{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPRMdvsAQ90h",
        "outputId": "987a9a65-b3f3-4e0e-c268-0597ef5a4644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import random\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Ssw-ZXAnRlPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/CS412/Project/german_credit_data.csv') # enter the file path on your drive for the csv file\n",
        "data.head() # print first 5 rows to check the data"
      ],
      "metadata": {
        "id": "NxxATMUQRPVW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "cabd2b1f-9da8-4c66-c34b-c9b5f4a5d4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  Age     Sex  Job Housing Saving accounts Checking account  \\\n",
              "0           0   67    male    2     own             NaN           little   \n",
              "1           1   22  female    2     own          little         moderate   \n",
              "2           2   49    male    1     own          little              NaN   \n",
              "3           3   45    male    2    free          little           little   \n",
              "4           4   53    male    2    free          little           little   \n",
              "\n",
              "   Credit amount  Duration              Purpose  Risk  \n",
              "0           1169         6             radio/TV  good  \n",
              "1           5951        48             radio/TV   bad  \n",
              "2           2096        12            education  good  \n",
              "3           7882        42  furniture/equipment  good  \n",
              "4           4870        24                  car   bad  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2046fc11-de3a-4065-b3a3-aec2ad2e92ee\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Age</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Job</th>\n",
              "      <th>Housing</th>\n",
              "      <th>Saving accounts</th>\n",
              "      <th>Checking account</th>\n",
              "      <th>Credit amount</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Purpose</th>\n",
              "      <th>Risk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>male</td>\n",
              "      <td>2</td>\n",
              "      <td>own</td>\n",
              "      <td>NaN</td>\n",
              "      <td>little</td>\n",
              "      <td>1169</td>\n",
              "      <td>6</td>\n",
              "      <td>radio/TV</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>female</td>\n",
              "      <td>2</td>\n",
              "      <td>own</td>\n",
              "      <td>little</td>\n",
              "      <td>moderate</td>\n",
              "      <td>5951</td>\n",
              "      <td>48</td>\n",
              "      <td>radio/TV</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>49</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>own</td>\n",
              "      <td>little</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2096</td>\n",
              "      <td>12</td>\n",
              "      <td>education</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>45</td>\n",
              "      <td>male</td>\n",
              "      <td>2</td>\n",
              "      <td>free</td>\n",
              "      <td>little</td>\n",
              "      <td>little</td>\n",
              "      <td>7882</td>\n",
              "      <td>42</td>\n",
              "      <td>furniture/equipment</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>53</td>\n",
              "      <td>male</td>\n",
              "      <td>2</td>\n",
              "      <td>free</td>\n",
              "      <td>little</td>\n",
              "      <td>little</td>\n",
              "      <td>4870</td>\n",
              "      <td>24</td>\n",
              "      <td>car</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2046fc11-de3a-4065-b3a3-aec2ad2e92ee')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2046fc11-de3a-4065-b3a3-aec2ad2e92ee button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2046fc11-de3a-4065-b3a3-aec2ad2e92ee');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**DATA PREPROCESSING**\n"
      ],
      "metadata": {
        "id": "V9KyQgqyU7Us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This method of imputation of missing values is based on getting the percentages of values of each variable and filling NaN values randomly but according to the percanteges.\n",
        "imputated_data = data.copy()\n",
        "\n",
        "col = 'Saving accounts'\n",
        "counts = imputated_data[col].value_counts(normalize=True)  # get the distribution of the categories\n",
        "missing = imputated_data[col].isnull()\n",
        "\n",
        "col_checking = 'Checking account'\n",
        "counts_checking = imputated_data[col_checking].value_counts(normalize=True)  # get the distribution of the categories\n",
        "missing_checking = imputated_data[col_checking].isnull()\n",
        "\n",
        "# Fill the missing values with categories according to the distribution\n",
        "imputated_data.loc[missing, col] = np.random.choice(counts.index, size=len(data[missing]), p=counts.values)\n",
        "imputated_data.loc[missing_checking, col_checking] = np.random.choice(counts_checking.index, size=len(data[missing_checking]), p=counts_checking.values)\n",
        "\n",
        "imputated_data.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "L1twodnMU4wl",
        "outputId": "7d0b4e2e-3ef0-4f29-d006-b4356af187fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  Age     Sex  Job Housing Saving accounts Checking account  \\\n",
              "0           0   67    male    2     own          little           little   \n",
              "1           1   22  female    2     own          little         moderate   \n",
              "2           2   49    male    1     own          little         moderate   \n",
              "3           3   45    male    2    free          little           little   \n",
              "4           4   53    male    2    free          little           little   \n",
              "\n",
              "   Credit amount  Duration              Purpose  Risk  \n",
              "0           1169         6             radio/TV  good  \n",
              "1           5951        48             radio/TV   bad  \n",
              "2           2096        12            education  good  \n",
              "3           7882        42  furniture/equipment  good  \n",
              "4           4870        24                  car   bad  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ccb8090-c977-44c7-bbfe-62d18063e298\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Age</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Job</th>\n",
              "      <th>Housing</th>\n",
              "      <th>Saving accounts</th>\n",
              "      <th>Checking account</th>\n",
              "      <th>Credit amount</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Purpose</th>\n",
              "      <th>Risk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>male</td>\n",
              "      <td>2</td>\n",
              "      <td>own</td>\n",
              "      <td>little</td>\n",
              "      <td>little</td>\n",
              "      <td>1169</td>\n",
              "      <td>6</td>\n",
              "      <td>radio/TV</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>female</td>\n",
              "      <td>2</td>\n",
              "      <td>own</td>\n",
              "      <td>little</td>\n",
              "      <td>moderate</td>\n",
              "      <td>5951</td>\n",
              "      <td>48</td>\n",
              "      <td>radio/TV</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>49</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>own</td>\n",
              "      <td>little</td>\n",
              "      <td>moderate</td>\n",
              "      <td>2096</td>\n",
              "      <td>12</td>\n",
              "      <td>education</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>45</td>\n",
              "      <td>male</td>\n",
              "      <td>2</td>\n",
              "      <td>free</td>\n",
              "      <td>little</td>\n",
              "      <td>little</td>\n",
              "      <td>7882</td>\n",
              "      <td>42</td>\n",
              "      <td>furniture/equipment</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>53</td>\n",
              "      <td>male</td>\n",
              "      <td>2</td>\n",
              "      <td>free</td>\n",
              "      <td>little</td>\n",
              "      <td>little</td>\n",
              "      <td>4870</td>\n",
              "      <td>24</td>\n",
              "      <td>car</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ccb8090-c977-44c7-bbfe-62d18063e298')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ccb8090-c977-44c7-bbfe-62d18063e298 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ccb8090-c977-44c7-bbfe-62d18063e298');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['Saving accounts'] = data['Saving accounts'].fillna(data['Saving accounts'].mode()[0])\n",
        "data['Checking account'] = data['Checking account'].fillna(data['Checking account'].mode()[0])\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ly-r8GAVgTyI",
        "outputId": "ec76bef7-d83a-4efc-cbff-ed6db3a3aef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  Age     Sex  Job Housing Saving accounts Checking account  \\\n",
              "0           0   67    male    2     own          little           little   \n",
              "1           1   22  female    2     own          little         moderate   \n",
              "2           2   49    male    1     own          little           little   \n",
              "3           3   45    male    2    free          little           little   \n",
              "4           4   53    male    2    free          little           little   \n",
              "\n",
              "   Credit amount  Duration              Purpose  Risk  \n",
              "0           1169         6             radio/TV  good  \n",
              "1           5951        48             radio/TV   bad  \n",
              "2           2096        12            education  good  \n",
              "3           7882        42  furniture/equipment  good  \n",
              "4           4870        24                  car   bad  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab4109e1-8ab8-4299-8f71-fb3bb73ed363\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Age</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Job</th>\n",
              "      <th>Housing</th>\n",
              "      <th>Saving accounts</th>\n",
              "      <th>Checking account</th>\n",
              "      <th>Credit amount</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Purpose</th>\n",
              "      <th>Risk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>male</td>\n",
              "      <td>2</td>\n",
              "      <td>own</td>\n",
              "      <td>little</td>\n",
              "      <td>little</td>\n",
              "      <td>1169</td>\n",
              "      <td>6</td>\n",
              "      <td>radio/TV</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>female</td>\n",
              "      <td>2</td>\n",
              "      <td>own</td>\n",
              "      <td>little</td>\n",
              "      <td>moderate</td>\n",
              "      <td>5951</td>\n",
              "      <td>48</td>\n",
              "      <td>radio/TV</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>49</td>\n",
              "      <td>male</td>\n",
              "      <td>1</td>\n",
              "      <td>own</td>\n",
              "      <td>little</td>\n",
              "      <td>little</td>\n",
              "      <td>2096</td>\n",
              "      <td>12</td>\n",
              "      <td>education</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>45</td>\n",
              "      <td>male</td>\n",
              "      <td>2</td>\n",
              "      <td>free</td>\n",
              "      <td>little</td>\n",
              "      <td>little</td>\n",
              "      <td>7882</td>\n",
              "      <td>42</td>\n",
              "      <td>furniture/equipment</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>53</td>\n",
              "      <td>male</td>\n",
              "      <td>2</td>\n",
              "      <td>free</td>\n",
              "      <td>little</td>\n",
              "      <td>little</td>\n",
              "      <td>4870</td>\n",
              "      <td>24</td>\n",
              "      <td>car</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab4109e1-8ab8-4299-8f71-fb3bb73ed363')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ab4109e1-8ab8-4299-8f71-fb3bb73ed363 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ab4109e1-8ab8-4299-8f71-fb3bb73ed363');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the order of your categories\n",
        "housing_encode = [['free', 'rent', 'own']] # free = 0.0 /// rent = 1.0 /// own = 2.0\n",
        "saving_encode = [['little', 'moderate', 'quite rich', 'rich']] # little = 0.0 /// moderate = 1.0 /// quite rich = 2.0 /// rich = 3.0\n",
        "checking_encode = [['little', 'moderate', 'rich']] # little = 0.0 /// moderate = 1.0 /// rich = 2.0\n",
        "risk_encode = [['good', 'bad']] # good = 0.0 /// bad = 1.0\n",
        "\n",
        "\n",
        "# Create the encoder\n",
        "ordinal_encoder_housing = OrdinalEncoder(categories=housing_encode)\n",
        "ordinal_encoder_saving = OrdinalEncoder(categories=saving_encode)\n",
        "ordinal_encoder_checking = OrdinalEncoder(categories=checking_encode)\n",
        "#Since 'risk is going to be our tager colunm, we don't want to encode it with one-hot encoding because that would create two instances of targes.'\n",
        "ordinal_encoder_risk = OrdinalEncoder(categories=risk_encode)\n",
        "\n",
        "# Fit and transform the data and replace the column in the dataframe\n",
        "imputated_data['Housing'] = ordinal_encoder_housing.fit_transform(imputated_data[['Housing']])\n",
        "imputated_data['Saving accounts'] = ordinal_encoder_saving.fit_transform(imputated_data[['Saving accounts']])\n",
        "imputated_data['Checking account'] = ordinal_encoder_checking.fit_transform(imputated_data[['Checking account']])\n",
        "imputated_data['Risk'] = ordinal_encoder_risk.fit_transform(imputated_data[['Risk']])\n",
        "\n",
        "print(imputated_data.head())  # print first 5 rows to check the data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlmGz4svUTPH",
        "outputId": "9ce41292-7534-4965-c1f9-3a856ecba845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  Age     Sex  Job  Housing  Saving accounts  Checking account  \\\n",
            "0           0   67    male    2      2.0              0.0               0.0   \n",
            "1           1   22  female    2      2.0              0.0               1.0   \n",
            "2           2   49    male    1      2.0              0.0               1.0   \n",
            "3           3   45    male    2      0.0              0.0               0.0   \n",
            "4           4   53    male    2      0.0              0.0               0.0   \n",
            "\n",
            "   Credit amount  Duration              Purpose  Risk  \n",
            "0           1169         6             radio/TV   0.0  \n",
            "1           5951        48             radio/TV   1.0  \n",
            "2           2096        12            education   0.0  \n",
            "3           7882        42  furniture/equipment   0.0  \n",
            "4           4870        24                  car   1.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Since seks and purpose variables are not an ordered categorical data (they cannot be put in a logical order),\n",
        "# we implemented one-hot encoding insted of the previous ordinal encoding for these\n",
        "\n",
        "imputated_data = pd.get_dummies(imputated_data, columns=['Sex'], prefix='Sex')\n",
        "imputated_data = pd.get_dummies(imputated_data, columns=['Purpose'], prefix='Purpose')\n",
        "print(imputated_data.head())  # print first 5 rows to check the data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEYr0MmPTJZR",
        "outputId": "69716e09-b29e-4fab-d103-13cd789143f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  Age  Job  Housing  Saving accounts  Checking account  \\\n",
            "0           0   67    2      2.0              0.0               0.0   \n",
            "1           1   22    2      2.0              0.0               1.0   \n",
            "2           2   49    1      2.0              0.0               1.0   \n",
            "3           3   45    2      0.0              0.0               0.0   \n",
            "4           4   53    2      0.0              0.0               0.0   \n",
            "\n",
            "   Credit amount  Duration  Risk  Sex_female  Sex_male  Purpose_business  \\\n",
            "0           1169         6   0.0           0         1                 0   \n",
            "1           5951        48   1.0           1         0                 0   \n",
            "2           2096        12   0.0           0         1                 0   \n",
            "3           7882        42   0.0           0         1                 0   \n",
            "4           4870        24   1.0           0         1                 0   \n",
            "\n",
            "   Purpose_car  Purpose_domestic appliances  Purpose_education  \\\n",
            "0            0                            0                  0   \n",
            "1            0                            0                  0   \n",
            "2            0                            0                  1   \n",
            "3            0                            0                  0   \n",
            "4            1                            0                  0   \n",
            "\n",
            "   Purpose_furniture/equipment  Purpose_radio/TV  Purpose_repairs  \\\n",
            "0                            0                 1                0   \n",
            "1                            0                 1                0   \n",
            "2                            0                 0                0   \n",
            "3                            1                 0                0   \n",
            "4                            0                 0                0   \n",
            "\n",
            "   Purpose_vacation/others  \n",
            "0                        0  \n",
            "1                        0  \n",
            "2                        0  \n",
            "3                        0  \n",
            "4                        0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**KNN CLASSIFICATION STARTS!!!**"
      ],
      "metadata": {
        "id": "aw2HS8WZq9-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = imputated_data.drop('Risk', axis=1)  # features\n",
        "y = imputated_data['Risk']  # target\n",
        "\n",
        "# first split the data into a training set and a temporary set using an 80-20 split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# then split the temporary set into validation and test sets using a 50-50 split\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "t-_dBYD-e64p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We have to standartize out data since the numerical differences is too large\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#In order to make them 'Dataframe' objects from 'numpy.ndarray' we have to implement this code:\n",
        "\n",
        "X_train = pd.DataFrame(X_train, columns=X.columns)\n",
        "X_val = pd.DataFrame(X_val, columns=X.columns)\n",
        "X_test = pd.DataFrame(X_test, columns=X.columns)"
      ],
      "metadata": {
        "id": "qE38ibotn57R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "k_values = [1, 3, 5, 7, 9, 11, 13] # k values we are going to try to find the best fitting\n",
        "\n",
        "best_mean_acc = -1\n",
        "best_k = None\n",
        "results_knn = []\n",
        "\n",
        "for k in k_values:\n",
        "    model = KNeighborsClassifier(n_neighbors=k) #initilize the model\n",
        "    # Perform 5-fold cross-validation on the training data\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "    mean_acc_score = scores.mean()\n",
        "    print('Validation accuracy for k=', k, ':', mean_acc_score)\n",
        "\n",
        "    results_knn.append((k, mean_acc_score))\n",
        "\n",
        "    # if mean validation accuracy is better than best_mean_acc, update best_mean_acc and best_k\n",
        "    if (best_mean_acc < mean_acc_score):\n",
        "        best_mean_acc = mean_acc_score\n",
        "        best_k = k\n",
        "\n",
        "# Fit the final model with the best k on the full training and validation set,\n",
        "# and evaluate on the test set\n",
        "final_model = KNeighborsClassifier(n_neighbors=best_k)\n",
        "\n",
        "# Combine the training and validation sets for final model training\n",
        "X_test_full = pd.concat([X_test, X_val])\n",
        "y_test_full = pd.concat([y_test, y_val])\n",
        "\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "final_y_pred = final_model.predict(X_test_full)\n",
        "final_acc = accuracy_score(y_test_full, final_y_pred)\n",
        "\n",
        "print('Best mean validation accuracy (', best_mean_acc, ') is achieved with k=', best_k)\n",
        "print('The final model accuracy on test set is: ', final_acc)\n",
        "\n",
        "cm = confusion_matrix(y_test_full, final_y_pred)\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(f'Confusion Matrix for KNN Classifier')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "VwK8n_EiKZgj",
        "outputId": "05fa4f2b-7e55-45c9-c340-0e26748522a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy for k= 1 : 0.635\n",
            "Validation accuracy for k= 3 : 0.6487499999999999\n",
            "Validation accuracy for k= 5 : 0.6725\n",
            "Validation accuracy for k= 7 : 0.6725\n",
            "Validation accuracy for k= 9 : 0.7037500000000001\n",
            "Validation accuracy for k= 11 : 0.6925\n",
            "Validation accuracy for k= 13 : 0.6987500000000001\n",
            "Best mean validation accuracy ( 0.7037500000000001 ) is achieved with k= 9\n",
            "The final model accuracy on test set is:  0.695\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**DECISION TREES STARTS!!!!!**"
      ],
      "metadata": {
        "id": "Ki8k3mt-vJ9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "X_test_full = pd.concat([X_test, X_val])\n",
        "y_test_full = pd.concat([y_test, y_val])\n",
        "\n",
        "#Hyper parameters:\n",
        "max_depth = list(range(1,30))\n",
        "min_leaf = list(range(1,100))\n",
        "\n",
        "best_mean_acc = -1\n",
        "best_depth = None\n",
        "best_leaves = None\n",
        "results_tree = []\n",
        "\n",
        "for depth in max_depth:\n",
        "  for leaf in min_leaf:\n",
        "    tree = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=leaf) #initilize the model\n",
        "\n",
        "    # Perform 5-fold cross-validation on the training data\n",
        "    scores = cross_val_score(tree, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "    mean_acc_score = scores.mean()\n",
        "    print('Validation accuracy for depth=', depth,\" and min leaf=\", leaf, ':', mean_acc_score)\n",
        "\n",
        "    results_tree.append((depth, leaf, mean_acc_score))\n",
        "\n",
        "    # if mean validation accuracy is better than best_mean_acc, update best_mean_acc and hyperparameters\n",
        "    if (best_mean_acc < mean_acc_score):\n",
        "        best_mean_acc = mean_acc_score\n",
        "        best_depth = depth\n",
        "        best_leaves = leaf\n",
        "\n",
        "# Fit the final model with the best k on the full training set and evaluate on the validation set\n",
        "tree_model = DecisionTreeClassifier(max_depth=best_depth, min_samples_leaf=best_leaves)\n",
        "tree_model.fit(X_train, y_train)\n",
        "tree_model_pred = tree_model.predict(X_test_full)\n",
        "final_val_acc = accuracy_score(y_test_full, tree_model_pred)\n",
        "\n",
        "print('Best mean validation accuracy (', best_mean_acc, ') is achieved with max depth=', best_depth, \" and min leaf=\", best_leaves)\n",
        "print('The final model accuracy on validation set is: ', final_val_acc)\n",
        "\n",
        "cm = confusion_matrix(y_test_full, tree_model_pred)\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(f'Confusion Matrix for Decision Tree')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9R-3GkNlvHsQ",
        "outputId": "f185389a-e15d-4b74-92df-fb3465e8215d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy for depth= 1  and min leaf= 1 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 2 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 3 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 4 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 5 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 6 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 7 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 8 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 9 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 10 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 11 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 12 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 13 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 14 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 15 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 16 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 17 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 18 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 19 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 20 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 21 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 22 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 23 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 24 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 25 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 26 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 27 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 28 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 29 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 30 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 31 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 32 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 33 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 34 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 35 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 36 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 37 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 38 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 39 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 40 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 41 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 42 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 43 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 44 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 45 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 46 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 47 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 48 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 49 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 50 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 51 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 52 : 0.69375\n",
            "Validation accuracy for depth= 1  and min leaf= 53 : 0.6925000000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 54 : 0.6925000000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 55 : 0.6925000000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 56 : 0.6925000000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 57 : 0.6925000000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 58 : 0.6925000000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 59 : 0.6925000000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 60 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 61 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 62 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 63 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 64 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 65 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 66 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 67 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 68 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 69 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 70 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 71 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 72 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 73 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 74 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 75 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 76 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 77 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 78 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 79 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 80 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 81 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 82 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 83 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 84 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 85 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 86 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 87 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 88 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 89 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 90 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 91 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 92 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 93 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 94 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 95 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 96 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 97 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 98 : 0.6987500000000001\n",
            "Validation accuracy for depth= 1  and min leaf= 99 : 0.6987500000000001\n",
            "Validation accuracy for depth= 2  and min leaf= 1 : 0.6799999999999999\n",
            "Validation accuracy for depth= 2  and min leaf= 2 : 0.6799999999999999\n",
            "Validation accuracy for depth= 2  and min leaf= 3 : 0.6799999999999999\n",
            "Validation accuracy for depth= 2  and min leaf= 4 : 0.6799999999999999\n",
            "Validation accuracy for depth= 2  and min leaf= 5 : 0.68125\n",
            "Validation accuracy for depth= 2  and min leaf= 6 : 0.68125\n",
            "Validation accuracy for depth= 2  and min leaf= 7 : 0.68125\n",
            "Validation accuracy for depth= 2  and min leaf= 8 : 0.68125\n",
            "Validation accuracy for depth= 2  and min leaf= 9 : 0.68125\n",
            "Validation accuracy for depth= 2  and min leaf= 10 : 0.68625\n",
            "Validation accuracy for depth= 2  and min leaf= 11 : 0.68625\n",
            "Validation accuracy for depth= 2  and min leaf= 12 : 0.68625\n",
            "Validation accuracy for depth= 2  and min leaf= 13 : 0.68625\n",
            "Validation accuracy for depth= 2  and min leaf= 14 : 0.68625\n",
            "Validation accuracy for depth= 2  and min leaf= 15 : 0.68375\n",
            "Validation accuracy for depth= 2  and min leaf= 16 : 0.68375\n",
            "Validation accuracy for depth= 2  and min leaf= 17 : 0.68375\n",
            "Validation accuracy for depth= 2  and min leaf= 18 : 0.68375\n",
            "Validation accuracy for depth= 2  and min leaf= 19 : 0.68375\n",
            "Validation accuracy for depth= 2  and min leaf= 20 : 0.68375\n",
            "Validation accuracy for depth= 2  and min leaf= 21 : 0.68375\n",
            "Validation accuracy for depth= 2  and min leaf= 22 : 0.67875\n",
            "Validation accuracy for depth= 2  and min leaf= 23 : 0.67875\n",
            "Validation accuracy for depth= 2  and min leaf= 24 : 0.6912499999999999\n",
            "Validation accuracy for depth= 2  and min leaf= 25 : 0.68875\n",
            "Validation accuracy for depth= 2  and min leaf= 26 : 0.68875\n",
            "Validation accuracy for depth= 2  and min leaf= 27 : 0.68875\n",
            "Validation accuracy for depth= 2  and min leaf= 28 : 0.68875\n",
            "Validation accuracy for depth= 2  and min leaf= 29 : 0.68875\n",
            "Validation accuracy for depth= 2  and min leaf= 30 : 0.68875\n",
            "Validation accuracy for depth= 2  and min leaf= 31 : 0.68875\n",
            "Validation accuracy for depth= 2  and min leaf= 32 : 0.68875\n",
            "Validation accuracy for depth= 2  and min leaf= 33 : 0.6825\n",
            "Validation accuracy for depth= 2  and min leaf= 34 : 0.6825\n",
            "Validation accuracy for depth= 2  and min leaf= 35 : 0.6825\n",
            "Validation accuracy for depth= 2  and min leaf= 36 : 0.6825\n",
            "Validation accuracy for depth= 2  and min leaf= 37 : 0.6825\n",
            "Validation accuracy for depth= 2  and min leaf= 38 : 0.68125\n",
            "Validation accuracy for depth= 2  and min leaf= 39 : 0.685\n",
            "Validation accuracy for depth= 2  and min leaf= 40 : 0.685\n",
            "Validation accuracy for depth= 2  and min leaf= 41 : 0.68125\n",
            "Validation accuracy for depth= 2  and min leaf= 42 : 0.68125\n",
            "Validation accuracy for depth= 2  and min leaf= 43 : 0.68125\n",
            "Validation accuracy for depth= 2  and min leaf= 44 : 0.68125\n",
            "Validation accuracy for depth= 2  and min leaf= 45 : 0.685\n",
            "Validation accuracy for depth= 2  and min leaf= 46 : 0.685\n",
            "Validation accuracy for depth= 2  and min leaf= 47 : 0.685\n",
            "Validation accuracy for depth= 2  and min leaf= 48 : 0.68375\n",
            "Validation accuracy for depth= 2  and min leaf= 49 : 0.68375\n",
            "Validation accuracy for depth= 2  and min leaf= 50 : 0.68375\n",
            "Validation accuracy for depth= 2  and min leaf= 51 : 0.68375\n",
            "Validation accuracy for depth= 2  and min leaf= 52 : 0.68375\n",
            "Validation accuracy for depth= 2  and min leaf= 53 : 0.6825\n",
            "Validation accuracy for depth= 2  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 2  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 2  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 2  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 2  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 2  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 2  and min leaf= 60 : 0.70125\n",
            "Validation accuracy for depth= 2  and min leaf= 61 : 0.70125\n",
            "Validation accuracy for depth= 2  and min leaf= 62 : 0.70125\n",
            "Validation accuracy for depth= 2  and min leaf= 63 : 0.70125\n",
            "Validation accuracy for depth= 2  and min leaf= 64 : 0.70125\n",
            "Validation accuracy for depth= 2  and min leaf= 65 : 0.6875000000000001\n",
            "Validation accuracy for depth= 2  and min leaf= 66 : 0.6875000000000001\n",
            "Validation accuracy for depth= 2  and min leaf= 67 : 0.6875000000000001\n",
            "Validation accuracy for depth= 2  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 2  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 2  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 2  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 2  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 2  and min leaf= 73 : 0.68875\n",
            "Validation accuracy for depth= 2  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 2  and min leaf= 75 : 0.6962499999999999\n",
            "Validation accuracy for depth= 2  and min leaf= 76 : 0.68625\n",
            "Validation accuracy for depth= 2  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 2  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 2  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 1 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 2 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 3 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 4 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 5 : 0.6900000000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 6 : 0.6912499999999999\n",
            "Validation accuracy for depth= 3  and min leaf= 7 : 0.6887500000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 8 : 0.68625\n",
            "Validation accuracy for depth= 3  and min leaf= 9 : 0.68625\n",
            "Validation accuracy for depth= 3  and min leaf= 10 : 0.6875\n",
            "Validation accuracy for depth= 3  and min leaf= 11 : 0.6925000000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 12 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 13 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 14 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 15 : 0.6925000000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 16 : 0.6925000000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 17 : 0.6925000000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 18 : 0.6925000000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 19 : 0.6900000000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 20 : 0.6937500000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 21 : 0.6937500000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 22 : 0.6900000000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 23 : 0.6900000000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 24 : 0.6849999999999999\n",
            "Validation accuracy for depth= 3  and min leaf= 25 : 0.6849999999999999\n",
            "Validation accuracy for depth= 3  and min leaf= 26 : 0.6849999999999999\n",
            "Validation accuracy for depth= 3  and min leaf= 27 : 0.6849999999999999\n",
            "Validation accuracy for depth= 3  and min leaf= 28 : 0.6849999999999999\n",
            "Validation accuracy for depth= 3  and min leaf= 29 : 0.6849999999999999\n",
            "Validation accuracy for depth= 3  and min leaf= 30 : 0.6837500000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 31 : 0.6837500000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 32 : 0.6799999999999999\n",
            "Validation accuracy for depth= 3  and min leaf= 33 : 0.68125\n",
            "Validation accuracy for depth= 3  and min leaf= 34 : 0.68125\n",
            "Validation accuracy for depth= 3  and min leaf= 35 : 0.68125\n",
            "Validation accuracy for depth= 3  and min leaf= 36 : 0.68125\n",
            "Validation accuracy for depth= 3  and min leaf= 37 : 0.6825\n",
            "Validation accuracy for depth= 3  and min leaf= 38 : 0.68625\n",
            "Validation accuracy for depth= 3  and min leaf= 39 : 0.69\n",
            "Validation accuracy for depth= 3  and min leaf= 40 : 0.69\n",
            "Validation accuracy for depth= 3  and min leaf= 41 : 0.68625\n",
            "Validation accuracy for depth= 3  and min leaf= 42 : 0.68625\n",
            "Validation accuracy for depth= 3  and min leaf= 43 : 0.68625\n",
            "Validation accuracy for depth= 3  and min leaf= 44 : 0.68625\n",
            "Validation accuracy for depth= 3  and min leaf= 45 : 0.68875\n",
            "Validation accuracy for depth= 3  and min leaf= 46 : 0.68875\n",
            "Validation accuracy for depth= 3  and min leaf= 47 : 0.68875\n",
            "Validation accuracy for depth= 3  and min leaf= 48 : 0.68875\n",
            "Validation accuracy for depth= 3  and min leaf= 49 : 0.68875\n",
            "Validation accuracy for depth= 3  and min leaf= 50 : 0.68875\n",
            "Validation accuracy for depth= 3  and min leaf= 51 : 0.68875\n",
            "Validation accuracy for depth= 3  and min leaf= 52 : 0.68875\n",
            "Validation accuracy for depth= 3  and min leaf= 53 : 0.6825\n",
            "Validation accuracy for depth= 3  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 3  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 3  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 3  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 3  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 3  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 3  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 3  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 3  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 3  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 3  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 3  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 3  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 3  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 3  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 3  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 3  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 3  and min leaf= 73 : 0.6962499999999999\n",
            "Validation accuracy for depth= 3  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 3  and min leaf= 75 : 0.68875\n",
            "Validation accuracy for depth= 3  and min leaf= 76 : 0.68625\n",
            "Validation accuracy for depth= 3  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 3  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 3  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 1 : 0.68875\n",
            "Validation accuracy for depth= 4  and min leaf= 2 : 0.6849999999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 3 : 0.68625\n",
            "Validation accuracy for depth= 4  and min leaf= 4 : 0.6825\n",
            "Validation accuracy for depth= 4  and min leaf= 5 : 0.6912499999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 6 : 0.6912499999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 7 : 0.6912499999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 8 : 0.6900000000000001\n",
            "Validation accuracy for depth= 4  and min leaf= 9 : 0.6675000000000001\n",
            "Validation accuracy for depth= 4  and min leaf= 10 : 0.6699999999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 11 : 0.665\n",
            "Validation accuracy for depth= 4  and min leaf= 12 : 0.6637500000000001\n",
            "Validation accuracy for depth= 4  and min leaf= 13 : 0.6637500000000001\n",
            "Validation accuracy for depth= 4  and min leaf= 14 : 0.66125\n",
            "Validation accuracy for depth= 4  and min leaf= 15 : 0.6575000000000001\n",
            "Validation accuracy for depth= 4  and min leaf= 16 : 0.675\n",
            "Validation accuracy for depth= 4  and min leaf= 17 : 0.6737500000000001\n",
            "Validation accuracy for depth= 4  and min leaf= 18 : 0.6737500000000001\n",
            "Validation accuracy for depth= 4  and min leaf= 19 : 0.67125\n",
            "Validation accuracy for depth= 4  and min leaf= 20 : 0.6812500000000001\n",
            "Validation accuracy for depth= 4  and min leaf= 21 : 0.67625\n",
            "Validation accuracy for depth= 4  and min leaf= 22 : 0.66625\n",
            "Validation accuracy for depth= 4  and min leaf= 23 : 0.6725\n",
            "Validation accuracy for depth= 4  and min leaf= 24 : 0.67375\n",
            "Validation accuracy for depth= 4  and min leaf= 25 : 0.6787500000000001\n",
            "Validation accuracy for depth= 4  and min leaf= 26 : 0.6787500000000001\n",
            "Validation accuracy for depth= 4  and min leaf= 27 : 0.665\n",
            "Validation accuracy for depth= 4  and min leaf= 28 : 0.665\n",
            "Validation accuracy for depth= 4  and min leaf= 29 : 0.665\n",
            "Validation accuracy for depth= 4  and min leaf= 30 : 0.6775\n",
            "Validation accuracy for depth= 4  and min leaf= 31 : 0.6775\n",
            "Validation accuracy for depth= 4  and min leaf= 32 : 0.6737500000000001\n",
            "Validation accuracy for depth= 4  and min leaf= 33 : 0.6775\n",
            "Validation accuracy for depth= 4  and min leaf= 34 : 0.6775\n",
            "Validation accuracy for depth= 4  and min leaf= 35 : 0.67375\n",
            "Validation accuracy for depth= 4  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 4  and min leaf= 37 : 0.6637500000000001\n",
            "Validation accuracy for depth= 4  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 4  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 4  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 4  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 4  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 4  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 4  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 4  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 4  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 4  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 4  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 4  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 4  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 4  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 4  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 4  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 4  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 4  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 4  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 4  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 4  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 4  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 4  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 4  and min leaf= 73 : 0.68875\n",
            "Validation accuracy for depth= 4  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 75 : 0.6962499999999999\n",
            "Validation accuracy for depth= 4  and min leaf= 76 : 0.68625\n",
            "Validation accuracy for depth= 4  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 4  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 4  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 1 : 0.6612500000000001\n",
            "Validation accuracy for depth= 5  and min leaf= 2 : 0.6637500000000001\n",
            "Validation accuracy for depth= 5  and min leaf= 3 : 0.665\n",
            "Validation accuracy for depth= 5  and min leaf= 4 : 0.67125\n",
            "Validation accuracy for depth= 5  and min leaf= 5 : 0.6775\n",
            "Validation accuracy for depth= 5  and min leaf= 6 : 0.68\n",
            "Validation accuracy for depth= 5  and min leaf= 7 : 0.6849999999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 8 : 0.6799999999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 9 : 0.665\n",
            "Validation accuracy for depth= 5  and min leaf= 10 : 0.6675\n",
            "Validation accuracy for depth= 5  and min leaf= 11 : 0.67875\n",
            "Validation accuracy for depth= 5  and min leaf= 12 : 0.6812499999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 13 : 0.6874999999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 14 : 0.6725\n",
            "Validation accuracy for depth= 5  and min leaf= 15 : 0.6725\n",
            "Validation accuracy for depth= 5  and min leaf= 16 : 0.6699999999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 17 : 0.6700000000000002\n",
            "Validation accuracy for depth= 5  and min leaf= 18 : 0.665\n",
            "Validation accuracy for depth= 5  and min leaf= 19 : 0.6612500000000001\n",
            "Validation accuracy for depth= 5  and min leaf= 20 : 0.6599999999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 21 : 0.6625\n",
            "Validation accuracy for depth= 5  and min leaf= 22 : 0.65875\n",
            "Validation accuracy for depth= 5  and min leaf= 23 : 0.6637500000000001\n",
            "Validation accuracy for depth= 5  and min leaf= 24 : 0.665\n",
            "Validation accuracy for depth= 5  and min leaf= 25 : 0.6687500000000001\n",
            "Validation accuracy for depth= 5  and min leaf= 26 : 0.6687500000000001\n",
            "Validation accuracy for depth= 5  and min leaf= 27 : 0.6637500000000001\n",
            "Validation accuracy for depth= 5  and min leaf= 28 : 0.6637500000000001\n",
            "Validation accuracy for depth= 5  and min leaf= 29 : 0.6637500000000001\n",
            "Validation accuracy for depth= 5  and min leaf= 30 : 0.67625\n",
            "Validation accuracy for depth= 5  and min leaf= 31 : 0.67625\n",
            "Validation accuracy for depth= 5  and min leaf= 32 : 0.6737500000000001\n",
            "Validation accuracy for depth= 5  and min leaf= 33 : 0.6775\n",
            "Validation accuracy for depth= 5  and min leaf= 34 : 0.6775\n",
            "Validation accuracy for depth= 5  and min leaf= 35 : 0.67375\n",
            "Validation accuracy for depth= 5  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 5  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 5  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 5  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 5  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 5  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 5  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 5  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 5  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 5  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 5  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 5  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 5  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 5  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 5  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 5  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 5  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 5  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 5  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 5  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 5  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 5  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 5  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 5  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 5  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 5  and min leaf= 73 : 0.6962499999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 74 : 0.68875\n",
            "Validation accuracy for depth= 5  and min leaf= 75 : 0.6962499999999999\n",
            "Validation accuracy for depth= 5  and min leaf= 76 : 0.68625\n",
            "Validation accuracy for depth= 5  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 5  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 5  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 1 : 0.6512500000000001\n",
            "Validation accuracy for depth= 6  and min leaf= 2 : 0.65875\n",
            "Validation accuracy for depth= 6  and min leaf= 3 : 0.65125\n",
            "Validation accuracy for depth= 6  and min leaf= 4 : 0.65375\n",
            "Validation accuracy for depth= 6  and min leaf= 5 : 0.6625\n",
            "Validation accuracy for depth= 6  and min leaf= 6 : 0.67125\n",
            "Validation accuracy for depth= 6  and min leaf= 7 : 0.685\n",
            "Validation accuracy for depth= 6  and min leaf= 8 : 0.665\n",
            "Validation accuracy for depth= 6  and min leaf= 9 : 0.6725\n",
            "Validation accuracy for depth= 6  and min leaf= 10 : 0.665\n",
            "Validation accuracy for depth= 6  and min leaf= 11 : 0.665\n",
            "Validation accuracy for depth= 6  and min leaf= 12 : 0.6675000000000001\n",
            "Validation accuracy for depth= 6  and min leaf= 13 : 0.66125\n",
            "Validation accuracy for depth= 6  and min leaf= 14 : 0.6512500000000001\n",
            "Validation accuracy for depth= 6  and min leaf= 15 : 0.65625\n",
            "Validation accuracy for depth= 6  and min leaf= 16 : 0.66875\n",
            "Validation accuracy for depth= 6  and min leaf= 17 : 0.6587500000000001\n",
            "Validation accuracy for depth= 6  and min leaf= 18 : 0.655\n",
            "Validation accuracy for depth= 6  and min leaf= 19 : 0.6475\n",
            "Validation accuracy for depth= 6  and min leaf= 20 : 0.6487499999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 21 : 0.65125\n",
            "Validation accuracy for depth= 6  and min leaf= 22 : 0.6475\n",
            "Validation accuracy for depth= 6  and min leaf= 23 : 0.6525\n",
            "Validation accuracy for depth= 6  and min leaf= 24 : 0.65375\n",
            "Validation accuracy for depth= 6  and min leaf= 25 : 0.6575\n",
            "Validation accuracy for depth= 6  and min leaf= 26 : 0.6575\n",
            "Validation accuracy for depth= 6  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 6  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 6  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 6  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 6  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 6  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 6  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 6  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 6  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 6  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 6  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 6  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 6  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 6  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 6  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 6  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 6  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 6  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 6  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 6  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 6  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 6  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 6  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 6  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 6  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 6  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 6  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 6  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 6  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 6  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 6  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 6  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 6  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 6  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 6  and min leaf= 73 : 0.6962499999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 75 : 0.6962499999999999\n",
            "Validation accuracy for depth= 6  and min leaf= 76 : 0.68625\n",
            "Validation accuracy for depth= 6  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 6  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 6  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 1 : 0.6375\n",
            "Validation accuracy for depth= 7  and min leaf= 2 : 0.65\n",
            "Validation accuracy for depth= 7  and min leaf= 3 : 0.6512500000000001\n",
            "Validation accuracy for depth= 7  and min leaf= 4 : 0.6612500000000001\n",
            "Validation accuracy for depth= 7  and min leaf= 5 : 0.6575\n",
            "Validation accuracy for depth= 7  and min leaf= 6 : 0.6625\n",
            "Validation accuracy for depth= 7  and min leaf= 7 : 0.675\n",
            "Validation accuracy for depth= 7  and min leaf= 8 : 0.6775\n",
            "Validation accuracy for depth= 7  and min leaf= 9 : 0.6775\n",
            "Validation accuracy for depth= 7  and min leaf= 10 : 0.6625\n",
            "Validation accuracy for depth= 7  and min leaf= 11 : 0.6587500000000001\n",
            "Validation accuracy for depth= 7  and min leaf= 12 : 0.6599999999999999\n",
            "Validation accuracy for depth= 7  and min leaf= 13 : 0.655\n",
            "Validation accuracy for depth= 7  and min leaf= 14 : 0.6425000000000001\n",
            "Validation accuracy for depth= 7  and min leaf= 15 : 0.645\n",
            "Validation accuracy for depth= 7  and min leaf= 16 : 0.66\n",
            "Validation accuracy for depth= 7  and min leaf= 17 : 0.6625\n",
            "Validation accuracy for depth= 7  and min leaf= 18 : 0.6525\n",
            "Validation accuracy for depth= 7  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 7  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 7  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 7  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 7  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 7  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 7  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 7  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 7  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 7  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 7  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 7  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 7  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 7  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 7  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 7  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 7  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 7  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 7  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 7  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 7  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 7  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 7  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 7  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 7  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 7  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 7  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 7  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 7  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 7  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 7  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 7  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 7  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 7  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 7  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 7  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 7  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 7  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 7  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 7  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 7  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 7  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 7  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 7  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 7  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 7  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 7  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 7  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 7  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 7  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 7  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 7  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 7  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 7  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 7  and min leaf= 73 : 0.68875\n",
            "Validation accuracy for depth= 7  and min leaf= 74 : 0.68875\n",
            "Validation accuracy for depth= 7  and min leaf= 75 : 0.68875\n",
            "Validation accuracy for depth= 7  and min leaf= 76 : 0.68625\n",
            "Validation accuracy for depth= 7  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 7  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 7  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 1 : 0.6275000000000001\n",
            "Validation accuracy for depth= 8  and min leaf= 2 : 0.62625\n",
            "Validation accuracy for depth= 8  and min leaf= 3 : 0.635\n",
            "Validation accuracy for depth= 8  and min leaf= 4 : 0.63625\n",
            "Validation accuracy for depth= 8  and min leaf= 5 : 0.6387499999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 6 : 0.6625\n",
            "Validation accuracy for depth= 8  and min leaf= 7 : 0.66625\n",
            "Validation accuracy for depth= 8  and min leaf= 8 : 0.6675000000000001\n",
            "Validation accuracy for depth= 8  and min leaf= 9 : 0.6775\n",
            "Validation accuracy for depth= 8  and min leaf= 10 : 0.6675000000000001\n",
            "Validation accuracy for depth= 8  and min leaf= 11 : 0.66625\n",
            "Validation accuracy for depth= 8  and min leaf= 12 : 0.6712499999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 13 : 0.66875\n",
            "Validation accuracy for depth= 8  and min leaf= 14 : 0.65375\n",
            "Validation accuracy for depth= 8  and min leaf= 15 : 0.6575\n",
            "Validation accuracy for depth= 8  and min leaf= 16 : 0.66875\n",
            "Validation accuracy for depth= 8  and min leaf= 17 : 0.6637500000000001\n",
            "Validation accuracy for depth= 8  and min leaf= 18 : 0.6537499999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 19 : 0.645\n",
            "Validation accuracy for depth= 8  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 8  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 8  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 8  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 8  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 8  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 8  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 8  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 8  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 8  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 8  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 8  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 8  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 8  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 8  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 8  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 8  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 8  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 8  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 8  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 8  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 8  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 8  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 8  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 8  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 8  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 8  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 8  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 8  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 8  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 8  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 8  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 8  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 8  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 8  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 8  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 8  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 8  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 8  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 8  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 8  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 8  and min leaf= 73 : 0.68875\n",
            "Validation accuracy for depth= 8  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 75 : 0.6962499999999999\n",
            "Validation accuracy for depth= 8  and min leaf= 76 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 8  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 8  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 1 : 0.62625\n",
            "Validation accuracy for depth= 9  and min leaf= 2 : 0.64375\n",
            "Validation accuracy for depth= 9  and min leaf= 3 : 0.6362500000000001\n",
            "Validation accuracy for depth= 9  and min leaf= 4 : 0.635\n",
            "Validation accuracy for depth= 9  and min leaf= 5 : 0.64\n",
            "Validation accuracy for depth= 9  and min leaf= 6 : 0.6537499999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 7 : 0.66625\n",
            "Validation accuracy for depth= 9  and min leaf= 8 : 0.6625\n",
            "Validation accuracy for depth= 9  and min leaf= 9 : 0.6725\n",
            "Validation accuracy for depth= 9  and min leaf= 10 : 0.6599999999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 11 : 0.6612500000000001\n",
            "Validation accuracy for depth= 9  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 9  and min leaf= 13 : 0.6675000000000001\n",
            "Validation accuracy for depth= 9  and min leaf= 14 : 0.6525000000000001\n",
            "Validation accuracy for depth= 9  and min leaf= 15 : 0.65375\n",
            "Validation accuracy for depth= 9  and min leaf= 16 : 0.6675\n",
            "Validation accuracy for depth= 9  and min leaf= 17 : 0.6625\n",
            "Validation accuracy for depth= 9  and min leaf= 18 : 0.6525\n",
            "Validation accuracy for depth= 9  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 9  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 9  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 9  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 9  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 9  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 9  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 9  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 9  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 9  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 9  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 9  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 9  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 9  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 9  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 9  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 9  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 9  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 9  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 9  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 9  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 9  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 9  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 9  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 9  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 9  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 9  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 9  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 9  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 9  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 9  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 9  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 9  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 9  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 9  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 9  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 9  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 9  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 9  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 9  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 9  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 9  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 9  and min leaf= 73 : 0.68875\n",
            "Validation accuracy for depth= 9  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 75 : 0.6962499999999999\n",
            "Validation accuracy for depth= 9  and min leaf= 76 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 9  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 9  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 1 : 0.6300000000000001\n",
            "Validation accuracy for depth= 10  and min leaf= 2 : 0.6487499999999999\n",
            "Validation accuracy for depth= 10  and min leaf= 3 : 0.63875\n",
            "Validation accuracy for depth= 10  and min leaf= 4 : 0.6375\n",
            "Validation accuracy for depth= 10  and min leaf= 5 : 0.65\n",
            "Validation accuracy for depth= 10  and min leaf= 6 : 0.655\n",
            "Validation accuracy for depth= 10  and min leaf= 7 : 0.66875\n",
            "Validation accuracy for depth= 10  and min leaf= 8 : 0.6575000000000001\n",
            "Validation accuracy for depth= 10  and min leaf= 9 : 0.67875\n",
            "Validation accuracy for depth= 10  and min leaf= 10 : 0.66625\n",
            "Validation accuracy for depth= 10  and min leaf= 11 : 0.66125\n",
            "Validation accuracy for depth= 10  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 10  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 10  and min leaf= 14 : 0.65375\n",
            "Validation accuracy for depth= 10  and min leaf= 15 : 0.6575\n",
            "Validation accuracy for depth= 10  and min leaf= 16 : 0.66875\n",
            "Validation accuracy for depth= 10  and min leaf= 17 : 0.6637500000000001\n",
            "Validation accuracy for depth= 10  and min leaf= 18 : 0.6537499999999999\n",
            "Validation accuracy for depth= 10  and min leaf= 19 : 0.645\n",
            "Validation accuracy for depth= 10  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 10  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 10  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 10  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 10  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 10  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 10  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 10  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 10  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 10  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 10  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 10  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 10  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 10  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 10  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 10  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 10  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 10  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 10  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 10  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 10  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 10  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 10  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 10  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 10  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 10  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 10  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 10  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 10  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 10  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 10  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 10  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 10  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 10  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 10  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 10  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 10  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 10  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 10  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 10  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 10  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 10  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 10  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 10  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 10  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 10  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 10  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 10  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 10  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 10  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 10  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 10  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 10  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 10  and min leaf= 73 : 0.68875\n",
            "Validation accuracy for depth= 10  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 10  and min leaf= 75 : 0.68875\n",
            "Validation accuracy for depth= 10  and min leaf= 76 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 10  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 10  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 1 : 0.6012500000000001\n",
            "Validation accuracy for depth= 11  and min leaf= 2 : 0.64\n",
            "Validation accuracy for depth= 11  and min leaf= 3 : 0.6125\n",
            "Validation accuracy for depth= 11  and min leaf= 4 : 0.6225\n",
            "Validation accuracy for depth= 11  and min leaf= 5 : 0.6399999999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 6 : 0.6487499999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 7 : 0.6675000000000001\n",
            "Validation accuracy for depth= 11  and min leaf= 8 : 0.6599999999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 9 : 0.6825000000000001\n",
            "Validation accuracy for depth= 11  and min leaf= 10 : 0.6625\n",
            "Validation accuracy for depth= 11  and min leaf= 11 : 0.6625\n",
            "Validation accuracy for depth= 11  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 11  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 11  and min leaf= 14 : 0.6525000000000001\n",
            "Validation accuracy for depth= 11  and min leaf= 15 : 0.65375\n",
            "Validation accuracy for depth= 11  and min leaf= 16 : 0.66875\n",
            "Validation accuracy for depth= 11  and min leaf= 17 : 0.6637500000000001\n",
            "Validation accuracy for depth= 11  and min leaf= 18 : 0.6525\n",
            "Validation accuracy for depth= 11  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 11  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 11  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 11  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 11  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 11  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 11  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 11  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 11  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 11  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 11  and min leaf= 29 : 0.665\n",
            "Validation accuracy for depth= 11  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 11  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 11  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 11  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 11  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 11  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 11  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 11  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 11  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 11  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 11  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 11  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 11  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 11  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 11  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 11  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 11  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 11  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 11  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 11  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 11  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 11  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 11  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 11  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 11  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 11  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 11  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 11  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 11  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 11  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 11  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 11  and min leaf= 73 : 0.6962499999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 74 : 0.68875\n",
            "Validation accuracy for depth= 11  and min leaf= 75 : 0.6962499999999999\n",
            "Validation accuracy for depth= 11  and min leaf= 76 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 11  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 11  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 1 : 0.6212500000000001\n",
            "Validation accuracy for depth= 12  and min leaf= 2 : 0.64625\n",
            "Validation accuracy for depth= 12  and min leaf= 3 : 0.6287499999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 4 : 0.6325000000000001\n",
            "Validation accuracy for depth= 12  and min leaf= 5 : 0.6300000000000001\n",
            "Validation accuracy for depth= 12  and min leaf= 6 : 0.65625\n",
            "Validation accuracy for depth= 12  and min leaf= 7 : 0.67125\n",
            "Validation accuracy for depth= 12  and min leaf= 8 : 0.6612499999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 9 : 0.68\n",
            "Validation accuracy for depth= 12  and min leaf= 10 : 0.6675\n",
            "Validation accuracy for depth= 12  and min leaf= 11 : 0.6637500000000001\n",
            "Validation accuracy for depth= 12  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 12  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 12  and min leaf= 14 : 0.6525000000000001\n",
            "Validation accuracy for depth= 12  and min leaf= 15 : 0.6575\n",
            "Validation accuracy for depth= 12  and min leaf= 16 : 0.665\n",
            "Validation accuracy for depth= 12  and min leaf= 17 : 0.6625\n",
            "Validation accuracy for depth= 12  and min leaf= 18 : 0.6537499999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 12  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 12  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 12  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 12  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 12  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 12  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 12  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 12  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 12  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 12  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 12  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 12  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 12  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 12  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 12  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 12  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 12  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 12  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 12  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 12  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 12  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 12  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 12  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 12  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 12  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 12  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 12  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 12  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 12  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 12  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 12  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 12  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 12  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 12  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 12  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 12  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 12  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 12  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 12  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 12  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 12  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 12  and min leaf= 73 : 0.6962499999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 12  and min leaf= 75 : 0.68875\n",
            "Validation accuracy for depth= 12  and min leaf= 76 : 0.68625\n",
            "Validation accuracy for depth= 12  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 12  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 12  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 1 : 0.61\n",
            "Validation accuracy for depth= 13  and min leaf= 2 : 0.6275000000000001\n",
            "Validation accuracy for depth= 13  and min leaf= 3 : 0.64\n",
            "Validation accuracy for depth= 13  and min leaf= 4 : 0.6275000000000001\n",
            "Validation accuracy for depth= 13  and min leaf= 5 : 0.63375\n",
            "Validation accuracy for depth= 13  and min leaf= 6 : 0.65\n",
            "Validation accuracy for depth= 13  and min leaf= 7 : 0.66875\n",
            "Validation accuracy for depth= 13  and min leaf= 8 : 0.6575\n",
            "Validation accuracy for depth= 13  and min leaf= 9 : 0.68\n",
            "Validation accuracy for depth= 13  and min leaf= 10 : 0.66625\n",
            "Validation accuracy for depth= 13  and min leaf= 11 : 0.66125\n",
            "Validation accuracy for depth= 13  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 13  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 13  and min leaf= 14 : 0.64625\n",
            "Validation accuracy for depth= 13  and min leaf= 15 : 0.65375\n",
            "Validation accuracy for depth= 13  and min leaf= 16 : 0.665\n",
            "Validation accuracy for depth= 13  and min leaf= 17 : 0.6625\n",
            "Validation accuracy for depth= 13  and min leaf= 18 : 0.6537499999999999\n",
            "Validation accuracy for depth= 13  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 13  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 13  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 13  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 13  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 13  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 13  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 13  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 13  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 13  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 13  and min leaf= 29 : 0.665\n",
            "Validation accuracy for depth= 13  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 13  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 13  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 13  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 13  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 13  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 13  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 13  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 13  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 13  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 13  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 13  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 13  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 13  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 13  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 13  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 13  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 13  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 13  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 13  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 13  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 13  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 13  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 13  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 13  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 13  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 13  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 13  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 13  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 13  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 13  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 13  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 13  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 13  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 13  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 13  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 13  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 13  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 13  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 13  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 13  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 13  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 13  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 13  and min leaf= 73 : 0.68875\n",
            "Validation accuracy for depth= 13  and min leaf= 74 : 0.68875\n",
            "Validation accuracy for depth= 13  and min leaf= 75 : 0.68875\n",
            "Validation accuracy for depth= 13  and min leaf= 76 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 13  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 13  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 1 : 0.5962500000000001\n",
            "Validation accuracy for depth= 14  and min leaf= 2 : 0.63375\n",
            "Validation accuracy for depth= 14  and min leaf= 3 : 0.635\n",
            "Validation accuracy for depth= 14  and min leaf= 4 : 0.6275000000000001\n",
            "Validation accuracy for depth= 14  and min leaf= 5 : 0.6325000000000001\n",
            "Validation accuracy for depth= 14  and min leaf= 6 : 0.6537499999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 7 : 0.6699999999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 8 : 0.6587500000000001\n",
            "Validation accuracy for depth= 14  and min leaf= 9 : 0.68\n",
            "Validation accuracy for depth= 14  and min leaf= 10 : 0.66625\n",
            "Validation accuracy for depth= 14  and min leaf= 11 : 0.66125\n",
            "Validation accuracy for depth= 14  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 14  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 14  and min leaf= 14 : 0.6525000000000001\n",
            "Validation accuracy for depth= 14  and min leaf= 15 : 0.65375\n",
            "Validation accuracy for depth= 14  and min leaf= 16 : 0.66875\n",
            "Validation accuracy for depth= 14  and min leaf= 17 : 0.6625\n",
            "Validation accuracy for depth= 14  and min leaf= 18 : 0.6537499999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 14  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 14  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 14  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 14  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 14  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 14  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 14  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 14  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 14  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 14  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 14  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 14  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 14  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 14  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 14  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 14  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 14  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 14  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 14  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 14  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 14  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 14  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 14  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 14  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 14  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 14  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 14  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 14  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 14  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 14  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 14  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 14  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 14  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 14  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 14  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 14  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 14  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 14  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 14  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 14  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 14  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 14  and min leaf= 73 : 0.68875\n",
            "Validation accuracy for depth= 14  and min leaf= 74 : 0.68875\n",
            "Validation accuracy for depth= 14  and min leaf= 75 : 0.6962499999999999\n",
            "Validation accuracy for depth= 14  and min leaf= 76 : 0.68625\n",
            "Validation accuracy for depth= 14  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 14  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 14  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 1 : 0.595\n",
            "Validation accuracy for depth= 15  and min leaf= 2 : 0.63875\n",
            "Validation accuracy for depth= 15  and min leaf= 3 : 0.6375\n",
            "Validation accuracy for depth= 15  and min leaf= 4 : 0.6325\n",
            "Validation accuracy for depth= 15  and min leaf= 5 : 0.63125\n",
            "Validation accuracy for depth= 15  and min leaf= 6 : 0.64625\n",
            "Validation accuracy for depth= 15  and min leaf= 7 : 0.6675000000000001\n",
            "Validation accuracy for depth= 15  and min leaf= 8 : 0.6599999999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 9 : 0.68\n",
            "Validation accuracy for depth= 15  and min leaf= 10 : 0.6675\n",
            "Validation accuracy for depth= 15  and min leaf= 11 : 0.6637500000000001\n",
            "Validation accuracy for depth= 15  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 15  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 15  and min leaf= 14 : 0.65375\n",
            "Validation accuracy for depth= 15  and min leaf= 15 : 0.65625\n",
            "Validation accuracy for depth= 15  and min leaf= 16 : 0.665\n",
            "Validation accuracy for depth= 15  and min leaf= 17 : 0.6637500000000001\n",
            "Validation accuracy for depth= 15  and min leaf= 18 : 0.6525\n",
            "Validation accuracy for depth= 15  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 15  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 15  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 15  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 15  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 15  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 15  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 15  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 15  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 15  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 15  and min leaf= 29 : 0.665\n",
            "Validation accuracy for depth= 15  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 15  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 15  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 15  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 15  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 15  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 15  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 15  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 15  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 15  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 15  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 15  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 15  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 15  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 15  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 15  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 15  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 15  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 15  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 15  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 15  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 15  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 15  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 15  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 15  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 15  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 15  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 15  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 15  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 15  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 15  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 15  and min leaf= 73 : 0.6962499999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 75 : 0.6962499999999999\n",
            "Validation accuracy for depth= 15  and min leaf= 76 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 15  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 15  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 1 : 0.6125\n",
            "Validation accuracy for depth= 16  and min leaf= 2 : 0.6362500000000001\n",
            "Validation accuracy for depth= 16  and min leaf= 3 : 0.63\n",
            "Validation accuracy for depth= 16  and min leaf= 4 : 0.63375\n",
            "Validation accuracy for depth= 16  and min leaf= 5 : 0.6325000000000001\n",
            "Validation accuracy for depth= 16  and min leaf= 6 : 0.6525000000000001\n",
            "Validation accuracy for depth= 16  and min leaf= 7 : 0.66625\n",
            "Validation accuracy for depth= 16  and min leaf= 8 : 0.6575\n",
            "Validation accuracy for depth= 16  and min leaf= 9 : 0.68\n",
            "Validation accuracy for depth= 16  and min leaf= 10 : 0.66875\n",
            "Validation accuracy for depth= 16  and min leaf= 11 : 0.66125\n",
            "Validation accuracy for depth= 16  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 16  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 16  and min leaf= 14 : 0.65375\n",
            "Validation accuracy for depth= 16  and min leaf= 15 : 0.6575\n",
            "Validation accuracy for depth= 16  and min leaf= 16 : 0.665\n",
            "Validation accuracy for depth= 16  and min leaf= 17 : 0.6625\n",
            "Validation accuracy for depth= 16  and min leaf= 18 : 0.6525\n",
            "Validation accuracy for depth= 16  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 16  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 16  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 16  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 16  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 16  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 16  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 16  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 16  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 16  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 16  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 16  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 16  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 16  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 16  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 16  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 16  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 16  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 16  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 16  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 16  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 16  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 16  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 16  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 16  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 16  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 16  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 16  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 16  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 16  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 16  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 16  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 16  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 16  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 16  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 16  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 16  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 16  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 16  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 16  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 16  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 16  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 16  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 16  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 16  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 16  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 16  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 16  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 16  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 16  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 16  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 16  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 16  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 16  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 16  and min leaf= 73 : 0.6962499999999999\n",
            "Validation accuracy for depth= 16  and min leaf= 74 : 0.68875\n",
            "Validation accuracy for depth= 16  and min leaf= 75 : 0.6962499999999999\n",
            "Validation accuracy for depth= 16  and min leaf= 76 : 0.68625\n",
            "Validation accuracy for depth= 16  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 16  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 16  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 1 : 0.60875\n",
            "Validation accuracy for depth= 17  and min leaf= 2 : 0.6425\n",
            "Validation accuracy for depth= 17  and min leaf= 3 : 0.6399999999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 4 : 0.63\n",
            "Validation accuracy for depth= 17  and min leaf= 5 : 0.6325\n",
            "Validation accuracy for depth= 17  and min leaf= 6 : 0.64625\n",
            "Validation accuracy for depth= 17  and min leaf= 7 : 0.6625\n",
            "Validation accuracy for depth= 17  and min leaf= 8 : 0.6599999999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 9 : 0.68125\n",
            "Validation accuracy for depth= 17  and min leaf= 10 : 0.66625\n",
            "Validation accuracy for depth= 17  and min leaf= 11 : 0.66125\n",
            "Validation accuracy for depth= 17  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 17  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 17  and min leaf= 14 : 0.65375\n",
            "Validation accuracy for depth= 17  and min leaf= 15 : 0.65375\n",
            "Validation accuracy for depth= 17  and min leaf= 16 : 0.6675\n",
            "Validation accuracy for depth= 17  and min leaf= 17 : 0.6637500000000001\n",
            "Validation accuracy for depth= 17  and min leaf= 18 : 0.6525\n",
            "Validation accuracy for depth= 17  and min leaf= 19 : 0.645\n",
            "Validation accuracy for depth= 17  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 17  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 17  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 17  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 17  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 17  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 17  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 17  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 17  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 17  and min leaf= 29 : 0.665\n",
            "Validation accuracy for depth= 17  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 17  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 17  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 17  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 17  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 17  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 17  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 17  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 17  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 17  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 17  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 17  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 17  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 17  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 17  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 17  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 17  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 17  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 17  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 17  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 17  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 17  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 17  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 17  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 17  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 17  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 17  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 17  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 17  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 17  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 17  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 17  and min leaf= 73 : 0.6962499999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 74 : 0.68875\n",
            "Validation accuracy for depth= 17  and min leaf= 75 : 0.6962499999999999\n",
            "Validation accuracy for depth= 17  and min leaf= 76 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 17  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 17  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 1 : 0.5862499999999999\n",
            "Validation accuracy for depth= 18  and min leaf= 2 : 0.6375\n",
            "Validation accuracy for depth= 18  and min leaf= 3 : 0.64125\n",
            "Validation accuracy for depth= 18  and min leaf= 4 : 0.625\n",
            "Validation accuracy for depth= 18  and min leaf= 5 : 0.6375\n",
            "Validation accuracy for depth= 18  and min leaf= 6 : 0.65\n",
            "Validation accuracy for depth= 18  and min leaf= 7 : 0.66625\n",
            "Validation accuracy for depth= 18  and min leaf= 8 : 0.6587500000000001\n",
            "Validation accuracy for depth= 18  and min leaf= 9 : 0.6762500000000001\n",
            "Validation accuracy for depth= 18  and min leaf= 10 : 0.6675\n",
            "Validation accuracy for depth= 18  and min leaf= 11 : 0.6637500000000001\n",
            "Validation accuracy for depth= 18  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 18  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 18  and min leaf= 14 : 0.6525000000000001\n",
            "Validation accuracy for depth= 18  and min leaf= 15 : 0.6575\n",
            "Validation accuracy for depth= 18  and min leaf= 16 : 0.665\n",
            "Validation accuracy for depth= 18  and min leaf= 17 : 0.6637500000000001\n",
            "Validation accuracy for depth= 18  and min leaf= 18 : 0.6525\n",
            "Validation accuracy for depth= 18  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 18  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 18  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 18  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 18  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 18  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 18  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 18  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 18  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 18  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 18  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 18  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 18  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 18  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 18  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 18  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 18  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 18  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 18  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 18  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 18  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 18  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 18  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 18  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 18  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 18  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 18  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 18  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 18  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 18  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 18  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 18  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 18  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 18  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 18  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 18  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 18  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 18  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 18  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 18  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 18  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 18  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 18  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 18  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 18  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 18  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 18  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 18  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 18  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 18  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 18  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 18  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 18  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 18  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 18  and min leaf= 73 : 0.6962499999999999\n",
            "Validation accuracy for depth= 18  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 18  and min leaf= 75 : 0.68875\n",
            "Validation accuracy for depth= 18  and min leaf= 76 : 0.68625\n",
            "Validation accuracy for depth= 18  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 18  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 18  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 1 : 0.61625\n",
            "Validation accuracy for depth= 19  and min leaf= 2 : 0.6537499999999999\n",
            "Validation accuracy for depth= 19  and min leaf= 3 : 0.6450000000000001\n",
            "Validation accuracy for depth= 19  and min leaf= 4 : 0.6325\n",
            "Validation accuracy for depth= 19  and min leaf= 5 : 0.63375\n",
            "Validation accuracy for depth= 19  and min leaf= 6 : 0.65\n",
            "Validation accuracy for depth= 19  and min leaf= 7 : 0.66875\n",
            "Validation accuracy for depth= 19  and min leaf= 8 : 0.6575000000000001\n",
            "Validation accuracy for depth= 19  and min leaf= 9 : 0.68125\n",
            "Validation accuracy for depth= 19  and min leaf= 10 : 0.66625\n",
            "Validation accuracy for depth= 19  and min leaf= 11 : 0.66125\n",
            "Validation accuracy for depth= 19  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 19  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 19  and min leaf= 14 : 0.65375\n",
            "Validation accuracy for depth= 19  and min leaf= 15 : 0.6525000000000001\n",
            "Validation accuracy for depth= 19  and min leaf= 16 : 0.6637500000000001\n",
            "Validation accuracy for depth= 19  and min leaf= 17 : 0.6625\n",
            "Validation accuracy for depth= 19  and min leaf= 18 : 0.6537499999999999\n",
            "Validation accuracy for depth= 19  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 19  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 19  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 19  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 19  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 19  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 19  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 19  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 19  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 19  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 19  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 19  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 19  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 19  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 19  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 19  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 19  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 19  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 19  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 19  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 19  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 19  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 19  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 19  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 19  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 19  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 19  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 19  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 19  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 19  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 19  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 19  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 19  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 19  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 19  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 19  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 19  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 19  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 19  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 19  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 19  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 19  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 19  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 19  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 19  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 19  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 19  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 19  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 19  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 19  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 19  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 19  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 19  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 19  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 19  and min leaf= 73 : 0.6962499999999999\n",
            "Validation accuracy for depth= 19  and min leaf= 74 : 0.68875\n",
            "Validation accuracy for depth= 19  and min leaf= 75 : 0.68875\n",
            "Validation accuracy for depth= 19  and min leaf= 76 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 19  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 19  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 1 : 0.61125\n",
            "Validation accuracy for depth= 20  and min leaf= 2 : 0.6537499999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 3 : 0.63625\n",
            "Validation accuracy for depth= 20  and min leaf= 4 : 0.6312499999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 5 : 0.6362500000000001\n",
            "Validation accuracy for depth= 20  and min leaf= 6 : 0.655\n",
            "Validation accuracy for depth= 20  and min leaf= 7 : 0.6675000000000001\n",
            "Validation accuracy for depth= 20  and min leaf= 8 : 0.6575\n",
            "Validation accuracy for depth= 20  and min leaf= 9 : 0.68\n",
            "Validation accuracy for depth= 20  and min leaf= 10 : 0.66875\n",
            "Validation accuracy for depth= 20  and min leaf= 11 : 0.6599999999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 20  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 20  and min leaf= 14 : 0.64625\n",
            "Validation accuracy for depth= 20  and min leaf= 15 : 0.65375\n",
            "Validation accuracy for depth= 20  and min leaf= 16 : 0.66875\n",
            "Validation accuracy for depth= 20  and min leaf= 17 : 0.6625\n",
            "Validation accuracy for depth= 20  and min leaf= 18 : 0.6525\n",
            "Validation accuracy for depth= 20  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 20  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 20  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 20  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 20  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 20  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 20  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 20  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 20  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 20  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 20  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 20  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 20  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 20  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 20  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 20  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 20  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 20  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 20  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 20  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 20  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 20  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 20  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 20  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 20  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 20  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 20  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 20  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 20  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 20  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 20  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 20  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 20  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 20  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 20  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 20  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 20  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 20  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 20  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 20  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 20  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 20  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 20  and min leaf= 73 : 0.68875\n",
            "Validation accuracy for depth= 20  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 75 : 0.6962499999999999\n",
            "Validation accuracy for depth= 20  and min leaf= 76 : 0.68625\n",
            "Validation accuracy for depth= 20  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 20  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 20  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 1 : 0.57875\n",
            "Validation accuracy for depth= 21  and min leaf= 2 : 0.63\n",
            "Validation accuracy for depth= 21  and min leaf= 3 : 0.64375\n",
            "Validation accuracy for depth= 21  and min leaf= 4 : 0.6312500000000001\n",
            "Validation accuracy for depth= 21  and min leaf= 5 : 0.6325000000000001\n",
            "Validation accuracy for depth= 21  and min leaf= 6 : 0.64875\n",
            "Validation accuracy for depth= 21  and min leaf= 7 : 0.6637500000000001\n",
            "Validation accuracy for depth= 21  and min leaf= 8 : 0.6587500000000001\n",
            "Validation accuracy for depth= 21  and min leaf= 9 : 0.67875\n",
            "Validation accuracy for depth= 21  and min leaf= 10 : 0.6637500000000001\n",
            "Validation accuracy for depth= 21  and min leaf= 11 : 0.6637500000000001\n",
            "Validation accuracy for depth= 21  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 21  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 21  and min leaf= 14 : 0.6525000000000001\n",
            "Validation accuracy for depth= 21  and min leaf= 15 : 0.6575\n",
            "Validation accuracy for depth= 21  and min leaf= 16 : 0.665\n",
            "Validation accuracy for depth= 21  and min leaf= 17 : 0.6625\n",
            "Validation accuracy for depth= 21  and min leaf= 18 : 0.6537499999999999\n",
            "Validation accuracy for depth= 21  and min leaf= 19 : 0.645\n",
            "Validation accuracy for depth= 21  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 21  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 21  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 21  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 21  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 21  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 21  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 21  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 21  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 21  and min leaf= 29 : 0.665\n",
            "Validation accuracy for depth= 21  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 21  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 21  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 21  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 21  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 21  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 21  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 21  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 21  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 21  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 21  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 21  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 21  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 21  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 21  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 21  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 21  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 21  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 21  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 21  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 21  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 21  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 21  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 21  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 21  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 21  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 21  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 21  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 21  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 21  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 21  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 21  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 21  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 21  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 21  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 21  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 21  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 21  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 21  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 21  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 21  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 21  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 21  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 21  and min leaf= 73 : 0.68875\n",
            "Validation accuracy for depth= 21  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 21  and min leaf= 75 : 0.68875\n",
            "Validation accuracy for depth= 21  and min leaf= 76 : 0.68625\n",
            "Validation accuracy for depth= 21  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 21  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 21  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 1 : 0.58375\n",
            "Validation accuracy for depth= 22  and min leaf= 2 : 0.64125\n",
            "Validation accuracy for depth= 22  and min leaf= 3 : 0.635\n",
            "Validation accuracy for depth= 22  and min leaf= 4 : 0.63125\n",
            "Validation accuracy for depth= 22  and min leaf= 5 : 0.635\n",
            "Validation accuracy for depth= 22  and min leaf= 6 : 0.6525000000000001\n",
            "Validation accuracy for depth= 22  and min leaf= 7 : 0.665\n",
            "Validation accuracy for depth= 22  and min leaf= 8 : 0.6599999999999999\n",
            "Validation accuracy for depth= 22  and min leaf= 9 : 0.6825000000000001\n",
            "Validation accuracy for depth= 22  and min leaf= 10 : 0.6675\n",
            "Validation accuracy for depth= 22  and min leaf= 11 : 0.6625\n",
            "Validation accuracy for depth= 22  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 22  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 22  and min leaf= 14 : 0.64625\n",
            "Validation accuracy for depth= 22  and min leaf= 15 : 0.65625\n",
            "Validation accuracy for depth= 22  and min leaf= 16 : 0.66875\n",
            "Validation accuracy for depth= 22  and min leaf= 17 : 0.6637500000000001\n",
            "Validation accuracy for depth= 22  and min leaf= 18 : 0.6525\n",
            "Validation accuracy for depth= 22  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 22  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 22  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 22  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 22  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 22  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 22  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 22  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 22  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 22  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 22  and min leaf= 29 : 0.665\n",
            "Validation accuracy for depth= 22  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 22  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 22  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 22  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 22  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 22  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 22  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 22  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 22  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 22  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 22  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 22  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 22  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 22  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 22  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 22  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 22  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 22  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 22  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 22  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 22  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 22  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 22  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 22  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 22  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 22  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 22  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 22  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 22  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 22  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 22  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 22  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 22  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 22  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 22  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 22  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 22  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 22  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 22  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 22  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 22  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 22  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 22  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 22  and min leaf= 73 : 0.68875\n",
            "Validation accuracy for depth= 22  and min leaf= 74 : 0.68875\n",
            "Validation accuracy for depth= 22  and min leaf= 75 : 0.68875\n",
            "Validation accuracy for depth= 22  and min leaf= 76 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 22  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 22  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 1 : 0.5974999999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 2 : 0.6437499999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 3 : 0.6399999999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 4 : 0.625\n",
            "Validation accuracy for depth= 23  and min leaf= 5 : 0.63625\n",
            "Validation accuracy for depth= 23  and min leaf= 6 : 0.65125\n",
            "Validation accuracy for depth= 23  and min leaf= 7 : 0.66625\n",
            "Validation accuracy for depth= 23  and min leaf= 8 : 0.6599999999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 9 : 0.68125\n",
            "Validation accuracy for depth= 23  and min leaf= 10 : 0.66125\n",
            "Validation accuracy for depth= 23  and min leaf= 11 : 0.6625\n",
            "Validation accuracy for depth= 23  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 23  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 23  and min leaf= 14 : 0.65375\n",
            "Validation accuracy for depth= 23  and min leaf= 15 : 0.6575\n",
            "Validation accuracy for depth= 23  and min leaf= 16 : 0.665\n",
            "Validation accuracy for depth= 23  and min leaf= 17 : 0.6637500000000001\n",
            "Validation accuracy for depth= 23  and min leaf= 18 : 0.6537499999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 23  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 23  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 23  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 23  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 23  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 23  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 23  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 23  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 23  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 23  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 23  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 23  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 23  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 23  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 23  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 23  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 23  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 23  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 23  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 23  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 23  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 23  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 23  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 23  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 23  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 23  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 23  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 23  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 23  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 23  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 23  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 23  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 23  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 23  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 23  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 23  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 23  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 23  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 23  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 23  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 23  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 23  and min leaf= 73 : 0.6962499999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 23  and min leaf= 75 : 0.68875\n",
            "Validation accuracy for depth= 23  and min leaf= 76 : 0.68625\n",
            "Validation accuracy for depth= 23  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 23  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 23  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 1 : 0.61\n",
            "Validation accuracy for depth= 24  and min leaf= 2 : 0.6300000000000001\n",
            "Validation accuracy for depth= 24  and min leaf= 3 : 0.62625\n",
            "Validation accuracy for depth= 24  and min leaf= 4 : 0.6362500000000001\n",
            "Validation accuracy for depth= 24  and min leaf= 5 : 0.63625\n",
            "Validation accuracy for depth= 24  and min leaf= 6 : 0.65125\n",
            "Validation accuracy for depth= 24  and min leaf= 7 : 0.6699999999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 8 : 0.6575000000000001\n",
            "Validation accuracy for depth= 24  and min leaf= 9 : 0.6762500000000001\n",
            "Validation accuracy for depth= 24  and min leaf= 10 : 0.6675000000000001\n",
            "Validation accuracy for depth= 24  and min leaf= 11 : 0.6637500000000001\n",
            "Validation accuracy for depth= 24  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 24  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 24  and min leaf= 14 : 0.6525000000000001\n",
            "Validation accuracy for depth= 24  and min leaf= 15 : 0.65375\n",
            "Validation accuracy for depth= 24  and min leaf= 16 : 0.665\n",
            "Validation accuracy for depth= 24  and min leaf= 17 : 0.6637500000000001\n",
            "Validation accuracy for depth= 24  and min leaf= 18 : 0.6512499999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 24  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 24  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 24  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 24  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 24  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 24  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 24  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 24  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 24  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 24  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 24  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 24  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 24  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 24  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 24  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 24  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 24  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 24  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 24  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 24  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 24  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 24  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 24  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 24  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 24  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 24  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 24  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 24  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 24  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 24  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 24  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 24  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 24  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 24  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 24  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 24  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 24  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 24  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 24  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 24  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 24  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 24  and min leaf= 73 : 0.6962499999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 24  and min leaf= 75 : 0.68875\n",
            "Validation accuracy for depth= 24  and min leaf= 76 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 24  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 24  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 1 : 0.6024999999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 2 : 0.64875\n",
            "Validation accuracy for depth= 25  and min leaf= 3 : 0.63125\n",
            "Validation accuracy for depth= 25  and min leaf= 4 : 0.6325000000000001\n",
            "Validation accuracy for depth= 25  and min leaf= 5 : 0.6399999999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 6 : 0.65125\n",
            "Validation accuracy for depth= 25  and min leaf= 7 : 0.665\n",
            "Validation accuracy for depth= 25  and min leaf= 8 : 0.6587500000000001\n",
            "Validation accuracy for depth= 25  and min leaf= 9 : 0.6825000000000001\n",
            "Validation accuracy for depth= 25  and min leaf= 10 : 0.66125\n",
            "Validation accuracy for depth= 25  and min leaf= 11 : 0.66125\n",
            "Validation accuracy for depth= 25  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 25  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 25  and min leaf= 14 : 0.65375\n",
            "Validation accuracy for depth= 25  and min leaf= 15 : 0.65625\n",
            "Validation accuracy for depth= 25  and min leaf= 16 : 0.6637500000000001\n",
            "Validation accuracy for depth= 25  and min leaf= 17 : 0.6625\n",
            "Validation accuracy for depth= 25  and min leaf= 18 : 0.6525\n",
            "Validation accuracy for depth= 25  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 25  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 25  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 25  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 25  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 25  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 25  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 25  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 25  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 25  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 25  and min leaf= 29 : 0.665\n",
            "Validation accuracy for depth= 25  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 25  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 25  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 25  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 25  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 25  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 25  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 25  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 25  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 25  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 25  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 25  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 25  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 25  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 25  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 25  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 25  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 25  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 25  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 25  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 25  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 25  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 25  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 25  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 25  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 25  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 25  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 25  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 25  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 25  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 25  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 25  and min leaf= 73 : 0.6962499999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 75 : 0.6962499999999999\n",
            "Validation accuracy for depth= 25  and min leaf= 76 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 25  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 25  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 1 : 0.5962500000000001\n",
            "Validation accuracy for depth= 26  and min leaf= 2 : 0.6425000000000001\n",
            "Validation accuracy for depth= 26  and min leaf= 3 : 0.63375\n",
            "Validation accuracy for depth= 26  and min leaf= 4 : 0.62375\n",
            "Validation accuracy for depth= 26  and min leaf= 5 : 0.6362499999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 6 : 0.6487499999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 7 : 0.66625\n",
            "Validation accuracy for depth= 26  and min leaf= 8 : 0.6612499999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 9 : 0.68125\n",
            "Validation accuracy for depth= 26  and min leaf= 10 : 0.66875\n",
            "Validation accuracy for depth= 26  and min leaf= 11 : 0.66125\n",
            "Validation accuracy for depth= 26  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 26  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 26  and min leaf= 14 : 0.64625\n",
            "Validation accuracy for depth= 26  and min leaf= 15 : 0.6575\n",
            "Validation accuracy for depth= 26  and min leaf= 16 : 0.6637500000000001\n",
            "Validation accuracy for depth= 26  and min leaf= 17 : 0.6625\n",
            "Validation accuracy for depth= 26  and min leaf= 18 : 0.6537499999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 26  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 26  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 26  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 26  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 26  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 26  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 26  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 26  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 26  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 26  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 26  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 26  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 26  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 26  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 26  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 26  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 26  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 26  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 26  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 26  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 26  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 26  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 26  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 26  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 26  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 26  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 26  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 26  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 26  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 26  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 26  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 26  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 26  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 26  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 26  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 26  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 26  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 26  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 26  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 26  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 26  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 26  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 26  and min leaf= 73 : 0.68875\n",
            "Validation accuracy for depth= 26  and min leaf= 74 : 0.68875\n",
            "Validation accuracy for depth= 26  and min leaf= 75 : 0.68875\n",
            "Validation accuracy for depth= 26  and min leaf= 76 : 0.68625\n",
            "Validation accuracy for depth= 26  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 26  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 26  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 1 : 0.6074999999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 2 : 0.64375\n",
            "Validation accuracy for depth= 27  and min leaf= 3 : 0.64125\n",
            "Validation accuracy for depth= 27  and min leaf= 4 : 0.6325\n",
            "Validation accuracy for depth= 27  and min leaf= 5 : 0.635\n",
            "Validation accuracy for depth= 27  and min leaf= 6 : 0.6487499999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 7 : 0.66625\n",
            "Validation accuracy for depth= 27  and min leaf= 8 : 0.6612499999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 9 : 0.68125\n",
            "Validation accuracy for depth= 27  and min leaf= 10 : 0.66625\n",
            "Validation accuracy for depth= 27  and min leaf= 11 : 0.6637500000000001\n",
            "Validation accuracy for depth= 27  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 27  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 27  and min leaf= 14 : 0.64625\n",
            "Validation accuracy for depth= 27  and min leaf= 15 : 0.65375\n",
            "Validation accuracy for depth= 27  and min leaf= 16 : 0.665\n",
            "Validation accuracy for depth= 27  and min leaf= 17 : 0.6637500000000001\n",
            "Validation accuracy for depth= 27  and min leaf= 18 : 0.6525\n",
            "Validation accuracy for depth= 27  and min leaf= 19 : 0.64625\n",
            "Validation accuracy for depth= 27  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 27  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 27  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 27  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 27  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 27  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 27  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 27  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 27  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 27  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 27  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 27  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 27  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 27  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 27  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 27  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 27  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 27  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 27  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 27  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 27  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 27  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 27  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 27  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 27  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 27  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 27  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 27  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 27  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 27  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 27  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 27  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 27  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 27  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 27  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 27  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 27  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 27  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 27  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 27  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 27  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 27  and min leaf= 73 : 0.68875\n",
            "Validation accuracy for depth= 27  and min leaf= 74 : 0.68875\n",
            "Validation accuracy for depth= 27  and min leaf= 75 : 0.6962499999999999\n",
            "Validation accuracy for depth= 27  and min leaf= 76 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 27  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 27  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 1 : 0.6050000000000001\n",
            "Validation accuracy for depth= 28  and min leaf= 2 : 0.6449999999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 3 : 0.6399999999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 4 : 0.6325000000000001\n",
            "Validation accuracy for depth= 28  and min leaf= 5 : 0.6325\n",
            "Validation accuracy for depth= 28  and min leaf= 6 : 0.65625\n",
            "Validation accuracy for depth= 28  and min leaf= 7 : 0.66875\n",
            "Validation accuracy for depth= 28  and min leaf= 8 : 0.6587500000000001\n",
            "Validation accuracy for depth= 28  and min leaf= 9 : 0.6825000000000001\n",
            "Validation accuracy for depth= 28  and min leaf= 10 : 0.66875\n",
            "Validation accuracy for depth= 28  and min leaf= 11 : 0.6637500000000001\n",
            "Validation accuracy for depth= 28  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 28  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 28  and min leaf= 14 : 0.65375\n",
            "Validation accuracy for depth= 28  and min leaf= 15 : 0.6575\n",
            "Validation accuracy for depth= 28  and min leaf= 16 : 0.665\n",
            "Validation accuracy for depth= 28  and min leaf= 17 : 0.6637500000000001\n",
            "Validation accuracy for depth= 28  and min leaf= 18 : 0.6512499999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 19 : 0.645\n",
            "Validation accuracy for depth= 28  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 28  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 28  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 28  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 28  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 28  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 28  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 28  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 28  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 28  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 28  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 28  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 28  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 28  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 28  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 28  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 28  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 28  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 28  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 28  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 28  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 28  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 28  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 28  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 28  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 28  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 28  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 28  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 28  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 28  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 28  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 28  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 28  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 28  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 28  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 28  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 28  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 28  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 28  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 28  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 28  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 28  and min leaf= 73 : 0.6962499999999999\n",
            "Validation accuracy for depth= 28  and min leaf= 74 : 0.68875\n",
            "Validation accuracy for depth= 28  and min leaf= 75 : 0.68875\n",
            "Validation accuracy for depth= 28  and min leaf= 76 : 0.68625\n",
            "Validation accuracy for depth= 28  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 28  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 28  and min leaf= 99 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 1 : 0.6025\n",
            "Validation accuracy for depth= 29  and min leaf= 2 : 0.64\n",
            "Validation accuracy for depth= 29  and min leaf= 3 : 0.63125\n",
            "Validation accuracy for depth= 29  and min leaf= 4 : 0.6325000000000001\n",
            "Validation accuracy for depth= 29  and min leaf= 5 : 0.64\n",
            "Validation accuracy for depth= 29  and min leaf= 6 : 0.6475\n",
            "Validation accuracy for depth= 29  and min leaf= 7 : 0.6675000000000001\n",
            "Validation accuracy for depth= 29  and min leaf= 8 : 0.6587500000000001\n",
            "Validation accuracy for depth= 29  and min leaf= 9 : 0.6775\n",
            "Validation accuracy for depth= 29  and min leaf= 10 : 0.6675000000000001\n",
            "Validation accuracy for depth= 29  and min leaf= 11 : 0.66125\n",
            "Validation accuracy for depth= 29  and min leaf= 12 : 0.6725\n",
            "Validation accuracy for depth= 29  and min leaf= 13 : 0.6700000000000002\n",
            "Validation accuracy for depth= 29  and min leaf= 14 : 0.64625\n",
            "Validation accuracy for depth= 29  and min leaf= 15 : 0.6575\n",
            "Validation accuracy for depth= 29  and min leaf= 16 : 0.6637500000000001\n",
            "Validation accuracy for depth= 29  and min leaf= 17 : 0.6625\n",
            "Validation accuracy for depth= 29  and min leaf= 18 : 0.6537499999999999\n",
            "Validation accuracy for depth= 29  and min leaf= 19 : 0.645\n",
            "Validation accuracy for depth= 29  and min leaf= 20 : 0.6475\n",
            "Validation accuracy for depth= 29  and min leaf= 21 : 0.65\n",
            "Validation accuracy for depth= 29  and min leaf= 22 : 0.645\n",
            "Validation accuracy for depth= 29  and min leaf= 23 : 0.65\n",
            "Validation accuracy for depth= 29  and min leaf= 24 : 0.65125\n",
            "Validation accuracy for depth= 29  and min leaf= 25 : 0.655\n",
            "Validation accuracy for depth= 29  and min leaf= 26 : 0.655\n",
            "Validation accuracy for depth= 29  and min leaf= 27 : 0.6525000000000001\n",
            "Validation accuracy for depth= 29  and min leaf= 28 : 0.6525000000000001\n",
            "Validation accuracy for depth= 29  and min leaf= 29 : 0.65375\n",
            "Validation accuracy for depth= 29  and min leaf= 30 : 0.67125\n",
            "Validation accuracy for depth= 29  and min leaf= 31 : 0.67125\n",
            "Validation accuracy for depth= 29  and min leaf= 32 : 0.66875\n",
            "Validation accuracy for depth= 29  and min leaf= 33 : 0.6725000000000001\n",
            "Validation accuracy for depth= 29  and min leaf= 34 : 0.6725000000000001\n",
            "Validation accuracy for depth= 29  and min leaf= 35 : 0.66875\n",
            "Validation accuracy for depth= 29  and min leaf= 36 : 0.67375\n",
            "Validation accuracy for depth= 29  and min leaf= 37 : 0.675\n",
            "Validation accuracy for depth= 29  and min leaf= 38 : 0.67\n",
            "Validation accuracy for depth= 29  and min leaf= 39 : 0.67\n",
            "Validation accuracy for depth= 29  and min leaf= 40 : 0.67\n",
            "Validation accuracy for depth= 29  and min leaf= 41 : 0.66625\n",
            "Validation accuracy for depth= 29  and min leaf= 42 : 0.6712499999999999\n",
            "Validation accuracy for depth= 29  and min leaf= 43 : 0.6712499999999999\n",
            "Validation accuracy for depth= 29  and min leaf= 44 : 0.6725\n",
            "Validation accuracy for depth= 29  and min leaf= 45 : 0.675\n",
            "Validation accuracy for depth= 29  and min leaf= 46 : 0.675\n",
            "Validation accuracy for depth= 29  and min leaf= 47 : 0.675\n",
            "Validation accuracy for depth= 29  and min leaf= 48 : 0.6799999999999999\n",
            "Validation accuracy for depth= 29  and min leaf= 49 : 0.6799999999999999\n",
            "Validation accuracy for depth= 29  and min leaf= 50 : 0.6799999999999999\n",
            "Validation accuracy for depth= 29  and min leaf= 51 : 0.6799999999999999\n",
            "Validation accuracy for depth= 29  and min leaf= 52 : 0.6799999999999999\n",
            "Validation accuracy for depth= 29  and min leaf= 53 : 0.67375\n",
            "Validation accuracy for depth= 29  and min leaf= 54 : 0.6825\n",
            "Validation accuracy for depth= 29  and min leaf= 55 : 0.6812499999999999\n",
            "Validation accuracy for depth= 29  and min leaf= 56 : 0.6799999999999999\n",
            "Validation accuracy for depth= 29  and min leaf= 57 : 0.6812499999999999\n",
            "Validation accuracy for depth= 29  and min leaf= 58 : 0.6812499999999999\n",
            "Validation accuracy for depth= 29  and min leaf= 59 : 0.6812499999999999\n",
            "Validation accuracy for depth= 29  and min leaf= 60 : 0.6925\n",
            "Validation accuracy for depth= 29  and min leaf= 61 : 0.6925\n",
            "Validation accuracy for depth= 29  and min leaf= 62 : 0.6925\n",
            "Validation accuracy for depth= 29  and min leaf= 63 : 0.6925\n",
            "Validation accuracy for depth= 29  and min leaf= 64 : 0.6925\n",
            "Validation accuracy for depth= 29  and min leaf= 65 : 0.6787500000000001\n",
            "Validation accuracy for depth= 29  and min leaf= 66 : 0.6787500000000001\n",
            "Validation accuracy for depth= 29  and min leaf= 67 : 0.6787500000000001\n",
            "Validation accuracy for depth= 29  and min leaf= 68 : 0.6975\n",
            "Validation accuracy for depth= 29  and min leaf= 69 : 0.6975\n",
            "Validation accuracy for depth= 29  and min leaf= 70 : 0.6975\n",
            "Validation accuracy for depth= 29  and min leaf= 71 : 0.6975\n",
            "Validation accuracy for depth= 29  and min leaf= 72 : 0.6975\n",
            "Validation accuracy for depth= 29  and min leaf= 73 : 0.68875\n",
            "Validation accuracy for depth= 29  and min leaf= 74 : 0.6962499999999999\n",
            "Validation accuracy for depth= 29  and min leaf= 75 : 0.68875\n",
            "Validation accuracy for depth= 29  and min leaf= 76 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 77 : 0.68625\n",
            "Validation accuracy for depth= 29  and min leaf= 78 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 79 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 80 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 81 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 82 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 83 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 84 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 85 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 86 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 87 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 88 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 89 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 90 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 91 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 92 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 93 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 94 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 95 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 96 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 97 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 98 : 0.69375\n",
            "Validation accuracy for depth= 29  and min leaf= 99 : 0.69375\n",
            "Best mean validation accuracy ( 0.70125 ) is achieved with max depth= 2  and min leaf= 60\n",
            "The final model accuracy on validation set is:  0.725\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**MULTILAYER PERCEPTRON!!!!!!!!!**"
      ],
      "metadata": {
        "id": "Y2o1Hf29z-mH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "hidden_layer_list = [2, 3, 4, 5, 6]\n",
        "epoch_list = [100, 150, 200, 250]\n",
        "learning_rate = [0.1, 0.01, 0.001]\n",
        "\n",
        "best_mean_acc = -1\n",
        "best_lr = None\n",
        "best_epoch = None\n",
        "best_layer = None\n",
        "results_mlp = []\n",
        "\n",
        "for lr in learning_rate:\n",
        "  for epoch in epoch_list:\n",
        "    for layer in hidden_layer_list:\n",
        "      # Initialize the MLPClassifier\n",
        "      mlp = MLPClassifier(hidden_layer_sizes=(layer,), max_iter=epoch, alpha=1e-4,\n",
        "                          solver='sgd', verbose=10, random_state=42,\n",
        "                          learning_rate_init=lr)\n",
        "\n",
        "      # Perform 5-fold cross-validation on the training data\n",
        "      scores = cross_val_score(mlp, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "      mean_acc_score = scores.mean()\n",
        "      print('Validation accuracy for learning rate=', lr,\" and epoaches=\", epoch, \"and for layer number\", layer, ':', mean_acc_score)\n",
        "\n",
        "      results_mlp.append((lr, epoch, layer, mean_acc_score))\n",
        "\n",
        "      # if mean validation accuracy is better than best_mean_acc, update best_mean_acc and hyperparameters\n",
        "      if (best_mean_acc < mean_acc_score):\n",
        "          best_mean_acc = mean_acc_score\n",
        "          best_lr = lr\n",
        "          best_epoch = epoch\n",
        "          best_layer = layer\n",
        "\n",
        "print('Best mean validation accuracy (', best_mean_acc, ') is achieved with learning rate=', best_lr,\" and epoaches=\", best_epoch, \"and for layer number\", best_layer)\n",
        "\n",
        "# Initialize the MLPClassifier\n",
        "best_mlp = MLPClassifier(hidden_layer_sizes=(best_layer,), max_iter=best_epoch, alpha=1e-4,\n",
        "                    solver='sgd', verbose=10, random_state=42,\n",
        "                    learning_rate_init=best_lr)\n",
        "\n",
        "# Train the classifier with the training data\n",
        "best_mlp.fit(X_train, y_train)\n",
        "\n",
        "# Predict the responses for the test dataset\n",
        "y_pred = best_mlp.predict(X_test_full)\n",
        "\n",
        "# Print the accuracy score of the MLP classifier\n",
        "print('Test accuracy: ', accuracy_score(y_test_full, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test_full, y_pred)\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(f'Confusion Matrix for Multilayer Perceptron')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oU5bZ-trqXJ",
        "outputId": "f78a44f6-d5e7-472d-c660-102d7b271700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.61616344\n",
            "Iteration 2, loss = 0.61047446\n",
            "Iteration 3, loss = 0.60478519\n",
            "Iteration 4, loss = 0.60040729\n",
            "Iteration 5, loss = 0.59683886\n",
            "Iteration 6, loss = 0.59352251\n",
            "Iteration 7, loss = 0.59032056\n",
            "Iteration 8, loss = 0.58709569\n",
            "Iteration 9, loss = 0.58414076\n",
            "Iteration 10, loss = 0.58165635\n",
            "Iteration 11, loss = 0.57911060\n",
            "Iteration 12, loss = 0.57790214\n",
            "Iteration 13, loss = 0.57579006\n",
            "Iteration 14, loss = 0.57293731\n",
            "Iteration 15, loss = 0.56984478\n",
            "Iteration 16, loss = 0.56633085\n",
            "Iteration 17, loss = 0.56403178\n",
            "Iteration 18, loss = 0.56081149\n",
            "Iteration 19, loss = 0.55656498\n",
            "Iteration 20, loss = 0.55483738\n",
            "Iteration 21, loss = 0.55165056\n",
            "Iteration 22, loss = 0.55122710\n",
            "Iteration 23, loss = 0.54983455\n",
            "Iteration 24, loss = 0.54824201\n",
            "Iteration 25, loss = 0.54789085\n",
            "Iteration 26, loss = 0.54627285\n",
            "Iteration 27, loss = 0.54523166\n",
            "Iteration 28, loss = 0.54485553\n",
            "Iteration 29, loss = 0.54476942\n",
            "Iteration 30, loss = 0.54341941\n",
            "Iteration 31, loss = 0.54275300\n",
            "Iteration 32, loss = 0.54166800\n",
            "Iteration 33, loss = 0.54111902\n",
            "Iteration 34, loss = 0.54465660\n",
            "Iteration 35, loss = 0.54087738\n",
            "Iteration 36, loss = 0.53954129\n",
            "Iteration 37, loss = 0.54069694\n",
            "Iteration 38, loss = 0.53983298\n",
            "Iteration 39, loss = 0.53907627\n",
            "Iteration 40, loss = 0.53860343\n",
            "Iteration 41, loss = 0.53932428\n",
            "Iteration 42, loss = 0.53915650\n",
            "Iteration 43, loss = 0.53861646\n",
            "Iteration 44, loss = 0.53889930\n",
            "Iteration 45, loss = 0.53773883\n",
            "Iteration 46, loss = 0.53837864\n",
            "Iteration 47, loss = 0.53835545\n",
            "Iteration 48, loss = 0.53696691\n",
            "Iteration 49, loss = 0.53629861\n",
            "Iteration 50, loss = 0.53652009\n",
            "Iteration 51, loss = 0.53844088\n",
            "Iteration 52, loss = 0.53710225\n",
            "Iteration 53, loss = 0.53678021\n",
            "Iteration 54, loss = 0.53589026\n",
            "Iteration 55, loss = 0.53500914\n",
            "Iteration 56, loss = 0.53512312\n",
            "Iteration 57, loss = 0.53536117\n",
            "Iteration 58, loss = 0.53590119\n",
            "Iteration 59, loss = 0.53510110\n",
            "Iteration 60, loss = 0.53468041\n",
            "Iteration 61, loss = 0.53525025\n",
            "Iteration 62, loss = 0.53545549\n",
            "Iteration 63, loss = 0.53428015\n",
            "Iteration 64, loss = 0.53534948\n",
            "Iteration 65, loss = 0.53517115\n",
            "Iteration 66, loss = 0.53455759\n",
            "Iteration 67, loss = 0.53467990\n",
            "Iteration 68, loss = 0.53436042\n",
            "Iteration 69, loss = 0.53448972\n",
            "Iteration 70, loss = 0.53444176\n",
            "Iteration 71, loss = 0.53474912\n",
            "Iteration 72, loss = 0.53471431\n",
            "Iteration 73, loss = 0.53404600\n",
            "Iteration 74, loss = 0.53413797\n",
            "Iteration 75, loss = 0.53474503\n",
            "Iteration 76, loss = 0.53460398\n",
            "Iteration 77, loss = 0.53496211\n",
            "Iteration 78, loss = 0.53504312\n",
            "Iteration 79, loss = 0.53361651\n",
            "Iteration 80, loss = 0.53294444\n",
            "Iteration 81, loss = 0.53551544\n",
            "Iteration 82, loss = 0.53583933\n",
            "Iteration 83, loss = 0.53495348\n",
            "Iteration 84, loss = 0.53577048\n",
            "Iteration 85, loss = 0.53327554\n",
            "Iteration 86, loss = 0.53315024\n",
            "Iteration 87, loss = 0.53348090\n",
            "Iteration 88, loss = 0.53214641\n",
            "Iteration 89, loss = 0.53452553\n",
            "Iteration 90, loss = 0.53339749\n",
            "Iteration 91, loss = 0.53408545\n",
            "Iteration 92, loss = 0.53402317\n",
            "Iteration 93, loss = 0.53249531\n",
            "Iteration 94, loss = 0.53232751\n",
            "Iteration 95, loss = 0.53441131\n",
            "Iteration 96, loss = 0.53359358\n",
            "Iteration 97, loss = 0.53233717\n",
            "Iteration 98, loss = 0.53477685\n",
            "Iteration 99, loss = 0.53576123\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61685270\n",
            "Iteration 2, loss = 0.61073358\n",
            "Iteration 3, loss = 0.60482747\n",
            "Iteration 4, loss = 0.60021840\n",
            "Iteration 5, loss = 0.59678875\n",
            "Iteration 6, loss = 0.59294638\n",
            "Iteration 7, loss = 0.58982852\n",
            "Iteration 8, loss = 0.58727008\n",
            "Iteration 9, loss = 0.58561090\n",
            "Iteration 10, loss = 0.58412723\n",
            "Iteration 11, loss = 0.58439637\n",
            "Iteration 12, loss = 0.58235275\n",
            "Iteration 13, loss = 0.58096798\n",
            "Iteration 14, loss = 0.57891433\n",
            "Iteration 15, loss = 0.57650903\n",
            "Iteration 16, loss = 0.57419914\n",
            "Iteration 17, loss = 0.57181285\n",
            "Iteration 18, loss = 0.56990192\n",
            "Iteration 19, loss = 0.56742489\n",
            "Iteration 20, loss = 0.56484876\n",
            "Iteration 21, loss = 0.56171823\n",
            "Iteration 22, loss = 0.55955647\n",
            "Iteration 23, loss = 0.55886560\n",
            "Iteration 24, loss = 0.55706339\n",
            "Iteration 25, loss = 0.55553422\n",
            "Iteration 26, loss = 0.55262330\n",
            "Iteration 27, loss = 0.55047921\n",
            "Iteration 28, loss = 0.54972788\n",
            "Iteration 29, loss = 0.54861285\n",
            "Iteration 30, loss = 0.54843663\n",
            "Iteration 31, loss = 0.54698559\n",
            "Iteration 32, loss = 0.54512202\n",
            "Iteration 33, loss = 0.54496236\n",
            "Iteration 34, loss = 0.54537399\n",
            "Iteration 35, loss = 0.54621125\n",
            "Iteration 36, loss = 0.54307091\n",
            "Iteration 37, loss = 0.54376705\n",
            "Iteration 38, loss = 0.54240030\n",
            "Iteration 39, loss = 0.54055163\n",
            "Iteration 40, loss = 0.53948219\n",
            "Iteration 41, loss = 0.53937518\n",
            "Iteration 42, loss = 0.53894503\n",
            "Iteration 43, loss = 0.53814838\n",
            "Iteration 44, loss = 0.53717987\n",
            "Iteration 45, loss = 0.53577462\n",
            "Iteration 46, loss = 0.53457548\n",
            "Iteration 47, loss = 0.53370548\n",
            "Iteration 48, loss = 0.53374536\n",
            "Iteration 49, loss = 0.53227638\n",
            "Iteration 50, loss = 0.53107787\n",
            "Iteration 51, loss = 0.53314206\n",
            "Iteration 52, loss = 0.53196477\n",
            "Iteration 53, loss = 0.53088992\n",
            "Iteration 54, loss = 0.53029162\n",
            "Iteration 55, loss = 0.52988151\n",
            "Iteration 56, loss = 0.52888568\n",
            "Iteration 57, loss = 0.52890005\n",
            "Iteration 58, loss = 0.52924797\n",
            "Iteration 59, loss = 0.52921454\n",
            "Iteration 60, loss = 0.52868950\n",
            "Iteration 61, loss = 0.52862123\n",
            "Iteration 62, loss = 0.52993570\n",
            "Iteration 63, loss = 0.52877129\n",
            "Iteration 64, loss = 0.52946871\n",
            "Iteration 65, loss = 0.52981092\n",
            "Iteration 66, loss = 0.53015241\n",
            "Iteration 67, loss = 0.52990268\n",
            "Iteration 68, loss = 0.52903022\n",
            "Iteration 69, loss = 0.52935976\n",
            "Iteration 70, loss = 0.52849976\n",
            "Iteration 71, loss = 0.52799905\n",
            "Iteration 72, loss = 0.52879280\n",
            "Iteration 73, loss = 0.52821299\n",
            "Iteration 74, loss = 0.52730704\n",
            "Iteration 75, loss = 0.52866449\n",
            "Iteration 76, loss = 0.52869782\n",
            "Iteration 77, loss = 0.52738765\n",
            "Iteration 78, loss = 0.52743448\n",
            "Iteration 79, loss = 0.52749131\n",
            "Iteration 80, loss = 0.52713528\n",
            "Iteration 81, loss = 0.52811449\n",
            "Iteration 82, loss = 0.52784675\n",
            "Iteration 83, loss = 0.52592258\n",
            "Iteration 84, loss = 0.52574880\n",
            "Iteration 85, loss = 0.52562074\n",
            "Iteration 86, loss = 0.52562702\n",
            "Iteration 87, loss = 0.52489044\n",
            "Iteration 88, loss = 0.52673234\n",
            "Iteration 89, loss = 0.52780336\n",
            "Iteration 90, loss = 0.52606327\n",
            "Iteration 91, loss = 0.52617697\n",
            "Iteration 92, loss = 0.52557126\n",
            "Iteration 93, loss = 0.52481340\n",
            "Iteration 94, loss = 0.52480812\n",
            "Iteration 95, loss = 0.52540942\n",
            "Iteration 96, loss = 0.52637237\n",
            "Iteration 97, loss = 0.52433287\n",
            "Iteration 98, loss = 0.52539716\n",
            "Iteration 99, loss = 0.52543487\n",
            "Iteration 100, loss = 0.52402267\n",
            "Iteration 1, loss = 0.61686379\n",
            "Iteration 2, loss = 0.61078213\n",
            "Iteration 3, loss = 0.60373906\n",
            "Iteration 4, loss = 0.59835244\n",
            "Iteration 5, loss = 0.59457664\n",
            "Iteration 6, loss = 0.59038982\n",
            "Iteration 7, loss = 0.58694864\n",
            "Iteration 8, loss = 0.58122019\n",
            "Iteration 9, loss = 0.57559384\n",
            "Iteration 10, loss = 0.57111279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 11, loss = 0.56767019\n",
            "Iteration 12, loss = 0.56484322\n",
            "Iteration 13, loss = 0.55949772\n",
            "Iteration 14, loss = 0.55649967\n",
            "Iteration 15, loss = 0.55290617\n",
            "Iteration 16, loss = 0.54970478\n",
            "Iteration 17, loss = 0.54607332\n",
            "Iteration 18, loss = 0.54403178\n",
            "Iteration 19, loss = 0.54132068\n",
            "Iteration 20, loss = 0.53854210\n",
            "Iteration 21, loss = 0.53710357\n",
            "Iteration 22, loss = 0.53429746\n",
            "Iteration 23, loss = 0.53375008\n",
            "Iteration 24, loss = 0.53094852\n",
            "Iteration 25, loss = 0.52965028\n",
            "Iteration 26, loss = 0.52815839\n",
            "Iteration 27, loss = 0.52688496\n",
            "Iteration 28, loss = 0.52553353\n",
            "Iteration 29, loss = 0.52394223\n",
            "Iteration 30, loss = 0.52356500\n",
            "Iteration 31, loss = 0.52294680\n",
            "Iteration 32, loss = 0.52183150\n",
            "Iteration 33, loss = 0.52255860\n",
            "Iteration 34, loss = 0.52142071\n",
            "Iteration 35, loss = 0.52125496\n",
            "Iteration 36, loss = 0.52111537\n",
            "Iteration 37, loss = 0.52026383\n",
            "Iteration 38, loss = 0.52045544\n",
            "Iteration 39, loss = 0.52091092\n",
            "Iteration 40, loss = 0.52008196\n",
            "Iteration 41, loss = 0.51990365\n",
            "Iteration 42, loss = 0.51974732\n",
            "Iteration 43, loss = 0.51904939\n",
            "Iteration 44, loss = 0.51941507\n",
            "Iteration 45, loss = 0.51917314\n",
            "Iteration 46, loss = 0.51924421\n",
            "Iteration 47, loss = 0.51928018\n",
            "Iteration 48, loss = 0.51909289\n",
            "Iteration 49, loss = 0.51842013\n",
            "Iteration 50, loss = 0.51802283\n",
            "Iteration 51, loss = 0.51819928\n",
            "Iteration 52, loss = 0.51951413\n",
            "Iteration 53, loss = 0.51949918\n",
            "Iteration 54, loss = 0.52004138\n",
            "Iteration 55, loss = 0.51851564\n",
            "Iteration 56, loss = 0.51716944\n",
            "Iteration 57, loss = 0.51662275\n",
            "Iteration 58, loss = 0.51660941\n",
            "Iteration 59, loss = 0.51655928\n",
            "Iteration 60, loss = 0.51630371\n",
            "Iteration 61, loss = 0.51683368\n",
            "Iteration 62, loss = 0.51674972\n",
            "Iteration 63, loss = 0.51545783\n",
            "Iteration 64, loss = 0.51608693\n",
            "Iteration 65, loss = 0.51590192\n",
            "Iteration 66, loss = 0.51553887\n",
            "Iteration 67, loss = 0.51569551\n",
            "Iteration 68, loss = 0.51562204\n",
            "Iteration 69, loss = 0.51538478\n",
            "Iteration 70, loss = 0.51492148\n",
            "Iteration 71, loss = 0.51418214\n",
            "Iteration 72, loss = 0.51644749\n",
            "Iteration 73, loss = 0.51636934\n",
            "Iteration 74, loss = 0.51514015\n",
            "Iteration 75, loss = 0.51694397\n",
            "Iteration 76, loss = 0.51490108\n",
            "Iteration 77, loss = 0.51324531\n",
            "Iteration 78, loss = 0.51423000\n",
            "Iteration 79, loss = 0.51396088\n",
            "Iteration 80, loss = 0.51433935\n",
            "Iteration 81, loss = 0.51317206\n",
            "Iteration 82, loss = 0.51380235\n",
            "Iteration 83, loss = 0.51273531\n",
            "Iteration 84, loss = 0.51314830\n",
            "Iteration 85, loss = 0.51374298\n",
            "Iteration 86, loss = 0.51415859\n",
            "Iteration 87, loss = 0.51239317\n",
            "Iteration 88, loss = 0.51355738\n",
            "Iteration 89, loss = 0.51420369\n",
            "Iteration 90, loss = 0.51490013\n",
            "Iteration 91, loss = 0.51391767\n",
            "Iteration 92, loss = 0.51283594\n",
            "Iteration 93, loss = 0.51340462\n",
            "Iteration 94, loss = 0.51341917\n",
            "Iteration 95, loss = 0.51351797\n",
            "Iteration 96, loss = 0.51398162\n",
            "Iteration 97, loss = 0.51293432\n",
            "Iteration 98, loss = 0.51275042\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61997020\n",
            "Iteration 2, loss = 0.61412378\n",
            "Iteration 3, loss = 0.60919162\n",
            "Iteration 4, loss = 0.60490434\n",
            "Iteration 5, loss = 0.60251389\n",
            "Iteration 6, loss = 0.59997429\n",
            "Iteration 7, loss = 0.59738037\n",
            "Iteration 8, loss = 0.59269736\n",
            "Iteration 9, loss = 0.58767828\n",
            "Iteration 10, loss = 0.58446028\n",
            "Iteration 11, loss = 0.58104889\n",
            "Iteration 12, loss = 0.57867536\n",
            "Iteration 13, loss = 0.57465469\n",
            "Iteration 14, loss = 0.57118039\n",
            "Iteration 15, loss = 0.56831045\n",
            "Iteration 16, loss = 0.56705962\n",
            "Iteration 17, loss = 0.56455372\n",
            "Iteration 18, loss = 0.56249116\n",
            "Iteration 19, loss = 0.56029054\n",
            "Iteration 20, loss = 0.55870059\n",
            "Iteration 21, loss = 0.55790136\n",
            "Iteration 22, loss = 0.55542813\n",
            "Iteration 23, loss = 0.55448658\n",
            "Iteration 24, loss = 0.55341077\n",
            "Iteration 25, loss = 0.55234914\n",
            "Iteration 26, loss = 0.55099291\n",
            "Iteration 27, loss = 0.54992709\n",
            "Iteration 28, loss = 0.54865229\n",
            "Iteration 29, loss = 0.54789818\n",
            "Iteration 30, loss = 0.54655713\n",
            "Iteration 31, loss = 0.54464837\n",
            "Iteration 32, loss = 0.54318115\n",
            "Iteration 33, loss = 0.54176155\n",
            "Iteration 34, loss = 0.53991255\n",
            "Iteration 35, loss = 0.53939289\n",
            "Iteration 36, loss = 0.53903124\n",
            "Iteration 37, loss = 0.53655695\n",
            "Iteration 38, loss = 0.53469995\n",
            "Iteration 39, loss = 0.53484729\n",
            "Iteration 40, loss = 0.53341719\n",
            "Iteration 41, loss = 0.53159598\n",
            "Iteration 42, loss = 0.53132522\n",
            "Iteration 43, loss = 0.53072165\n",
            "Iteration 44, loss = 0.53090682\n",
            "Iteration 45, loss = 0.53074961\n",
            "Iteration 46, loss = 0.53065336\n",
            "Iteration 47, loss = 0.53033300\n",
            "Iteration 48, loss = 0.52958211\n",
            "Iteration 49, loss = 0.52900039\n",
            "Iteration 50, loss = 0.52795815\n",
            "Iteration 51, loss = 0.52848891\n",
            "Iteration 52, loss = 0.53032016\n",
            "Iteration 53, loss = 0.52827347\n",
            "Iteration 54, loss = 0.52872219\n",
            "Iteration 55, loss = 0.52744500\n",
            "Iteration 56, loss = 0.52682941\n",
            "Iteration 57, loss = 0.52662720\n",
            "Iteration 58, loss = 0.52671375\n",
            "Iteration 59, loss = 0.52668943\n",
            "Iteration 60, loss = 0.52675084\n",
            "Iteration 61, loss = 0.52698153\n",
            "Iteration 62, loss = 0.52731884\n",
            "Iteration 63, loss = 0.52657748\n",
            "Iteration 64, loss = 0.52577505\n",
            "Iteration 65, loss = 0.52717356\n",
            "Iteration 66, loss = 0.52654840\n",
            "Iteration 67, loss = 0.52671599\n",
            "Iteration 68, loss = 0.52702098\n",
            "Iteration 69, loss = 0.52778156\n",
            "Iteration 70, loss = 0.52659366\n",
            "Iteration 71, loss = 0.52593393\n",
            "Iteration 72, loss = 0.52768568\n",
            "Iteration 73, loss = 0.52725121\n",
            "Iteration 74, loss = 0.52653828\n",
            "Iteration 75, loss = 0.52815880\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61941224\n",
            "Iteration 2, loss = 0.61318364\n",
            "Iteration 3, loss = 0.60864231\n",
            "Iteration 4, loss = 0.60454530\n",
            "Iteration 5, loss = 0.60243915\n",
            "Iteration 6, loss = 0.60246689\n",
            "Iteration 7, loss = 0.60083027\n",
            "Iteration 8, loss = 0.59640653\n",
            "Iteration 9, loss = 0.59331881\n",
            "Iteration 10, loss = 0.59175851\n",
            "Iteration 11, loss = 0.58960825\n",
            "Iteration 12, loss = 0.58941496\n",
            "Iteration 13, loss = 0.58594276\n",
            "Iteration 14, loss = 0.58312039\n",
            "Iteration 15, loss = 0.58096844\n",
            "Iteration 16, loss = 0.57971808\n",
            "Iteration 17, loss = 0.58006510\n",
            "Iteration 18, loss = 0.57982617\n",
            "Iteration 19, loss = 0.57840057\n",
            "Iteration 20, loss = 0.57747882\n",
            "Iteration 21, loss = 0.57701133\n",
            "Iteration 22, loss = 0.57534329\n",
            "Iteration 23, loss = 0.57568807\n",
            "Iteration 24, loss = 0.57479372\n",
            "Iteration 25, loss = 0.57414903\n",
            "Iteration 26, loss = 0.57320683\n",
            "Iteration 27, loss = 0.57351214\n",
            "Iteration 28, loss = 0.57212556\n",
            "Iteration 29, loss = 0.57182505\n",
            "Iteration 30, loss = 0.57175012\n",
            "Iteration 31, loss = 0.57021913\n",
            "Iteration 32, loss = 0.56935480\n",
            "Iteration 33, loss = 0.56839796\n",
            "Iteration 34, loss = 0.56693242\n",
            "Iteration 35, loss = 0.56572092\n",
            "Iteration 36, loss = 0.56395992\n",
            "Iteration 37, loss = 0.56338466\n",
            "Iteration 38, loss = 0.56170087\n",
            "Iteration 39, loss = 0.55877045\n",
            "Iteration 40, loss = 0.55793019\n",
            "Iteration 41, loss = 0.55614417\n",
            "Iteration 42, loss = 0.55506357\n",
            "Iteration 43, loss = 0.55343058\n",
            "Iteration 44, loss = 0.55340796\n",
            "Iteration 45, loss = 0.55087697\n",
            "Iteration 46, loss = 0.54966880\n",
            "Iteration 47, loss = 0.54774331\n",
            "Iteration 48, loss = 0.54593504\n",
            "Iteration 49, loss = 0.54453803\n",
            "Iteration 50, loss = 0.54326147\n",
            "Iteration 51, loss = 0.54251671\n",
            "Iteration 52, loss = 0.54193067\n",
            "Iteration 53, loss = 0.54062192\n",
            "Iteration 54, loss = 0.54211250\n",
            "Iteration 55, loss = 0.54021127\n",
            "Iteration 56, loss = 0.53903735\n",
            "Iteration 57, loss = 0.53750337\n",
            "Iteration 58, loss = 0.53657233\n",
            "Iteration 59, loss = 0.53494783\n",
            "Iteration 60, loss = 0.53609512\n",
            "Iteration 61, loss = 0.53605135\n",
            "Iteration 62, loss = 0.53444912\n",
            "Iteration 63, loss = 0.53426413\n",
            "Iteration 64, loss = 0.53256619\n",
            "Iteration 65, loss = 0.53272366\n",
            "Iteration 66, loss = 0.53213028\n",
            "Iteration 67, loss = 0.53294015\n",
            "Iteration 68, loss = 0.53405626\n",
            "Iteration 69, loss = 0.53502634\n",
            "Iteration 70, loss = 0.53220216\n",
            "Iteration 71, loss = 0.53183938\n",
            "Iteration 72, loss = 0.53266801\n",
            "Iteration 73, loss = 0.53181568\n",
            "Iteration 74, loss = 0.53103228\n",
            "Iteration 75, loss = 0.53109650\n",
            "Iteration 76, loss = 0.53120562\n",
            "Iteration 77, loss = 0.53139712\n",
            "Iteration 78, loss = 0.53105354\n",
            "Iteration 79, loss = 0.53115344\n",
            "Iteration 80, loss = 0.53351865\n",
            "Iteration 81, loss = 0.53029266\n",
            "Iteration 82, loss = 0.53141297\n",
            "Iteration 83, loss = 0.53030700\n",
            "Iteration 84, loss = 0.53077603\n",
            "Iteration 85, loss = 0.53117501\n",
            "Iteration 86, loss = 0.53017275\n",
            "Iteration 87, loss = 0.53167091\n",
            "Iteration 88, loss = 0.53110833\n",
            "Iteration 89, loss = 0.53082480\n",
            "Iteration 90, loss = 0.53097012\n",
            "Iteration 91, loss = 0.52953903\n",
            "Iteration 92, loss = 0.53041845\n",
            "Iteration 93, loss = 0.53062090\n",
            "Iteration 94, loss = 0.53060511\n",
            "Iteration 95, loss = 0.52943256\n",
            "Iteration 96, loss = 0.52966999\n",
            "Iteration 97, loss = 0.53005740\n",
            "Iteration 98, loss = 0.53237745\n",
            "Iteration 99, loss = 0.53421484\n",
            "Iteration 100, loss = 0.53217815\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 100 and for layer number 2 : 0.7125\n",
            "Iteration 1, loss = 0.81149998\n",
            "Iteration 2, loss = 0.65873604\n",
            "Iteration 3, loss = 0.59987267\n",
            "Iteration 4, loss = 0.58584785\n",
            "Iteration 5, loss = 0.57524774\n",
            "Iteration 6, loss = 0.57117799\n",
            "Iteration 7, loss = 0.56778662\n",
            "Iteration 8, loss = 0.56432353\n",
            "Iteration 9, loss = 0.56078602\n",
            "Iteration 10, loss = 0.55775868\n",
            "Iteration 11, loss = 0.55650784\n",
            "Iteration 12, loss = 0.55523370\n",
            "Iteration 13, loss = 0.55379450\n",
            "Iteration 14, loss = 0.55218434\n",
            "Iteration 15, loss = 0.54947622\n",
            "Iteration 16, loss = 0.54665060\n",
            "Iteration 17, loss = 0.54742043\n",
            "Iteration 18, loss = 0.54599748\n",
            "Iteration 19, loss = 0.54336523\n",
            "Iteration 20, loss = 0.54192285\n",
            "Iteration 21, loss = 0.54095123\n",
            "Iteration 22, loss = 0.53904375\n",
            "Iteration 23, loss = 0.53750011\n",
            "Iteration 24, loss = 0.53633557\n",
            "Iteration 25, loss = 0.53686758\n",
            "Iteration 26, loss = 0.53616775\n",
            "Iteration 27, loss = 0.53438193\n",
            "Iteration 28, loss = 0.53371599\n",
            "Iteration 29, loss = 0.53315282\n",
            "Iteration 30, loss = 0.53305221\n",
            "Iteration 31, loss = 0.53267854\n",
            "Iteration 32, loss = 0.53127495\n",
            "Iteration 33, loss = 0.53042640\n",
            "Iteration 34, loss = 0.52970462\n",
            "Iteration 35, loss = 0.52915225\n",
            "Iteration 36, loss = 0.52950701\n",
            "Iteration 37, loss = 0.53004409\n",
            "Iteration 38, loss = 0.52794971\n",
            "Iteration 39, loss = 0.52762295\n",
            "Iteration 40, loss = 0.52731455\n",
            "Iteration 41, loss = 0.52624037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 42, loss = 0.52665690\n",
            "Iteration 43, loss = 0.52605307\n",
            "Iteration 44, loss = 0.52585806\n",
            "Iteration 45, loss = 0.52614887\n",
            "Iteration 46, loss = 0.52391730\n",
            "Iteration 47, loss = 0.52342006\n",
            "Iteration 48, loss = 0.52281511\n",
            "Iteration 49, loss = 0.52151818\n",
            "Iteration 50, loss = 0.52013343\n",
            "Iteration 51, loss = 0.52039141\n",
            "Iteration 52, loss = 0.52155114\n",
            "Iteration 53, loss = 0.51973222\n",
            "Iteration 54, loss = 0.51937204\n",
            "Iteration 55, loss = 0.52073481\n",
            "Iteration 56, loss = 0.52026603\n",
            "Iteration 57, loss = 0.51854102\n",
            "Iteration 58, loss = 0.51726925\n",
            "Iteration 59, loss = 0.51684776\n",
            "Iteration 60, loss = 0.51709917\n",
            "Iteration 61, loss = 0.51740898\n",
            "Iteration 62, loss = 0.51849608\n",
            "Iteration 63, loss = 0.51816904\n",
            "Iteration 64, loss = 0.51797494\n",
            "Iteration 65, loss = 0.51884720\n",
            "Iteration 66, loss = 0.51748484\n",
            "Iteration 67, loss = 0.51679103\n",
            "Iteration 68, loss = 0.51567568\n",
            "Iteration 69, loss = 0.51513003\n",
            "Iteration 70, loss = 0.51663152\n",
            "Iteration 71, loss = 0.51592509\n",
            "Iteration 72, loss = 0.51354275\n",
            "Iteration 73, loss = 0.51737624\n",
            "Iteration 74, loss = 0.51699973\n",
            "Iteration 75, loss = 0.51609717\n",
            "Iteration 76, loss = 0.51411590\n",
            "Iteration 77, loss = 0.51467742\n",
            "Iteration 78, loss = 0.51476699\n",
            "Iteration 79, loss = 0.51310993\n",
            "Iteration 80, loss = 0.51284290\n",
            "Iteration 81, loss = 0.51462620\n",
            "Iteration 82, loss = 0.51531153\n",
            "Iteration 83, loss = 0.51651873\n",
            "Iteration 84, loss = 0.51583456\n",
            "Iteration 85, loss = 0.51574633\n",
            "Iteration 86, loss = 0.51492935\n",
            "Iteration 87, loss = 0.51491277\n",
            "Iteration 88, loss = 0.51469364\n",
            "Iteration 89, loss = 0.51383952\n",
            "Iteration 90, loss = 0.51359792\n",
            "Iteration 91, loss = 0.51379622\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81331306\n",
            "Iteration 2, loss = 0.66033007\n",
            "Iteration 3, loss = 0.60394338\n",
            "Iteration 4, loss = 0.58347678\n",
            "Iteration 5, loss = 0.57325470\n",
            "Iteration 6, loss = 0.56846700\n",
            "Iteration 7, loss = 0.56686121\n",
            "Iteration 8, loss = 0.56389750\n",
            "Iteration 9, loss = 0.55854005\n",
            "Iteration 10, loss = 0.55686836\n",
            "Iteration 11, loss = 0.55570604\n",
            "Iteration 12, loss = 0.55406456\n",
            "Iteration 13, loss = 0.55150024\n",
            "Iteration 14, loss = 0.54875073\n",
            "Iteration 15, loss = 0.54638142\n",
            "Iteration 16, loss = 0.54388926\n",
            "Iteration 17, loss = 0.54225501\n",
            "Iteration 18, loss = 0.54000220\n",
            "Iteration 19, loss = 0.53809810\n",
            "Iteration 20, loss = 0.53710679\n",
            "Iteration 21, loss = 0.53602823\n",
            "Iteration 22, loss = 0.53495535\n",
            "Iteration 23, loss = 0.53323611\n",
            "Iteration 24, loss = 0.53158803\n",
            "Iteration 25, loss = 0.53142116\n",
            "Iteration 26, loss = 0.53039578\n",
            "Iteration 27, loss = 0.52896039\n",
            "Iteration 28, loss = 0.52812962\n",
            "Iteration 29, loss = 0.52760837\n",
            "Iteration 30, loss = 0.52750333\n",
            "Iteration 31, loss = 0.52844395\n",
            "Iteration 32, loss = 0.52805030\n",
            "Iteration 33, loss = 0.52654364\n",
            "Iteration 34, loss = 0.52559491\n",
            "Iteration 35, loss = 0.52472209\n",
            "Iteration 36, loss = 0.52462304\n",
            "Iteration 37, loss = 0.52484838\n",
            "Iteration 38, loss = 0.52243952\n",
            "Iteration 39, loss = 0.52135493\n",
            "Iteration 40, loss = 0.52089779\n",
            "Iteration 41, loss = 0.52040099\n",
            "Iteration 42, loss = 0.52008310\n",
            "Iteration 43, loss = 0.51987458\n",
            "Iteration 44, loss = 0.51911580\n",
            "Iteration 45, loss = 0.51961977\n",
            "Iteration 46, loss = 0.51973649\n",
            "Iteration 47, loss = 0.51772961\n",
            "Iteration 48, loss = 0.51829609\n",
            "Iteration 49, loss = 0.51755155\n",
            "Iteration 50, loss = 0.51591976\n",
            "Iteration 51, loss = 0.51644725\n",
            "Iteration 52, loss = 0.51663831\n",
            "Iteration 53, loss = 0.51686230\n",
            "Iteration 54, loss = 0.51725454\n",
            "Iteration 55, loss = 0.51699703\n",
            "Iteration 56, loss = 0.51687145\n",
            "Iteration 57, loss = 0.51589051\n",
            "Iteration 58, loss = 0.51582670\n",
            "Iteration 59, loss = 0.51564801\n",
            "Iteration 60, loss = 0.51650497\n",
            "Iteration 61, loss = 0.51722020\n",
            "Iteration 62, loss = 0.51686858\n",
            "Iteration 63, loss = 0.51537984\n",
            "Iteration 64, loss = 0.51598666\n",
            "Iteration 65, loss = 0.51758055\n",
            "Iteration 66, loss = 0.51641849\n",
            "Iteration 67, loss = 0.51479609\n",
            "Iteration 68, loss = 0.51482381\n",
            "Iteration 69, loss = 0.51634873\n",
            "Iteration 70, loss = 0.51664928\n",
            "Iteration 71, loss = 0.51580978\n",
            "Iteration 72, loss = 0.51464183\n",
            "Iteration 73, loss = 0.51606701\n",
            "Iteration 74, loss = 0.51718934\n",
            "Iteration 75, loss = 0.51586765\n",
            "Iteration 76, loss = 0.51353713\n",
            "Iteration 77, loss = 0.51510832\n",
            "Iteration 78, loss = 0.51452434\n",
            "Iteration 79, loss = 0.51383952\n",
            "Iteration 80, loss = 0.51460585\n",
            "Iteration 81, loss = 0.51428523\n",
            "Iteration 82, loss = 0.51447232\n",
            "Iteration 83, loss = 0.51480264\n",
            "Iteration 84, loss = 0.51529507\n",
            "Iteration 85, loss = 0.51354914\n",
            "Iteration 86, loss = 0.51193067\n",
            "Iteration 87, loss = 0.51160359\n",
            "Iteration 88, loss = 0.51109447\n",
            "Iteration 89, loss = 0.51041741\n",
            "Iteration 90, loss = 0.51053244\n",
            "Iteration 91, loss = 0.51303529\n",
            "Iteration 92, loss = 0.51296644\n",
            "Iteration 93, loss = 0.50982758\n",
            "Iteration 94, loss = 0.51173122\n",
            "Iteration 95, loss = 0.51226543\n",
            "Iteration 96, loss = 0.51108616\n",
            "Iteration 97, loss = 0.51200241\n",
            "Iteration 98, loss = 0.51100175\n",
            "Iteration 99, loss = 0.51246029\n",
            "Iteration 100, loss = 0.51000404\n",
            "Iteration 1, loss = 0.82932651\n",
            "Iteration 2, loss = 0.66164239\n",
            "Iteration 3, loss = 0.60009028\n",
            "Iteration 4, loss = 0.57580262\n",
            "Iteration 5, loss = 0.56470176\n",
            "Iteration 6, loss = 0.55748571\n",
            "Iteration 7, loss = 0.55627910\n",
            "Iteration 8, loss = 0.55132255\n",
            "Iteration 9, loss = 0.54679100\n",
            "Iteration 10, loss = 0.54440088\n",
            "Iteration 11, loss = 0.54314934\n",
            "Iteration 12, loss = 0.54115322\n",
            "Iteration 13, loss = 0.53896820\n",
            "Iteration 14, loss = 0.53738517\n",
            "Iteration 15, loss = 0.53610209\n",
            "Iteration 16, loss = 0.53548329\n",
            "Iteration 17, loss = 0.53479088\n",
            "Iteration 18, loss = 0.53420721\n",
            "Iteration 19, loss = 0.53365687\n",
            "Iteration 20, loss = 0.53360199\n",
            "Iteration 21, loss = 0.53262289\n",
            "Iteration 22, loss = 0.53272172\n",
            "Iteration 23, loss = 0.53160057\n",
            "Iteration 24, loss = 0.53020057\n",
            "Iteration 25, loss = 0.53032962\n",
            "Iteration 26, loss = 0.52946469\n",
            "Iteration 27, loss = 0.52945571\n",
            "Iteration 28, loss = 0.52875327\n",
            "Iteration 29, loss = 0.52877389\n",
            "Iteration 30, loss = 0.52682750\n",
            "Iteration 31, loss = 0.52605181\n",
            "Iteration 32, loss = 0.52571340\n",
            "Iteration 33, loss = 0.52546378\n",
            "Iteration 34, loss = 0.52562253\n",
            "Iteration 35, loss = 0.52363862\n",
            "Iteration 36, loss = 0.52212134\n",
            "Iteration 37, loss = 0.52251926\n",
            "Iteration 38, loss = 0.52218800\n",
            "Iteration 39, loss = 0.52165898\n",
            "Iteration 40, loss = 0.52026508\n",
            "Iteration 41, loss = 0.51934207\n",
            "Iteration 42, loss = 0.51863317\n",
            "Iteration 43, loss = 0.51868689\n",
            "Iteration 44, loss = 0.51721053\n",
            "Iteration 45, loss = 0.51841107\n",
            "Iteration 46, loss = 0.51880717\n",
            "Iteration 47, loss = 0.51778831\n",
            "Iteration 48, loss = 0.51753928\n",
            "Iteration 49, loss = 0.51562444\n",
            "Iteration 50, loss = 0.51508967\n",
            "Iteration 51, loss = 0.51559385\n",
            "Iteration 52, loss = 0.51571383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 53, loss = 0.51504503\n",
            "Iteration 54, loss = 0.51481022\n",
            "Iteration 55, loss = 0.51387321\n",
            "Iteration 56, loss = 0.51488602\n",
            "Iteration 57, loss = 0.51372919\n",
            "Iteration 58, loss = 0.51402763\n",
            "Iteration 59, loss = 0.51448928\n",
            "Iteration 60, loss = 0.51337426\n",
            "Iteration 61, loss = 0.51427479\n",
            "Iteration 62, loss = 0.51474731\n",
            "Iteration 63, loss = 0.51357626\n",
            "Iteration 64, loss = 0.51252725\n",
            "Iteration 65, loss = 0.51231619\n",
            "Iteration 66, loss = 0.51255882\n",
            "Iteration 67, loss = 0.51204635\n",
            "Iteration 68, loss = 0.51242474\n",
            "Iteration 69, loss = 0.51219609\n",
            "Iteration 70, loss = 0.51366912\n",
            "Iteration 71, loss = 0.51423931\n",
            "Iteration 72, loss = 0.51245169\n",
            "Iteration 73, loss = 0.51357885\n",
            "Iteration 74, loss = 0.51272285\n",
            "Iteration 75, loss = 0.51279332\n",
            "Iteration 76, loss = 0.51178815\n",
            "Iteration 77, loss = 0.51023404\n",
            "Iteration 78, loss = 0.51053386\n",
            "Iteration 79, loss = 0.51100373\n",
            "Iteration 80, loss = 0.50993851\n",
            "Iteration 81, loss = 0.50974919\n",
            "Iteration 82, loss = 0.50959666\n",
            "Iteration 83, loss = 0.51106230\n",
            "Iteration 84, loss = 0.51063345\n",
            "Iteration 85, loss = 0.51076724\n",
            "Iteration 86, loss = 0.50930014\n",
            "Iteration 87, loss = 0.50845813\n",
            "Iteration 88, loss = 0.50816720\n",
            "Iteration 89, loss = 0.50865641\n",
            "Iteration 90, loss = 0.50926082\n",
            "Iteration 91, loss = 0.51053776\n",
            "Iteration 92, loss = 0.51101996\n",
            "Iteration 93, loss = 0.50967905\n",
            "Iteration 94, loss = 0.51493859\n",
            "Iteration 95, loss = 0.51080848\n",
            "Iteration 96, loss = 0.51116814\n",
            "Iteration 97, loss = 0.51019227\n",
            "Iteration 98, loss = 0.51022928\n",
            "Iteration 99, loss = 0.50991010\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.80972489\n",
            "Iteration 2, loss = 0.65904043\n",
            "Iteration 3, loss = 0.60553834\n",
            "Iteration 4, loss = 0.58737349\n",
            "Iteration 5, loss = 0.57959275\n",
            "Iteration 6, loss = 0.57310858\n",
            "Iteration 7, loss = 0.57025941\n",
            "Iteration 8, loss = 0.56762812\n",
            "Iteration 9, loss = 0.56369167\n",
            "Iteration 10, loss = 0.56030709\n",
            "Iteration 11, loss = 0.55715604\n",
            "Iteration 12, loss = 0.55424368\n",
            "Iteration 13, loss = 0.55086767\n",
            "Iteration 14, loss = 0.54968789\n",
            "Iteration 15, loss = 0.54804813\n",
            "Iteration 16, loss = 0.54716522\n",
            "Iteration 17, loss = 0.54589839\n",
            "Iteration 18, loss = 0.54464670\n",
            "Iteration 19, loss = 0.54270658\n",
            "Iteration 20, loss = 0.54195373\n",
            "Iteration 21, loss = 0.54050459\n",
            "Iteration 22, loss = 0.54002333\n",
            "Iteration 23, loss = 0.53829247\n",
            "Iteration 24, loss = 0.53714600\n",
            "Iteration 25, loss = 0.53752867\n",
            "Iteration 26, loss = 0.53664465\n",
            "Iteration 27, loss = 0.53524365\n",
            "Iteration 28, loss = 0.53398566\n",
            "Iteration 29, loss = 0.53388843\n",
            "Iteration 30, loss = 0.53308004\n",
            "Iteration 31, loss = 0.53183431\n",
            "Iteration 32, loss = 0.53173172\n",
            "Iteration 33, loss = 0.53088996\n",
            "Iteration 34, loss = 0.53055194\n",
            "Iteration 35, loss = 0.52922236\n",
            "Iteration 36, loss = 0.52768203\n",
            "Iteration 37, loss = 0.52816267\n",
            "Iteration 38, loss = 0.52732724\n",
            "Iteration 39, loss = 0.52567655\n",
            "Iteration 40, loss = 0.52483423\n",
            "Iteration 41, loss = 0.52393807\n",
            "Iteration 42, loss = 0.52282236\n",
            "Iteration 43, loss = 0.52273856\n",
            "Iteration 44, loss = 0.52322500\n",
            "Iteration 45, loss = 0.52272876\n",
            "Iteration 46, loss = 0.52240351\n",
            "Iteration 47, loss = 0.52226690\n",
            "Iteration 48, loss = 0.52143150\n",
            "Iteration 49, loss = 0.51896197\n",
            "Iteration 50, loss = 0.51832271\n",
            "Iteration 51, loss = 0.51789919\n",
            "Iteration 52, loss = 0.51599498\n",
            "Iteration 53, loss = 0.51552360\n",
            "Iteration 54, loss = 0.51513925\n",
            "Iteration 55, loss = 0.51481940\n",
            "Iteration 56, loss = 0.51382834\n",
            "Iteration 57, loss = 0.51223555\n",
            "Iteration 58, loss = 0.51167038\n",
            "Iteration 59, loss = 0.51239758\n",
            "Iteration 60, loss = 0.51254387\n",
            "Iteration 61, loss = 0.51088149\n",
            "Iteration 62, loss = 0.51097122\n",
            "Iteration 63, loss = 0.50975030\n",
            "Iteration 64, loss = 0.51038171\n",
            "Iteration 65, loss = 0.51104626\n",
            "Iteration 66, loss = 0.51060992\n",
            "Iteration 67, loss = 0.50930488\n",
            "Iteration 68, loss = 0.50885791\n",
            "Iteration 69, loss = 0.50957961\n",
            "Iteration 70, loss = 0.50996656\n",
            "Iteration 71, loss = 0.51323337\n",
            "Iteration 72, loss = 0.51190285\n",
            "Iteration 73, loss = 0.51171478\n",
            "Iteration 74, loss = 0.51071170\n",
            "Iteration 75, loss = 0.51021603\n",
            "Iteration 76, loss = 0.51167047\n",
            "Iteration 77, loss = 0.50976451\n",
            "Iteration 78, loss = 0.50861749\n",
            "Iteration 79, loss = 0.51106608\n",
            "Iteration 80, loss = 0.50849303\n",
            "Iteration 81, loss = 0.50817405\n",
            "Iteration 82, loss = 0.50984304\n",
            "Iteration 83, loss = 0.50968674\n",
            "Iteration 84, loss = 0.50810151\n",
            "Iteration 85, loss = 0.50825494\n",
            "Iteration 86, loss = 0.50784555\n",
            "Iteration 87, loss = 0.50798596\n",
            "Iteration 88, loss = 0.50880394\n",
            "Iteration 89, loss = 0.50803305\n",
            "Iteration 90, loss = 0.50877266\n",
            "Iteration 91, loss = 0.50864130\n",
            "Iteration 92, loss = 0.50836387\n",
            "Iteration 93, loss = 0.50758278\n",
            "Iteration 94, loss = 0.50909866\n",
            "Iteration 95, loss = 0.50893600\n",
            "Iteration 96, loss = 0.51058805\n",
            "Iteration 97, loss = 0.50825085\n",
            "Iteration 98, loss = 0.50973095\n",
            "Iteration 99, loss = 0.50818597\n",
            "Iteration 100, loss = 0.50770833\n",
            "Iteration 1, loss = 0.80475080\n",
            "Iteration 2, loss = 0.65487955\n",
            "Iteration 3, loss = 0.60390614\n",
            "Iteration 4, loss = 0.58819751\n",
            "Iteration 5, loss = 0.57865468\n",
            "Iteration 6, loss = 0.57206376\n",
            "Iteration 7, loss = 0.57135814\n",
            "Iteration 8, loss = 0.56979090\n",
            "Iteration 9, loss = 0.56702828\n",
            "Iteration 10, loss = 0.56515424\n",
            "Iteration 11, loss = 0.56408998\n",
            "Iteration 12, loss = 0.56223931\n",
            "Iteration 13, loss = 0.55712257\n",
            "Iteration 14, loss = 0.55543343\n",
            "Iteration 15, loss = 0.55460630\n",
            "Iteration 16, loss = 0.55343467\n",
            "Iteration 17, loss = 0.55204421\n",
            "Iteration 18, loss = 0.55203563\n",
            "Iteration 19, loss = 0.55038470\n",
            "Iteration 20, loss = 0.54916114\n",
            "Iteration 21, loss = 0.54790691\n",
            "Iteration 22, loss = 0.54823634\n",
            "Iteration 23, loss = 0.54770996\n",
            "Iteration 24, loss = 0.54647616\n",
            "Iteration 25, loss = 0.54569847\n",
            "Iteration 26, loss = 0.54635954\n",
            "Iteration 27, loss = 0.54486154\n",
            "Iteration 28, loss = 0.54269137\n",
            "Iteration 29, loss = 0.54393359\n",
            "Iteration 30, loss = 0.54525630\n",
            "Iteration 31, loss = 0.54050479\n",
            "Iteration 32, loss = 0.54251410\n",
            "Iteration 33, loss = 0.54337415\n",
            "Iteration 34, loss = 0.54084148\n",
            "Iteration 35, loss = 0.53936802\n",
            "Iteration 36, loss = 0.53834473\n",
            "Iteration 37, loss = 0.53647467\n",
            "Iteration 38, loss = 0.53587492\n",
            "Iteration 39, loss = 0.53480966\n",
            "Iteration 40, loss = 0.53401460\n",
            "Iteration 41, loss = 0.53291611\n",
            "Iteration 42, loss = 0.53377357\n",
            "Iteration 43, loss = 0.53302631\n",
            "Iteration 44, loss = 0.53221832\n",
            "Iteration 45, loss = 0.53264785\n",
            "Iteration 46, loss = 0.52988870\n",
            "Iteration 47, loss = 0.52953298\n",
            "Iteration 48, loss = 0.52807396\n",
            "Iteration 49, loss = 0.52674531\n",
            "Iteration 50, loss = 0.52573679\n",
            "Iteration 51, loss = 0.52553847\n",
            "Iteration 52, loss = 0.52517267\n",
            "Iteration 53, loss = 0.52597777\n",
            "Iteration 54, loss = 0.52580652\n",
            "Iteration 55, loss = 0.52542780\n",
            "Iteration 56, loss = 0.52500573\n",
            "Iteration 57, loss = 0.52407180\n",
            "Iteration 58, loss = 0.52276023\n",
            "Iteration 59, loss = 0.52280461\n",
            "Iteration 60, loss = 0.52352107\n",
            "Iteration 61, loss = 0.52285380\n",
            "Iteration 62, loss = 0.52188201\n",
            "Iteration 63, loss = 0.52059995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 64, loss = 0.52139367\n",
            "Iteration 65, loss = 0.52289242\n",
            "Iteration 66, loss = 0.52192389\n",
            "Iteration 67, loss = 0.52091656\n",
            "Iteration 68, loss = 0.52019784\n",
            "Iteration 69, loss = 0.51985503\n",
            "Iteration 70, loss = 0.51990398\n",
            "Iteration 71, loss = 0.52013587\n",
            "Iteration 72, loss = 0.52161612\n",
            "Iteration 73, loss = 0.52083912\n",
            "Iteration 74, loss = 0.51885686\n",
            "Iteration 75, loss = 0.51883100\n",
            "Iteration 76, loss = 0.52128410\n",
            "Iteration 77, loss = 0.52155454\n",
            "Iteration 78, loss = 0.51890984\n",
            "Iteration 79, loss = 0.52133313\n",
            "Iteration 80, loss = 0.51892092\n",
            "Iteration 81, loss = 0.51817384\n",
            "Iteration 82, loss = 0.52172643\n",
            "Iteration 83, loss = 0.52130464\n",
            "Iteration 84, loss = 0.52020023\n",
            "Iteration 85, loss = 0.51936990\n",
            "Iteration 86, loss = 0.51910924\n",
            "Iteration 87, loss = 0.51918844\n",
            "Iteration 88, loss = 0.51849875\n",
            "Iteration 89, loss = 0.51964345\n",
            "Iteration 90, loss = 0.51967115\n",
            "Iteration 91, loss = 0.51912139\n",
            "Iteration 92, loss = 0.51886701\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 100 and for layer number 3 : 0.6987500000000001\n",
            "Iteration 1, loss = 0.89843010\n",
            "Iteration 2, loss = 0.76513132\n",
            "Iteration 3, loss = 0.65384082\n",
            "Iteration 4, loss = 0.60270237\n",
            "Iteration 5, loss = 0.58704637\n",
            "Iteration 6, loss = 0.57794274\n",
            "Iteration 7, loss = 0.57124497\n",
            "Iteration 8, loss = 0.56673704\n",
            "Iteration 9, loss = 0.56287505\n",
            "Iteration 10, loss = 0.55892982\n",
            "Iteration 11, loss = 0.55698535\n",
            "Iteration 12, loss = 0.55516473\n",
            "Iteration 13, loss = 0.55305959\n",
            "Iteration 14, loss = 0.55269439\n",
            "Iteration 15, loss = 0.55268796\n",
            "Iteration 16, loss = 0.55141608\n",
            "Iteration 17, loss = 0.55198928\n",
            "Iteration 18, loss = 0.55193711\n",
            "Iteration 19, loss = 0.55199247\n",
            "Iteration 20, loss = 0.55087278\n",
            "Iteration 21, loss = 0.54920550\n",
            "Iteration 22, loss = 0.54957461\n",
            "Iteration 23, loss = 0.55002924\n",
            "Iteration 24, loss = 0.54902125\n",
            "Iteration 25, loss = 0.54833007\n",
            "Iteration 26, loss = 0.54641390\n",
            "Iteration 27, loss = 0.54527628\n",
            "Iteration 28, loss = 0.54547347\n",
            "Iteration 29, loss = 0.54497476\n",
            "Iteration 30, loss = 0.54315303\n",
            "Iteration 31, loss = 0.54269913\n",
            "Iteration 32, loss = 0.54345058\n",
            "Iteration 33, loss = 0.54133958\n",
            "Iteration 34, loss = 0.54006651\n",
            "Iteration 35, loss = 0.54181417\n",
            "Iteration 36, loss = 0.54064432\n",
            "Iteration 37, loss = 0.54176667\n",
            "Iteration 38, loss = 0.53892320\n",
            "Iteration 39, loss = 0.53749626\n",
            "Iteration 40, loss = 0.53643073\n",
            "Iteration 41, loss = 0.53749149\n",
            "Iteration 42, loss = 0.53688622\n",
            "Iteration 43, loss = 0.53409458\n",
            "Iteration 44, loss = 0.53520926\n",
            "Iteration 45, loss = 0.53517666\n",
            "Iteration 46, loss = 0.53266025\n",
            "Iteration 47, loss = 0.53317645\n",
            "Iteration 48, loss = 0.53170110\n",
            "Iteration 49, loss = 0.53091427\n",
            "Iteration 50, loss = 0.52962578\n",
            "Iteration 51, loss = 0.52946461\n",
            "Iteration 52, loss = 0.52939386\n",
            "Iteration 53, loss = 0.52910654\n",
            "Iteration 54, loss = 0.52878266\n",
            "Iteration 55, loss = 0.52787557\n",
            "Iteration 56, loss = 0.52630304\n",
            "Iteration 57, loss = 0.52688899\n",
            "Iteration 58, loss = 0.52679798\n",
            "Iteration 59, loss = 0.52907269\n",
            "Iteration 60, loss = 0.52540211\n",
            "Iteration 61, loss = 0.52761757\n",
            "Iteration 62, loss = 0.52778592\n",
            "Iteration 63, loss = 0.52798004\n",
            "Iteration 64, loss = 0.52820011\n",
            "Iteration 65, loss = 0.52612079\n",
            "Iteration 66, loss = 0.52629661\n",
            "Iteration 67, loss = 0.52709228\n",
            "Iteration 68, loss = 0.52815700\n",
            "Iteration 69, loss = 0.52708967\n",
            "Iteration 70, loss = 0.52691176\n",
            "Iteration 71, loss = 0.52816892\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.89647974\n",
            "Iteration 2, loss = 0.76503390\n",
            "Iteration 3, loss = 0.65738862\n",
            "Iteration 4, loss = 0.60451657\n",
            "Iteration 5, loss = 0.58466759\n",
            "Iteration 6, loss = 0.57504316\n",
            "Iteration 7, loss = 0.56767168\n",
            "Iteration 8, loss = 0.56133735\n",
            "Iteration 9, loss = 0.55764613\n",
            "Iteration 10, loss = 0.55376055\n",
            "Iteration 11, loss = 0.55274203\n",
            "Iteration 12, loss = 0.55149150\n",
            "Iteration 13, loss = 0.54874116\n",
            "Iteration 14, loss = 0.54757801\n",
            "Iteration 15, loss = 0.54687336\n",
            "Iteration 16, loss = 0.54592392\n",
            "Iteration 17, loss = 0.54463412\n",
            "Iteration 18, loss = 0.54412666\n",
            "Iteration 19, loss = 0.54401242\n",
            "Iteration 20, loss = 0.54346391\n",
            "Iteration 21, loss = 0.54321807\n",
            "Iteration 22, loss = 0.54308250\n",
            "Iteration 23, loss = 0.54393627\n",
            "Iteration 24, loss = 0.54504573\n",
            "Iteration 25, loss = 0.54398395\n",
            "Iteration 26, loss = 0.54195910\n",
            "Iteration 27, loss = 0.54102551\n",
            "Iteration 28, loss = 0.54000856\n",
            "Iteration 29, loss = 0.53864889\n",
            "Iteration 30, loss = 0.53780552\n",
            "Iteration 31, loss = 0.53879175\n",
            "Iteration 32, loss = 0.53899122\n",
            "Iteration 33, loss = 0.53803797\n",
            "Iteration 34, loss = 0.53777915\n",
            "Iteration 35, loss = 0.53787486\n",
            "Iteration 36, loss = 0.53457417\n",
            "Iteration 37, loss = 0.53692346\n",
            "Iteration 38, loss = 0.53428080\n",
            "Iteration 39, loss = 0.53379512\n",
            "Iteration 40, loss = 0.53249173\n",
            "Iteration 41, loss = 0.53436636\n",
            "Iteration 42, loss = 0.53136372\n",
            "Iteration 43, loss = 0.52905241\n",
            "Iteration 44, loss = 0.52976079\n",
            "Iteration 45, loss = 0.52792724\n",
            "Iteration 46, loss = 0.52571268\n",
            "Iteration 47, loss = 0.52518925\n",
            "Iteration 48, loss = 0.52391380\n",
            "Iteration 49, loss = 0.52326992\n",
            "Iteration 50, loss = 0.52120426\n",
            "Iteration 51, loss = 0.52140533\n",
            "Iteration 52, loss = 0.52240227\n",
            "Iteration 53, loss = 0.52119785\n",
            "Iteration 54, loss = 0.52163119\n",
            "Iteration 55, loss = 0.52027640\n",
            "Iteration 56, loss = 0.52069217\n",
            "Iteration 57, loss = 0.52106924\n",
            "Iteration 58, loss = 0.51928000\n",
            "Iteration 59, loss = 0.51916456\n",
            "Iteration 60, loss = 0.52327257\n",
            "Iteration 61, loss = 0.51895711\n",
            "Iteration 62, loss = 0.51532216\n",
            "Iteration 63, loss = 0.51514669\n",
            "Iteration 64, loss = 0.51379276\n",
            "Iteration 65, loss = 0.51532715\n",
            "Iteration 66, loss = 0.51897733\n",
            "Iteration 67, loss = 0.51749566\n",
            "Iteration 68, loss = 0.51430419\n",
            "Iteration 69, loss = 0.51448492\n",
            "Iteration 70, loss = 0.51420734\n",
            "Iteration 71, loss = 0.51086393\n",
            "Iteration 72, loss = 0.50569609\n",
            "Iteration 73, loss = 0.50630985\n",
            "Iteration 74, loss = 0.50555591\n",
            "Iteration 75, loss = 0.50792682\n",
            "Iteration 76, loss = 0.50417952\n",
            "Iteration 77, loss = 0.50254183\n",
            "Iteration 78, loss = 0.50139324\n",
            "Iteration 79, loss = 0.50452224\n",
            "Iteration 80, loss = 0.50252965\n",
            "Iteration 81, loss = 0.50458088\n",
            "Iteration 82, loss = 0.49910814\n",
            "Iteration 83, loss = 0.50903244\n",
            "Iteration 84, loss = 0.49569914\n",
            "Iteration 85, loss = 0.50412909\n",
            "Iteration 86, loss = 0.49595235\n",
            "Iteration 87, loss = 0.49687348\n",
            "Iteration 88, loss = 0.49134680\n",
            "Iteration 89, loss = 0.49834615\n",
            "Iteration 90, loss = 0.48980261\n",
            "Iteration 91, loss = 0.49338704\n",
            "Iteration 92, loss = 0.48902344\n",
            "Iteration 93, loss = 0.49145853\n",
            "Iteration 94, loss = 0.48747800\n",
            "Iteration 95, loss = 0.48887107\n",
            "Iteration 96, loss = 0.49037425\n",
            "Iteration 97, loss = 0.49859268\n",
            "Iteration 98, loss = 0.48675766\n",
            "Iteration 99, loss = 0.49052318\n",
            "Iteration 100, loss = 0.48894740\n",
            "Iteration 1, loss = 0.90447953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2, loss = 0.76691485\n",
            "Iteration 3, loss = 0.65636497\n",
            "Iteration 4, loss = 0.59577920\n",
            "Iteration 5, loss = 0.57707171\n",
            "Iteration 6, loss = 0.56749853\n",
            "Iteration 7, loss = 0.56265587\n",
            "Iteration 8, loss = 0.55936371\n",
            "Iteration 9, loss = 0.55478568\n",
            "Iteration 10, loss = 0.55004024\n",
            "Iteration 11, loss = 0.54769876\n",
            "Iteration 12, loss = 0.54630747\n",
            "Iteration 13, loss = 0.54357485\n",
            "Iteration 14, loss = 0.54209368\n",
            "Iteration 15, loss = 0.54201538\n",
            "Iteration 16, loss = 0.54178238\n",
            "Iteration 17, loss = 0.54056641\n",
            "Iteration 18, loss = 0.53921933\n",
            "Iteration 19, loss = 0.53823331\n",
            "Iteration 20, loss = 0.53717933\n",
            "Iteration 21, loss = 0.53771539\n",
            "Iteration 22, loss = 0.53632534\n",
            "Iteration 23, loss = 0.53647887\n",
            "Iteration 24, loss = 0.53782796\n",
            "Iteration 25, loss = 0.53758771\n",
            "Iteration 26, loss = 0.53530753\n",
            "Iteration 27, loss = 0.53393838\n",
            "Iteration 28, loss = 0.53190523\n",
            "Iteration 29, loss = 0.53044664\n",
            "Iteration 30, loss = 0.52968979\n",
            "Iteration 31, loss = 0.52941003\n",
            "Iteration 32, loss = 0.52925026\n",
            "Iteration 33, loss = 0.52945278\n",
            "Iteration 34, loss = 0.52754751\n",
            "Iteration 35, loss = 0.52513913\n",
            "Iteration 36, loss = 0.52465087\n",
            "Iteration 37, loss = 0.52490959\n",
            "Iteration 38, loss = 0.52384790\n",
            "Iteration 39, loss = 0.52392522\n",
            "Iteration 40, loss = 0.52349030\n",
            "Iteration 41, loss = 0.52296316\n",
            "Iteration 42, loss = 0.52142275\n",
            "Iteration 43, loss = 0.52025813\n",
            "Iteration 44, loss = 0.52140690\n",
            "Iteration 45, loss = 0.51916030\n",
            "Iteration 46, loss = 0.51798468\n",
            "Iteration 47, loss = 0.51756444\n",
            "Iteration 48, loss = 0.51911537\n",
            "Iteration 49, loss = 0.51761755\n",
            "Iteration 50, loss = 0.51629200\n",
            "Iteration 51, loss = 0.51601996\n",
            "Iteration 52, loss = 0.51569139\n",
            "Iteration 53, loss = 0.51522243\n",
            "Iteration 54, loss = 0.51449337\n",
            "Iteration 55, loss = 0.51487080\n",
            "Iteration 56, loss = 0.51443166\n",
            "Iteration 57, loss = 0.51406125\n",
            "Iteration 58, loss = 0.51403510\n",
            "Iteration 59, loss = 0.51353227\n",
            "Iteration 60, loss = 0.51573735\n",
            "Iteration 61, loss = 0.51559794\n",
            "Iteration 62, loss = 0.51492730\n",
            "Iteration 63, loss = 0.51295094\n",
            "Iteration 64, loss = 0.51173244\n",
            "Iteration 65, loss = 0.51256462\n",
            "Iteration 66, loss = 0.51539152\n",
            "Iteration 67, loss = 0.51606858\n",
            "Iteration 68, loss = 0.51419822\n",
            "Iteration 69, loss = 0.51133250\n",
            "Iteration 70, loss = 0.51035396\n",
            "Iteration 71, loss = 0.51073227\n",
            "Iteration 72, loss = 0.51138410\n",
            "Iteration 73, loss = 0.50991133\n",
            "Iteration 74, loss = 0.50943337\n",
            "Iteration 75, loss = 0.50905278\n",
            "Iteration 76, loss = 0.50930612\n",
            "Iteration 77, loss = 0.50761673\n",
            "Iteration 78, loss = 0.50834262\n",
            "Iteration 79, loss = 0.50950490\n",
            "Iteration 80, loss = 0.50716846\n",
            "Iteration 81, loss = 0.50880714\n",
            "Iteration 82, loss = 0.50480841\n",
            "Iteration 83, loss = 0.50816916\n",
            "Iteration 84, loss = 0.50715524\n",
            "Iteration 85, loss = 0.51112409\n",
            "Iteration 86, loss = 0.50938590\n",
            "Iteration 87, loss = 0.50451063\n",
            "Iteration 88, loss = 0.50303698\n",
            "Iteration 89, loss = 0.50204349\n",
            "Iteration 90, loss = 0.50549412\n",
            "Iteration 91, loss = 0.50266367\n",
            "Iteration 92, loss = 0.50142561\n",
            "Iteration 93, loss = 0.50091745\n",
            "Iteration 94, loss = 0.50182045\n",
            "Iteration 95, loss = 0.50059640\n",
            "Iteration 96, loss = 0.50122117\n",
            "Iteration 97, loss = 0.50394290\n",
            "Iteration 98, loss = 0.50454859\n",
            "Iteration 99, loss = 0.50382952\n",
            "Iteration 100, loss = 0.50259390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.90198656\n",
            "Iteration 2, loss = 0.76291623\n",
            "Iteration 3, loss = 0.65742388\n",
            "Iteration 4, loss = 0.60221160\n",
            "Iteration 5, loss = 0.58317327\n",
            "Iteration 6, loss = 0.57433136\n",
            "Iteration 7, loss = 0.56872138\n",
            "Iteration 8, loss = 0.56549704\n",
            "Iteration 9, loss = 0.56136635\n",
            "Iteration 10, loss = 0.55708719\n",
            "Iteration 11, loss = 0.55448963\n",
            "Iteration 12, loss = 0.55290078\n",
            "Iteration 13, loss = 0.55099528\n",
            "Iteration 14, loss = 0.54931756\n",
            "Iteration 15, loss = 0.54756538\n",
            "Iteration 16, loss = 0.54790434\n",
            "Iteration 17, loss = 0.54674437\n",
            "Iteration 18, loss = 0.54610576\n",
            "Iteration 19, loss = 0.54541517\n",
            "Iteration 20, loss = 0.54421269\n",
            "Iteration 21, loss = 0.54342487\n",
            "Iteration 22, loss = 0.54277383\n",
            "Iteration 23, loss = 0.54073149\n",
            "Iteration 24, loss = 0.53939535\n",
            "Iteration 25, loss = 0.53830286\n",
            "Iteration 26, loss = 0.53745869\n",
            "Iteration 27, loss = 0.53549679\n",
            "Iteration 28, loss = 0.53399954\n",
            "Iteration 29, loss = 0.53321457\n",
            "Iteration 30, loss = 0.53192810\n",
            "Iteration 31, loss = 0.53203935\n",
            "Iteration 32, loss = 0.53023525\n",
            "Iteration 33, loss = 0.53114364\n",
            "Iteration 34, loss = 0.52945194\n",
            "Iteration 35, loss = 0.52907989\n",
            "Iteration 36, loss = 0.52834601\n",
            "Iteration 37, loss = 0.52729509\n",
            "Iteration 38, loss = 0.52699016\n",
            "Iteration 39, loss = 0.52728567\n",
            "Iteration 40, loss = 0.52614239\n",
            "Iteration 41, loss = 0.52530819\n",
            "Iteration 42, loss = 0.52660269\n",
            "Iteration 43, loss = 0.52394303\n",
            "Iteration 44, loss = 0.52572657\n",
            "Iteration 45, loss = 0.52671551\n",
            "Iteration 46, loss = 0.52209890\n",
            "Iteration 47, loss = 0.52253360\n",
            "Iteration 48, loss = 0.52161146\n",
            "Iteration 49, loss = 0.51878445\n",
            "Iteration 50, loss = 0.51725599\n",
            "Iteration 51, loss = 0.51697770\n",
            "Iteration 52, loss = 0.51832703\n",
            "Iteration 53, loss = 0.51684925\n",
            "Iteration 54, loss = 0.51487529\n",
            "Iteration 55, loss = 0.51478777\n",
            "Iteration 56, loss = 0.51717655\n",
            "Iteration 57, loss = 0.51394420\n",
            "Iteration 58, loss = 0.51565967\n",
            "Iteration 59, loss = 0.51330813\n",
            "Iteration 60, loss = 0.51544560\n",
            "Iteration 61, loss = 0.51703005\n",
            "Iteration 62, loss = 0.51640741\n",
            "Iteration 63, loss = 0.51268030\n",
            "Iteration 64, loss = 0.51199019\n",
            "Iteration 65, loss = 0.51365689\n",
            "Iteration 66, loss = 0.51556396\n",
            "Iteration 67, loss = 0.51719521\n",
            "Iteration 68, loss = 0.51272104\n",
            "Iteration 69, loss = 0.50991236\n",
            "Iteration 70, loss = 0.51058489\n",
            "Iteration 71, loss = 0.50994532\n",
            "Iteration 72, loss = 0.51339412\n",
            "Iteration 73, loss = 0.51098616\n",
            "Iteration 74, loss = 0.51041455\n",
            "Iteration 75, loss = 0.51109968\n",
            "Iteration 76, loss = 0.51151649\n",
            "Iteration 77, loss = 0.51007244\n",
            "Iteration 78, loss = 0.50933724\n",
            "Iteration 79, loss = 0.50888062\n",
            "Iteration 80, loss = 0.50800370\n",
            "Iteration 81, loss = 0.50708513\n",
            "Iteration 82, loss = 0.50945677\n",
            "Iteration 83, loss = 0.50837970\n",
            "Iteration 84, loss = 0.50710538\n",
            "Iteration 85, loss = 0.50540758\n",
            "Iteration 86, loss = 0.50423081\n",
            "Iteration 87, loss = 0.50390843\n",
            "Iteration 88, loss = 0.50198227\n",
            "Iteration 89, loss = 0.50078540\n",
            "Iteration 90, loss = 0.49806554\n",
            "Iteration 91, loss = 0.49996821\n",
            "Iteration 92, loss = 0.49663444\n",
            "Iteration 93, loss = 0.49797597\n",
            "Iteration 94, loss = 0.49714516\n",
            "Iteration 95, loss = 0.49746944\n",
            "Iteration 96, loss = 0.49608519\n",
            "Iteration 97, loss = 0.49377143\n",
            "Iteration 98, loss = 0.49313557\n",
            "Iteration 99, loss = 0.49273804\n",
            "Iteration 100, loss = 0.49246199\n",
            "Iteration 1, loss = 0.90952303\n",
            "Iteration 2, loss = 0.77155333\n",
            "Iteration 3, loss = 0.67094423\n",
            "Iteration 4, loss = 0.62256267\n",
            "Iteration 5, loss = 0.60199531\n",
            "Iteration 6, loss = 0.59268166\n",
            "Iteration 7, loss = 0.58716016\n",
            "Iteration 8, loss = 0.58316253\n",
            "Iteration 9, loss = 0.58015802\n",
            "Iteration 10, loss = 0.57589625\n",
            "Iteration 11, loss = 0.57238103\n",
            "Iteration 12, loss = 0.56950780\n",
            "Iteration 13, loss = 0.56551747\n",
            "Iteration 14, loss = 0.56266632\n",
            "Iteration 15, loss = 0.56045276\n",
            "Iteration 16, loss = 0.55996391\n",
            "Iteration 17, loss = 0.56033396\n",
            "Iteration 18, loss = 0.55859220\n",
            "Iteration 19, loss = 0.55699684\n",
            "Iteration 20, loss = 0.55853159\n",
            "Iteration 21, loss = 0.55800404\n",
            "Iteration 22, loss = 0.55612568\n",
            "Iteration 23, loss = 0.55443691\n",
            "Iteration 24, loss = 0.55519688\n",
            "Iteration 25, loss = 0.55540400\n",
            "Iteration 26, loss = 0.55363007\n",
            "Iteration 27, loss = 0.55216340\n",
            "Iteration 28, loss = 0.55081333\n",
            "Iteration 29, loss = 0.55203330\n",
            "Iteration 30, loss = 0.55126833\n",
            "Iteration 31, loss = 0.55193759\n",
            "Iteration 32, loss = 0.54991332\n",
            "Iteration 33, loss = 0.54961661\n",
            "Iteration 34, loss = 0.54899348\n",
            "Iteration 35, loss = 0.54876516\n",
            "Iteration 36, loss = 0.54808754\n",
            "Iteration 37, loss = 0.54874480\n",
            "Iteration 38, loss = 0.54786161\n",
            "Iteration 39, loss = 0.54812496\n",
            "Iteration 40, loss = 0.54753323\n",
            "Iteration 41, loss = 0.54695736\n",
            "Iteration 42, loss = 0.54787232\n",
            "Iteration 43, loss = 0.54571606\n",
            "Iteration 44, loss = 0.54937040\n",
            "Iteration 45, loss = 0.54782186\n",
            "Iteration 46, loss = 0.54661798\n",
            "Iteration 47, loss = 0.54675632\n",
            "Iteration 48, loss = 0.54689029\n",
            "Iteration 49, loss = 0.54523948\n",
            "Iteration 50, loss = 0.54503063\n",
            "Iteration 51, loss = 0.54525127\n",
            "Iteration 52, loss = 0.54468624\n",
            "Iteration 53, loss = 0.54400302\n",
            "Iteration 54, loss = 0.54391423\n",
            "Iteration 55, loss = 0.54280670\n",
            "Iteration 56, loss = 0.54259832\n",
            "Iteration 57, loss = 0.54213790\n",
            "Iteration 58, loss = 0.54282466\n",
            "Iteration 59, loss = 0.54178980\n",
            "Iteration 60, loss = 0.54152020\n",
            "Iteration 61, loss = 0.54155992\n",
            "Iteration 62, loss = 0.54119488\n",
            "Iteration 63, loss = 0.54071023\n",
            "Iteration 64, loss = 0.54030722\n",
            "Iteration 65, loss = 0.53913105\n",
            "Iteration 66, loss = 0.53989860\n",
            "Iteration 67, loss = 0.54074834\n",
            "Iteration 68, loss = 0.54150445\n",
            "Iteration 69, loss = 0.53777836\n",
            "Iteration 70, loss = 0.53800642\n",
            "Iteration 71, loss = 0.53700912\n",
            "Iteration 72, loss = 0.53829864\n",
            "Iteration 73, loss = 0.53702260\n",
            "Iteration 74, loss = 0.53586493\n",
            "Iteration 75, loss = 0.54016969\n",
            "Iteration 76, loss = 0.53650803\n",
            "Iteration 77, loss = 0.53598665\n",
            "Iteration 78, loss = 0.53578596\n",
            "Iteration 79, loss = 0.53630111\n",
            "Iteration 80, loss = 0.53602261\n",
            "Iteration 81, loss = 0.53346023\n",
            "Iteration 82, loss = 0.53441862\n",
            "Iteration 83, loss = 0.53432345\n",
            "Iteration 84, loss = 0.53342583\n",
            "Iteration 85, loss = 0.53437527\n",
            "Iteration 86, loss = 0.53154339\n",
            "Iteration 87, loss = 0.53295855\n",
            "Iteration 88, loss = 0.53567582\n",
            "Iteration 89, loss = 0.53415989\n",
            "Iteration 90, loss = 0.52991798\n",
            "Iteration 91, loss = 0.53273745\n",
            "Iteration 92, loss = 0.52870751\n",
            "Iteration 93, loss = 0.52784626\n",
            "Iteration 94, loss = 0.52855230\n",
            "Iteration 95, loss = 0.53029506\n",
            "Iteration 96, loss = 0.52741036\n",
            "Iteration 97, loss = 0.52717025\n",
            "Iteration 98, loss = 0.52456752\n",
            "Iteration 99, loss = 0.52656043\n",
            "Iteration 100, loss = 0.52491723\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 100 and for layer number 4 : 0.69125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.84283593\n",
            "Iteration 2, loss = 0.67832748\n",
            "Iteration 3, loss = 0.61850827\n",
            "Iteration 4, loss = 0.60382734\n",
            "Iteration 5, loss = 0.59477653\n",
            "Iteration 6, loss = 0.59056898\n",
            "Iteration 7, loss = 0.58162134\n",
            "Iteration 8, loss = 0.57169835\n",
            "Iteration 9, loss = 0.56715602\n",
            "Iteration 10, loss = 0.56250128\n",
            "Iteration 11, loss = 0.56056030\n",
            "Iteration 12, loss = 0.55803894\n",
            "Iteration 13, loss = 0.55555691\n",
            "Iteration 14, loss = 0.55356033\n",
            "Iteration 15, loss = 0.54894640\n",
            "Iteration 16, loss = 0.54914943\n",
            "Iteration 17, loss = 0.54874113\n",
            "Iteration 18, loss = 0.54701632\n",
            "Iteration 19, loss = 0.54482747\n",
            "Iteration 20, loss = 0.54364637\n",
            "Iteration 21, loss = 0.54290350\n",
            "Iteration 22, loss = 0.54365119\n",
            "Iteration 23, loss = 0.54035673\n",
            "Iteration 24, loss = 0.53845438\n",
            "Iteration 25, loss = 0.53723289\n",
            "Iteration 26, loss = 0.53636142\n",
            "Iteration 27, loss = 0.53484490\n",
            "Iteration 28, loss = 0.53512642\n",
            "Iteration 29, loss = 0.53544812\n",
            "Iteration 30, loss = 0.53257999\n",
            "Iteration 31, loss = 0.53591516\n",
            "Iteration 32, loss = 0.53339687\n",
            "Iteration 33, loss = 0.53051295\n",
            "Iteration 34, loss = 0.52991251\n",
            "Iteration 35, loss = 0.52986860\n",
            "Iteration 36, loss = 0.52797787\n",
            "Iteration 37, loss = 0.52785962\n",
            "Iteration 38, loss = 0.52742171\n",
            "Iteration 39, loss = 0.52639113\n",
            "Iteration 40, loss = 0.52615978\n",
            "Iteration 41, loss = 0.52616046\n",
            "Iteration 42, loss = 0.52438233\n",
            "Iteration 43, loss = 0.52450988\n",
            "Iteration 44, loss = 0.52431472\n",
            "Iteration 45, loss = 0.52376981\n",
            "Iteration 46, loss = 0.52195023\n",
            "Iteration 47, loss = 0.52206280\n",
            "Iteration 48, loss = 0.52291998\n",
            "Iteration 49, loss = 0.52195095\n",
            "Iteration 50, loss = 0.52108269\n",
            "Iteration 51, loss = 0.52130216\n",
            "Iteration 52, loss = 0.52001568\n",
            "Iteration 53, loss = 0.52004530\n",
            "Iteration 54, loss = 0.52191695\n",
            "Iteration 55, loss = 0.52080266\n",
            "Iteration 56, loss = 0.51808299\n",
            "Iteration 57, loss = 0.51748947\n",
            "Iteration 58, loss = 0.51716362\n",
            "Iteration 59, loss = 0.51694316\n",
            "Iteration 60, loss = 0.51658528\n",
            "Iteration 61, loss = 0.51648336\n",
            "Iteration 62, loss = 0.51510877\n",
            "Iteration 63, loss = 0.51749582\n",
            "Iteration 64, loss = 0.51449864\n",
            "Iteration 65, loss = 0.51468475\n",
            "Iteration 66, loss = 0.51606903\n",
            "Iteration 67, loss = 0.51518097\n",
            "Iteration 68, loss = 0.51455031\n",
            "Iteration 69, loss = 0.51350630\n",
            "Iteration 70, loss = 0.51362597\n",
            "Iteration 71, loss = 0.51534284\n",
            "Iteration 72, loss = 0.51294395\n",
            "Iteration 73, loss = 0.51181823\n",
            "Iteration 74, loss = 0.51195315\n",
            "Iteration 75, loss = 0.51020922\n",
            "Iteration 76, loss = 0.50961620\n",
            "Iteration 77, loss = 0.50872978\n",
            "Iteration 78, loss = 0.50980573\n",
            "Iteration 79, loss = 0.50981380\n",
            "Iteration 80, loss = 0.51025917\n",
            "Iteration 81, loss = 0.51557245\n",
            "Iteration 82, loss = 0.51404262\n",
            "Iteration 83, loss = 0.50811045\n",
            "Iteration 84, loss = 0.51117944\n",
            "Iteration 85, loss = 0.51408823\n",
            "Iteration 86, loss = 0.50714546\n",
            "Iteration 87, loss = 0.50648920\n",
            "Iteration 88, loss = 0.50683883\n",
            "Iteration 89, loss = 0.50483630\n",
            "Iteration 90, loss = 0.50519714\n",
            "Iteration 91, loss = 0.50637432\n",
            "Iteration 92, loss = 0.50509134\n",
            "Iteration 93, loss = 0.50572160\n",
            "Iteration 94, loss = 0.50471110\n",
            "Iteration 95, loss = 0.50551280\n",
            "Iteration 96, loss = 0.50375059\n",
            "Iteration 97, loss = 0.50444488\n",
            "Iteration 98, loss = 0.50495500\n",
            "Iteration 99, loss = 0.50372446\n",
            "Iteration 100, loss = 0.50397249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.84211562\n",
            "Iteration 2, loss = 0.67390273\n",
            "Iteration 3, loss = 0.61479397\n",
            "Iteration 4, loss = 0.59817770\n",
            "Iteration 5, loss = 0.58643524\n",
            "Iteration 6, loss = 0.57940279\n",
            "Iteration 7, loss = 0.57108334\n",
            "Iteration 8, loss = 0.56345206\n",
            "Iteration 9, loss = 0.55760032\n",
            "Iteration 10, loss = 0.55137476\n",
            "Iteration 11, loss = 0.54982593\n",
            "Iteration 12, loss = 0.54759444\n",
            "Iteration 13, loss = 0.54480473\n",
            "Iteration 14, loss = 0.54105592\n",
            "Iteration 15, loss = 0.53799031\n",
            "Iteration 16, loss = 0.53554759\n",
            "Iteration 17, loss = 0.53296703\n",
            "Iteration 18, loss = 0.53051284\n",
            "Iteration 19, loss = 0.52790683\n",
            "Iteration 20, loss = 0.52677164\n",
            "Iteration 21, loss = 0.52485005\n",
            "Iteration 22, loss = 0.52480127\n",
            "Iteration 23, loss = 0.52143077\n",
            "Iteration 24, loss = 0.52039570\n",
            "Iteration 25, loss = 0.51926551\n",
            "Iteration 26, loss = 0.51855653\n",
            "Iteration 27, loss = 0.51683205\n",
            "Iteration 28, loss = 0.51569551\n",
            "Iteration 29, loss = 0.51493485\n",
            "Iteration 30, loss = 0.51421020\n",
            "Iteration 31, loss = 0.51674064\n",
            "Iteration 32, loss = 0.51260548\n",
            "Iteration 33, loss = 0.51040642\n",
            "Iteration 34, loss = 0.50939175\n",
            "Iteration 35, loss = 0.50888292\n",
            "Iteration 36, loss = 0.50876223\n",
            "Iteration 37, loss = 0.50628136\n",
            "Iteration 38, loss = 0.50496776\n",
            "Iteration 39, loss = 0.50289192\n",
            "Iteration 40, loss = 0.50200486\n",
            "Iteration 41, loss = 0.50190053\n",
            "Iteration 42, loss = 0.50099544\n",
            "Iteration 43, loss = 0.50033700\n",
            "Iteration 44, loss = 0.49990491\n",
            "Iteration 45, loss = 0.49703635\n",
            "Iteration 46, loss = 0.49645873\n",
            "Iteration 47, loss = 0.49625000\n",
            "Iteration 48, loss = 0.49583677\n",
            "Iteration 49, loss = 0.49334537\n",
            "Iteration 50, loss = 0.49351033\n",
            "Iteration 51, loss = 0.49339818\n",
            "Iteration 52, loss = 0.49169668\n",
            "Iteration 53, loss = 0.49150701\n",
            "Iteration 54, loss = 0.49566927\n",
            "Iteration 55, loss = 0.49062902\n",
            "Iteration 56, loss = 0.48908149\n",
            "Iteration 57, loss = 0.48730963\n",
            "Iteration 58, loss = 0.48737917\n",
            "Iteration 59, loss = 0.48786171\n",
            "Iteration 60, loss = 0.48735379\n",
            "Iteration 61, loss = 0.48919848\n",
            "Iteration 62, loss = 0.48604695\n",
            "Iteration 63, loss = 0.48791279\n",
            "Iteration 64, loss = 0.48447282\n",
            "Iteration 65, loss = 0.48392292\n",
            "Iteration 66, loss = 0.48710397\n",
            "Iteration 67, loss = 0.48497721\n",
            "Iteration 68, loss = 0.48189699\n",
            "Iteration 69, loss = 0.48215848\n",
            "Iteration 70, loss = 0.48396642\n",
            "Iteration 71, loss = 0.48472021\n",
            "Iteration 72, loss = 0.48269523\n",
            "Iteration 73, loss = 0.48006770\n",
            "Iteration 74, loss = 0.47949046\n",
            "Iteration 75, loss = 0.47917436\n",
            "Iteration 76, loss = 0.47986172\n",
            "Iteration 77, loss = 0.47652529\n",
            "Iteration 78, loss = 0.47572204\n",
            "Iteration 79, loss = 0.47605137\n",
            "Iteration 80, loss = 0.47486116\n",
            "Iteration 81, loss = 0.47661630\n",
            "Iteration 82, loss = 0.47626359\n",
            "Iteration 83, loss = 0.47833453\n",
            "Iteration 84, loss = 0.47666945\n",
            "Iteration 85, loss = 0.48468544\n",
            "Iteration 86, loss = 0.47513421\n",
            "Iteration 87, loss = 0.47577963\n",
            "Iteration 88, loss = 0.47356521\n",
            "Iteration 89, loss = 0.47354569\n",
            "Iteration 90, loss = 0.47358204\n",
            "Iteration 91, loss = 0.47377450\n",
            "Iteration 92, loss = 0.47338047\n",
            "Iteration 93, loss = 0.47247782\n",
            "Iteration 94, loss = 0.47194345\n",
            "Iteration 95, loss = 0.47182012\n",
            "Iteration 96, loss = 0.47099111\n",
            "Iteration 97, loss = 0.47485566\n",
            "Iteration 98, loss = 0.47394550\n",
            "Iteration 99, loss = 0.47321010\n",
            "Iteration 100, loss = 0.47317758\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.84194648\n",
            "Iteration 2, loss = 0.66554851\n",
            "Iteration 3, loss = 0.61225024\n",
            "Iteration 4, loss = 0.59987886\n",
            "Iteration 5, loss = 0.59202196\n",
            "Iteration 6, loss = 0.58325873\n",
            "Iteration 7, loss = 0.57298718\n",
            "Iteration 8, loss = 0.56206000\n",
            "Iteration 9, loss = 0.55315409\n",
            "Iteration 10, loss = 0.54698521\n",
            "Iteration 11, loss = 0.54442773\n",
            "Iteration 12, loss = 0.54226291\n",
            "Iteration 13, loss = 0.53980072\n",
            "Iteration 14, loss = 0.53599988\n",
            "Iteration 15, loss = 0.53313323\n",
            "Iteration 16, loss = 0.52934416\n",
            "Iteration 17, loss = 0.52638109\n",
            "Iteration 18, loss = 0.52580879\n",
            "Iteration 19, loss = 0.52426243\n",
            "Iteration 20, loss = 0.52290946\n",
            "Iteration 21, loss = 0.52268451\n",
            "Iteration 22, loss = 0.52343882\n",
            "Iteration 23, loss = 0.52150082\n",
            "Iteration 24, loss = 0.52105498\n",
            "Iteration 25, loss = 0.51995877\n",
            "Iteration 26, loss = 0.51906444\n",
            "Iteration 27, loss = 0.51760723\n",
            "Iteration 28, loss = 0.51746923\n",
            "Iteration 29, loss = 0.51659429\n",
            "Iteration 30, loss = 0.51628310\n",
            "Iteration 31, loss = 0.51696056\n",
            "Iteration 32, loss = 0.51532952\n",
            "Iteration 33, loss = 0.51492982\n",
            "Iteration 34, loss = 0.51395807\n",
            "Iteration 35, loss = 0.51293487\n",
            "Iteration 36, loss = 0.51311644\n",
            "Iteration 37, loss = 0.51206223\n",
            "Iteration 38, loss = 0.51187460\n",
            "Iteration 39, loss = 0.51152712\n",
            "Iteration 40, loss = 0.51025039\n",
            "Iteration 41, loss = 0.51022046\n",
            "Iteration 42, loss = 0.51081281\n",
            "Iteration 43, loss = 0.51040966\n",
            "Iteration 44, loss = 0.51026804\n",
            "Iteration 45, loss = 0.50922244\n",
            "Iteration 46, loss = 0.50916438\n",
            "Iteration 47, loss = 0.50888024\n",
            "Iteration 48, loss = 0.50907711\n",
            "Iteration 49, loss = 0.50774179\n",
            "Iteration 50, loss = 0.50710401\n",
            "Iteration 51, loss = 0.50764802\n",
            "Iteration 52, loss = 0.50624387\n",
            "Iteration 53, loss = 0.50559071\n",
            "Iteration 54, loss = 0.50518696\n",
            "Iteration 55, loss = 0.50350782\n",
            "Iteration 56, loss = 0.50403498\n",
            "Iteration 57, loss = 0.50308545\n",
            "Iteration 58, loss = 0.50276083\n",
            "Iteration 59, loss = 0.50331295\n",
            "Iteration 60, loss = 0.50236108\n",
            "Iteration 61, loss = 0.50171369\n",
            "Iteration 62, loss = 0.50260973\n",
            "Iteration 63, loss = 0.50167126\n",
            "Iteration 64, loss = 0.50123775\n",
            "Iteration 65, loss = 0.49866415\n",
            "Iteration 66, loss = 0.50089686\n",
            "Iteration 67, loss = 0.49977304\n",
            "Iteration 68, loss = 0.49736768\n",
            "Iteration 69, loss = 0.49733328\n",
            "Iteration 70, loss = 0.49723094\n",
            "Iteration 71, loss = 0.49939928\n",
            "Iteration 72, loss = 0.49655591\n",
            "Iteration 73, loss = 0.49371061\n",
            "Iteration 74, loss = 0.49284754\n",
            "Iteration 75, loss = 0.49211623\n",
            "Iteration 76, loss = 0.49079801\n",
            "Iteration 77, loss = 0.48971083\n",
            "Iteration 78, loss = 0.48842846\n",
            "Iteration 79, loss = 0.48863192\n",
            "Iteration 80, loss = 0.48769454\n",
            "Iteration 81, loss = 0.48869401\n",
            "Iteration 82, loss = 0.48656822\n",
            "Iteration 83, loss = 0.48627403\n",
            "Iteration 84, loss = 0.48614048\n",
            "Iteration 85, loss = 0.48758305\n",
            "Iteration 86, loss = 0.48417598\n",
            "Iteration 87, loss = 0.48175374\n",
            "Iteration 88, loss = 0.48032217\n",
            "Iteration 89, loss = 0.48057079\n",
            "Iteration 90, loss = 0.48163173\n",
            "Iteration 91, loss = 0.48031648\n",
            "Iteration 92, loss = 0.47936347\n",
            "Iteration 93, loss = 0.47888769\n",
            "Iteration 94, loss = 0.47807031\n",
            "Iteration 95, loss = 0.47792276\n",
            "Iteration 96, loss = 0.47669075\n",
            "Iteration 97, loss = 0.47703885\n",
            "Iteration 98, loss = 0.47652764\n",
            "Iteration 99, loss = 0.47698218\n",
            "Iteration 100, loss = 0.47609304\n",
            "Iteration 1, loss = 0.83713456\n",
            "Iteration 2, loss = 0.66083566\n",
            "Iteration 3, loss = 0.62354131\n",
            "Iteration 4, loss = 0.60812662\n",
            "Iteration 5, loss = 0.59948788\n",
            "Iteration 6, loss = 0.59562370\n",
            "Iteration 7, loss = 0.58895751\n",
            "Iteration 8, loss = 0.57947492\n",
            "Iteration 9, loss = 0.57067064\n",
            "Iteration 10, loss = 0.56219342\n",
            "Iteration 11, loss = 0.55698371\n",
            "Iteration 12, loss = 0.55391316\n",
            "Iteration 13, loss = 0.55129977\n",
            "Iteration 14, loss = 0.54890671\n",
            "Iteration 15, loss = 0.54643885\n",
            "Iteration 16, loss = 0.54297314\n",
            "Iteration 17, loss = 0.54102246\n",
            "Iteration 18, loss = 0.54129445\n",
            "Iteration 19, loss = 0.53813457\n",
            "Iteration 20, loss = 0.53584825\n",
            "Iteration 21, loss = 0.53505185\n",
            "Iteration 22, loss = 0.53380029\n",
            "Iteration 23, loss = 0.53263117\n",
            "Iteration 24, loss = 0.53059466\n",
            "Iteration 25, loss = 0.53043742\n",
            "Iteration 26, loss = 0.52878689\n",
            "Iteration 27, loss = 0.52721957\n",
            "Iteration 28, loss = 0.52509053\n",
            "Iteration 29, loss = 0.52463802\n",
            "Iteration 30, loss = 0.52600622\n",
            "Iteration 31, loss = 0.52533686\n",
            "Iteration 32, loss = 0.52282068\n",
            "Iteration 33, loss = 0.52207370\n",
            "Iteration 34, loss = 0.52197151\n",
            "Iteration 35, loss = 0.52050410\n",
            "Iteration 36, loss = 0.51890510\n",
            "Iteration 37, loss = 0.51812398\n",
            "Iteration 38, loss = 0.51735240\n",
            "Iteration 39, loss = 0.51699532\n",
            "Iteration 40, loss = 0.51686667\n",
            "Iteration 41, loss = 0.51610695\n",
            "Iteration 42, loss = 0.51669172\n",
            "Iteration 43, loss = 0.51322902\n",
            "Iteration 44, loss = 0.51318415\n",
            "Iteration 45, loss = 0.51185272\n",
            "Iteration 46, loss = 0.51094082\n",
            "Iteration 47, loss = 0.51111908\n",
            "Iteration 48, loss = 0.50958323\n",
            "Iteration 49, loss = 0.51071506\n",
            "Iteration 50, loss = 0.50797224\n",
            "Iteration 51, loss = 0.50855548\n",
            "Iteration 52, loss = 0.50619339\n",
            "Iteration 53, loss = 0.50607627\n",
            "Iteration 54, loss = 0.50619348\n",
            "Iteration 55, loss = 0.50297138\n",
            "Iteration 56, loss = 0.50363331\n",
            "Iteration 57, loss = 0.50120507\n",
            "Iteration 58, loss = 0.50208289\n",
            "Iteration 59, loss = 0.50219130\n",
            "Iteration 60, loss = 0.50081060\n",
            "Iteration 61, loss = 0.50034864\n",
            "Iteration 62, loss = 0.49877738\n",
            "Iteration 63, loss = 0.49889385\n",
            "Iteration 64, loss = 0.49910741\n",
            "Iteration 65, loss = 0.49653541\n",
            "Iteration 66, loss = 0.49809058\n",
            "Iteration 67, loss = 0.49694000\n",
            "Iteration 68, loss = 0.49540907\n",
            "Iteration 69, loss = 0.49503097\n",
            "Iteration 70, loss = 0.49322977\n",
            "Iteration 71, loss = 0.49398712\n",
            "Iteration 72, loss = 0.49298884\n",
            "Iteration 73, loss = 0.49221089\n",
            "Iteration 74, loss = 0.49056898\n",
            "Iteration 75, loss = 0.49090439\n",
            "Iteration 76, loss = 0.49178349\n",
            "Iteration 77, loss = 0.48995355\n",
            "Iteration 78, loss = 0.48955926\n",
            "Iteration 79, loss = 0.49071445\n",
            "Iteration 80, loss = 0.48789645\n",
            "Iteration 81, loss = 0.49334601\n",
            "Iteration 82, loss = 0.48766543\n",
            "Iteration 83, loss = 0.49117912\n",
            "Iteration 84, loss = 0.49090990\n",
            "Iteration 85, loss = 0.49150852\n",
            "Iteration 86, loss = 0.48896881\n",
            "Iteration 87, loss = 0.48385274\n",
            "Iteration 88, loss = 0.48921794\n",
            "Iteration 89, loss = 0.48589666\n",
            "Iteration 90, loss = 0.48460451\n",
            "Iteration 91, loss = 0.48482953\n",
            "Iteration 92, loss = 0.48133251\n",
            "Iteration 93, loss = 0.47927352\n",
            "Iteration 94, loss = 0.47865817\n",
            "Iteration 95, loss = 0.47757052\n",
            "Iteration 96, loss = 0.47917336\n",
            "Iteration 97, loss = 0.47724566\n",
            "Iteration 98, loss = 0.47747433\n",
            "Iteration 99, loss = 0.47479210\n",
            "Iteration 100, loss = 0.47134891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.84068350\n",
            "Iteration 2, loss = 0.66163454\n",
            "Iteration 3, loss = 0.62089339\n",
            "Iteration 4, loss = 0.60367884\n",
            "Iteration 5, loss = 0.59758809\n",
            "Iteration 6, loss = 0.59053744\n",
            "Iteration 7, loss = 0.58355859\n",
            "Iteration 8, loss = 0.57575257\n",
            "Iteration 9, loss = 0.56752153\n",
            "Iteration 10, loss = 0.56250510\n",
            "Iteration 11, loss = 0.55940582\n",
            "Iteration 12, loss = 0.55725922\n",
            "Iteration 13, loss = 0.55539372\n",
            "Iteration 14, loss = 0.55261931\n",
            "Iteration 15, loss = 0.55125222\n",
            "Iteration 16, loss = 0.54938656\n",
            "Iteration 17, loss = 0.54723075\n",
            "Iteration 18, loss = 0.54611201\n",
            "Iteration 19, loss = 0.54445852\n",
            "Iteration 20, loss = 0.54260693\n",
            "Iteration 21, loss = 0.54191735\n",
            "Iteration 22, loss = 0.54076356\n",
            "Iteration 23, loss = 0.54072044\n",
            "Iteration 24, loss = 0.53816883\n",
            "Iteration 25, loss = 0.53735542\n",
            "Iteration 26, loss = 0.53648829\n",
            "Iteration 27, loss = 0.53660585\n",
            "Iteration 28, loss = 0.53461130\n",
            "Iteration 29, loss = 0.53387902\n",
            "Iteration 30, loss = 0.53469368\n",
            "Iteration 31, loss = 0.53202572\n",
            "Iteration 32, loss = 0.52926070\n",
            "Iteration 33, loss = 0.52892232\n",
            "Iteration 34, loss = 0.52792346\n",
            "Iteration 35, loss = 0.52631842\n",
            "Iteration 36, loss = 0.52525013\n",
            "Iteration 37, loss = 0.52366796\n",
            "Iteration 38, loss = 0.52165717\n",
            "Iteration 39, loss = 0.52017580\n",
            "Iteration 40, loss = 0.52051896\n",
            "Iteration 41, loss = 0.51962111\n",
            "Iteration 42, loss = 0.51778834\n",
            "Iteration 43, loss = 0.51575968\n",
            "Iteration 44, loss = 0.51410563\n",
            "Iteration 45, loss = 0.51679264\n",
            "Iteration 46, loss = 0.51113185\n",
            "Iteration 47, loss = 0.51272641\n",
            "Iteration 48, loss = 0.51033363\n",
            "Iteration 49, loss = 0.51149875\n",
            "Iteration 50, loss = 0.50972279\n",
            "Iteration 51, loss = 0.50659960\n",
            "Iteration 52, loss = 0.50812025\n",
            "Iteration 53, loss = 0.50583664\n",
            "Iteration 54, loss = 0.50413478\n",
            "Iteration 55, loss = 0.50309056\n",
            "Iteration 56, loss = 0.50207817\n",
            "Iteration 57, loss = 0.50064145\n",
            "Iteration 58, loss = 0.50182169\n",
            "Iteration 59, loss = 0.50027190\n",
            "Iteration 60, loss = 0.50034093\n",
            "Iteration 61, loss = 0.50228740\n",
            "Iteration 62, loss = 0.49936254\n",
            "Iteration 63, loss = 0.49841749\n",
            "Iteration 64, loss = 0.49726227\n",
            "Iteration 65, loss = 0.49574820\n",
            "Iteration 66, loss = 0.49597583\n",
            "Iteration 67, loss = 0.49666907\n",
            "Iteration 68, loss = 0.49781259\n",
            "Iteration 69, loss = 0.49598924\n",
            "Iteration 70, loss = 0.49504260\n",
            "Iteration 71, loss = 0.49511654\n",
            "Iteration 72, loss = 0.49587326\n",
            "Iteration 73, loss = 0.49325396\n",
            "Iteration 74, loss = 0.49171675\n",
            "Iteration 75, loss = 0.49379349\n",
            "Iteration 76, loss = 0.49232947\n",
            "Iteration 77, loss = 0.49154613\n",
            "Iteration 78, loss = 0.48956218\n",
            "Iteration 79, loss = 0.49085117\n",
            "Iteration 80, loss = 0.48951285\n",
            "Iteration 81, loss = 0.48964164\n",
            "Iteration 82, loss = 0.48776003\n",
            "Iteration 83, loss = 0.48866045\n",
            "Iteration 84, loss = 0.48743867\n",
            "Iteration 85, loss = 0.48959427\n",
            "Iteration 86, loss = 0.48893899\n",
            "Iteration 87, loss = 0.48581596\n",
            "Iteration 88, loss = 0.48958197\n",
            "Iteration 89, loss = 0.48734166\n",
            "Iteration 90, loss = 0.48549654\n",
            "Iteration 91, loss = 0.48475242\n",
            "Iteration 92, loss = 0.48860945\n",
            "Iteration 93, loss = 0.48724563\n",
            "Iteration 94, loss = 0.48767697\n",
            "Iteration 95, loss = 0.48842445\n",
            "Iteration 96, loss = 0.48431945\n",
            "Iteration 97, loss = 0.48893577\n",
            "Iteration 98, loss = 0.48836483\n",
            "Iteration 99, loss = 0.48372760\n",
            "Iteration 100, loss = 0.48331204\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 100 and for layer number 5 : 0.6849999999999999\n",
            "Iteration 1, loss = 1.17918922\n",
            "Iteration 2, loss = 0.68685959\n",
            "Iteration 3, loss = 0.66908384\n",
            "Iteration 4, loss = 0.63908765\n",
            "Iteration 5, loss = 0.59631417\n",
            "Iteration 6, loss = 0.57411322\n",
            "Iteration 7, loss = 0.56515132\n",
            "Iteration 8, loss = 0.56235259\n",
            "Iteration 9, loss = 0.55879602\n",
            "Iteration 10, loss = 0.55343311\n",
            "Iteration 11, loss = 0.54937387\n",
            "Iteration 12, loss = 0.54565812\n",
            "Iteration 13, loss = 0.54212281\n",
            "Iteration 14, loss = 0.53924739\n",
            "Iteration 15, loss = 0.53679909\n",
            "Iteration 16, loss = 0.53377760\n",
            "Iteration 17, loss = 0.53062494\n",
            "Iteration 18, loss = 0.52882499\n",
            "Iteration 19, loss = 0.52691208\n",
            "Iteration 20, loss = 0.52429166\n",
            "Iteration 21, loss = 0.52287004\n",
            "Iteration 22, loss = 0.52153317\n",
            "Iteration 23, loss = 0.52034916\n",
            "Iteration 24, loss = 0.51827497\n",
            "Iteration 25, loss = 0.51660905\n",
            "Iteration 26, loss = 0.51371597\n",
            "Iteration 27, loss = 0.51319660\n",
            "Iteration 28, loss = 0.51266851\n",
            "Iteration 29, loss = 0.51153150\n",
            "Iteration 30, loss = 0.51012592\n",
            "Iteration 31, loss = 0.50761414\n",
            "Iteration 32, loss = 0.50575164\n",
            "Iteration 33, loss = 0.50502220\n",
            "Iteration 34, loss = 0.50257502\n",
            "Iteration 35, loss = 0.50116088\n",
            "Iteration 36, loss = 0.50085492\n",
            "Iteration 37, loss = 0.50006507\n",
            "Iteration 38, loss = 0.50045593\n",
            "Iteration 39, loss = 0.49817018\n",
            "Iteration 40, loss = 0.49616517\n",
            "Iteration 41, loss = 0.49493665\n",
            "Iteration 42, loss = 0.49382371\n",
            "Iteration 43, loss = 0.49116192\n",
            "Iteration 44, loss = 0.49137707\n",
            "Iteration 45, loss = 0.49019324\n",
            "Iteration 46, loss = 0.48951993\n",
            "Iteration 47, loss = 0.48910536\n",
            "Iteration 48, loss = 0.48741551\n",
            "Iteration 49, loss = 0.48718603\n",
            "Iteration 50, loss = 0.48788111\n",
            "Iteration 51, loss = 0.48658319\n",
            "Iteration 52, loss = 0.48428395\n",
            "Iteration 53, loss = 0.48301730\n",
            "Iteration 54, loss = 0.48312539\n",
            "Iteration 55, loss = 0.48457488\n",
            "Iteration 56, loss = 0.48226929\n",
            "Iteration 57, loss = 0.48317217\n",
            "Iteration 58, loss = 0.48192920\n",
            "Iteration 59, loss = 0.48146113\n",
            "Iteration 60, loss = 0.47980896\n",
            "Iteration 61, loss = 0.48194676\n",
            "Iteration 62, loss = 0.48177424\n",
            "Iteration 63, loss = 0.47940223\n",
            "Iteration 64, loss = 0.47986086\n",
            "Iteration 65, loss = 0.47906337\n",
            "Iteration 66, loss = 0.48176154\n",
            "Iteration 67, loss = 0.47980551\n",
            "Iteration 68, loss = 0.47421164\n",
            "Iteration 69, loss = 0.47866929\n",
            "Iteration 70, loss = 0.47818193\n",
            "Iteration 71, loss = 0.47648211\n",
            "Iteration 72, loss = 0.47809994\n",
            "Iteration 73, loss = 0.47671165\n",
            "Iteration 74, loss = 0.47807209\n",
            "Iteration 75, loss = 0.47386657\n",
            "Iteration 76, loss = 0.47979068\n",
            "Iteration 77, loss = 0.47641987\n",
            "Iteration 78, loss = 0.47519644\n",
            "Iteration 79, loss = 0.47597674\n",
            "Iteration 80, loss = 0.47752150\n",
            "Iteration 81, loss = 0.47264753\n",
            "Iteration 82, loss = 0.47679573\n",
            "Iteration 83, loss = 0.47137165\n",
            "Iteration 84, loss = 0.47241433\n",
            "Iteration 85, loss = 0.47296123\n",
            "Iteration 86, loss = 0.47121904\n",
            "Iteration 87, loss = 0.46978595\n",
            "Iteration 88, loss = 0.46902987\n",
            "Iteration 89, loss = 0.47068566\n",
            "Iteration 90, loss = 0.47041782\n",
            "Iteration 91, loss = 0.46913431\n",
            "Iteration 92, loss = 0.46712164\n",
            "Iteration 93, loss = 0.46786863\n",
            "Iteration 94, loss = 0.47072284\n",
            "Iteration 95, loss = 0.46650386\n",
            "Iteration 96, loss = 0.46759921\n",
            "Iteration 97, loss = 0.46723491\n",
            "Iteration 98, loss = 0.46798436\n",
            "Iteration 99, loss = 0.46617021\n",
            "Iteration 100, loss = 0.46677471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.16748585\n",
            "Iteration 2, loss = 0.68435580\n",
            "Iteration 3, loss = 0.66513531\n",
            "Iteration 4, loss = 0.63465831\n",
            "Iteration 5, loss = 0.59114244\n",
            "Iteration 6, loss = 0.57003818\n",
            "Iteration 7, loss = 0.56114980\n",
            "Iteration 8, loss = 0.55536206\n",
            "Iteration 9, loss = 0.55068060\n",
            "Iteration 10, loss = 0.54421841\n",
            "Iteration 11, loss = 0.53917315\n",
            "Iteration 12, loss = 0.53756731\n",
            "Iteration 13, loss = 0.53401793\n",
            "Iteration 14, loss = 0.53102906\n",
            "Iteration 15, loss = 0.52923240\n",
            "Iteration 16, loss = 0.52842161\n",
            "Iteration 17, loss = 0.52571400\n",
            "Iteration 18, loss = 0.52352226\n",
            "Iteration 19, loss = 0.52236147\n",
            "Iteration 20, loss = 0.52094060\n",
            "Iteration 21, loss = 0.51926264\n",
            "Iteration 22, loss = 0.51803912\n",
            "Iteration 23, loss = 0.51743550\n",
            "Iteration 24, loss = 0.51611098\n",
            "Iteration 25, loss = 0.51484374\n",
            "Iteration 26, loss = 0.51366008\n",
            "Iteration 27, loss = 0.51252271\n",
            "Iteration 28, loss = 0.51171310\n",
            "Iteration 29, loss = 0.51033110\n",
            "Iteration 30, loss = 0.51004992\n",
            "Iteration 31, loss = 0.50854952\n",
            "Iteration 32, loss = 0.50724227\n",
            "Iteration 33, loss = 0.50680377\n",
            "Iteration 34, loss = 0.50552425\n",
            "Iteration 35, loss = 0.50445364\n",
            "Iteration 36, loss = 0.50348245\n",
            "Iteration 37, loss = 0.50306294\n",
            "Iteration 38, loss = 0.50302057\n",
            "Iteration 39, loss = 0.50034962\n",
            "Iteration 40, loss = 0.49685972\n",
            "Iteration 41, loss = 0.49725617\n",
            "Iteration 42, loss = 0.49689740\n",
            "Iteration 43, loss = 0.49322015\n",
            "Iteration 44, loss = 0.49091816\n",
            "Iteration 45, loss = 0.49002955\n",
            "Iteration 46, loss = 0.48930378\n",
            "Iteration 47, loss = 0.48750406\n",
            "Iteration 48, loss = 0.48563755\n",
            "Iteration 49, loss = 0.48510987\n",
            "Iteration 50, loss = 0.48263117\n",
            "Iteration 51, loss = 0.48163564\n",
            "Iteration 52, loss = 0.48019692\n",
            "Iteration 53, loss = 0.47899467\n",
            "Iteration 54, loss = 0.47797759\n",
            "Iteration 55, loss = 0.47806081\n",
            "Iteration 56, loss = 0.47472758\n",
            "Iteration 57, loss = 0.47475328\n",
            "Iteration 58, loss = 0.48037708\n",
            "Iteration 59, loss = 0.47454856\n",
            "Iteration 60, loss = 0.47150624\n",
            "Iteration 61, loss = 0.47368171\n",
            "Iteration 62, loss = 0.47159185\n",
            "Iteration 63, loss = 0.47018719\n",
            "Iteration 64, loss = 0.47013049\n",
            "Iteration 65, loss = 0.46900835\n",
            "Iteration 66, loss = 0.47222578\n",
            "Iteration 67, loss = 0.46624930\n",
            "Iteration 68, loss = 0.46616882\n",
            "Iteration 69, loss = 0.46744872\n",
            "Iteration 70, loss = 0.46542103\n",
            "Iteration 71, loss = 0.46563050\n",
            "Iteration 72, loss = 0.46410833\n",
            "Iteration 73, loss = 0.46574532\n",
            "Iteration 74, loss = 0.46582344\n",
            "Iteration 75, loss = 0.46387258\n",
            "Iteration 76, loss = 0.46665653\n",
            "Iteration 77, loss = 0.46445674\n",
            "Iteration 78, loss = 0.45919444\n",
            "Iteration 79, loss = 0.46157350\n",
            "Iteration 80, loss = 0.45949043\n",
            "Iteration 81, loss = 0.45832037\n",
            "Iteration 82, loss = 0.46105550\n",
            "Iteration 83, loss = 0.45602345\n",
            "Iteration 84, loss = 0.45494166\n",
            "Iteration 85, loss = 0.45595838\n",
            "Iteration 86, loss = 0.45507863\n",
            "Iteration 87, loss = 0.45255795\n",
            "Iteration 88, loss = 0.45389850\n",
            "Iteration 89, loss = 0.45556347\n",
            "Iteration 90, loss = 0.45402081\n",
            "Iteration 91, loss = 0.45350112\n",
            "Iteration 92, loss = 0.45323924\n",
            "Iteration 93, loss = 0.45354713\n",
            "Iteration 94, loss = 0.45469343\n",
            "Iteration 95, loss = 0.45434926\n",
            "Iteration 96, loss = 0.45160621\n",
            "Iteration 97, loss = 0.45766711\n",
            "Iteration 98, loss = 0.45407384\n",
            "Iteration 99, loss = 0.45107557\n",
            "Iteration 100, loss = 0.45150522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.16546862\n",
            "Iteration 2, loss = 0.68633174\n",
            "Iteration 3, loss = 0.66932821\n",
            "Iteration 4, loss = 0.63316451\n",
            "Iteration 5, loss = 0.58313492\n",
            "Iteration 6, loss = 0.56010652\n",
            "Iteration 7, loss = 0.55169390\n",
            "Iteration 8, loss = 0.54727909\n",
            "Iteration 9, loss = 0.54056381\n",
            "Iteration 10, loss = 0.53462264\n",
            "Iteration 11, loss = 0.53074940\n",
            "Iteration 12, loss = 0.52810966\n",
            "Iteration 13, loss = 0.52335327\n",
            "Iteration 14, loss = 0.51907926\n",
            "Iteration 15, loss = 0.51620450\n",
            "Iteration 16, loss = 0.51215303\n",
            "Iteration 17, loss = 0.50909448\n",
            "Iteration 18, loss = 0.50678571\n",
            "Iteration 19, loss = 0.50463134\n",
            "Iteration 20, loss = 0.50275679\n",
            "Iteration 21, loss = 0.50193707\n",
            "Iteration 22, loss = 0.50067535\n",
            "Iteration 23, loss = 0.49933591\n",
            "Iteration 24, loss = 0.49650228\n",
            "Iteration 25, loss = 0.49555558\n",
            "Iteration 26, loss = 0.49408503\n",
            "Iteration 27, loss = 0.49212077\n",
            "Iteration 28, loss = 0.49039763\n",
            "Iteration 29, loss = 0.49091874\n",
            "Iteration 30, loss = 0.48948791\n",
            "Iteration 31, loss = 0.48581534\n",
            "Iteration 32, loss = 0.48869327\n",
            "Iteration 33, loss = 0.48897904\n",
            "Iteration 34, loss = 0.48535231\n",
            "Iteration 35, loss = 0.48291656\n",
            "Iteration 36, loss = 0.48277159\n",
            "Iteration 37, loss = 0.48203690\n",
            "Iteration 38, loss = 0.48011133\n",
            "Iteration 39, loss = 0.47814519\n",
            "Iteration 40, loss = 0.47752528\n",
            "Iteration 41, loss = 0.47808257\n",
            "Iteration 42, loss = 0.47548653\n",
            "Iteration 43, loss = 0.47488039\n",
            "Iteration 44, loss = 0.47215938\n",
            "Iteration 45, loss = 0.47241277\n",
            "Iteration 46, loss = 0.47041245\n",
            "Iteration 47, loss = 0.46855368\n",
            "Iteration 48, loss = 0.46766108\n",
            "Iteration 49, loss = 0.46726617\n",
            "Iteration 50, loss = 0.46619583\n",
            "Iteration 51, loss = 0.46808418\n",
            "Iteration 52, loss = 0.46650417\n",
            "Iteration 53, loss = 0.46421970\n",
            "Iteration 54, loss = 0.46349735\n",
            "Iteration 55, loss = 0.46203259\n",
            "Iteration 56, loss = 0.45977442\n",
            "Iteration 57, loss = 0.46027057\n",
            "Iteration 58, loss = 0.46988778\n",
            "Iteration 59, loss = 0.46233902\n",
            "Iteration 60, loss = 0.45988533\n",
            "Iteration 61, loss = 0.46002719\n",
            "Iteration 62, loss = 0.45684772\n",
            "Iteration 63, loss = 0.45614815\n",
            "Iteration 64, loss = 0.45659778\n",
            "Iteration 65, loss = 0.45477147\n",
            "Iteration 66, loss = 0.45365227\n",
            "Iteration 67, loss = 0.45417324\n",
            "Iteration 68, loss = 0.45558637\n",
            "Iteration 69, loss = 0.45327549\n",
            "Iteration 70, loss = 0.45172777\n",
            "Iteration 71, loss = 0.45299898\n",
            "Iteration 72, loss = 0.44959190\n",
            "Iteration 73, loss = 0.45170402\n",
            "Iteration 74, loss = 0.44775769\n",
            "Iteration 75, loss = 0.45124258\n",
            "Iteration 76, loss = 0.45279056\n",
            "Iteration 77, loss = 0.45367221\n",
            "Iteration 78, loss = 0.45209065\n",
            "Iteration 79, loss = 0.44550968\n",
            "Iteration 80, loss = 0.44876667\n",
            "Iteration 81, loss = 0.45109419\n",
            "Iteration 82, loss = 0.44660740\n",
            "Iteration 83, loss = 0.44514269\n",
            "Iteration 84, loss = 0.44699843\n",
            "Iteration 85, loss = 0.44512059\n",
            "Iteration 86, loss = 0.44512139\n",
            "Iteration 87, loss = 0.44485383\n",
            "Iteration 88, loss = 0.44676667\n",
            "Iteration 89, loss = 0.44270317\n",
            "Iteration 90, loss = 0.44603391\n",
            "Iteration 91, loss = 0.44541570\n",
            "Iteration 92, loss = 0.44201438\n",
            "Iteration 93, loss = 0.44779921\n",
            "Iteration 94, loss = 0.44237476\n",
            "Iteration 95, loss = 0.44077190\n",
            "Iteration 96, loss = 0.44335455\n",
            "Iteration 97, loss = 0.44102656\n",
            "Iteration 98, loss = 0.44041186\n",
            "Iteration 99, loss = 0.44373161\n",
            "Iteration 100, loss = 0.44043808\n",
            "Iteration 1, loss = 1.17232581\n",
            "Iteration 2, loss = 0.68571124\n",
            "Iteration 3, loss = 0.67888710\n",
            "Iteration 4, loss = 0.64673825\n",
            "Iteration 5, loss = 0.59794892\n",
            "Iteration 6, loss = 0.57312271\n",
            "Iteration 7, loss = 0.56466572\n",
            "Iteration 8, loss = 0.56229608\n",
            "Iteration 9, loss = 0.55561310\n",
            "Iteration 10, loss = 0.54912067\n",
            "Iteration 11, loss = 0.54482135\n",
            "Iteration 12, loss = 0.54181354\n",
            "Iteration 13, loss = 0.53790741\n",
            "Iteration 14, loss = 0.53433278\n",
            "Iteration 15, loss = 0.52898001\n",
            "Iteration 16, loss = 0.52559670\n",
            "Iteration 17, loss = 0.52245148\n",
            "Iteration 18, loss = 0.52076866\n",
            "Iteration 19, loss = 0.51855342\n",
            "Iteration 20, loss = 0.51651247\n",
            "Iteration 21, loss = 0.51437681\n",
            "Iteration 22, loss = 0.51293226\n",
            "Iteration 23, loss = 0.51158435\n",
            "Iteration 24, loss = 0.51007435\n",
            "Iteration 25, loss = 0.50918096\n",
            "Iteration 26, loss = 0.50769243\n",
            "Iteration 27, loss = 0.50590206\n",
            "Iteration 28, loss = 0.50344794\n",
            "Iteration 29, loss = 0.50376017\n",
            "Iteration 30, loss = 0.50259131\n",
            "Iteration 31, loss = 0.49779779\n",
            "Iteration 32, loss = 0.50038877\n",
            "Iteration 33, loss = 0.49986861\n",
            "Iteration 34, loss = 0.49659424\n",
            "Iteration 35, loss = 0.49537589\n",
            "Iteration 36, loss = 0.49442566\n",
            "Iteration 37, loss = 0.49355529\n",
            "Iteration 38, loss = 0.49079018\n",
            "Iteration 39, loss = 0.49320428\n",
            "Iteration 40, loss = 0.49091291\n",
            "Iteration 41, loss = 0.49177786\n",
            "Iteration 42, loss = 0.48780397\n",
            "Iteration 43, loss = 0.48707865\n",
            "Iteration 44, loss = 0.48403883\n",
            "Iteration 45, loss = 0.48323445\n",
            "Iteration 46, loss = 0.48137168\n",
            "Iteration 47, loss = 0.48031872\n",
            "Iteration 48, loss = 0.47925227\n",
            "Iteration 49, loss = 0.47750569\n",
            "Iteration 50, loss = 0.47614650\n",
            "Iteration 51, loss = 0.47494394\n",
            "Iteration 52, loss = 0.47613063\n",
            "Iteration 53, loss = 0.47203151\n",
            "Iteration 54, loss = 0.47099025\n",
            "Iteration 55, loss = 0.47014201\n",
            "Iteration 56, loss = 0.46862588\n",
            "Iteration 57, loss = 0.46814231\n",
            "Iteration 58, loss = 0.46903360\n",
            "Iteration 59, loss = 0.46719988\n",
            "Iteration 60, loss = 0.46525691\n",
            "Iteration 61, loss = 0.46857972\n",
            "Iteration 62, loss = 0.46629160\n",
            "Iteration 63, loss = 0.46516734\n",
            "Iteration 64, loss = 0.46558111\n",
            "Iteration 65, loss = 0.46518520\n",
            "Iteration 66, loss = 0.46280131\n",
            "Iteration 67, loss = 0.46380183\n",
            "Iteration 68, loss = 0.46591917\n",
            "Iteration 69, loss = 0.46315412\n",
            "Iteration 70, loss = 0.46036406\n",
            "Iteration 71, loss = 0.46176288\n",
            "Iteration 72, loss = 0.45721844\n",
            "Iteration 73, loss = 0.46086504\n",
            "Iteration 74, loss = 0.45675920\n",
            "Iteration 75, loss = 0.45976963\n",
            "Iteration 76, loss = 0.45768641\n",
            "Iteration 77, loss = 0.45566741\n",
            "Iteration 78, loss = 0.45668412\n",
            "Iteration 79, loss = 0.45701608\n",
            "Iteration 80, loss = 0.45535166\n",
            "Iteration 81, loss = 0.45516166\n",
            "Iteration 82, loss = 0.45334298\n",
            "Iteration 83, loss = 0.45488815\n",
            "Iteration 84, loss = 0.45266330\n",
            "Iteration 85, loss = 0.45515735\n",
            "Iteration 86, loss = 0.45433561\n",
            "Iteration 87, loss = 0.45214455\n",
            "Iteration 88, loss = 0.45610070\n",
            "Iteration 89, loss = 0.45281270\n",
            "Iteration 90, loss = 0.45580547\n",
            "Iteration 91, loss = 0.45791238\n",
            "Iteration 92, loss = 0.45517448\n",
            "Iteration 93, loss = 0.45607128\n",
            "Iteration 94, loss = 0.45501342\n",
            "Iteration 95, loss = 0.45437550\n",
            "Iteration 96, loss = 0.45452545\n",
            "Iteration 97, loss = 0.45300895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 98, loss = 0.45116718\n",
            "Iteration 99, loss = 0.45147916\n",
            "Iteration 100, loss = 0.45130028\n",
            "Iteration 1, loss = 1.16332810\n",
            "Iteration 2, loss = 0.68618232\n",
            "Iteration 3, loss = 0.68913900\n",
            "Iteration 4, loss = 0.65398233\n",
            "Iteration 5, loss = 0.60390842\n",
            "Iteration 6, loss = 0.57885852\n",
            "Iteration 7, loss = 0.57025920\n",
            "Iteration 8, loss = 0.56663455\n",
            "Iteration 9, loss = 0.55823330\n",
            "Iteration 10, loss = 0.55248266\n",
            "Iteration 11, loss = 0.54848967\n",
            "Iteration 12, loss = 0.54592930\n",
            "Iteration 13, loss = 0.54329974\n",
            "Iteration 14, loss = 0.54033888\n",
            "Iteration 15, loss = 0.53572275\n",
            "Iteration 16, loss = 0.53346657\n",
            "Iteration 17, loss = 0.53074143\n",
            "Iteration 18, loss = 0.52813994\n",
            "Iteration 19, loss = 0.52519975\n",
            "Iteration 20, loss = 0.52278328\n",
            "Iteration 21, loss = 0.52152420\n",
            "Iteration 22, loss = 0.52051844\n",
            "Iteration 23, loss = 0.51838123\n",
            "Iteration 24, loss = 0.51528032\n",
            "Iteration 25, loss = 0.51406820\n",
            "Iteration 26, loss = 0.51326190\n",
            "Iteration 27, loss = 0.51152712\n",
            "Iteration 28, loss = 0.50905238\n",
            "Iteration 29, loss = 0.50776352\n",
            "Iteration 30, loss = 0.50738792\n",
            "Iteration 31, loss = 0.50674779\n",
            "Iteration 32, loss = 0.50443804\n",
            "Iteration 33, loss = 0.50413038\n",
            "Iteration 34, loss = 0.50267196\n",
            "Iteration 35, loss = 0.50345086\n",
            "Iteration 36, loss = 0.50351132\n",
            "Iteration 37, loss = 0.50228991\n",
            "Iteration 38, loss = 0.50019602\n",
            "Iteration 39, loss = 0.50010008\n",
            "Iteration 40, loss = 0.49891127\n",
            "Iteration 41, loss = 0.49782462\n",
            "Iteration 42, loss = 0.49613568\n",
            "Iteration 43, loss = 0.49638432\n",
            "Iteration 44, loss = 0.49150995\n",
            "Iteration 45, loss = 0.49590998\n",
            "Iteration 46, loss = 0.49315333\n",
            "Iteration 47, loss = 0.49098981\n",
            "Iteration 48, loss = 0.48868236\n",
            "Iteration 49, loss = 0.48880587\n",
            "Iteration 50, loss = 0.48782580\n",
            "Iteration 51, loss = 0.48398444\n",
            "Iteration 52, loss = 0.48677887\n",
            "Iteration 53, loss = 0.48429541\n",
            "Iteration 54, loss = 0.48259965\n",
            "Iteration 55, loss = 0.48282400\n",
            "Iteration 56, loss = 0.48193661\n",
            "Iteration 57, loss = 0.48142843\n",
            "Iteration 58, loss = 0.48238175\n",
            "Iteration 59, loss = 0.48125050\n",
            "Iteration 60, loss = 0.48075630\n",
            "Iteration 61, loss = 0.48005422\n",
            "Iteration 62, loss = 0.47693954\n",
            "Iteration 63, loss = 0.47850162\n",
            "Iteration 64, loss = 0.47706809\n",
            "Iteration 65, loss = 0.47560735\n",
            "Iteration 66, loss = 0.47619349\n",
            "Iteration 67, loss = 0.47423727\n",
            "Iteration 68, loss = 0.47714908\n",
            "Iteration 69, loss = 0.47224118\n",
            "Iteration 70, loss = 0.47332061\n",
            "Iteration 71, loss = 0.47321793\n",
            "Iteration 72, loss = 0.47258821\n",
            "Iteration 73, loss = 0.47189953\n",
            "Iteration 74, loss = 0.47093795\n",
            "Iteration 75, loss = 0.47350687\n",
            "Iteration 76, loss = 0.47030472\n",
            "Iteration 77, loss = 0.46734921\n",
            "Iteration 78, loss = 0.46952657\n",
            "Iteration 79, loss = 0.47024198\n",
            "Iteration 80, loss = 0.46871231\n",
            "Iteration 81, loss = 0.46987299\n",
            "Iteration 82, loss = 0.46711412\n",
            "Iteration 83, loss = 0.46694432\n",
            "Iteration 84, loss = 0.46505605\n",
            "Iteration 85, loss = 0.46497067\n",
            "Iteration 86, loss = 0.46634769\n",
            "Iteration 87, loss = 0.46120770\n",
            "Iteration 88, loss = 0.46237763\n",
            "Iteration 89, loss = 0.46308429\n",
            "Iteration 90, loss = 0.46505227\n",
            "Iteration 91, loss = 0.46288208\n",
            "Iteration 92, loss = 0.45916148\n",
            "Iteration 93, loss = 0.45643054\n",
            "Iteration 94, loss = 0.45848188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 95, loss = 0.45912885\n",
            "Iteration 96, loss = 0.45837258\n",
            "Iteration 97, loss = 0.45821094\n",
            "Iteration 98, loss = 0.45545213\n",
            "Iteration 99, loss = 0.45392220\n",
            "Iteration 100, loss = 0.45374146\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 100 and for layer number 6 : 0.6987500000000001\n",
            "Iteration 1, loss = 0.61616344\n",
            "Iteration 2, loss = 0.61047446\n",
            "Iteration 3, loss = 0.60478519\n",
            "Iteration 4, loss = 0.60040729\n",
            "Iteration 5, loss = 0.59683886\n",
            "Iteration 6, loss = 0.59352251\n",
            "Iteration 7, loss = 0.59032056\n",
            "Iteration 8, loss = 0.58709569\n",
            "Iteration 9, loss = 0.58414076\n",
            "Iteration 10, loss = 0.58165635\n",
            "Iteration 11, loss = 0.57911060\n",
            "Iteration 12, loss = 0.57790214\n",
            "Iteration 13, loss = 0.57579006\n",
            "Iteration 14, loss = 0.57293731\n",
            "Iteration 15, loss = 0.56984478\n",
            "Iteration 16, loss = 0.56633085\n",
            "Iteration 17, loss = 0.56403178\n",
            "Iteration 18, loss = 0.56081149\n",
            "Iteration 19, loss = 0.55656498\n",
            "Iteration 20, loss = 0.55483738\n",
            "Iteration 21, loss = 0.55165056\n",
            "Iteration 22, loss = 0.55122710\n",
            "Iteration 23, loss = 0.54983455\n",
            "Iteration 24, loss = 0.54824201\n",
            "Iteration 25, loss = 0.54789085\n",
            "Iteration 26, loss = 0.54627285\n",
            "Iteration 27, loss = 0.54523166\n",
            "Iteration 28, loss = 0.54485553\n",
            "Iteration 29, loss = 0.54476942\n",
            "Iteration 30, loss = 0.54341941\n",
            "Iteration 31, loss = 0.54275300\n",
            "Iteration 32, loss = 0.54166800\n",
            "Iteration 33, loss = 0.54111902\n",
            "Iteration 34, loss = 0.54465660\n",
            "Iteration 35, loss = 0.54087738\n",
            "Iteration 36, loss = 0.53954129\n",
            "Iteration 37, loss = 0.54069694\n",
            "Iteration 38, loss = 0.53983298\n",
            "Iteration 39, loss = 0.53907627\n",
            "Iteration 40, loss = 0.53860343\n",
            "Iteration 41, loss = 0.53932428\n",
            "Iteration 42, loss = 0.53915650\n",
            "Iteration 43, loss = 0.53861646\n",
            "Iteration 44, loss = 0.53889930\n",
            "Iteration 45, loss = 0.53773883\n",
            "Iteration 46, loss = 0.53837864\n",
            "Iteration 47, loss = 0.53835545\n",
            "Iteration 48, loss = 0.53696691\n",
            "Iteration 49, loss = 0.53629861\n",
            "Iteration 50, loss = 0.53652009\n",
            "Iteration 51, loss = 0.53844088\n",
            "Iteration 52, loss = 0.53710225\n",
            "Iteration 53, loss = 0.53678021\n",
            "Iteration 54, loss = 0.53589026\n",
            "Iteration 55, loss = 0.53500914\n",
            "Iteration 56, loss = 0.53512312\n",
            "Iteration 57, loss = 0.53536117\n",
            "Iteration 58, loss = 0.53590119\n",
            "Iteration 59, loss = 0.53510110\n",
            "Iteration 60, loss = 0.53468041\n",
            "Iteration 61, loss = 0.53525025\n",
            "Iteration 62, loss = 0.53545549\n",
            "Iteration 63, loss = 0.53428015\n",
            "Iteration 64, loss = 0.53534948\n",
            "Iteration 65, loss = 0.53517115\n",
            "Iteration 66, loss = 0.53455759\n",
            "Iteration 67, loss = 0.53467990\n",
            "Iteration 68, loss = 0.53436042\n",
            "Iteration 69, loss = 0.53448972\n",
            "Iteration 70, loss = 0.53444176\n",
            "Iteration 71, loss = 0.53474912\n",
            "Iteration 72, loss = 0.53471431\n",
            "Iteration 73, loss = 0.53404600\n",
            "Iteration 74, loss = 0.53413797\n",
            "Iteration 75, loss = 0.53474503\n",
            "Iteration 76, loss = 0.53460398\n",
            "Iteration 77, loss = 0.53496211\n",
            "Iteration 78, loss = 0.53504312\n",
            "Iteration 79, loss = 0.53361651\n",
            "Iteration 80, loss = 0.53294444\n",
            "Iteration 81, loss = 0.53551544\n",
            "Iteration 82, loss = 0.53583933\n",
            "Iteration 83, loss = 0.53495348\n",
            "Iteration 84, loss = 0.53577048\n",
            "Iteration 85, loss = 0.53327554\n",
            "Iteration 86, loss = 0.53315024\n",
            "Iteration 87, loss = 0.53348090\n",
            "Iteration 88, loss = 0.53214641\n",
            "Iteration 89, loss = 0.53452553\n",
            "Iteration 90, loss = 0.53339749\n",
            "Iteration 91, loss = 0.53408545\n",
            "Iteration 92, loss = 0.53402317\n",
            "Iteration 93, loss = 0.53249531\n",
            "Iteration 94, loss = 0.53232751\n",
            "Iteration 95, loss = 0.53441131\n",
            "Iteration 96, loss = 0.53359358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 97, loss = 0.53233717\n",
            "Iteration 98, loss = 0.53477685\n",
            "Iteration 99, loss = 0.53576123\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61685270\n",
            "Iteration 2, loss = 0.61073358\n",
            "Iteration 3, loss = 0.60482747\n",
            "Iteration 4, loss = 0.60021840\n",
            "Iteration 5, loss = 0.59678875\n",
            "Iteration 6, loss = 0.59294638\n",
            "Iteration 7, loss = 0.58982852\n",
            "Iteration 8, loss = 0.58727008\n",
            "Iteration 9, loss = 0.58561090\n",
            "Iteration 10, loss = 0.58412723\n",
            "Iteration 11, loss = 0.58439637\n",
            "Iteration 12, loss = 0.58235275\n",
            "Iteration 13, loss = 0.58096798\n",
            "Iteration 14, loss = 0.57891433\n",
            "Iteration 15, loss = 0.57650903\n",
            "Iteration 16, loss = 0.57419914\n",
            "Iteration 17, loss = 0.57181285\n",
            "Iteration 18, loss = 0.56990192\n",
            "Iteration 19, loss = 0.56742489\n",
            "Iteration 20, loss = 0.56484876\n",
            "Iteration 21, loss = 0.56171823\n",
            "Iteration 22, loss = 0.55955647\n",
            "Iteration 23, loss = 0.55886560\n",
            "Iteration 24, loss = 0.55706339\n",
            "Iteration 25, loss = 0.55553422\n",
            "Iteration 26, loss = 0.55262330\n",
            "Iteration 27, loss = 0.55047921\n",
            "Iteration 28, loss = 0.54972788\n",
            "Iteration 29, loss = 0.54861285\n",
            "Iteration 30, loss = 0.54843663\n",
            "Iteration 31, loss = 0.54698559\n",
            "Iteration 32, loss = 0.54512202\n",
            "Iteration 33, loss = 0.54496236\n",
            "Iteration 34, loss = 0.54537399\n",
            "Iteration 35, loss = 0.54621125\n",
            "Iteration 36, loss = 0.54307091\n",
            "Iteration 37, loss = 0.54376705\n",
            "Iteration 38, loss = 0.54240030\n",
            "Iteration 39, loss = 0.54055163\n",
            "Iteration 40, loss = 0.53948219\n",
            "Iteration 41, loss = 0.53937518\n",
            "Iteration 42, loss = 0.53894503\n",
            "Iteration 43, loss = 0.53814838\n",
            "Iteration 44, loss = 0.53717987\n",
            "Iteration 45, loss = 0.53577462\n",
            "Iteration 46, loss = 0.53457548\n",
            "Iteration 47, loss = 0.53370548\n",
            "Iteration 48, loss = 0.53374536\n",
            "Iteration 49, loss = 0.53227638\n",
            "Iteration 50, loss = 0.53107787\n",
            "Iteration 51, loss = 0.53314206\n",
            "Iteration 52, loss = 0.53196477\n",
            "Iteration 53, loss = 0.53088992\n",
            "Iteration 54, loss = 0.53029162\n",
            "Iteration 55, loss = 0.52988151\n",
            "Iteration 56, loss = 0.52888568\n",
            "Iteration 57, loss = 0.52890005\n",
            "Iteration 58, loss = 0.52924797\n",
            "Iteration 59, loss = 0.52921454\n",
            "Iteration 60, loss = 0.52868950\n",
            "Iteration 61, loss = 0.52862123\n",
            "Iteration 62, loss = 0.52993570\n",
            "Iteration 63, loss = 0.52877129\n",
            "Iteration 64, loss = 0.52946871\n",
            "Iteration 65, loss = 0.52981092\n",
            "Iteration 66, loss = 0.53015241\n",
            "Iteration 67, loss = 0.52990268\n",
            "Iteration 68, loss = 0.52903022\n",
            "Iteration 69, loss = 0.52935976\n",
            "Iteration 70, loss = 0.52849976\n",
            "Iteration 71, loss = 0.52799905\n",
            "Iteration 72, loss = 0.52879280\n",
            "Iteration 73, loss = 0.52821299\n",
            "Iteration 74, loss = 0.52730704\n",
            "Iteration 75, loss = 0.52866449\n",
            "Iteration 76, loss = 0.52869782\n",
            "Iteration 77, loss = 0.52738765\n",
            "Iteration 78, loss = 0.52743448\n",
            "Iteration 79, loss = 0.52749131\n",
            "Iteration 80, loss = 0.52713528\n",
            "Iteration 81, loss = 0.52811449\n",
            "Iteration 82, loss = 0.52784675\n",
            "Iteration 83, loss = 0.52592258\n",
            "Iteration 84, loss = 0.52574880\n",
            "Iteration 85, loss = 0.52562074\n",
            "Iteration 86, loss = 0.52562702\n",
            "Iteration 87, loss = 0.52489044\n",
            "Iteration 88, loss = 0.52673234\n",
            "Iteration 89, loss = 0.52780336\n",
            "Iteration 90, loss = 0.52606327\n",
            "Iteration 91, loss = 0.52617697\n",
            "Iteration 92, loss = 0.52557126\n",
            "Iteration 93, loss = 0.52481340\n",
            "Iteration 94, loss = 0.52480812\n",
            "Iteration 95, loss = 0.52540942\n",
            "Iteration 96, loss = 0.52637237\n",
            "Iteration 97, loss = 0.52433287\n",
            "Iteration 98, loss = 0.52539716\n",
            "Iteration 99, loss = 0.52543487\n",
            "Iteration 100, loss = 0.52402267\n",
            "Iteration 101, loss = 0.52394746\n",
            "Iteration 102, loss = 0.52557838\n",
            "Iteration 103, loss = 0.52511001\n",
            "Iteration 104, loss = 0.52432925\n",
            "Iteration 105, loss = 0.52443864\n",
            "Iteration 106, loss = 0.52406957\n",
            "Iteration 107, loss = 0.52459945\n",
            "Iteration 108, loss = 0.52394326\n",
            "Iteration 109, loss = 0.52307189\n",
            "Iteration 110, loss = 0.52400625\n",
            "Iteration 111, loss = 0.52296824\n",
            "Iteration 112, loss = 0.52321968\n",
            "Iteration 113, loss = 0.52327495\n",
            "Iteration 114, loss = 0.52374982\n",
            "Iteration 115, loss = 0.52267798\n",
            "Iteration 116, loss = 0.52300414\n",
            "Iteration 117, loss = 0.52305182\n",
            "Iteration 118, loss = 0.52330732\n",
            "Iteration 119, loss = 0.52477222\n",
            "Iteration 120, loss = 0.52362954\n",
            "Iteration 121, loss = 0.52294899\n",
            "Iteration 122, loss = 0.52279810\n",
            "Iteration 123, loss = 0.52346034\n",
            "Iteration 124, loss = 0.52288044\n",
            "Iteration 125, loss = 0.52290763\n",
            "Iteration 126, loss = 0.52227996\n",
            "Iteration 127, loss = 0.52311677\n",
            "Iteration 128, loss = 0.52422479\n",
            "Iteration 129, loss = 0.52620033\n",
            "Iteration 130, loss = 0.52289523\n",
            "Iteration 131, loss = 0.52251810\n",
            "Iteration 132, loss = 0.52295944\n",
            "Iteration 133, loss = 0.52369208\n",
            "Iteration 134, loss = 0.52439265\n",
            "Iteration 135, loss = 0.52252922\n",
            "Iteration 136, loss = 0.52396035\n",
            "Iteration 137, loss = 0.52261474\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61686379\n",
            "Iteration 2, loss = 0.61078213\n",
            "Iteration 3, loss = 0.60373906\n",
            "Iteration 4, loss = 0.59835244\n",
            "Iteration 5, loss = 0.59457664\n",
            "Iteration 6, loss = 0.59038982\n",
            "Iteration 7, loss = 0.58694864\n",
            "Iteration 8, loss = 0.58122019\n",
            "Iteration 9, loss = 0.57559384\n",
            "Iteration 10, loss = 0.57111279\n",
            "Iteration 11, loss = 0.56767019\n",
            "Iteration 12, loss = 0.56484322\n",
            "Iteration 13, loss = 0.55949772\n",
            "Iteration 14, loss = 0.55649967\n",
            "Iteration 15, loss = 0.55290617\n",
            "Iteration 16, loss = 0.54970478\n",
            "Iteration 17, loss = 0.54607332\n",
            "Iteration 18, loss = 0.54403178\n",
            "Iteration 19, loss = 0.54132068\n",
            "Iteration 20, loss = 0.53854210\n",
            "Iteration 21, loss = 0.53710357\n",
            "Iteration 22, loss = 0.53429746\n",
            "Iteration 23, loss = 0.53375008\n",
            "Iteration 24, loss = 0.53094852\n",
            "Iteration 25, loss = 0.52965028\n",
            "Iteration 26, loss = 0.52815839\n",
            "Iteration 27, loss = 0.52688496\n",
            "Iteration 28, loss = 0.52553353\n",
            "Iteration 29, loss = 0.52394223\n",
            "Iteration 30, loss = 0.52356500\n",
            "Iteration 31, loss = 0.52294680\n",
            "Iteration 32, loss = 0.52183150\n",
            "Iteration 33, loss = 0.52255860\n",
            "Iteration 34, loss = 0.52142071\n",
            "Iteration 35, loss = 0.52125496\n",
            "Iteration 36, loss = 0.52111537\n",
            "Iteration 37, loss = 0.52026383\n",
            "Iteration 38, loss = 0.52045544\n",
            "Iteration 39, loss = 0.52091092\n",
            "Iteration 40, loss = 0.52008196\n",
            "Iteration 41, loss = 0.51990365\n",
            "Iteration 42, loss = 0.51974732\n",
            "Iteration 43, loss = 0.51904939\n",
            "Iteration 44, loss = 0.51941507\n",
            "Iteration 45, loss = 0.51917314\n",
            "Iteration 46, loss = 0.51924421\n",
            "Iteration 47, loss = 0.51928018\n",
            "Iteration 48, loss = 0.51909289\n",
            "Iteration 49, loss = 0.51842013\n",
            "Iteration 50, loss = 0.51802283\n",
            "Iteration 51, loss = 0.51819928\n",
            "Iteration 52, loss = 0.51951413\n",
            "Iteration 53, loss = 0.51949918\n",
            "Iteration 54, loss = 0.52004138\n",
            "Iteration 55, loss = 0.51851564\n",
            "Iteration 56, loss = 0.51716944\n",
            "Iteration 57, loss = 0.51662275\n",
            "Iteration 58, loss = 0.51660941\n",
            "Iteration 59, loss = 0.51655928\n",
            "Iteration 60, loss = 0.51630371\n",
            "Iteration 61, loss = 0.51683368\n",
            "Iteration 62, loss = 0.51674972\n",
            "Iteration 63, loss = 0.51545783\n",
            "Iteration 64, loss = 0.51608693\n",
            "Iteration 65, loss = 0.51590192\n",
            "Iteration 66, loss = 0.51553887\n",
            "Iteration 67, loss = 0.51569551\n",
            "Iteration 68, loss = 0.51562204\n",
            "Iteration 69, loss = 0.51538478\n",
            "Iteration 70, loss = 0.51492148\n",
            "Iteration 71, loss = 0.51418214\n",
            "Iteration 72, loss = 0.51644749\n",
            "Iteration 73, loss = 0.51636934\n",
            "Iteration 74, loss = 0.51514015\n",
            "Iteration 75, loss = 0.51694397\n",
            "Iteration 76, loss = 0.51490108\n",
            "Iteration 77, loss = 0.51324531\n",
            "Iteration 78, loss = 0.51423000\n",
            "Iteration 79, loss = 0.51396088\n",
            "Iteration 80, loss = 0.51433935\n",
            "Iteration 81, loss = 0.51317206\n",
            "Iteration 82, loss = 0.51380235\n",
            "Iteration 83, loss = 0.51273531\n",
            "Iteration 84, loss = 0.51314830\n",
            "Iteration 85, loss = 0.51374298\n",
            "Iteration 86, loss = 0.51415859\n",
            "Iteration 87, loss = 0.51239317\n",
            "Iteration 88, loss = 0.51355738\n",
            "Iteration 89, loss = 0.51420369\n",
            "Iteration 90, loss = 0.51490013\n",
            "Iteration 91, loss = 0.51391767\n",
            "Iteration 92, loss = 0.51283594\n",
            "Iteration 93, loss = 0.51340462\n",
            "Iteration 94, loss = 0.51341917\n",
            "Iteration 95, loss = 0.51351797\n",
            "Iteration 96, loss = 0.51398162\n",
            "Iteration 97, loss = 0.51293432\n",
            "Iteration 98, loss = 0.51275042\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61997020\n",
            "Iteration 2, loss = 0.61412378\n",
            "Iteration 3, loss = 0.60919162\n",
            "Iteration 4, loss = 0.60490434\n",
            "Iteration 5, loss = 0.60251389\n",
            "Iteration 6, loss = 0.59997429\n",
            "Iteration 7, loss = 0.59738037\n",
            "Iteration 8, loss = 0.59269736\n",
            "Iteration 9, loss = 0.58767828\n",
            "Iteration 10, loss = 0.58446028\n",
            "Iteration 11, loss = 0.58104889\n",
            "Iteration 12, loss = 0.57867536\n",
            "Iteration 13, loss = 0.57465469\n",
            "Iteration 14, loss = 0.57118039\n",
            "Iteration 15, loss = 0.56831045\n",
            "Iteration 16, loss = 0.56705962\n",
            "Iteration 17, loss = 0.56455372\n",
            "Iteration 18, loss = 0.56249116\n",
            "Iteration 19, loss = 0.56029054\n",
            "Iteration 20, loss = 0.55870059\n",
            "Iteration 21, loss = 0.55790136\n",
            "Iteration 22, loss = 0.55542813\n",
            "Iteration 23, loss = 0.55448658\n",
            "Iteration 24, loss = 0.55341077\n",
            "Iteration 25, loss = 0.55234914\n",
            "Iteration 26, loss = 0.55099291\n",
            "Iteration 27, loss = 0.54992709\n",
            "Iteration 28, loss = 0.54865229\n",
            "Iteration 29, loss = 0.54789818\n",
            "Iteration 30, loss = 0.54655713\n",
            "Iteration 31, loss = 0.54464837\n",
            "Iteration 32, loss = 0.54318115\n",
            "Iteration 33, loss = 0.54176155\n",
            "Iteration 34, loss = 0.53991255\n",
            "Iteration 35, loss = 0.53939289\n",
            "Iteration 36, loss = 0.53903124\n",
            "Iteration 37, loss = 0.53655695\n",
            "Iteration 38, loss = 0.53469995\n",
            "Iteration 39, loss = 0.53484729\n",
            "Iteration 40, loss = 0.53341719\n",
            "Iteration 41, loss = 0.53159598\n",
            "Iteration 42, loss = 0.53132522\n",
            "Iteration 43, loss = 0.53072165\n",
            "Iteration 44, loss = 0.53090682\n",
            "Iteration 45, loss = 0.53074961\n",
            "Iteration 46, loss = 0.53065336\n",
            "Iteration 47, loss = 0.53033300\n",
            "Iteration 48, loss = 0.52958211\n",
            "Iteration 49, loss = 0.52900039\n",
            "Iteration 50, loss = 0.52795815\n",
            "Iteration 51, loss = 0.52848891\n",
            "Iteration 52, loss = 0.53032016\n",
            "Iteration 53, loss = 0.52827347\n",
            "Iteration 54, loss = 0.52872219\n",
            "Iteration 55, loss = 0.52744500\n",
            "Iteration 56, loss = 0.52682941\n",
            "Iteration 57, loss = 0.52662720\n",
            "Iteration 58, loss = 0.52671375\n",
            "Iteration 59, loss = 0.52668943\n",
            "Iteration 60, loss = 0.52675084\n",
            "Iteration 61, loss = 0.52698153\n",
            "Iteration 62, loss = 0.52731884\n",
            "Iteration 63, loss = 0.52657748\n",
            "Iteration 64, loss = 0.52577505\n",
            "Iteration 65, loss = 0.52717356\n",
            "Iteration 66, loss = 0.52654840\n",
            "Iteration 67, loss = 0.52671599\n",
            "Iteration 68, loss = 0.52702098\n",
            "Iteration 69, loss = 0.52778156\n",
            "Iteration 70, loss = 0.52659366\n",
            "Iteration 71, loss = 0.52593393\n",
            "Iteration 72, loss = 0.52768568\n",
            "Iteration 73, loss = 0.52725121\n",
            "Iteration 74, loss = 0.52653828\n",
            "Iteration 75, loss = 0.52815880\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61941224\n",
            "Iteration 2, loss = 0.61318364\n",
            "Iteration 3, loss = 0.60864231\n",
            "Iteration 4, loss = 0.60454530\n",
            "Iteration 5, loss = 0.60243915\n",
            "Iteration 6, loss = 0.60246689\n",
            "Iteration 7, loss = 0.60083027\n",
            "Iteration 8, loss = 0.59640653\n",
            "Iteration 9, loss = 0.59331881\n",
            "Iteration 10, loss = 0.59175851\n",
            "Iteration 11, loss = 0.58960825\n",
            "Iteration 12, loss = 0.58941496\n",
            "Iteration 13, loss = 0.58594276\n",
            "Iteration 14, loss = 0.58312039\n",
            "Iteration 15, loss = 0.58096844\n",
            "Iteration 16, loss = 0.57971808\n",
            "Iteration 17, loss = 0.58006510\n",
            "Iteration 18, loss = 0.57982617\n",
            "Iteration 19, loss = 0.57840057\n",
            "Iteration 20, loss = 0.57747882\n",
            "Iteration 21, loss = 0.57701133\n",
            "Iteration 22, loss = 0.57534329\n",
            "Iteration 23, loss = 0.57568807\n",
            "Iteration 24, loss = 0.57479372\n",
            "Iteration 25, loss = 0.57414903\n",
            "Iteration 26, loss = 0.57320683\n",
            "Iteration 27, loss = 0.57351214\n",
            "Iteration 28, loss = 0.57212556\n",
            "Iteration 29, loss = 0.57182505\n",
            "Iteration 30, loss = 0.57175012\n",
            "Iteration 31, loss = 0.57021913\n",
            "Iteration 32, loss = 0.56935480\n",
            "Iteration 33, loss = 0.56839796\n",
            "Iteration 34, loss = 0.56693242\n",
            "Iteration 35, loss = 0.56572092\n",
            "Iteration 36, loss = 0.56395992\n",
            "Iteration 37, loss = 0.56338466\n",
            "Iteration 38, loss = 0.56170087\n",
            "Iteration 39, loss = 0.55877045\n",
            "Iteration 40, loss = 0.55793019\n",
            "Iteration 41, loss = 0.55614417\n",
            "Iteration 42, loss = 0.55506357\n",
            "Iteration 43, loss = 0.55343058\n",
            "Iteration 44, loss = 0.55340796\n",
            "Iteration 45, loss = 0.55087697\n",
            "Iteration 46, loss = 0.54966880\n",
            "Iteration 47, loss = 0.54774331\n",
            "Iteration 48, loss = 0.54593504\n",
            "Iteration 49, loss = 0.54453803\n",
            "Iteration 50, loss = 0.54326147\n",
            "Iteration 51, loss = 0.54251671\n",
            "Iteration 52, loss = 0.54193067\n",
            "Iteration 53, loss = 0.54062192\n",
            "Iteration 54, loss = 0.54211250\n",
            "Iteration 55, loss = 0.54021127\n",
            "Iteration 56, loss = 0.53903735\n",
            "Iteration 57, loss = 0.53750337\n",
            "Iteration 58, loss = 0.53657233\n",
            "Iteration 59, loss = 0.53494783\n",
            "Iteration 60, loss = 0.53609512\n",
            "Iteration 61, loss = 0.53605135\n",
            "Iteration 62, loss = 0.53444912\n",
            "Iteration 63, loss = 0.53426413\n",
            "Iteration 64, loss = 0.53256619\n",
            "Iteration 65, loss = 0.53272366\n",
            "Iteration 66, loss = 0.53213028\n",
            "Iteration 67, loss = 0.53294015\n",
            "Iteration 68, loss = 0.53405626\n",
            "Iteration 69, loss = 0.53502634\n",
            "Iteration 70, loss = 0.53220216\n",
            "Iteration 71, loss = 0.53183938\n",
            "Iteration 72, loss = 0.53266801\n",
            "Iteration 73, loss = 0.53181568\n",
            "Iteration 74, loss = 0.53103228\n",
            "Iteration 75, loss = 0.53109650\n",
            "Iteration 76, loss = 0.53120562\n",
            "Iteration 77, loss = 0.53139712\n",
            "Iteration 78, loss = 0.53105354\n",
            "Iteration 79, loss = 0.53115344\n",
            "Iteration 80, loss = 0.53351865\n",
            "Iteration 81, loss = 0.53029266\n",
            "Iteration 82, loss = 0.53141297\n",
            "Iteration 83, loss = 0.53030700\n",
            "Iteration 84, loss = 0.53077603\n",
            "Iteration 85, loss = 0.53117501\n",
            "Iteration 86, loss = 0.53017275\n",
            "Iteration 87, loss = 0.53167091\n",
            "Iteration 88, loss = 0.53110833\n",
            "Iteration 89, loss = 0.53082480\n",
            "Iteration 90, loss = 0.53097012\n",
            "Iteration 91, loss = 0.52953903\n",
            "Iteration 92, loss = 0.53041845\n",
            "Iteration 93, loss = 0.53062090\n",
            "Iteration 94, loss = 0.53060511\n",
            "Iteration 95, loss = 0.52943256\n",
            "Iteration 96, loss = 0.52966999\n",
            "Iteration 97, loss = 0.53005740\n",
            "Iteration 98, loss = 0.53237745\n",
            "Iteration 99, loss = 0.53421484\n",
            "Iteration 100, loss = 0.53217815\n",
            "Iteration 101, loss = 0.53011092\n",
            "Iteration 102, loss = 0.53037191\n",
            "Iteration 103, loss = 0.52964197\n",
            "Iteration 104, loss = 0.53177603\n",
            "Iteration 105, loss = 0.53014765\n",
            "Iteration 106, loss = 0.53351194\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 150 and for layer number 2 : 0.70625\n",
            "Iteration 1, loss = 0.81149998\n",
            "Iteration 2, loss = 0.65873604\n",
            "Iteration 3, loss = 0.59987267\n",
            "Iteration 4, loss = 0.58584785\n",
            "Iteration 5, loss = 0.57524774\n",
            "Iteration 6, loss = 0.57117799\n",
            "Iteration 7, loss = 0.56778662\n",
            "Iteration 8, loss = 0.56432353\n",
            "Iteration 9, loss = 0.56078602\n",
            "Iteration 10, loss = 0.55775868\n",
            "Iteration 11, loss = 0.55650784\n",
            "Iteration 12, loss = 0.55523370\n",
            "Iteration 13, loss = 0.55379450\n",
            "Iteration 14, loss = 0.55218434\n",
            "Iteration 15, loss = 0.54947622\n",
            "Iteration 16, loss = 0.54665060\n",
            "Iteration 17, loss = 0.54742043\n",
            "Iteration 18, loss = 0.54599748\n",
            "Iteration 19, loss = 0.54336523\n",
            "Iteration 20, loss = 0.54192285\n",
            "Iteration 21, loss = 0.54095123\n",
            "Iteration 22, loss = 0.53904375\n",
            "Iteration 23, loss = 0.53750011\n",
            "Iteration 24, loss = 0.53633557\n",
            "Iteration 25, loss = 0.53686758\n",
            "Iteration 26, loss = 0.53616775\n",
            "Iteration 27, loss = 0.53438193\n",
            "Iteration 28, loss = 0.53371599\n",
            "Iteration 29, loss = 0.53315282\n",
            "Iteration 30, loss = 0.53305221\n",
            "Iteration 31, loss = 0.53267854\n",
            "Iteration 32, loss = 0.53127495\n",
            "Iteration 33, loss = 0.53042640\n",
            "Iteration 34, loss = 0.52970462\n",
            "Iteration 35, loss = 0.52915225\n",
            "Iteration 36, loss = 0.52950701\n",
            "Iteration 37, loss = 0.53004409\n",
            "Iteration 38, loss = 0.52794971\n",
            "Iteration 39, loss = 0.52762295\n",
            "Iteration 40, loss = 0.52731455\n",
            "Iteration 41, loss = 0.52624037\n",
            "Iteration 42, loss = 0.52665690\n",
            "Iteration 43, loss = 0.52605307\n",
            "Iteration 44, loss = 0.52585806\n",
            "Iteration 45, loss = 0.52614887\n",
            "Iteration 46, loss = 0.52391730\n",
            "Iteration 47, loss = 0.52342006\n",
            "Iteration 48, loss = 0.52281511\n",
            "Iteration 49, loss = 0.52151818\n",
            "Iteration 50, loss = 0.52013343\n",
            "Iteration 51, loss = 0.52039141\n",
            "Iteration 52, loss = 0.52155114\n",
            "Iteration 53, loss = 0.51973222\n",
            "Iteration 54, loss = 0.51937204\n",
            "Iteration 55, loss = 0.52073481\n",
            "Iteration 56, loss = 0.52026603\n",
            "Iteration 57, loss = 0.51854102\n",
            "Iteration 58, loss = 0.51726925\n",
            "Iteration 59, loss = 0.51684776\n",
            "Iteration 60, loss = 0.51709917\n",
            "Iteration 61, loss = 0.51740898\n",
            "Iteration 62, loss = 0.51849608\n",
            "Iteration 63, loss = 0.51816904\n",
            "Iteration 64, loss = 0.51797494\n",
            "Iteration 65, loss = 0.51884720\n",
            "Iteration 66, loss = 0.51748484\n",
            "Iteration 67, loss = 0.51679103\n",
            "Iteration 68, loss = 0.51567568\n",
            "Iteration 69, loss = 0.51513003\n",
            "Iteration 70, loss = 0.51663152\n",
            "Iteration 71, loss = 0.51592509\n",
            "Iteration 72, loss = 0.51354275\n",
            "Iteration 73, loss = 0.51737624\n",
            "Iteration 74, loss = 0.51699973\n",
            "Iteration 75, loss = 0.51609717\n",
            "Iteration 76, loss = 0.51411590\n",
            "Iteration 77, loss = 0.51467742\n",
            "Iteration 78, loss = 0.51476699\n",
            "Iteration 79, loss = 0.51310993\n",
            "Iteration 80, loss = 0.51284290\n",
            "Iteration 81, loss = 0.51462620\n",
            "Iteration 82, loss = 0.51531153\n",
            "Iteration 83, loss = 0.51651873\n",
            "Iteration 84, loss = 0.51583456\n",
            "Iteration 85, loss = 0.51574633\n",
            "Iteration 86, loss = 0.51492935\n",
            "Iteration 87, loss = 0.51491277\n",
            "Iteration 88, loss = 0.51469364\n",
            "Iteration 89, loss = 0.51383952\n",
            "Iteration 90, loss = 0.51359792\n",
            "Iteration 91, loss = 0.51379622\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81331306\n",
            "Iteration 2, loss = 0.66033007\n",
            "Iteration 3, loss = 0.60394338\n",
            "Iteration 4, loss = 0.58347678\n",
            "Iteration 5, loss = 0.57325470\n",
            "Iteration 6, loss = 0.56846700\n",
            "Iteration 7, loss = 0.56686121\n",
            "Iteration 8, loss = 0.56389750\n",
            "Iteration 9, loss = 0.55854005\n",
            "Iteration 10, loss = 0.55686836\n",
            "Iteration 11, loss = 0.55570604\n",
            "Iteration 12, loss = 0.55406456\n",
            "Iteration 13, loss = 0.55150024\n",
            "Iteration 14, loss = 0.54875073\n",
            "Iteration 15, loss = 0.54638142\n",
            "Iteration 16, loss = 0.54388926\n",
            "Iteration 17, loss = 0.54225501\n",
            "Iteration 18, loss = 0.54000220\n",
            "Iteration 19, loss = 0.53809810\n",
            "Iteration 20, loss = 0.53710679\n",
            "Iteration 21, loss = 0.53602823\n",
            "Iteration 22, loss = 0.53495535\n",
            "Iteration 23, loss = 0.53323611\n",
            "Iteration 24, loss = 0.53158803\n",
            "Iteration 25, loss = 0.53142116\n",
            "Iteration 26, loss = 0.53039578\n",
            "Iteration 27, loss = 0.52896039\n",
            "Iteration 28, loss = 0.52812962\n",
            "Iteration 29, loss = 0.52760837\n",
            "Iteration 30, loss = 0.52750333\n",
            "Iteration 31, loss = 0.52844395\n",
            "Iteration 32, loss = 0.52805030\n",
            "Iteration 33, loss = 0.52654364\n",
            "Iteration 34, loss = 0.52559491\n",
            "Iteration 35, loss = 0.52472209\n",
            "Iteration 36, loss = 0.52462304\n",
            "Iteration 37, loss = 0.52484838\n",
            "Iteration 38, loss = 0.52243952\n",
            "Iteration 39, loss = 0.52135493\n",
            "Iteration 40, loss = 0.52089779\n",
            "Iteration 41, loss = 0.52040099\n",
            "Iteration 42, loss = 0.52008310\n",
            "Iteration 43, loss = 0.51987458\n",
            "Iteration 44, loss = 0.51911580\n",
            "Iteration 45, loss = 0.51961977\n",
            "Iteration 46, loss = 0.51973649\n",
            "Iteration 47, loss = 0.51772961\n",
            "Iteration 48, loss = 0.51829609\n",
            "Iteration 49, loss = 0.51755155\n",
            "Iteration 50, loss = 0.51591976\n",
            "Iteration 51, loss = 0.51644725\n",
            "Iteration 52, loss = 0.51663831\n",
            "Iteration 53, loss = 0.51686230\n",
            "Iteration 54, loss = 0.51725454\n",
            "Iteration 55, loss = 0.51699703\n",
            "Iteration 56, loss = 0.51687145\n",
            "Iteration 57, loss = 0.51589051\n",
            "Iteration 58, loss = 0.51582670\n",
            "Iteration 59, loss = 0.51564801\n",
            "Iteration 60, loss = 0.51650497\n",
            "Iteration 61, loss = 0.51722020\n",
            "Iteration 62, loss = 0.51686858\n",
            "Iteration 63, loss = 0.51537984\n",
            "Iteration 64, loss = 0.51598666\n",
            "Iteration 65, loss = 0.51758055\n",
            "Iteration 66, loss = 0.51641849\n",
            "Iteration 67, loss = 0.51479609\n",
            "Iteration 68, loss = 0.51482381\n",
            "Iteration 69, loss = 0.51634873\n",
            "Iteration 70, loss = 0.51664928\n",
            "Iteration 71, loss = 0.51580978\n",
            "Iteration 72, loss = 0.51464183\n",
            "Iteration 73, loss = 0.51606701\n",
            "Iteration 74, loss = 0.51718934\n",
            "Iteration 75, loss = 0.51586765\n",
            "Iteration 76, loss = 0.51353713\n",
            "Iteration 77, loss = 0.51510832\n",
            "Iteration 78, loss = 0.51452434\n",
            "Iteration 79, loss = 0.51383952\n",
            "Iteration 80, loss = 0.51460585\n",
            "Iteration 81, loss = 0.51428523\n",
            "Iteration 82, loss = 0.51447232\n",
            "Iteration 83, loss = 0.51480264\n",
            "Iteration 84, loss = 0.51529507\n",
            "Iteration 85, loss = 0.51354914\n",
            "Iteration 86, loss = 0.51193067\n",
            "Iteration 87, loss = 0.51160359\n",
            "Iteration 88, loss = 0.51109447\n",
            "Iteration 89, loss = 0.51041741\n",
            "Iteration 90, loss = 0.51053244\n",
            "Iteration 91, loss = 0.51303529\n",
            "Iteration 92, loss = 0.51296644\n",
            "Iteration 93, loss = 0.50982758\n",
            "Iteration 94, loss = 0.51173122\n",
            "Iteration 95, loss = 0.51226543\n",
            "Iteration 96, loss = 0.51108616\n",
            "Iteration 97, loss = 0.51200241\n",
            "Iteration 98, loss = 0.51100175\n",
            "Iteration 99, loss = 0.51246029\n",
            "Iteration 100, loss = 0.51000404\n",
            "Iteration 101, loss = 0.51138436\n",
            "Iteration 102, loss = 0.51236465\n",
            "Iteration 103, loss = 0.51172918\n",
            "Iteration 104, loss = 0.50855530\n",
            "Iteration 105, loss = 0.50935817\n",
            "Iteration 106, loss = 0.51052172\n",
            "Iteration 107, loss = 0.50964702\n",
            "Iteration 108, loss = 0.51046143\n",
            "Iteration 109, loss = 0.50950127\n",
            "Iteration 110, loss = 0.50842225\n",
            "Iteration 111, loss = 0.50861293\n",
            "Iteration 112, loss = 0.50905976\n",
            "Iteration 113, loss = 0.50864322\n",
            "Iteration 114, loss = 0.50808864\n",
            "Iteration 115, loss = 0.50867589\n",
            "Iteration 116, loss = 0.50841214\n",
            "Iteration 117, loss = 0.50897808\n",
            "Iteration 118, loss = 0.51029751\n",
            "Iteration 119, loss = 0.51017563\n",
            "Iteration 120, loss = 0.51101536\n",
            "Iteration 121, loss = 0.51041526\n",
            "Iteration 122, loss = 0.50764798\n",
            "Iteration 123, loss = 0.50879865\n",
            "Iteration 124, loss = 0.50951965\n",
            "Iteration 125, loss = 0.50867811\n",
            "Iteration 126, loss = 0.50849240\n",
            "Iteration 127, loss = 0.50884949\n",
            "Iteration 128, loss = 0.50952545\n",
            "Iteration 129, loss = 0.50750653\n",
            "Iteration 130, loss = 0.50831559\n",
            "Iteration 131, loss = 0.50812065\n",
            "Iteration 132, loss = 0.50747709\n",
            "Iteration 133, loss = 0.50883508\n",
            "Iteration 134, loss = 0.50905708\n",
            "Iteration 135, loss = 0.50842161\n",
            "Iteration 136, loss = 0.50808287\n",
            "Iteration 137, loss = 0.50961933\n",
            "Iteration 138, loss = 0.51003489\n",
            "Iteration 139, loss = 0.50941765\n",
            "Iteration 140, loss = 0.50742270\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.82932651\n",
            "Iteration 2, loss = 0.66164239\n",
            "Iteration 3, loss = 0.60009028\n",
            "Iteration 4, loss = 0.57580262\n",
            "Iteration 5, loss = 0.56470176\n",
            "Iteration 6, loss = 0.55748571\n",
            "Iteration 7, loss = 0.55627910\n",
            "Iteration 8, loss = 0.55132255\n",
            "Iteration 9, loss = 0.54679100\n",
            "Iteration 10, loss = 0.54440088\n",
            "Iteration 11, loss = 0.54314934\n",
            "Iteration 12, loss = 0.54115322\n",
            "Iteration 13, loss = 0.53896820\n",
            "Iteration 14, loss = 0.53738517\n",
            "Iteration 15, loss = 0.53610209\n",
            "Iteration 16, loss = 0.53548329\n",
            "Iteration 17, loss = 0.53479088\n",
            "Iteration 18, loss = 0.53420721\n",
            "Iteration 19, loss = 0.53365687\n",
            "Iteration 20, loss = 0.53360199\n",
            "Iteration 21, loss = 0.53262289\n",
            "Iteration 22, loss = 0.53272172\n",
            "Iteration 23, loss = 0.53160057\n",
            "Iteration 24, loss = 0.53020057\n",
            "Iteration 25, loss = 0.53032962\n",
            "Iteration 26, loss = 0.52946469\n",
            "Iteration 27, loss = 0.52945571\n",
            "Iteration 28, loss = 0.52875327\n",
            "Iteration 29, loss = 0.52877389\n",
            "Iteration 30, loss = 0.52682750\n",
            "Iteration 31, loss = 0.52605181\n",
            "Iteration 32, loss = 0.52571340\n",
            "Iteration 33, loss = 0.52546378\n",
            "Iteration 34, loss = 0.52562253\n",
            "Iteration 35, loss = 0.52363862\n",
            "Iteration 36, loss = 0.52212134\n",
            "Iteration 37, loss = 0.52251926\n",
            "Iteration 38, loss = 0.52218800\n",
            "Iteration 39, loss = 0.52165898\n",
            "Iteration 40, loss = 0.52026508\n",
            "Iteration 41, loss = 0.51934207\n",
            "Iteration 42, loss = 0.51863317\n",
            "Iteration 43, loss = 0.51868689\n",
            "Iteration 44, loss = 0.51721053\n",
            "Iteration 45, loss = 0.51841107\n",
            "Iteration 46, loss = 0.51880717\n",
            "Iteration 47, loss = 0.51778831\n",
            "Iteration 48, loss = 0.51753928\n",
            "Iteration 49, loss = 0.51562444\n",
            "Iteration 50, loss = 0.51508967\n",
            "Iteration 51, loss = 0.51559385\n",
            "Iteration 52, loss = 0.51571383\n",
            "Iteration 53, loss = 0.51504503\n",
            "Iteration 54, loss = 0.51481022\n",
            "Iteration 55, loss = 0.51387321\n",
            "Iteration 56, loss = 0.51488602\n",
            "Iteration 57, loss = 0.51372919\n",
            "Iteration 58, loss = 0.51402763\n",
            "Iteration 59, loss = 0.51448928\n",
            "Iteration 60, loss = 0.51337426\n",
            "Iteration 61, loss = 0.51427479\n",
            "Iteration 62, loss = 0.51474731\n",
            "Iteration 63, loss = 0.51357626\n",
            "Iteration 64, loss = 0.51252725\n",
            "Iteration 65, loss = 0.51231619\n",
            "Iteration 66, loss = 0.51255882\n",
            "Iteration 67, loss = 0.51204635\n",
            "Iteration 68, loss = 0.51242474\n",
            "Iteration 69, loss = 0.51219609\n",
            "Iteration 70, loss = 0.51366912\n",
            "Iteration 71, loss = 0.51423931\n",
            "Iteration 72, loss = 0.51245169\n",
            "Iteration 73, loss = 0.51357885\n",
            "Iteration 74, loss = 0.51272285\n",
            "Iteration 75, loss = 0.51279332\n",
            "Iteration 76, loss = 0.51178815\n",
            "Iteration 77, loss = 0.51023404\n",
            "Iteration 78, loss = 0.51053386\n",
            "Iteration 79, loss = 0.51100373\n",
            "Iteration 80, loss = 0.50993851\n",
            "Iteration 81, loss = 0.50974919\n",
            "Iteration 82, loss = 0.50959666\n",
            "Iteration 83, loss = 0.51106230\n",
            "Iteration 84, loss = 0.51063345\n",
            "Iteration 85, loss = 0.51076724\n",
            "Iteration 86, loss = 0.50930014\n",
            "Iteration 87, loss = 0.50845813\n",
            "Iteration 88, loss = 0.50816720\n",
            "Iteration 89, loss = 0.50865641\n",
            "Iteration 90, loss = 0.50926082\n",
            "Iteration 91, loss = 0.51053776\n",
            "Iteration 92, loss = 0.51101996\n",
            "Iteration 93, loss = 0.50967905\n",
            "Iteration 94, loss = 0.51493859\n",
            "Iteration 95, loss = 0.51080848\n",
            "Iteration 96, loss = 0.51116814\n",
            "Iteration 97, loss = 0.51019227\n",
            "Iteration 98, loss = 0.51022928\n",
            "Iteration 99, loss = 0.50991010\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.80972489\n",
            "Iteration 2, loss = 0.65904043\n",
            "Iteration 3, loss = 0.60553834\n",
            "Iteration 4, loss = 0.58737349\n",
            "Iteration 5, loss = 0.57959275\n",
            "Iteration 6, loss = 0.57310858\n",
            "Iteration 7, loss = 0.57025941\n",
            "Iteration 8, loss = 0.56762812\n",
            "Iteration 9, loss = 0.56369167\n",
            "Iteration 10, loss = 0.56030709\n",
            "Iteration 11, loss = 0.55715604\n",
            "Iteration 12, loss = 0.55424368\n",
            "Iteration 13, loss = 0.55086767\n",
            "Iteration 14, loss = 0.54968789\n",
            "Iteration 15, loss = 0.54804813\n",
            "Iteration 16, loss = 0.54716522\n",
            "Iteration 17, loss = 0.54589839\n",
            "Iteration 18, loss = 0.54464670\n",
            "Iteration 19, loss = 0.54270658\n",
            "Iteration 20, loss = 0.54195373\n",
            "Iteration 21, loss = 0.54050459\n",
            "Iteration 22, loss = 0.54002333\n",
            "Iteration 23, loss = 0.53829247\n",
            "Iteration 24, loss = 0.53714600\n",
            "Iteration 25, loss = 0.53752867\n",
            "Iteration 26, loss = 0.53664465\n",
            "Iteration 27, loss = 0.53524365\n",
            "Iteration 28, loss = 0.53398566\n",
            "Iteration 29, loss = 0.53388843\n",
            "Iteration 30, loss = 0.53308004\n",
            "Iteration 31, loss = 0.53183431\n",
            "Iteration 32, loss = 0.53173172\n",
            "Iteration 33, loss = 0.53088996\n",
            "Iteration 34, loss = 0.53055194\n",
            "Iteration 35, loss = 0.52922236\n",
            "Iteration 36, loss = 0.52768203\n",
            "Iteration 37, loss = 0.52816267\n",
            "Iteration 38, loss = 0.52732724\n",
            "Iteration 39, loss = 0.52567655\n",
            "Iteration 40, loss = 0.52483423\n",
            "Iteration 41, loss = 0.52393807\n",
            "Iteration 42, loss = 0.52282236\n",
            "Iteration 43, loss = 0.52273856\n",
            "Iteration 44, loss = 0.52322500\n",
            "Iteration 45, loss = 0.52272876\n",
            "Iteration 46, loss = 0.52240351\n",
            "Iteration 47, loss = 0.52226690\n",
            "Iteration 48, loss = 0.52143150\n",
            "Iteration 49, loss = 0.51896197\n",
            "Iteration 50, loss = 0.51832271\n",
            "Iteration 51, loss = 0.51789919\n",
            "Iteration 52, loss = 0.51599498\n",
            "Iteration 53, loss = 0.51552360\n",
            "Iteration 54, loss = 0.51513925\n",
            "Iteration 55, loss = 0.51481940\n",
            "Iteration 56, loss = 0.51382834\n",
            "Iteration 57, loss = 0.51223555\n",
            "Iteration 58, loss = 0.51167038\n",
            "Iteration 59, loss = 0.51239758\n",
            "Iteration 60, loss = 0.51254387\n",
            "Iteration 61, loss = 0.51088149\n",
            "Iteration 62, loss = 0.51097122\n",
            "Iteration 63, loss = 0.50975030\n",
            "Iteration 64, loss = 0.51038171\n",
            "Iteration 65, loss = 0.51104626\n",
            "Iteration 66, loss = 0.51060992\n",
            "Iteration 67, loss = 0.50930488\n",
            "Iteration 68, loss = 0.50885791\n",
            "Iteration 69, loss = 0.50957961\n",
            "Iteration 70, loss = 0.50996656\n",
            "Iteration 71, loss = 0.51323337\n",
            "Iteration 72, loss = 0.51190285\n",
            "Iteration 73, loss = 0.51171478\n",
            "Iteration 74, loss = 0.51071170\n",
            "Iteration 75, loss = 0.51021603\n",
            "Iteration 76, loss = 0.51167047\n",
            "Iteration 77, loss = 0.50976451\n",
            "Iteration 78, loss = 0.50861749\n",
            "Iteration 79, loss = 0.51106608\n",
            "Iteration 80, loss = 0.50849303\n",
            "Iteration 81, loss = 0.50817405\n",
            "Iteration 82, loss = 0.50984304\n",
            "Iteration 83, loss = 0.50968674\n",
            "Iteration 84, loss = 0.50810151\n",
            "Iteration 85, loss = 0.50825494\n",
            "Iteration 86, loss = 0.50784555\n",
            "Iteration 87, loss = 0.50798596\n",
            "Iteration 88, loss = 0.50880394\n",
            "Iteration 89, loss = 0.50803305\n",
            "Iteration 90, loss = 0.50877266\n",
            "Iteration 91, loss = 0.50864130\n",
            "Iteration 92, loss = 0.50836387\n",
            "Iteration 93, loss = 0.50758278\n",
            "Iteration 94, loss = 0.50909866\n",
            "Iteration 95, loss = 0.50893600\n",
            "Iteration 96, loss = 0.51058805\n",
            "Iteration 97, loss = 0.50825085\n",
            "Iteration 98, loss = 0.50973095\n",
            "Iteration 99, loss = 0.50818597\n",
            "Iteration 100, loss = 0.50770833\n",
            "Iteration 101, loss = 0.50783601\n",
            "Iteration 102, loss = 0.50693994\n",
            "Iteration 103, loss = 0.50820330\n",
            "Iteration 104, loss = 0.50955333\n",
            "Iteration 105, loss = 0.50794062\n",
            "Iteration 106, loss = 0.50728308\n",
            "Iteration 107, loss = 0.50720195\n",
            "Iteration 108, loss = 0.50774986\n",
            "Iteration 109, loss = 0.50752158\n",
            "Iteration 110, loss = 0.50663107\n",
            "Iteration 111, loss = 0.50605649\n",
            "Iteration 112, loss = 0.50666184\n",
            "Iteration 113, loss = 0.50683693\n",
            "Iteration 114, loss = 0.50618463\n",
            "Iteration 115, loss = 0.50648116\n",
            "Iteration 116, loss = 0.50631940\n",
            "Iteration 117, loss = 0.50758764\n",
            "Iteration 118, loss = 0.50727017\n",
            "Iteration 119, loss = 0.50610174\n",
            "Iteration 120, loss = 0.50615163\n",
            "Iteration 121, loss = 0.50730000\n",
            "Iteration 122, loss = 0.50583915\n",
            "Iteration 123, loss = 0.50618300\n",
            "Iteration 124, loss = 0.50521044\n",
            "Iteration 125, loss = 0.50601022\n",
            "Iteration 126, loss = 0.50563019\n",
            "Iteration 127, loss = 0.50482379\n",
            "Iteration 128, loss = 0.50540935\n",
            "Iteration 129, loss = 0.50458830\n",
            "Iteration 130, loss = 0.50497663\n",
            "Iteration 131, loss = 0.50588508\n",
            "Iteration 132, loss = 0.50589222\n",
            "Iteration 133, loss = 0.50604027\n",
            "Iteration 134, loss = 0.50603218\n",
            "Iteration 135, loss = 0.50531004\n",
            "Iteration 136, loss = 0.50503691\n",
            "Iteration 137, loss = 0.50465889\n",
            "Iteration 138, loss = 0.50606272\n",
            "Iteration 139, loss = 0.50520856\n",
            "Iteration 140, loss = 0.50477656\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.80475080\n",
            "Iteration 2, loss = 0.65487955\n",
            "Iteration 3, loss = 0.60390614\n",
            "Iteration 4, loss = 0.58819751\n",
            "Iteration 5, loss = 0.57865468\n",
            "Iteration 6, loss = 0.57206376\n",
            "Iteration 7, loss = 0.57135814\n",
            "Iteration 8, loss = 0.56979090\n",
            "Iteration 9, loss = 0.56702828\n",
            "Iteration 10, loss = 0.56515424\n",
            "Iteration 11, loss = 0.56408998\n",
            "Iteration 12, loss = 0.56223931\n",
            "Iteration 13, loss = 0.55712257\n",
            "Iteration 14, loss = 0.55543343\n",
            "Iteration 15, loss = 0.55460630\n",
            "Iteration 16, loss = 0.55343467\n",
            "Iteration 17, loss = 0.55204421\n",
            "Iteration 18, loss = 0.55203563\n",
            "Iteration 19, loss = 0.55038470\n",
            "Iteration 20, loss = 0.54916114\n",
            "Iteration 21, loss = 0.54790691\n",
            "Iteration 22, loss = 0.54823634\n",
            "Iteration 23, loss = 0.54770996\n",
            "Iteration 24, loss = 0.54647616\n",
            "Iteration 25, loss = 0.54569847\n",
            "Iteration 26, loss = 0.54635954\n",
            "Iteration 27, loss = 0.54486154\n",
            "Iteration 28, loss = 0.54269137\n",
            "Iteration 29, loss = 0.54393359\n",
            "Iteration 30, loss = 0.54525630\n",
            "Iteration 31, loss = 0.54050479\n",
            "Iteration 32, loss = 0.54251410\n",
            "Iteration 33, loss = 0.54337415\n",
            "Iteration 34, loss = 0.54084148\n",
            "Iteration 35, loss = 0.53936802\n",
            "Iteration 36, loss = 0.53834473\n",
            "Iteration 37, loss = 0.53647467\n",
            "Iteration 38, loss = 0.53587492\n",
            "Iteration 39, loss = 0.53480966\n",
            "Iteration 40, loss = 0.53401460\n",
            "Iteration 41, loss = 0.53291611\n",
            "Iteration 42, loss = 0.53377357\n",
            "Iteration 43, loss = 0.53302631\n",
            "Iteration 44, loss = 0.53221832\n",
            "Iteration 45, loss = 0.53264785\n",
            "Iteration 46, loss = 0.52988870\n",
            "Iteration 47, loss = 0.52953298\n",
            "Iteration 48, loss = 0.52807396\n",
            "Iteration 49, loss = 0.52674531\n",
            "Iteration 50, loss = 0.52573679\n",
            "Iteration 51, loss = 0.52553847\n",
            "Iteration 52, loss = 0.52517267\n",
            "Iteration 53, loss = 0.52597777\n",
            "Iteration 54, loss = 0.52580652\n",
            "Iteration 55, loss = 0.52542780\n",
            "Iteration 56, loss = 0.52500573\n",
            "Iteration 57, loss = 0.52407180\n",
            "Iteration 58, loss = 0.52276023\n",
            "Iteration 59, loss = 0.52280461\n",
            "Iteration 60, loss = 0.52352107\n",
            "Iteration 61, loss = 0.52285380\n",
            "Iteration 62, loss = 0.52188201\n",
            "Iteration 63, loss = 0.52059995\n",
            "Iteration 64, loss = 0.52139367\n",
            "Iteration 65, loss = 0.52289242\n",
            "Iteration 66, loss = 0.52192389\n",
            "Iteration 67, loss = 0.52091656\n",
            "Iteration 68, loss = 0.52019784\n",
            "Iteration 69, loss = 0.51985503\n",
            "Iteration 70, loss = 0.51990398\n",
            "Iteration 71, loss = 0.52013587\n",
            "Iteration 72, loss = 0.52161612\n",
            "Iteration 73, loss = 0.52083912\n",
            "Iteration 74, loss = 0.51885686\n",
            "Iteration 75, loss = 0.51883100\n",
            "Iteration 76, loss = 0.52128410\n",
            "Iteration 77, loss = 0.52155454\n",
            "Iteration 78, loss = 0.51890984\n",
            "Iteration 79, loss = 0.52133313\n",
            "Iteration 80, loss = 0.51892092\n",
            "Iteration 81, loss = 0.51817384\n",
            "Iteration 82, loss = 0.52172643\n",
            "Iteration 83, loss = 0.52130464\n",
            "Iteration 84, loss = 0.52020023\n",
            "Iteration 85, loss = 0.51936990\n",
            "Iteration 86, loss = 0.51910924\n",
            "Iteration 87, loss = 0.51918844\n",
            "Iteration 88, loss = 0.51849875\n",
            "Iteration 89, loss = 0.51964345\n",
            "Iteration 90, loss = 0.51967115\n",
            "Iteration 91, loss = 0.51912139\n",
            "Iteration 92, loss = 0.51886701\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 150 and for layer number 3 : 0.6950000000000001\n",
            "Iteration 1, loss = 0.89843010\n",
            "Iteration 2, loss = 0.76513132\n",
            "Iteration 3, loss = 0.65384082\n",
            "Iteration 4, loss = 0.60270237\n",
            "Iteration 5, loss = 0.58704637\n",
            "Iteration 6, loss = 0.57794274\n",
            "Iteration 7, loss = 0.57124497\n",
            "Iteration 8, loss = 0.56673704\n",
            "Iteration 9, loss = 0.56287505\n",
            "Iteration 10, loss = 0.55892982\n",
            "Iteration 11, loss = 0.55698535\n",
            "Iteration 12, loss = 0.55516473\n",
            "Iteration 13, loss = 0.55305959\n",
            "Iteration 14, loss = 0.55269439\n",
            "Iteration 15, loss = 0.55268796\n",
            "Iteration 16, loss = 0.55141608\n",
            "Iteration 17, loss = 0.55198928\n",
            "Iteration 18, loss = 0.55193711\n",
            "Iteration 19, loss = 0.55199247\n",
            "Iteration 20, loss = 0.55087278\n",
            "Iteration 21, loss = 0.54920550\n",
            "Iteration 22, loss = 0.54957461\n",
            "Iteration 23, loss = 0.55002924\n",
            "Iteration 24, loss = 0.54902125\n",
            "Iteration 25, loss = 0.54833007\n",
            "Iteration 26, loss = 0.54641390\n",
            "Iteration 27, loss = 0.54527628\n",
            "Iteration 28, loss = 0.54547347\n",
            "Iteration 29, loss = 0.54497476\n",
            "Iteration 30, loss = 0.54315303\n",
            "Iteration 31, loss = 0.54269913\n",
            "Iteration 32, loss = 0.54345058\n",
            "Iteration 33, loss = 0.54133958\n",
            "Iteration 34, loss = 0.54006651\n",
            "Iteration 35, loss = 0.54181417\n",
            "Iteration 36, loss = 0.54064432\n",
            "Iteration 37, loss = 0.54176667\n",
            "Iteration 38, loss = 0.53892320\n",
            "Iteration 39, loss = 0.53749626\n",
            "Iteration 40, loss = 0.53643073\n",
            "Iteration 41, loss = 0.53749149\n",
            "Iteration 42, loss = 0.53688622\n",
            "Iteration 43, loss = 0.53409458\n",
            "Iteration 44, loss = 0.53520926\n",
            "Iteration 45, loss = 0.53517666\n",
            "Iteration 46, loss = 0.53266025\n",
            "Iteration 47, loss = 0.53317645\n",
            "Iteration 48, loss = 0.53170110\n",
            "Iteration 49, loss = 0.53091427\n",
            "Iteration 50, loss = 0.52962578\n",
            "Iteration 51, loss = 0.52946461\n",
            "Iteration 52, loss = 0.52939386\n",
            "Iteration 53, loss = 0.52910654\n",
            "Iteration 54, loss = 0.52878266\n",
            "Iteration 55, loss = 0.52787557\n",
            "Iteration 56, loss = 0.52630304\n",
            "Iteration 57, loss = 0.52688899\n",
            "Iteration 58, loss = 0.52679798\n",
            "Iteration 59, loss = 0.52907269\n",
            "Iteration 60, loss = 0.52540211\n",
            "Iteration 61, loss = 0.52761757\n",
            "Iteration 62, loss = 0.52778592\n",
            "Iteration 63, loss = 0.52798004\n",
            "Iteration 64, loss = 0.52820011\n",
            "Iteration 65, loss = 0.52612079\n",
            "Iteration 66, loss = 0.52629661\n",
            "Iteration 67, loss = 0.52709228\n",
            "Iteration 68, loss = 0.52815700\n",
            "Iteration 69, loss = 0.52708967\n",
            "Iteration 70, loss = 0.52691176\n",
            "Iteration 71, loss = 0.52816892\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.89647974\n",
            "Iteration 2, loss = 0.76503390\n",
            "Iteration 3, loss = 0.65738862\n",
            "Iteration 4, loss = 0.60451657\n",
            "Iteration 5, loss = 0.58466759\n",
            "Iteration 6, loss = 0.57504316\n",
            "Iteration 7, loss = 0.56767168\n",
            "Iteration 8, loss = 0.56133735\n",
            "Iteration 9, loss = 0.55764613\n",
            "Iteration 10, loss = 0.55376055\n",
            "Iteration 11, loss = 0.55274203\n",
            "Iteration 12, loss = 0.55149150\n",
            "Iteration 13, loss = 0.54874116\n",
            "Iteration 14, loss = 0.54757801\n",
            "Iteration 15, loss = 0.54687336\n",
            "Iteration 16, loss = 0.54592392\n",
            "Iteration 17, loss = 0.54463412\n",
            "Iteration 18, loss = 0.54412666\n",
            "Iteration 19, loss = 0.54401242\n",
            "Iteration 20, loss = 0.54346391\n",
            "Iteration 21, loss = 0.54321807\n",
            "Iteration 22, loss = 0.54308250\n",
            "Iteration 23, loss = 0.54393627\n",
            "Iteration 24, loss = 0.54504573\n",
            "Iteration 25, loss = 0.54398395\n",
            "Iteration 26, loss = 0.54195910\n",
            "Iteration 27, loss = 0.54102551\n",
            "Iteration 28, loss = 0.54000856\n",
            "Iteration 29, loss = 0.53864889\n",
            "Iteration 30, loss = 0.53780552\n",
            "Iteration 31, loss = 0.53879175\n",
            "Iteration 32, loss = 0.53899122\n",
            "Iteration 33, loss = 0.53803797\n",
            "Iteration 34, loss = 0.53777915\n",
            "Iteration 35, loss = 0.53787486\n",
            "Iteration 36, loss = 0.53457417\n",
            "Iteration 37, loss = 0.53692346\n",
            "Iteration 38, loss = 0.53428080\n",
            "Iteration 39, loss = 0.53379512\n",
            "Iteration 40, loss = 0.53249173\n",
            "Iteration 41, loss = 0.53436636\n",
            "Iteration 42, loss = 0.53136372\n",
            "Iteration 43, loss = 0.52905241\n",
            "Iteration 44, loss = 0.52976079\n",
            "Iteration 45, loss = 0.52792724\n",
            "Iteration 46, loss = 0.52571268\n",
            "Iteration 47, loss = 0.52518925\n",
            "Iteration 48, loss = 0.52391380\n",
            "Iteration 49, loss = 0.52326992\n",
            "Iteration 50, loss = 0.52120426\n",
            "Iteration 51, loss = 0.52140533\n",
            "Iteration 52, loss = 0.52240227\n",
            "Iteration 53, loss = 0.52119785\n",
            "Iteration 54, loss = 0.52163119\n",
            "Iteration 55, loss = 0.52027640\n",
            "Iteration 56, loss = 0.52069217\n",
            "Iteration 57, loss = 0.52106924\n",
            "Iteration 58, loss = 0.51928000\n",
            "Iteration 59, loss = 0.51916456\n",
            "Iteration 60, loss = 0.52327257\n",
            "Iteration 61, loss = 0.51895711\n",
            "Iteration 62, loss = 0.51532216\n",
            "Iteration 63, loss = 0.51514669\n",
            "Iteration 64, loss = 0.51379276\n",
            "Iteration 65, loss = 0.51532715\n",
            "Iteration 66, loss = 0.51897733\n",
            "Iteration 67, loss = 0.51749566\n",
            "Iteration 68, loss = 0.51430419\n",
            "Iteration 69, loss = 0.51448492\n",
            "Iteration 70, loss = 0.51420734\n",
            "Iteration 71, loss = 0.51086393\n",
            "Iteration 72, loss = 0.50569609\n",
            "Iteration 73, loss = 0.50630985\n",
            "Iteration 74, loss = 0.50555591\n",
            "Iteration 75, loss = 0.50792682\n",
            "Iteration 76, loss = 0.50417952\n",
            "Iteration 77, loss = 0.50254183\n",
            "Iteration 78, loss = 0.50139324\n",
            "Iteration 79, loss = 0.50452224\n",
            "Iteration 80, loss = 0.50252965\n",
            "Iteration 81, loss = 0.50458088\n",
            "Iteration 82, loss = 0.49910814\n",
            "Iteration 83, loss = 0.50903244\n",
            "Iteration 84, loss = 0.49569914\n",
            "Iteration 85, loss = 0.50412909\n",
            "Iteration 86, loss = 0.49595235\n",
            "Iteration 87, loss = 0.49687348\n",
            "Iteration 88, loss = 0.49134680\n",
            "Iteration 89, loss = 0.49834615\n",
            "Iteration 90, loss = 0.48980261\n",
            "Iteration 91, loss = 0.49338704\n",
            "Iteration 92, loss = 0.48902344\n",
            "Iteration 93, loss = 0.49145853\n",
            "Iteration 94, loss = 0.48747800\n",
            "Iteration 95, loss = 0.48887107\n",
            "Iteration 96, loss = 0.49037425\n",
            "Iteration 97, loss = 0.49859268\n",
            "Iteration 98, loss = 0.48675766\n",
            "Iteration 99, loss = 0.49052318\n",
            "Iteration 100, loss = 0.48894740\n",
            "Iteration 101, loss = 0.48919468\n",
            "Iteration 102, loss = 0.49201846\n",
            "Iteration 103, loss = 0.48345248\n",
            "Iteration 104, loss = 0.49059289\n",
            "Iteration 105, loss = 0.49260947\n",
            "Iteration 106, loss = 0.49523811\n",
            "Iteration 107, loss = 0.49297273\n",
            "Iteration 108, loss = 0.49140348\n",
            "Iteration 109, loss = 0.48191850\n",
            "Iteration 110, loss = 0.48719341\n",
            "Iteration 111, loss = 0.48443655\n",
            "Iteration 112, loss = 0.48192928\n",
            "Iteration 113, loss = 0.48586406\n",
            "Iteration 114, loss = 0.48068653\n",
            "Iteration 115, loss = 0.47871787\n",
            "Iteration 116, loss = 0.48071761\n",
            "Iteration 117, loss = 0.48186814\n",
            "Iteration 118, loss = 0.48641580\n",
            "Iteration 119, loss = 0.48315740\n",
            "Iteration 120, loss = 0.48302756\n",
            "Iteration 121, loss = 0.48043503\n",
            "Iteration 122, loss = 0.48211741\n",
            "Iteration 123, loss = 0.48912972\n",
            "Iteration 124, loss = 0.48290244\n",
            "Iteration 125, loss = 0.48100373\n",
            "Iteration 126, loss = 0.47845257\n",
            "Iteration 127, loss = 0.48121488\n",
            "Iteration 128, loss = 0.48829914\n",
            "Iteration 129, loss = 0.48272910\n",
            "Iteration 130, loss = 0.48383524\n",
            "Iteration 131, loss = 0.48492663\n",
            "Iteration 132, loss = 0.48394394\n",
            "Iteration 133, loss = 0.48218977\n",
            "Iteration 134, loss = 0.48130342\n",
            "Iteration 135, loss = 0.47719077\n",
            "Iteration 136, loss = 0.48500437\n",
            "Iteration 137, loss = 0.47948613\n",
            "Iteration 138, loss = 0.47840764\n",
            "Iteration 139, loss = 0.48790227\n",
            "Iteration 140, loss = 0.48435334\n",
            "Iteration 141, loss = 0.48877738\n",
            "Iteration 142, loss = 0.48881379\n",
            "Iteration 143, loss = 0.48103120\n",
            "Iteration 144, loss = 0.48172600\n",
            "Iteration 145, loss = 0.48632543\n",
            "Iteration 146, loss = 0.48128421\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.90447953\n",
            "Iteration 2, loss = 0.76691485\n",
            "Iteration 3, loss = 0.65636497\n",
            "Iteration 4, loss = 0.59577920\n",
            "Iteration 5, loss = 0.57707171\n",
            "Iteration 6, loss = 0.56749853\n",
            "Iteration 7, loss = 0.56265587\n",
            "Iteration 8, loss = 0.55936371\n",
            "Iteration 9, loss = 0.55478568\n",
            "Iteration 10, loss = 0.55004024\n",
            "Iteration 11, loss = 0.54769876\n",
            "Iteration 12, loss = 0.54630747\n",
            "Iteration 13, loss = 0.54357485\n",
            "Iteration 14, loss = 0.54209368\n",
            "Iteration 15, loss = 0.54201538\n",
            "Iteration 16, loss = 0.54178238\n",
            "Iteration 17, loss = 0.54056641\n",
            "Iteration 18, loss = 0.53921933\n",
            "Iteration 19, loss = 0.53823331\n",
            "Iteration 20, loss = 0.53717933\n",
            "Iteration 21, loss = 0.53771539\n",
            "Iteration 22, loss = 0.53632534\n",
            "Iteration 23, loss = 0.53647887\n",
            "Iteration 24, loss = 0.53782796\n",
            "Iteration 25, loss = 0.53758771\n",
            "Iteration 26, loss = 0.53530753\n",
            "Iteration 27, loss = 0.53393838\n",
            "Iteration 28, loss = 0.53190523\n",
            "Iteration 29, loss = 0.53044664\n",
            "Iteration 30, loss = 0.52968979\n",
            "Iteration 31, loss = 0.52941003\n",
            "Iteration 32, loss = 0.52925026\n",
            "Iteration 33, loss = 0.52945278\n",
            "Iteration 34, loss = 0.52754751\n",
            "Iteration 35, loss = 0.52513913\n",
            "Iteration 36, loss = 0.52465087\n",
            "Iteration 37, loss = 0.52490959\n",
            "Iteration 38, loss = 0.52384790\n",
            "Iteration 39, loss = 0.52392522\n",
            "Iteration 40, loss = 0.52349030\n",
            "Iteration 41, loss = 0.52296316\n",
            "Iteration 42, loss = 0.52142275\n",
            "Iteration 43, loss = 0.52025813\n",
            "Iteration 44, loss = 0.52140690\n",
            "Iteration 45, loss = 0.51916030\n",
            "Iteration 46, loss = 0.51798468\n",
            "Iteration 47, loss = 0.51756444\n",
            "Iteration 48, loss = 0.51911537\n",
            "Iteration 49, loss = 0.51761755\n",
            "Iteration 50, loss = 0.51629200\n",
            "Iteration 51, loss = 0.51601996\n",
            "Iteration 52, loss = 0.51569139\n",
            "Iteration 53, loss = 0.51522243\n",
            "Iteration 54, loss = 0.51449337\n",
            "Iteration 55, loss = 0.51487080\n",
            "Iteration 56, loss = 0.51443166\n",
            "Iteration 57, loss = 0.51406125\n",
            "Iteration 58, loss = 0.51403510\n",
            "Iteration 59, loss = 0.51353227\n",
            "Iteration 60, loss = 0.51573735\n",
            "Iteration 61, loss = 0.51559794\n",
            "Iteration 62, loss = 0.51492730\n",
            "Iteration 63, loss = 0.51295094\n",
            "Iteration 64, loss = 0.51173244\n",
            "Iteration 65, loss = 0.51256462\n",
            "Iteration 66, loss = 0.51539152\n",
            "Iteration 67, loss = 0.51606858\n",
            "Iteration 68, loss = 0.51419822\n",
            "Iteration 69, loss = 0.51133250\n",
            "Iteration 70, loss = 0.51035396\n",
            "Iteration 71, loss = 0.51073227\n",
            "Iteration 72, loss = 0.51138410\n",
            "Iteration 73, loss = 0.50991133\n",
            "Iteration 74, loss = 0.50943337\n",
            "Iteration 75, loss = 0.50905278\n",
            "Iteration 76, loss = 0.50930612\n",
            "Iteration 77, loss = 0.50761673\n",
            "Iteration 78, loss = 0.50834262\n",
            "Iteration 79, loss = 0.50950490\n",
            "Iteration 80, loss = 0.50716846\n",
            "Iteration 81, loss = 0.50880714\n",
            "Iteration 82, loss = 0.50480841\n",
            "Iteration 83, loss = 0.50816916\n",
            "Iteration 84, loss = 0.50715524\n",
            "Iteration 85, loss = 0.51112409\n",
            "Iteration 86, loss = 0.50938590\n",
            "Iteration 87, loss = 0.50451063\n",
            "Iteration 88, loss = 0.50303698\n",
            "Iteration 89, loss = 0.50204349\n",
            "Iteration 90, loss = 0.50549412\n",
            "Iteration 91, loss = 0.50266367\n",
            "Iteration 92, loss = 0.50142561\n",
            "Iteration 93, loss = 0.50091745\n",
            "Iteration 94, loss = 0.50182045\n",
            "Iteration 95, loss = 0.50059640\n",
            "Iteration 96, loss = 0.50122117\n",
            "Iteration 97, loss = 0.50394290\n",
            "Iteration 98, loss = 0.50454859\n",
            "Iteration 99, loss = 0.50382952\n",
            "Iteration 100, loss = 0.50259390\n",
            "Iteration 101, loss = 0.50723207\n",
            "Iteration 102, loss = 0.50419079\n",
            "Iteration 103, loss = 0.50751390\n",
            "Iteration 104, loss = 0.50523052\n",
            "Iteration 105, loss = 0.50583591\n",
            "Iteration 106, loss = 0.50369211\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.90198656\n",
            "Iteration 2, loss = 0.76291623\n",
            "Iteration 3, loss = 0.65742388\n",
            "Iteration 4, loss = 0.60221160\n",
            "Iteration 5, loss = 0.58317327\n",
            "Iteration 6, loss = 0.57433136\n",
            "Iteration 7, loss = 0.56872138\n",
            "Iteration 8, loss = 0.56549704\n",
            "Iteration 9, loss = 0.56136635\n",
            "Iteration 10, loss = 0.55708719\n",
            "Iteration 11, loss = 0.55448963\n",
            "Iteration 12, loss = 0.55290078\n",
            "Iteration 13, loss = 0.55099528\n",
            "Iteration 14, loss = 0.54931756\n",
            "Iteration 15, loss = 0.54756538\n",
            "Iteration 16, loss = 0.54790434\n",
            "Iteration 17, loss = 0.54674437\n",
            "Iteration 18, loss = 0.54610576\n",
            "Iteration 19, loss = 0.54541517\n",
            "Iteration 20, loss = 0.54421269\n",
            "Iteration 21, loss = 0.54342487\n",
            "Iteration 22, loss = 0.54277383\n",
            "Iteration 23, loss = 0.54073149\n",
            "Iteration 24, loss = 0.53939535\n",
            "Iteration 25, loss = 0.53830286\n",
            "Iteration 26, loss = 0.53745869\n",
            "Iteration 27, loss = 0.53549679\n",
            "Iteration 28, loss = 0.53399954\n",
            "Iteration 29, loss = 0.53321457\n",
            "Iteration 30, loss = 0.53192810\n",
            "Iteration 31, loss = 0.53203935\n",
            "Iteration 32, loss = 0.53023525\n",
            "Iteration 33, loss = 0.53114364\n",
            "Iteration 34, loss = 0.52945194\n",
            "Iteration 35, loss = 0.52907989\n",
            "Iteration 36, loss = 0.52834601\n",
            "Iteration 37, loss = 0.52729509\n",
            "Iteration 38, loss = 0.52699016\n",
            "Iteration 39, loss = 0.52728567\n",
            "Iteration 40, loss = 0.52614239\n",
            "Iteration 41, loss = 0.52530819\n",
            "Iteration 42, loss = 0.52660269\n",
            "Iteration 43, loss = 0.52394303\n",
            "Iteration 44, loss = 0.52572657\n",
            "Iteration 45, loss = 0.52671551\n",
            "Iteration 46, loss = 0.52209890\n",
            "Iteration 47, loss = 0.52253360\n",
            "Iteration 48, loss = 0.52161146\n",
            "Iteration 49, loss = 0.51878445\n",
            "Iteration 50, loss = 0.51725599\n",
            "Iteration 51, loss = 0.51697770\n",
            "Iteration 52, loss = 0.51832703\n",
            "Iteration 53, loss = 0.51684925\n",
            "Iteration 54, loss = 0.51487529\n",
            "Iteration 55, loss = 0.51478777\n",
            "Iteration 56, loss = 0.51717655\n",
            "Iteration 57, loss = 0.51394420\n",
            "Iteration 58, loss = 0.51565967\n",
            "Iteration 59, loss = 0.51330813\n",
            "Iteration 60, loss = 0.51544560\n",
            "Iteration 61, loss = 0.51703005\n",
            "Iteration 62, loss = 0.51640741\n",
            "Iteration 63, loss = 0.51268030\n",
            "Iteration 64, loss = 0.51199019\n",
            "Iteration 65, loss = 0.51365689\n",
            "Iteration 66, loss = 0.51556396\n",
            "Iteration 67, loss = 0.51719521\n",
            "Iteration 68, loss = 0.51272104\n",
            "Iteration 69, loss = 0.50991236\n",
            "Iteration 70, loss = 0.51058489\n",
            "Iteration 71, loss = 0.50994532\n",
            "Iteration 72, loss = 0.51339412\n",
            "Iteration 73, loss = 0.51098616\n",
            "Iteration 74, loss = 0.51041455\n",
            "Iteration 75, loss = 0.51109968\n",
            "Iteration 76, loss = 0.51151649\n",
            "Iteration 77, loss = 0.51007244\n",
            "Iteration 78, loss = 0.50933724\n",
            "Iteration 79, loss = 0.50888062\n",
            "Iteration 80, loss = 0.50800370\n",
            "Iteration 81, loss = 0.50708513\n",
            "Iteration 82, loss = 0.50945677\n",
            "Iteration 83, loss = 0.50837970\n",
            "Iteration 84, loss = 0.50710538\n",
            "Iteration 85, loss = 0.50540758\n",
            "Iteration 86, loss = 0.50423081\n",
            "Iteration 87, loss = 0.50390843\n",
            "Iteration 88, loss = 0.50198227\n",
            "Iteration 89, loss = 0.50078540\n",
            "Iteration 90, loss = 0.49806554\n",
            "Iteration 91, loss = 0.49996821\n",
            "Iteration 92, loss = 0.49663444\n",
            "Iteration 93, loss = 0.49797597\n",
            "Iteration 94, loss = 0.49714516\n",
            "Iteration 95, loss = 0.49746944\n",
            "Iteration 96, loss = 0.49608519\n",
            "Iteration 97, loss = 0.49377143\n",
            "Iteration 98, loss = 0.49313557\n",
            "Iteration 99, loss = 0.49273804\n",
            "Iteration 100, loss = 0.49246199\n",
            "Iteration 101, loss = 0.49582359\n",
            "Iteration 102, loss = 0.49465084\n",
            "Iteration 103, loss = 0.49043097\n",
            "Iteration 104, loss = 0.49075977\n",
            "Iteration 105, loss = 0.48804810\n",
            "Iteration 106, loss = 0.49138036\n",
            "Iteration 107, loss = 0.48862655\n",
            "Iteration 108, loss = 0.48664654\n",
            "Iteration 109, loss = 0.48721615\n",
            "Iteration 110, loss = 0.48491582\n",
            "Iteration 111, loss = 0.48582198\n",
            "Iteration 112, loss = 0.48424068\n",
            "Iteration 113, loss = 0.48887209\n",
            "Iteration 114, loss = 0.48474359\n",
            "Iteration 115, loss = 0.48135174\n",
            "Iteration 116, loss = 0.47917012\n",
            "Iteration 117, loss = 0.47815764\n",
            "Iteration 118, loss = 0.47774196\n",
            "Iteration 119, loss = 0.47968992\n",
            "Iteration 120, loss = 0.48299151\n",
            "Iteration 121, loss = 0.47731419\n",
            "Iteration 122, loss = 0.48110555\n",
            "Iteration 123, loss = 0.47994359\n",
            "Iteration 124, loss = 0.47881270\n",
            "Iteration 125, loss = 0.47609209\n",
            "Iteration 126, loss = 0.47652508\n",
            "Iteration 127, loss = 0.47701695\n",
            "Iteration 128, loss = 0.47673891\n",
            "Iteration 129, loss = 0.47503491\n",
            "Iteration 130, loss = 0.47374311\n",
            "Iteration 131, loss = 0.47586992\n",
            "Iteration 132, loss = 0.47706351\n",
            "Iteration 133, loss = 0.48030158\n",
            "Iteration 134, loss = 0.47672322\n",
            "Iteration 135, loss = 0.48001233\n",
            "Iteration 136, loss = 0.47768191\n",
            "Iteration 137, loss = 0.47533839\n",
            "Iteration 138, loss = 0.47186347\n",
            "Iteration 139, loss = 0.47333103\n",
            "Iteration 140, loss = 0.47154149\n",
            "Iteration 141, loss = 0.47557482\n",
            "Iteration 142, loss = 0.47545775\n",
            "Iteration 143, loss = 0.47501401\n",
            "Iteration 144, loss = 0.47262526\n",
            "Iteration 145, loss = 0.47461588\n",
            "Iteration 146, loss = 0.47290281\n",
            "Iteration 147, loss = 0.47154879\n",
            "Iteration 148, loss = 0.46919477\n",
            "Iteration 149, loss = 0.47140531\n",
            "Iteration 150, loss = 0.47070103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.90952303\n",
            "Iteration 2, loss = 0.77155333\n",
            "Iteration 3, loss = 0.67094423\n",
            "Iteration 4, loss = 0.62256267\n",
            "Iteration 5, loss = 0.60199531\n",
            "Iteration 6, loss = 0.59268166\n",
            "Iteration 7, loss = 0.58716016\n",
            "Iteration 8, loss = 0.58316253\n",
            "Iteration 9, loss = 0.58015802\n",
            "Iteration 10, loss = 0.57589625\n",
            "Iteration 11, loss = 0.57238103\n",
            "Iteration 12, loss = 0.56950780\n",
            "Iteration 13, loss = 0.56551747\n",
            "Iteration 14, loss = 0.56266632\n",
            "Iteration 15, loss = 0.56045276\n",
            "Iteration 16, loss = 0.55996391\n",
            "Iteration 17, loss = 0.56033396\n",
            "Iteration 18, loss = 0.55859220\n",
            "Iteration 19, loss = 0.55699684\n",
            "Iteration 20, loss = 0.55853159\n",
            "Iteration 21, loss = 0.55800404\n",
            "Iteration 22, loss = 0.55612568\n",
            "Iteration 23, loss = 0.55443691\n",
            "Iteration 24, loss = 0.55519688\n",
            "Iteration 25, loss = 0.55540400\n",
            "Iteration 26, loss = 0.55363007\n",
            "Iteration 27, loss = 0.55216340\n",
            "Iteration 28, loss = 0.55081333\n",
            "Iteration 29, loss = 0.55203330\n",
            "Iteration 30, loss = 0.55126833\n",
            "Iteration 31, loss = 0.55193759\n",
            "Iteration 32, loss = 0.54991332\n",
            "Iteration 33, loss = 0.54961661\n",
            "Iteration 34, loss = 0.54899348\n",
            "Iteration 35, loss = 0.54876516\n",
            "Iteration 36, loss = 0.54808754\n",
            "Iteration 37, loss = 0.54874480\n",
            "Iteration 38, loss = 0.54786161\n",
            "Iteration 39, loss = 0.54812496\n",
            "Iteration 40, loss = 0.54753323\n",
            "Iteration 41, loss = 0.54695736\n",
            "Iteration 42, loss = 0.54787232\n",
            "Iteration 43, loss = 0.54571606\n",
            "Iteration 44, loss = 0.54937040\n",
            "Iteration 45, loss = 0.54782186\n",
            "Iteration 46, loss = 0.54661798\n",
            "Iteration 47, loss = 0.54675632\n",
            "Iteration 48, loss = 0.54689029\n",
            "Iteration 49, loss = 0.54523948\n",
            "Iteration 50, loss = 0.54503063\n",
            "Iteration 51, loss = 0.54525127\n",
            "Iteration 52, loss = 0.54468624\n",
            "Iteration 53, loss = 0.54400302\n",
            "Iteration 54, loss = 0.54391423\n",
            "Iteration 55, loss = 0.54280670\n",
            "Iteration 56, loss = 0.54259832\n",
            "Iteration 57, loss = 0.54213790\n",
            "Iteration 58, loss = 0.54282466\n",
            "Iteration 59, loss = 0.54178980\n",
            "Iteration 60, loss = 0.54152020\n",
            "Iteration 61, loss = 0.54155992\n",
            "Iteration 62, loss = 0.54119488\n",
            "Iteration 63, loss = 0.54071023\n",
            "Iteration 64, loss = 0.54030722\n",
            "Iteration 65, loss = 0.53913105\n",
            "Iteration 66, loss = 0.53989860\n",
            "Iteration 67, loss = 0.54074834\n",
            "Iteration 68, loss = 0.54150445\n",
            "Iteration 69, loss = 0.53777836\n",
            "Iteration 70, loss = 0.53800642\n",
            "Iteration 71, loss = 0.53700912\n",
            "Iteration 72, loss = 0.53829864\n",
            "Iteration 73, loss = 0.53702260\n",
            "Iteration 74, loss = 0.53586493\n",
            "Iteration 75, loss = 0.54016969\n",
            "Iteration 76, loss = 0.53650803\n",
            "Iteration 77, loss = 0.53598665\n",
            "Iteration 78, loss = 0.53578596\n",
            "Iteration 79, loss = 0.53630111\n",
            "Iteration 80, loss = 0.53602261\n",
            "Iteration 81, loss = 0.53346023\n",
            "Iteration 82, loss = 0.53441862\n",
            "Iteration 83, loss = 0.53432345\n",
            "Iteration 84, loss = 0.53342583\n",
            "Iteration 85, loss = 0.53437527\n",
            "Iteration 86, loss = 0.53154339\n",
            "Iteration 87, loss = 0.53295855\n",
            "Iteration 88, loss = 0.53567582\n",
            "Iteration 89, loss = 0.53415989\n",
            "Iteration 90, loss = 0.52991798\n",
            "Iteration 91, loss = 0.53273745\n",
            "Iteration 92, loss = 0.52870751\n",
            "Iteration 93, loss = 0.52784626\n",
            "Iteration 94, loss = 0.52855230\n",
            "Iteration 95, loss = 0.53029506\n",
            "Iteration 96, loss = 0.52741036\n",
            "Iteration 97, loss = 0.52717025\n",
            "Iteration 98, loss = 0.52456752\n",
            "Iteration 99, loss = 0.52656043\n",
            "Iteration 100, loss = 0.52491723\n",
            "Iteration 101, loss = 0.52483333\n",
            "Iteration 102, loss = 0.52385748\n",
            "Iteration 103, loss = 0.52248737\n",
            "Iteration 104, loss = 0.52100438\n",
            "Iteration 105, loss = 0.52075252\n",
            "Iteration 106, loss = 0.52322728\n",
            "Iteration 107, loss = 0.52289920\n",
            "Iteration 108, loss = 0.51981161\n",
            "Iteration 109, loss = 0.52062543\n",
            "Iteration 110, loss = 0.51794407\n",
            "Iteration 111, loss = 0.52074644\n",
            "Iteration 112, loss = 0.51821047\n",
            "Iteration 113, loss = 0.51730661\n",
            "Iteration 114, loss = 0.51684496\n",
            "Iteration 115, loss = 0.51611415\n",
            "Iteration 116, loss = 0.51614487\n",
            "Iteration 117, loss = 0.51452083\n",
            "Iteration 118, loss = 0.51238568\n",
            "Iteration 119, loss = 0.51315305\n",
            "Iteration 120, loss = 0.51433388\n",
            "Iteration 121, loss = 0.51170358\n",
            "Iteration 122, loss = 0.51384342\n",
            "Iteration 123, loss = 0.51519640\n",
            "Iteration 124, loss = 0.51469395\n",
            "Iteration 125, loss = 0.51220491\n",
            "Iteration 126, loss = 0.50915291\n",
            "Iteration 127, loss = 0.50821678\n",
            "Iteration 128, loss = 0.50649950\n",
            "Iteration 129, loss = 0.50748697\n",
            "Iteration 130, loss = 0.50801146\n",
            "Iteration 131, loss = 0.50718197\n",
            "Iteration 132, loss = 0.50999426\n",
            "Iteration 133, loss = 0.50587073\n",
            "Iteration 134, loss = 0.50716233\n",
            "Iteration 135, loss = 0.50597473\n",
            "Iteration 136, loss = 0.50702109\n",
            "Iteration 137, loss = 0.50638497\n",
            "Iteration 138, loss = 0.50359542\n",
            "Iteration 139, loss = 0.50938738\n",
            "Iteration 140, loss = 0.50860421\n",
            "Iteration 141, loss = 0.50834190\n",
            "Iteration 142, loss = 0.50732540\n",
            "Iteration 143, loss = 0.50718784\n",
            "Iteration 144, loss = 0.50687248\n",
            "Iteration 145, loss = 0.50995060\n",
            "Iteration 146, loss = 0.50964205\n",
            "Iteration 147, loss = 0.50568991\n",
            "Iteration 148, loss = 0.50419144\n",
            "Iteration 149, loss = 0.50265334\n",
            "Iteration 150, loss = 0.50231836\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 150 and for layer number 4 : 0.68875\n",
            "Iteration 1, loss = 0.84283593\n",
            "Iteration 2, loss = 0.67832748\n",
            "Iteration 3, loss = 0.61850827\n",
            "Iteration 4, loss = 0.60382734\n",
            "Iteration 5, loss = 0.59477653\n",
            "Iteration 6, loss = 0.59056898\n",
            "Iteration 7, loss = 0.58162134\n",
            "Iteration 8, loss = 0.57169835"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 9, loss = 0.56715602\n",
            "Iteration 10, loss = 0.56250128\n",
            "Iteration 11, loss = 0.56056030\n",
            "Iteration 12, loss = 0.55803894\n",
            "Iteration 13, loss = 0.55555691\n",
            "Iteration 14, loss = 0.55356033\n",
            "Iteration 15, loss = 0.54894640\n",
            "Iteration 16, loss = 0.54914943\n",
            "Iteration 17, loss = 0.54874113\n",
            "Iteration 18, loss = 0.54701632\n",
            "Iteration 19, loss = 0.54482747\n",
            "Iteration 20, loss = 0.54364637\n",
            "Iteration 21, loss = 0.54290350\n",
            "Iteration 22, loss = 0.54365119\n",
            "Iteration 23, loss = 0.54035673\n",
            "Iteration 24, loss = 0.53845438\n",
            "Iteration 25, loss = 0.53723289\n",
            "Iteration 26, loss = 0.53636142\n",
            "Iteration 27, loss = 0.53484490\n",
            "Iteration 28, loss = 0.53512642\n",
            "Iteration 29, loss = 0.53544812\n",
            "Iteration 30, loss = 0.53257999\n",
            "Iteration 31, loss = 0.53591516\n",
            "Iteration 32, loss = 0.53339687\n",
            "Iteration 33, loss = 0.53051295\n",
            "Iteration 34, loss = 0.52991251\n",
            "Iteration 35, loss = 0.52986860\n",
            "Iteration 36, loss = 0.52797787\n",
            "Iteration 37, loss = 0.52785962\n",
            "Iteration 38, loss = 0.52742171\n",
            "Iteration 39, loss = 0.52639113\n",
            "Iteration 40, loss = 0.52615978\n",
            "Iteration 41, loss = 0.52616046\n",
            "Iteration 42, loss = 0.52438233\n",
            "Iteration 43, loss = 0.52450988\n",
            "Iteration 44, loss = 0.52431472\n",
            "Iteration 45, loss = 0.52376981\n",
            "Iteration 46, loss = 0.52195023\n",
            "Iteration 47, loss = 0.52206280\n",
            "Iteration 48, loss = 0.52291998\n",
            "Iteration 49, loss = 0.52195095\n",
            "Iteration 50, loss = 0.52108269\n",
            "Iteration 51, loss = 0.52130216\n",
            "Iteration 52, loss = 0.52001568\n",
            "Iteration 53, loss = 0.52004530\n",
            "Iteration 54, loss = 0.52191695\n",
            "Iteration 55, loss = 0.52080266\n",
            "Iteration 56, loss = 0.51808299\n",
            "Iteration 57, loss = 0.51748947\n",
            "Iteration 58, loss = 0.51716362\n",
            "Iteration 59, loss = 0.51694316\n",
            "Iteration 60, loss = 0.51658528\n",
            "Iteration 61, loss = 0.51648336\n",
            "Iteration 62, loss = 0.51510877\n",
            "Iteration 63, loss = 0.51749582\n",
            "Iteration 64, loss = 0.51449864\n",
            "Iteration 65, loss = 0.51468475\n",
            "Iteration 66, loss = 0.51606903\n",
            "Iteration 67, loss = 0.51518097\n",
            "Iteration 68, loss = 0.51455031\n",
            "Iteration 69, loss = 0.51350630\n",
            "Iteration 70, loss = 0.51362597\n",
            "Iteration 71, loss = 0.51534284\n",
            "Iteration 72, loss = 0.51294395\n",
            "Iteration 73, loss = 0.51181823\n",
            "Iteration 74, loss = 0.51195315\n",
            "Iteration 75, loss = 0.51020922\n",
            "Iteration 76, loss = 0.50961620\n",
            "Iteration 77, loss = 0.50872978\n",
            "Iteration 78, loss = 0.50980573\n",
            "Iteration 79, loss = 0.50981380\n",
            "Iteration 80, loss = 0.51025917\n",
            "Iteration 81, loss = 0.51557245\n",
            "Iteration 82, loss = 0.51404262\n",
            "Iteration 83, loss = 0.50811045\n",
            "Iteration 84, loss = 0.51117944\n",
            "Iteration 85, loss = 0.51408823\n",
            "Iteration 86, loss = 0.50714546\n",
            "Iteration 87, loss = 0.50648920\n",
            "Iteration 88, loss = 0.50683883\n",
            "Iteration 89, loss = 0.50483630\n",
            "Iteration 90, loss = 0.50519714\n",
            "Iteration 91, loss = 0.50637432\n",
            "Iteration 92, loss = 0.50509134\n",
            "Iteration 93, loss = 0.50572160\n",
            "Iteration 94, loss = 0.50471110\n",
            "Iteration 95, loss = 0.50551280\n",
            "Iteration 96, loss = 0.50375059\n",
            "Iteration 97, loss = 0.50444488\n",
            "Iteration 98, loss = 0.50495500\n",
            "Iteration 99, loss = 0.50372446\n",
            "Iteration 100, loss = 0.50397249\n",
            "Iteration 101, loss = 0.50584103\n",
            "Iteration 102, loss = 0.50538688\n",
            "Iteration 103, loss = 0.50533859\n",
            "Iteration 104, loss = 0.50522541\n",
            "Iteration 105, loss = 0.50313477\n",
            "Iteration 106, loss = 0.50375229\n",
            "Iteration 107, loss = 0.50157035\n",
            "Iteration 108, loss = 0.50024633\n",
            "Iteration 109, loss = 0.50053768\n",
            "Iteration 110, loss = 0.50180636\n",
            "Iteration 111, loss = 0.50104015\n",
            "Iteration 112, loss = 0.49996236\n",
            "Iteration 113, loss = 0.49994954\n",
            "Iteration 114, loss = 0.50009725\n",
            "Iteration 115, loss = 0.49886425\n",
            "Iteration 116, loss = 0.49794556\n",
            "Iteration 117, loss = 0.49915618\n",
            "Iteration 118, loss = 0.49714965\n",
            "Iteration 119, loss = 0.49676612\n",
            "Iteration 120, loss = 0.49755745\n",
            "Iteration 121, loss = 0.49675014\n",
            "Iteration 122, loss = 0.49640613\n",
            "Iteration 123, loss = 0.49767010\n",
            "Iteration 124, loss = 0.49764428\n",
            "Iteration 125, loss = 0.49774683\n",
            "Iteration 126, loss = 0.49739006\n",
            "Iteration 127, loss = 0.49544826\n",
            "Iteration 128, loss = 0.49784077\n",
            "Iteration 129, loss = 0.49644203\n",
            "Iteration 130, loss = 0.49657157\n",
            "Iteration 131, loss = 0.49466084\n",
            "Iteration 132, loss = 0.49522061\n",
            "Iteration 133, loss = 0.49614322\n",
            "Iteration 134, loss = 0.49233119\n",
            "Iteration 135, loss = 0.49259802\n",
            "Iteration 136, loss = 0.49261920\n",
            "Iteration 137, loss = 0.49548365\n",
            "Iteration 138, loss = 0.49462814\n",
            "Iteration 139, loss = 0.49515159\n",
            "Iteration 140, loss = 0.49499592\n",
            "Iteration 141, loss = 0.49543857\n",
            "Iteration 142, loss = 0.49233858\n",
            "Iteration 143, loss = 0.49309902\n",
            "Iteration 144, loss = 0.49020281\n",
            "Iteration 145, loss = 0.49189385\n",
            "Iteration 146, loss = 0.49148586\n",
            "Iteration 147, loss = 0.49266871\n",
            "Iteration 148, loss = 0.49050547\n",
            "Iteration 149, loss = 0.49223937\n",
            "Iteration 150, loss = 0.49102742\n",
            "Iteration 1, loss = 0.84211562\n",
            "Iteration 2, loss = 0.67390273\n",
            "Iteration 3, loss = 0.61479397\n",
            "Iteration 4, loss = 0.59817770\n",
            "Iteration 5, loss = 0.58643524\n",
            "Iteration 6, loss = 0.57940279\n",
            "Iteration 7, loss = 0.57108334\n",
            "Iteration 8, loss = 0.56345206\n",
            "Iteration 9, loss = 0.55760032\n",
            "Iteration 10, loss = 0.55137476\n",
            "Iteration 11, loss = 0.54982593\n",
            "Iteration 12, loss = 0.54759444\n",
            "Iteration 13, loss = 0.54480473\n",
            "Iteration 14, loss = 0.54105592\n",
            "Iteration 15, loss = 0.53799031\n",
            "Iteration 16, loss = 0.53554759\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 17, loss = 0.53296703\n",
            "Iteration 18, loss = 0.53051284\n",
            "Iteration 19, loss = 0.52790683\n",
            "Iteration 20, loss = 0.52677164\n",
            "Iteration 21, loss = 0.52485005\n",
            "Iteration 22, loss = 0.52480127\n",
            "Iteration 23, loss = 0.52143077\n",
            "Iteration 24, loss = 0.52039570\n",
            "Iteration 25, loss = 0.51926551\n",
            "Iteration 26, loss = 0.51855653\n",
            "Iteration 27, loss = 0.51683205\n",
            "Iteration 28, loss = 0.51569551\n",
            "Iteration 29, loss = 0.51493485\n",
            "Iteration 30, loss = 0.51421020\n",
            "Iteration 31, loss = 0.51674064\n",
            "Iteration 32, loss = 0.51260548\n",
            "Iteration 33, loss = 0.51040642\n",
            "Iteration 34, loss = 0.50939175\n",
            "Iteration 35, loss = 0.50888292\n",
            "Iteration 36, loss = 0.50876223\n",
            "Iteration 37, loss = 0.50628136\n",
            "Iteration 38, loss = 0.50496776\n",
            "Iteration 39, loss = 0.50289192\n",
            "Iteration 40, loss = 0.50200486\n",
            "Iteration 41, loss = 0.50190053\n",
            "Iteration 42, loss = 0.50099544\n",
            "Iteration 43, loss = 0.50033700\n",
            "Iteration 44, loss = 0.49990491\n",
            "Iteration 45, loss = 0.49703635\n",
            "Iteration 46, loss = 0.49645873\n",
            "Iteration 47, loss = 0.49625000\n",
            "Iteration 48, loss = 0.49583677\n",
            "Iteration 49, loss = 0.49334537\n",
            "Iteration 50, loss = 0.49351033\n",
            "Iteration 51, loss = 0.49339818\n",
            "Iteration 52, loss = 0.49169668\n",
            "Iteration 53, loss = 0.49150701\n",
            "Iteration 54, loss = 0.49566927\n",
            "Iteration 55, loss = 0.49062902\n",
            "Iteration 56, loss = 0.48908149\n",
            "Iteration 57, loss = 0.48730963\n",
            "Iteration 58, loss = 0.48737917\n",
            "Iteration 59, loss = 0.48786171\n",
            "Iteration 60, loss = 0.48735379\n",
            "Iteration 61, loss = 0.48919848\n",
            "Iteration 62, loss = 0.48604695\n",
            "Iteration 63, loss = 0.48791279\n",
            "Iteration 64, loss = 0.48447282\n",
            "Iteration 65, loss = 0.48392292\n",
            "Iteration 66, loss = 0.48710397\n",
            "Iteration 67, loss = 0.48497721\n",
            "Iteration 68, loss = 0.48189699\n",
            "Iteration 69, loss = 0.48215848\n",
            "Iteration 70, loss = 0.48396642\n",
            "Iteration 71, loss = 0.48472021\n",
            "Iteration 72, loss = 0.48269523\n",
            "Iteration 73, loss = 0.48006770\n",
            "Iteration 74, loss = 0.47949046\n",
            "Iteration 75, loss = 0.47917436\n",
            "Iteration 76, loss = 0.47986172\n",
            "Iteration 77, loss = 0.47652529\n",
            "Iteration 78, loss = 0.47572204\n",
            "Iteration 79, loss = 0.47605137\n",
            "Iteration 80, loss = 0.47486116\n",
            "Iteration 81, loss = 0.47661630\n",
            "Iteration 82, loss = 0.47626359\n",
            "Iteration 83, loss = 0.47833453\n",
            "Iteration 84, loss = 0.47666945\n",
            "Iteration 85, loss = 0.48468544\n",
            "Iteration 86, loss = 0.47513421\n",
            "Iteration 87, loss = 0.47577963\n",
            "Iteration 88, loss = 0.47356521\n",
            "Iteration 89, loss = 0.47354569\n",
            "Iteration 90, loss = 0.47358204\n",
            "Iteration 91, loss = 0.47377450\n",
            "Iteration 92, loss = 0.47338047\n",
            "Iteration 93, loss = 0.47247782\n",
            "Iteration 94, loss = 0.47194345\n",
            "Iteration 95, loss = 0.47182012\n",
            "Iteration 96, loss = 0.47099111\n",
            "Iteration 97, loss = 0.47485566\n",
            "Iteration 98, loss = 0.47394550\n",
            "Iteration 99, loss = 0.47321010\n",
            "Iteration 100, loss = 0.47317758\n",
            "Iteration 101, loss = 0.47147078\n",
            "Iteration 102, loss = 0.46973482\n",
            "Iteration 103, loss = 0.47088729\n",
            "Iteration 104, loss = 0.47040228\n",
            "Iteration 105, loss = 0.47415798\n",
            "Iteration 106, loss = 0.47081625\n",
            "Iteration 107, loss = 0.47069119\n",
            "Iteration 108, loss = 0.46858736\n",
            "Iteration 109, loss = 0.46535988\n",
            "Iteration 110, loss = 0.46921133\n",
            "Iteration 111, loss = 0.47624427\n",
            "Iteration 112, loss = 0.46987930\n",
            "Iteration 113, loss = 0.46703870\n",
            "Iteration 114, loss = 0.46861779\n",
            "Iteration 115, loss = 0.47201319\n",
            "Iteration 116, loss = 0.46866252\n",
            "Iteration 117, loss = 0.46893663\n",
            "Iteration 118, loss = 0.46594707\n",
            "Iteration 119, loss = 0.46587544\n",
            "Iteration 120, loss = 0.46586999\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.84194648\n",
            "Iteration 2, loss = 0.66554851\n",
            "Iteration 3, loss = 0.61225024\n",
            "Iteration 4, loss = 0.59987886\n",
            "Iteration 5, loss = 0.59202196\n",
            "Iteration 6, loss = 0.58325873\n",
            "Iteration 7, loss = 0.57298718\n",
            "Iteration 8, loss = 0.56206000\n",
            "Iteration 9, loss = 0.55315409\n",
            "Iteration 10, loss = 0.54698521\n",
            "Iteration 11, loss = 0.54442773\n",
            "Iteration 12, loss = 0.54226291\n",
            "Iteration 13, loss = 0.53980072\n",
            "Iteration 14, loss = 0.53599988\n",
            "Iteration 15, loss = 0.53313323\n",
            "Iteration 16, loss = 0.52934416\n",
            "Iteration 17, loss = 0.52638109\n",
            "Iteration 18, loss = 0.52580879\n",
            "Iteration 19, loss = 0.52426243\n",
            "Iteration 20, loss = 0.52290946\n",
            "Iteration 21, loss = 0.52268451\n",
            "Iteration 22, loss = 0.52343882\n",
            "Iteration 23, loss = 0.52150082\n",
            "Iteration 24, loss = 0.52105498\n",
            "Iteration 25, loss = 0.51995877\n",
            "Iteration 26, loss = 0.51906444\n",
            "Iteration 27, loss = 0.51760723\n",
            "Iteration 28, loss = 0.51746923\n",
            "Iteration 29, loss = 0.51659429\n",
            "Iteration 30, loss = 0.51628310\n",
            "Iteration 31, loss = 0.51696056\n",
            "Iteration 32, loss = 0.51532952\n",
            "Iteration 33, loss = 0.51492982\n",
            "Iteration 34, loss = 0.51395807\n",
            "Iteration 35, loss = 0.51293487\n",
            "Iteration 36, loss = 0.51311644\n",
            "Iteration 37, loss = 0.51206223\n",
            "Iteration 38, loss = 0.51187460\n",
            "Iteration 39, loss = 0.51152712\n",
            "Iteration 40, loss = 0.51025039\n",
            "Iteration 41, loss = 0.51022046\n",
            "Iteration 42, loss = 0.51081281\n",
            "Iteration 43, loss = 0.51040966\n",
            "Iteration 44, loss = 0.51026804\n",
            "Iteration 45, loss = 0.50922244\n",
            "Iteration 46, loss = 0.50916438\n",
            "Iteration 47, loss = 0.50888024\n",
            "Iteration 48, loss = 0.50907711\n",
            "Iteration 49, loss = 0.50774179\n",
            "Iteration 50, loss = 0.50710401\n",
            "Iteration 51, loss = 0.50764802\n",
            "Iteration 52, loss = 0.50624387\n",
            "Iteration 53, loss = 0.50559071\n",
            "Iteration 54, loss = 0.50518696\n",
            "Iteration 55, loss = 0.50350782\n",
            "Iteration 56, loss = 0.50403498\n",
            "Iteration 57, loss = 0.50308545\n",
            "Iteration 58, loss = 0.50276083\n",
            "Iteration 59, loss = 0.50331295\n",
            "Iteration 60, loss = 0.50236108\n",
            "Iteration 61, loss = 0.50171369\n",
            "Iteration 62, loss = 0.50260973\n",
            "Iteration 63, loss = 0.50167126\n",
            "Iteration 64, loss = 0.50123775\n",
            "Iteration 65, loss = 0.49866415\n",
            "Iteration 66, loss = 0.50089686\n",
            "Iteration 67, loss = 0.49977304\n",
            "Iteration 68, loss = 0.49736768\n",
            "Iteration 69, loss = 0.49733328\n",
            "Iteration 70, loss = 0.49723094\n",
            "Iteration 71, loss = 0.49939928\n",
            "Iteration 72, loss = 0.49655591\n",
            "Iteration 73, loss = 0.49371061\n",
            "Iteration 74, loss = 0.49284754\n",
            "Iteration 75, loss = 0.49211623\n",
            "Iteration 76, loss = 0.49079801\n",
            "Iteration 77, loss = 0.48971083\n",
            "Iteration 78, loss = 0.48842846\n",
            "Iteration 79, loss = 0.48863192\n",
            "Iteration 80, loss = 0.48769454\n",
            "Iteration 81, loss = 0.48869401\n",
            "Iteration 82, loss = 0.48656822\n",
            "Iteration 83, loss = 0.48627403\n",
            "Iteration 84, loss = 0.48614048\n",
            "Iteration 85, loss = 0.48758305\n",
            "Iteration 86, loss = 0.48417598\n",
            "Iteration 87, loss = 0.48175374\n",
            "Iteration 88, loss = 0.48032217\n",
            "Iteration 89, loss = 0.48057079\n",
            "Iteration 90, loss = 0.48163173\n",
            "Iteration 91, loss = 0.48031648\n",
            "Iteration 92, loss = 0.47936347\n",
            "Iteration 93, loss = 0.47888769\n",
            "Iteration 94, loss = 0.47807031\n",
            "Iteration 95, loss = 0.47792276\n",
            "Iteration 96, loss = 0.47669075\n",
            "Iteration 97, loss = 0.47703885\n",
            "Iteration 98, loss = 0.47652764\n",
            "Iteration 99, loss = 0.47698218\n",
            "Iteration 100, loss = 0.47609304\n",
            "Iteration 101, loss = 0.47529402\n",
            "Iteration 102, loss = 0.47466444\n",
            "Iteration 103, loss = 0.47422635\n",
            "Iteration 104, loss = 0.47371837\n",
            "Iteration 105, loss = 0.47493566\n",
            "Iteration 106, loss = 0.47400548\n",
            "Iteration 107, loss = 0.47320379\n",
            "Iteration 108, loss = 0.47309130\n",
            "Iteration 109, loss = 0.47056961\n",
            "Iteration 110, loss = 0.47351695\n",
            "Iteration 111, loss = 0.47489974\n",
            "Iteration 112, loss = 0.47332189\n",
            "Iteration 113, loss = 0.47146561\n",
            "Iteration 114, loss = 0.47004990\n",
            "Iteration 115, loss = 0.47355515\n",
            "Iteration 116, loss = 0.47397915\n",
            "Iteration 117, loss = 0.47466726\n",
            "Iteration 118, loss = 0.47146515\n",
            "Iteration 119, loss = 0.46944354\n",
            "Iteration 120, loss = 0.47083722\n",
            "Iteration 121, loss = 0.46829298\n",
            "Iteration 122, loss = 0.47049314\n",
            "Iteration 123, loss = 0.46855504\n",
            "Iteration 124, loss = 0.47046074\n",
            "Iteration 125, loss = 0.46915822\n",
            "Iteration 126, loss = 0.46790139\n",
            "Iteration 127, loss = 0.46876398\n",
            "Iteration 128, loss = 0.46816952\n",
            "Iteration 129, loss = 0.46819922\n",
            "Iteration 130, loss = 0.46776867\n",
            "Iteration 131, loss = 0.46596833\n",
            "Iteration 132, loss = 0.46531227\n",
            "Iteration 133, loss = 0.46661815\n",
            "Iteration 134, loss = 0.46529771\n",
            "Iteration 135, loss = 0.46387486\n",
            "Iteration 136, loss = 0.46544262\n",
            "Iteration 137, loss = 0.46535451\n",
            "Iteration 138, loss = 0.46541958\n",
            "Iteration 139, loss = 0.46533499\n",
            "Iteration 140, loss = 0.46329613\n",
            "Iteration 141, loss = 0.46399936\n",
            "Iteration 142, loss = 0.46287439\n",
            "Iteration 143, loss = 0.46369668\n",
            "Iteration 144, loss = 0.46330816\n",
            "Iteration 145, loss = 0.46335661\n",
            "Iteration 146, loss = 0.46403232\n",
            "Iteration 147, loss = 0.46531830\n",
            "Iteration 148, loss = 0.46581724\n",
            "Iteration 149, loss = 0.46551850\n",
            "Iteration 150, loss = 0.46611122\n",
            "Iteration 1, loss = 0.83713456\n",
            "Iteration 2, loss = 0.66083566\n",
            "Iteration 3, loss = 0.62354131\n",
            "Iteration 4, loss = 0.60812662\n",
            "Iteration 5, loss = 0.59948788\n",
            "Iteration 6, loss = 0.59562370\n",
            "Iteration 7, loss = 0.58895751\n",
            "Iteration 8, loss = 0.57947492\n",
            "Iteration 9, loss = 0.57067064\n",
            "Iteration 10, loss = 0.56219342\n",
            "Iteration 11, loss = 0.55698371\n",
            "Iteration 12, loss = 0.55391316\n",
            "Iteration 13, loss = 0.55129977\n",
            "Iteration 14, loss = 0.54890671\n",
            "Iteration 15, loss = 0.54643885\n",
            "Iteration 16, loss = 0.54297314\n",
            "Iteration 17, loss = 0.54102246\n",
            "Iteration 18, loss = 0.54129445\n",
            "Iteration 19, loss = 0.53813457\n",
            "Iteration 20, loss = 0.53584825\n",
            "Iteration 21, loss = 0.53505185\n",
            "Iteration 22, loss = 0.53380029\n",
            "Iteration 23, loss = 0.53263117\n",
            "Iteration 24, loss = 0.53059466\n",
            "Iteration 25, loss = 0.53043742\n",
            "Iteration 26, loss = 0.52878689\n",
            "Iteration 27, loss = 0.52721957\n",
            "Iteration 28, loss = 0.52509053\n",
            "Iteration 29, loss = 0.52463802\n",
            "Iteration 30, loss = 0.52600622\n",
            "Iteration 31, loss = 0.52533686\n",
            "Iteration 32, loss = 0.52282068\n",
            "Iteration 33, loss = 0.52207370\n",
            "Iteration 34, loss = 0.52197151\n",
            "Iteration 35, loss = 0.52050410\n",
            "Iteration 36, loss = 0.51890510\n",
            "Iteration 37, loss = 0.51812398\n",
            "Iteration 38, loss = 0.51735240\n",
            "Iteration 39, loss = 0.51699532\n",
            "Iteration 40, loss = 0.51686667\n",
            "Iteration 41, loss = 0.51610695\n",
            "Iteration 42, loss = 0.51669172\n",
            "Iteration 43, loss = 0.51322902\n",
            "Iteration 44, loss = 0.51318415\n",
            "Iteration 45, loss = 0.51185272\n",
            "Iteration 46, loss = 0.51094082\n",
            "Iteration 47, loss = 0.51111908\n",
            "Iteration 48, loss = 0.50958323\n",
            "Iteration 49, loss = 0.51071506\n",
            "Iteration 50, loss = 0.50797224\n",
            "Iteration 51, loss = 0.50855548\n",
            "Iteration 52, loss = 0.50619339\n",
            "Iteration 53, loss = 0.50607627"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 54, loss = 0.50619348\n",
            "Iteration 55, loss = 0.50297138\n",
            "Iteration 56, loss = 0.50363331\n",
            "Iteration 57, loss = 0.50120507\n",
            "Iteration 58, loss = 0.50208289\n",
            "Iteration 59, loss = 0.50219130\n",
            "Iteration 60, loss = 0.50081060\n",
            "Iteration 61, loss = 0.50034864\n",
            "Iteration 62, loss = 0.49877738\n",
            "Iteration 63, loss = 0.49889385\n",
            "Iteration 64, loss = 0.49910741\n",
            "Iteration 65, loss = 0.49653541\n",
            "Iteration 66, loss = 0.49809058\n",
            "Iteration 67, loss = 0.49694000\n",
            "Iteration 68, loss = 0.49540907\n",
            "Iteration 69, loss = 0.49503097\n",
            "Iteration 70, loss = 0.49322977\n",
            "Iteration 71, loss = 0.49398712\n",
            "Iteration 72, loss = 0.49298884\n",
            "Iteration 73, loss = 0.49221089\n",
            "Iteration 74, loss = 0.49056898\n",
            "Iteration 75, loss = 0.49090439\n",
            "Iteration 76, loss = 0.49178349\n",
            "Iteration 77, loss = 0.48995355\n",
            "Iteration 78, loss = 0.48955926\n",
            "Iteration 79, loss = 0.49071445\n",
            "Iteration 80, loss = 0.48789645\n",
            "Iteration 81, loss = 0.49334601\n",
            "Iteration 82, loss = 0.48766543\n",
            "Iteration 83, loss = 0.49117912\n",
            "Iteration 84, loss = 0.49090990\n",
            "Iteration 85, loss = 0.49150852\n",
            "Iteration 86, loss = 0.48896881\n",
            "Iteration 87, loss = 0.48385274\n",
            "Iteration 88, loss = 0.48921794\n",
            "Iteration 89, loss = 0.48589666\n",
            "Iteration 90, loss = 0.48460451\n",
            "Iteration 91, loss = 0.48482953\n",
            "Iteration 92, loss = 0.48133251\n",
            "Iteration 93, loss = 0.47927352\n",
            "Iteration 94, loss = 0.47865817\n",
            "Iteration 95, loss = 0.47757052\n",
            "Iteration 96, loss = 0.47917336\n",
            "Iteration 97, loss = 0.47724566\n",
            "Iteration 98, loss = 0.47747433\n",
            "Iteration 99, loss = 0.47479210\n",
            "Iteration 100, loss = 0.47134891\n",
            "Iteration 101, loss = 0.47284035\n",
            "Iteration 102, loss = 0.47352995\n",
            "Iteration 103, loss = 0.47224160\n",
            "Iteration 104, loss = 0.47212225\n",
            "Iteration 105, loss = 0.47184532\n",
            "Iteration 106, loss = 0.46958548\n",
            "Iteration 107, loss = 0.47249782\n",
            "Iteration 108, loss = 0.47339093\n",
            "Iteration 109, loss = 0.46895803\n",
            "Iteration 110, loss = 0.47348572\n",
            "Iteration 111, loss = 0.47387400\n",
            "Iteration 112, loss = 0.47071031\n",
            "Iteration 113, loss = 0.47299048\n",
            "Iteration 114, loss = 0.47027790\n",
            "Iteration 115, loss = 0.47575480\n",
            "Iteration 116, loss = 0.47479950\n",
            "Iteration 117, loss = 0.47291502\n",
            "Iteration 118, loss = 0.46806154\n",
            "Iteration 119, loss = 0.46810702\n",
            "Iteration 120, loss = 0.46852562\n",
            "Iteration 121, loss = 0.46785207\n",
            "Iteration 122, loss = 0.46712060\n",
            "Iteration 123, loss = 0.46379537\n",
            "Iteration 124, loss = 0.46692458\n",
            "Iteration 125, loss = 0.46876335\n",
            "Iteration 126, loss = 0.46553169\n",
            "Iteration 127, loss = 0.46911223\n",
            "Iteration 128, loss = 0.46725812\n",
            "Iteration 129, loss = 0.46746697\n",
            "Iteration 130, loss = 0.47360943\n",
            "Iteration 131, loss = 0.47004586\n",
            "Iteration 132, loss = 0.46820468\n",
            "Iteration 133, loss = 0.47028128\n",
            "Iteration 134, loss = 0.46296306\n",
            "Iteration 135, loss = 0.46389413\n",
            "Iteration 136, loss = 0.46786654\n",
            "Iteration 137, loss = 0.46407171\n",
            "Iteration 138, loss = 0.46591593\n",
            "Iteration 139, loss = 0.46129456\n",
            "Iteration 140, loss = 0.45925773\n",
            "Iteration 141, loss = 0.46237936\n",
            "Iteration 142, loss = 0.46106316\n",
            "Iteration 143, loss = 0.46256290\n",
            "Iteration 144, loss = 0.45925252\n",
            "Iteration 145, loss = 0.45847400\n",
            "Iteration 146, loss = 0.46023258\n",
            "Iteration 147, loss = 0.46050539\n",
            "Iteration 148, loss = 0.46045107\n",
            "Iteration 149, loss = 0.45892109\n",
            "Iteration 150, loss = 0.45637400\n",
            "Iteration 1, loss = 0.84068350\n",
            "Iteration 2, loss = 0.66163454\n",
            "Iteration 3, loss = 0.62089339\n",
            "Iteration 4, loss = 0.60367884\n",
            "Iteration 5, loss = 0.59758809\n",
            "Iteration 6, loss = 0.59053744\n",
            "Iteration 7, loss = 0.58355859\n",
            "Iteration 8, loss = 0.57575257\n",
            "Iteration 9, loss = 0.56752153\n",
            "Iteration 10, loss = 0.56250510\n",
            "Iteration 11, loss = 0.55940582\n",
            "Iteration 12, loss = 0.55725922\n",
            "Iteration 13, loss = 0.55539372\n",
            "Iteration 14, loss = 0.55261931\n",
            "Iteration 15, loss = 0.55125222\n",
            "Iteration 16, loss = 0.54938656\n",
            "Iteration 17, loss = 0.54723075\n",
            "Iteration 18, loss = 0.54611201\n",
            "Iteration 19, loss = 0.54445852\n",
            "Iteration 20, loss = 0.54260693\n",
            "Iteration 21, loss = 0.54191735\n",
            "Iteration 22, loss = 0.54076356\n",
            "Iteration 23, loss = 0.54072044\n",
            "Iteration 24, loss = 0.53816883\n",
            "Iteration 25, loss = 0.53735542\n",
            "Iteration 26, loss = 0.53648829\n",
            "Iteration 27, loss = 0.53660585\n",
            "Iteration 28, loss = 0.53461130\n",
            "Iteration 29, loss = 0.53387902\n",
            "Iteration 30, loss = 0.53469368\n",
            "Iteration 31, loss = 0.53202572\n",
            "Iteration 32, loss = 0.52926070\n",
            "Iteration 33, loss = 0.52892232\n",
            "Iteration 34, loss = 0.52792346\n",
            "Iteration 35, loss = 0.52631842\n",
            "Iteration 36, loss = 0.52525013\n",
            "Iteration 37, loss = 0.52366796\n",
            "Iteration 38, loss = 0.52165717\n",
            "Iteration 39, loss = 0.52017580\n",
            "Iteration 40, loss = 0.52051896\n",
            "Iteration 41, loss = 0.51962111\n",
            "Iteration 42, loss = 0.51778834\n",
            "Iteration 43, loss = 0.51575968\n",
            "Iteration 44, loss = 0.51410563\n",
            "Iteration 45, loss = 0.51679264\n",
            "Iteration 46, loss = 0.51113185\n",
            "Iteration 47, loss = 0.51272641\n",
            "Iteration 48, loss = 0.51033363\n",
            "Iteration 49, loss = 0.51149875\n",
            "Iteration 50, loss = 0.50972279\n",
            "Iteration 51, loss = 0.50659960\n",
            "Iteration 52, loss = 0.50812025\n",
            "Iteration 53, loss = 0.50583664\n",
            "Iteration 54, loss = 0.50413478\n",
            "Iteration 55, loss = 0.50309056\n",
            "Iteration 56, loss = 0.50207817\n",
            "Iteration 57, loss = 0.50064145\n",
            "Iteration 58, loss = 0.50182169\n",
            "Iteration 59, loss = 0.50027190\n",
            "Iteration 60, loss = 0.50034093\n",
            "Iteration 61, loss = 0.50228740\n",
            "Iteration 62, loss = 0.49936254\n",
            "Iteration 63, loss = 0.49841749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 64, loss = 0.49726227\n",
            "Iteration 65, loss = 0.49574820\n",
            "Iteration 66, loss = 0.49597583\n",
            "Iteration 67, loss = 0.49666907\n",
            "Iteration 68, loss = 0.49781259\n",
            "Iteration 69, loss = 0.49598924\n",
            "Iteration 70, loss = 0.49504260\n",
            "Iteration 71, loss = 0.49511654\n",
            "Iteration 72, loss = 0.49587326\n",
            "Iteration 73, loss = 0.49325396\n",
            "Iteration 74, loss = 0.49171675\n",
            "Iteration 75, loss = 0.49379349\n",
            "Iteration 76, loss = 0.49232947\n",
            "Iteration 77, loss = 0.49154613\n",
            "Iteration 78, loss = 0.48956218\n",
            "Iteration 79, loss = 0.49085117\n",
            "Iteration 80, loss = 0.48951285\n",
            "Iteration 81, loss = 0.48964164\n",
            "Iteration 82, loss = 0.48776003\n",
            "Iteration 83, loss = 0.48866045\n",
            "Iteration 84, loss = 0.48743867\n",
            "Iteration 85, loss = 0.48959427\n",
            "Iteration 86, loss = 0.48893899\n",
            "Iteration 87, loss = 0.48581596\n",
            "Iteration 88, loss = 0.48958197\n",
            "Iteration 89, loss = 0.48734166\n",
            "Iteration 90, loss = 0.48549654\n",
            "Iteration 91, loss = 0.48475242\n",
            "Iteration 92, loss = 0.48860945\n",
            "Iteration 93, loss = 0.48724563\n",
            "Iteration 94, loss = 0.48767697\n",
            "Iteration 95, loss = 0.48842445\n",
            "Iteration 96, loss = 0.48431945\n",
            "Iteration 97, loss = 0.48893577\n",
            "Iteration 98, loss = 0.48836483\n",
            "Iteration 99, loss = 0.48372760\n",
            "Iteration 100, loss = 0.48331204\n",
            "Iteration 101, loss = 0.48457879\n",
            "Iteration 102, loss = 0.48378776\n",
            "Iteration 103, loss = 0.48212422\n",
            "Iteration 104, loss = 0.48130812\n",
            "Iteration 105, loss = 0.48237457\n",
            "Iteration 106, loss = 0.48402408\n",
            "Iteration 107, loss = 0.48117475\n",
            "Iteration 108, loss = 0.48229236\n",
            "Iteration 109, loss = 0.48249911\n",
            "Iteration 110, loss = 0.48217632\n",
            "Iteration 111, loss = 0.48300399\n",
            "Iteration 112, loss = 0.48305660\n",
            "Iteration 113, loss = 0.48623112\n",
            "Iteration 114, loss = 0.48033446\n",
            "Iteration 115, loss = 0.48255182\n",
            "Iteration 116, loss = 0.48329862\n",
            "Iteration 117, loss = 0.48245303\n",
            "Iteration 118, loss = 0.47916846\n",
            "Iteration 119, loss = 0.48122489\n",
            "Iteration 120, loss = 0.48029741\n",
            "Iteration 121, loss = 0.48173266\n",
            "Iteration 122, loss = 0.48009463\n",
            "Iteration 123, loss = 0.48139827\n",
            "Iteration 124, loss = 0.48397352\n",
            "Iteration 125, loss = 0.48144800\n",
            "Iteration 126, loss = 0.47902017\n",
            "Iteration 127, loss = 0.48260405\n",
            "Iteration 128, loss = 0.48030403\n",
            "Iteration 129, loss = 0.48113867\n",
            "Iteration 130, loss = 0.48312732\n",
            "Iteration 131, loss = 0.48231551\n",
            "Iteration 132, loss = 0.48290192\n",
            "Iteration 133, loss = 0.48060189\n",
            "Iteration 134, loss = 0.47852087\n",
            "Iteration 135, loss = 0.47857566\n",
            "Iteration 136, loss = 0.47896620\n",
            "Iteration 137, loss = 0.48044220\n",
            "Iteration 138, loss = 0.48120303\n",
            "Iteration 139, loss = 0.48065450\n",
            "Iteration 140, loss = 0.48012169\n",
            "Iteration 141, loss = 0.48184580\n",
            "Iteration 142, loss = 0.47990577\n",
            "Iteration 143, loss = 0.47887212\n",
            "Iteration 144, loss = 0.47922702\n",
            "Iteration 145, loss = 0.47890527\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 150 and for layer number 5 : 0.6737500000000001\n",
            "Iteration 1, loss = 1.17918922\n",
            "Iteration 2, loss = 0.68685959\n",
            "Iteration 3, loss = 0.66908384\n",
            "Iteration 4, loss = 0.63908765\n",
            "Iteration 5, loss = 0.59631417\n",
            "Iteration 6, loss = 0.57411322\n",
            "Iteration 7, loss = 0.56515132\n",
            "Iteration 8, loss = 0.56235259\n",
            "Iteration 9, loss = 0.55879602\n",
            "Iteration 10, loss = 0.55343311\n",
            "Iteration 11, loss = 0.54937387\n",
            "Iteration 12, loss = 0.54565812\n",
            "Iteration 13, loss = 0.54212281\n",
            "Iteration 14, loss = 0.53924739\n",
            "Iteration 15, loss = 0.53679909\n",
            "Iteration 16, loss = 0.53377760\n",
            "Iteration 17, loss = 0.53062494\n",
            "Iteration 18, loss = 0.52882499\n",
            "Iteration 19, loss = 0.52691208\n",
            "Iteration 20, loss = 0.52429166\n",
            "Iteration 21, loss = 0.52287004\n",
            "Iteration 22, loss = 0.52153317\n",
            "Iteration 23, loss = 0.52034916\n",
            "Iteration 24, loss = 0.51827497\n",
            "Iteration 25, loss = 0.51660905\n",
            "Iteration 26, loss = 0.51371597\n",
            "Iteration 27, loss = 0.51319660\n",
            "Iteration 28, loss = 0.51266851\n",
            "Iteration 29, loss = 0.51153150\n",
            "Iteration 30, loss = 0.51012592\n",
            "Iteration 31, loss = 0.50761414\n",
            "Iteration 32, loss = 0.50575164\n",
            "Iteration 33, loss = 0.50502220\n",
            "Iteration 34, loss = 0.50257502\n",
            "Iteration 35, loss = 0.50116088\n",
            "Iteration 36, loss = 0.50085492\n",
            "Iteration 37, loss = 0.50006507\n",
            "Iteration 38, loss = 0.50045593\n",
            "Iteration 39, loss = 0.49817018\n",
            "Iteration 40, loss = 0.49616517\n",
            "Iteration 41, loss = 0.49493665\n",
            "Iteration 42, loss = 0.49382371\n",
            "Iteration 43, loss = 0.49116192\n",
            "Iteration 44, loss = 0.49137707\n",
            "Iteration 45, loss = 0.49019324\n",
            "Iteration 46, loss = 0.48951993\n",
            "Iteration 47, loss = 0.48910536\n",
            "Iteration 48, loss = 0.48741551\n",
            "Iteration 49, loss = 0.48718603\n",
            "Iteration 50, loss = 0.48788111\n",
            "Iteration 51, loss = 0.48658319\n",
            "Iteration 52, loss = 0.48428395\n",
            "Iteration 53, loss = 0.48301730\n",
            "Iteration 54, loss = 0.48312539\n",
            "Iteration 55, loss = 0.48457488\n",
            "Iteration 56, loss = 0.48226929\n",
            "Iteration 57, loss = 0.48317217\n",
            "Iteration 58, loss = 0.48192920\n",
            "Iteration 59, loss = 0.48146113\n",
            "Iteration 60, loss = 0.47980896\n",
            "Iteration 61, loss = 0.48194676\n",
            "Iteration 62, loss = 0.48177424\n",
            "Iteration 63, loss = 0.47940223\n",
            "Iteration 64, loss = 0.47986086\n",
            "Iteration 65, loss = 0.47906337\n",
            "Iteration 66, loss = 0.48176154\n",
            "Iteration 67, loss = 0.47980551\n",
            "Iteration 68, loss = 0.47421164\n",
            "Iteration 69, loss = 0.47866929\n",
            "Iteration 70, loss = 0.47818193\n",
            "Iteration 71, loss = 0.47648211\n",
            "Iteration 72, loss = 0.47809994\n",
            "Iteration 73, loss = 0.47671165\n",
            "Iteration 74, loss = 0.47807209\n",
            "Iteration 75, loss = 0.47386657\n",
            "Iteration 76, loss = 0.47979068\n",
            "Iteration 77, loss = 0.47641987\n",
            "Iteration 78, loss = 0.47519644\n",
            "Iteration 79, loss = 0.47597674\n",
            "Iteration 80, loss = 0.47752150\n",
            "Iteration 81, loss = 0.47264753\n",
            "Iteration 82, loss = 0.47679573\n",
            "Iteration 83, loss = 0.47137165\n",
            "Iteration 84, loss = 0.47241433\n",
            "Iteration 85, loss = 0.47296123\n",
            "Iteration 86, loss = 0.47121904\n",
            "Iteration 87, loss = 0.46978595\n",
            "Iteration 88, loss = 0.46902987\n",
            "Iteration 89, loss = 0.47068566\n",
            "Iteration 90, loss = 0.47041782\n",
            "Iteration 91, loss = 0.46913431\n",
            "Iteration 92, loss = 0.46712164\n",
            "Iteration 93, loss = 0.46786863\n",
            "Iteration 94, loss = 0.47072284\n",
            "Iteration 95, loss = 0.46650386\n",
            "Iteration 96, loss = 0.46759921\n",
            "Iteration 97, loss = 0.46723491\n",
            "Iteration 98, loss = 0.46798436\n",
            "Iteration 99, loss = 0.46617021\n",
            "Iteration 100, loss = 0.46677471\n",
            "Iteration 101, loss = 0.46883795\n",
            "Iteration 102, loss = 0.46501524\n",
            "Iteration 103, loss = 0.46521559\n",
            "Iteration 104, loss = 0.46640698\n",
            "Iteration 105, loss = 0.46541906\n",
            "Iteration 106, loss = 0.46309067\n",
            "Iteration 107, loss = 0.46109671\n",
            "Iteration 108, loss = 0.46360672\n",
            "Iteration 109, loss = 0.46457365\n",
            "Iteration 110, loss = 0.46823808\n",
            "Iteration 111, loss = 0.46414644\n",
            "Iteration 112, loss = 0.46096768\n",
            "Iteration 113, loss = 0.46169197\n",
            "Iteration 114, loss = 0.46468778\n",
            "Iteration 115, loss = 0.46661842\n",
            "Iteration 116, loss = 0.46006286\n",
            "Iteration 117, loss = 0.45994884\n",
            "Iteration 118, loss = 0.46063935\n",
            "Iteration 119, loss = 0.46146452\n",
            "Iteration 120, loss = 0.46024749\n",
            "Iteration 121, loss = 0.46181429\n",
            "Iteration 122, loss = 0.46016368\n",
            "Iteration 123, loss = 0.45925512\n",
            "Iteration 124, loss = 0.45811744\n",
            "Iteration 125, loss = 0.45703953\n",
            "Iteration 126, loss = 0.45884028\n",
            "Iteration 127, loss = 0.45823976\n",
            "Iteration 128, loss = 0.45926850\n",
            "Iteration 129, loss = 0.45980924\n",
            "Iteration 130, loss = 0.46215638\n",
            "Iteration 131, loss = 0.46053598\n",
            "Iteration 132, loss = 0.45790395\n",
            "Iteration 133, loss = 0.46100678\n",
            "Iteration 134, loss = 0.46189882\n",
            "Iteration 135, loss = 0.45768441\n",
            "Iteration 136, loss = 0.45600856\n",
            "Iteration 137, loss = 0.45665909\n",
            "Iteration 138, loss = 0.45685970\n",
            "Iteration 139, loss = 0.45704522\n",
            "Iteration 140, loss = 0.45598413\n",
            "Iteration 141, loss = 0.45634036\n",
            "Iteration 142, loss = 0.46182268\n",
            "Iteration 143, loss = 0.46050592\n",
            "Iteration 144, loss = 0.45835694\n",
            "Iteration 145, loss = 0.45599864\n",
            "Iteration 146, loss = 0.45600910\n",
            "Iteration 147, loss = 0.45545299\n",
            "Iteration 148, loss = 0.45865224\n",
            "Iteration 149, loss = 0.45750648\n",
            "Iteration 150, loss = 0.45908949\n",
            "Iteration 1, loss = 1.16748585\n",
            "Iteration 2, loss = 0.68435580\n",
            "Iteration 3, loss = 0.66513531\n",
            "Iteration 4, loss = 0.63465831\n",
            "Iteration 5, loss = 0.59114244\n",
            "Iteration 6, loss = 0.57003818\n",
            "Iteration 7, loss = 0.56114980\n",
            "Iteration 8, loss = 0.55536206\n",
            "Iteration 9, loss = 0.55068060\n",
            "Iteration 10, loss = 0.54421841\n",
            "Iteration 11, loss = 0.53917315\n",
            "Iteration 12, loss = 0.53756731\n",
            "Iteration 13, loss = 0.53401793\n",
            "Iteration 14, loss = 0.53102906\n",
            "Iteration 15, loss = 0.52923240\n",
            "Iteration 16, loss = 0.52842161\n",
            "Iteration 17, loss = 0.52571400\n",
            "Iteration 18, loss = 0.52352226\n",
            "Iteration 19, loss = 0.52236147\n",
            "Iteration 20, loss = 0.52094060\n",
            "Iteration 21, loss = 0.51926264\n",
            "Iteration 22, loss = 0.51803912\n",
            "Iteration 23, loss = 0.51743550\n",
            "Iteration 24, loss = 0.51611098\n",
            "Iteration 25, loss = 0.51484374\n",
            "Iteration 26, loss = 0.51366008\n",
            "Iteration 27, loss = 0.51252271\n",
            "Iteration 28, loss = 0.51171310\n",
            "Iteration 29, loss = 0.51033110\n",
            "Iteration 30, loss = 0.51004992\n",
            "Iteration 31, loss = 0.50854952\n",
            "Iteration 32, loss = 0.50724227\n",
            "Iteration 33, loss = 0.50680377\n",
            "Iteration 34, loss = 0.50552425\n",
            "Iteration 35, loss = 0.50445364\n",
            "Iteration 36, loss = 0.50348245\n",
            "Iteration 37, loss = 0.50306294\n",
            "Iteration 38, loss = 0.50302057\n",
            "Iteration 39, loss = 0.50034962\n",
            "Iteration 40, loss = 0.49685972\n",
            "Iteration 41, loss = 0.49725617\n",
            "Iteration 42, loss = 0.49689740\n",
            "Iteration 43, loss = 0.49322015\n",
            "Iteration 44, loss = 0.49091816\n",
            "Iteration 45, loss = 0.49002955\n",
            "Iteration 46, loss = 0.48930378\n",
            "Iteration 47, loss = 0.48750406\n",
            "Iteration 48, loss = 0.48563755\n",
            "Iteration 49, loss = 0.48510987\n",
            "Iteration 50, loss = 0.48263117\n",
            "Iteration 51, loss = 0.48163564\n",
            "Iteration 52, loss = 0.48019692\n",
            "Iteration 53, loss = 0.47899467\n",
            "Iteration 54, loss = 0.47797759\n",
            "Iteration 55, loss = 0.47806081\n",
            "Iteration 56, loss = 0.47472758\n",
            "Iteration 57, loss = 0.47475328\n",
            "Iteration 58, loss = 0.48037708\n",
            "Iteration 59, loss = 0.47454856\n",
            "Iteration 60, loss = 0.47150624\n",
            "Iteration 61, loss = 0.47368171\n",
            "Iteration 62, loss = 0.47159185\n",
            "Iteration 63, loss = 0.47018719\n",
            "Iteration 64, loss = 0.47013049\n",
            "Iteration 65, loss = 0.46900835\n",
            "Iteration 66, loss = 0.47222578\n",
            "Iteration 67, loss = 0.46624930\n",
            "Iteration 68, loss = 0.46616882\n",
            "Iteration 69, loss = 0.46744872\n",
            "Iteration 70, loss = 0.46542103\n",
            "Iteration 71, loss = 0.46563050\n",
            "Iteration 72, loss = 0.46410833\n",
            "Iteration 73, loss = 0.46574532\n",
            "Iteration 74, loss = 0.46582344\n",
            "Iteration 75, loss = 0.46387258\n",
            "Iteration 76, loss = 0.46665653\n",
            "Iteration 77, loss = 0.46445674\n",
            "Iteration 78, loss = 0.45919444\n",
            "Iteration 79, loss = 0.46157350\n",
            "Iteration 80, loss = 0.45949043\n",
            "Iteration 81, loss = 0.45832037\n",
            "Iteration 82, loss = 0.46105550\n",
            "Iteration 83, loss = 0.45602345\n",
            "Iteration 84, loss = 0.45494166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 85, loss = 0.45595838\n",
            "Iteration 86, loss = 0.45507863\n",
            "Iteration 87, loss = 0.45255795\n",
            "Iteration 88, loss = 0.45389850\n",
            "Iteration 89, loss = 0.45556347\n",
            "Iteration 90, loss = 0.45402081\n",
            "Iteration 91, loss = 0.45350112\n",
            "Iteration 92, loss = 0.45323924\n",
            "Iteration 93, loss = 0.45354713\n",
            "Iteration 94, loss = 0.45469343\n",
            "Iteration 95, loss = 0.45434926\n",
            "Iteration 96, loss = 0.45160621\n",
            "Iteration 97, loss = 0.45766711\n",
            "Iteration 98, loss = 0.45407384\n",
            "Iteration 99, loss = 0.45107557\n",
            "Iteration 100, loss = 0.45150522\n",
            "Iteration 101, loss = 0.45337602\n",
            "Iteration 102, loss = 0.45051003\n",
            "Iteration 103, loss = 0.45141026\n",
            "Iteration 104, loss = 0.45372546\n",
            "Iteration 105, loss = 0.45051399\n",
            "Iteration 106, loss = 0.45271310\n",
            "Iteration 107, loss = 0.45451704\n",
            "Iteration 108, loss = 0.45499414\n",
            "Iteration 109, loss = 0.45433798\n",
            "Iteration 110, loss = 0.45173734\n",
            "Iteration 111, loss = 0.45229799\n",
            "Iteration 112, loss = 0.45241619\n",
            "Iteration 113, loss = 0.44668247\n",
            "Iteration 114, loss = 0.45128533\n",
            "Iteration 115, loss = 0.45399400\n",
            "Iteration 116, loss = 0.45011937\n",
            "Iteration 117, loss = 0.44890083\n",
            "Iteration 118, loss = 0.45017289\n",
            "Iteration 119, loss = 0.44728390\n",
            "Iteration 120, loss = 0.45071777\n",
            "Iteration 121, loss = 0.44625580\n",
            "Iteration 122, loss = 0.44767060\n",
            "Iteration 123, loss = 0.44902646\n",
            "Iteration 124, loss = 0.44515681\n",
            "Iteration 125, loss = 0.44702235\n",
            "Iteration 126, loss = 0.44793449\n",
            "Iteration 127, loss = 0.45023562\n",
            "Iteration 128, loss = 0.45075530\n",
            "Iteration 129, loss = 0.45404378\n",
            "Iteration 130, loss = 0.44971471\n",
            "Iteration 131, loss = 0.44851051\n",
            "Iteration 132, loss = 0.44745229\n",
            "Iteration 133, loss = 0.45351603\n",
            "Iteration 134, loss = 0.45378934\n",
            "Iteration 135, loss = 0.44528727\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.16546862\n",
            "Iteration 2, loss = 0.68633174\n",
            "Iteration 3, loss = 0.66932821\n",
            "Iteration 4, loss = 0.63316451\n",
            "Iteration 5, loss = 0.58313492\n",
            "Iteration 6, loss = 0.56010652\n",
            "Iteration 7, loss = 0.55169390\n",
            "Iteration 8, loss = 0.54727909\n",
            "Iteration 9, loss = 0.54056381\n",
            "Iteration 10, loss = 0.53462264\n",
            "Iteration 11, loss = 0.53074940\n",
            "Iteration 12, loss = 0.52810966\n",
            "Iteration 13, loss = 0.52335327\n",
            "Iteration 14, loss = 0.51907926\n",
            "Iteration 15, loss = 0.51620450\n",
            "Iteration 16, loss = 0.51215303\n",
            "Iteration 17, loss = 0.50909448\n",
            "Iteration 18, loss = 0.50678571\n",
            "Iteration 19, loss = 0.50463134\n",
            "Iteration 20, loss = 0.50275679\n",
            "Iteration 21, loss = 0.50193707\n",
            "Iteration 22, loss = 0.50067535\n",
            "Iteration 23, loss = 0.49933591\n",
            "Iteration 24, loss = 0.49650228\n",
            "Iteration 25, loss = 0.49555558\n",
            "Iteration 26, loss = 0.49408503\n",
            "Iteration 27, loss = 0.49212077\n",
            "Iteration 28, loss = 0.49039763\n",
            "Iteration 29, loss = 0.49091874\n",
            "Iteration 30, loss = 0.48948791\n",
            "Iteration 31, loss = 0.48581534\n",
            "Iteration 32, loss = 0.48869327\n",
            "Iteration 33, loss = 0.48897904\n",
            "Iteration 34, loss = 0.48535231\n",
            "Iteration 35, loss = 0.48291656\n",
            "Iteration 36, loss = 0.48277159\n",
            "Iteration 37, loss = 0.48203690\n",
            "Iteration 38, loss = 0.48011133\n",
            "Iteration 39, loss = 0.47814519\n",
            "Iteration 40, loss = 0.47752528\n",
            "Iteration 41, loss = 0.47808257\n",
            "Iteration 42, loss = 0.47548653\n",
            "Iteration 43, loss = 0.47488039\n",
            "Iteration 44, loss = 0.47215938\n",
            "Iteration 45, loss = 0.47241277\n",
            "Iteration 46, loss = 0.47041245\n",
            "Iteration 47, loss = 0.46855368\n",
            "Iteration 48, loss = 0.46766108\n",
            "Iteration 49, loss = 0.46726617\n",
            "Iteration 50, loss = 0.46619583\n",
            "Iteration 51, loss = 0.46808418\n",
            "Iteration 52, loss = 0.46650417\n",
            "Iteration 53, loss = 0.46421970\n",
            "Iteration 54, loss = 0.46349735\n",
            "Iteration 55, loss = 0.46203259\n",
            "Iteration 56, loss = 0.45977442\n",
            "Iteration 57, loss = 0.46027057\n",
            "Iteration 58, loss = 0.46988778\n",
            "Iteration 59, loss = 0.46233902\n",
            "Iteration 60, loss = 0.45988533\n",
            "Iteration 61, loss = 0.46002719\n",
            "Iteration 62, loss = 0.45684772\n",
            "Iteration 63, loss = 0.45614815\n",
            "Iteration 64, loss = 0.45659778\n",
            "Iteration 65, loss = 0.45477147\n",
            "Iteration 66, loss = 0.45365227\n",
            "Iteration 67, loss = 0.45417324\n",
            "Iteration 68, loss = 0.45558637\n",
            "Iteration 69, loss = 0.45327549\n",
            "Iteration 70, loss = 0.45172777\n",
            "Iteration 71, loss = 0.45299898\n",
            "Iteration 72, loss = 0.44959190\n",
            "Iteration 73, loss = 0.45170402\n",
            "Iteration 74, loss = 0.44775769\n",
            "Iteration 75, loss = 0.45124258\n",
            "Iteration 76, loss = 0.45279056\n",
            "Iteration 77, loss = 0.45367221\n",
            "Iteration 78, loss = 0.45209065\n",
            "Iteration 79, loss = 0.44550968\n",
            "Iteration 80, loss = 0.44876667\n",
            "Iteration 81, loss = 0.45109419\n",
            "Iteration 82, loss = 0.44660740\n",
            "Iteration 83, loss = 0.44514269\n",
            "Iteration 84, loss = 0.44699843\n",
            "Iteration 85, loss = 0.44512059\n",
            "Iteration 86, loss = 0.44512139\n",
            "Iteration 87, loss = 0.44485383\n",
            "Iteration 88, loss = 0.44676667\n",
            "Iteration 89, loss = 0.44270317\n",
            "Iteration 90, loss = 0.44603391\n",
            "Iteration 91, loss = 0.44541570\n",
            "Iteration 92, loss = 0.44201438\n",
            "Iteration 93, loss = 0.44779921\n",
            "Iteration 94, loss = 0.44237476\n",
            "Iteration 95, loss = 0.44077190\n",
            "Iteration 96, loss = 0.44335455\n",
            "Iteration 97, loss = 0.44102656\n",
            "Iteration 98, loss = 0.44041186\n",
            "Iteration 99, loss = 0.44373161\n",
            "Iteration 100, loss = 0.44043808\n",
            "Iteration 101, loss = 0.44140721\n",
            "Iteration 102, loss = 0.44211328\n",
            "Iteration 103, loss = 0.44206878\n",
            "Iteration 104, loss = 0.44635574\n",
            "Iteration 105, loss = 0.44078835\n",
            "Iteration 106, loss = 0.43766046\n",
            "Iteration 107, loss = 0.44037017\n",
            "Iteration 108, loss = 0.44718351\n",
            "Iteration 109, loss = 0.44289678\n",
            "Iteration 110, loss = 0.43875981\n",
            "Iteration 111, loss = 0.44048935\n",
            "Iteration 112, loss = 0.44324779\n",
            "Iteration 113, loss = 0.43909179\n",
            "Iteration 114, loss = 0.44425033\n",
            "Iteration 115, loss = 0.43701218\n",
            "Iteration 116, loss = 0.44133170\n",
            "Iteration 117, loss = 0.43576260\n",
            "Iteration 118, loss = 0.43681952\n",
            "Iteration 119, loss = 0.44194694\n",
            "Iteration 120, loss = 0.44437692\n",
            "Iteration 121, loss = 0.43976692\n",
            "Iteration 122, loss = 0.44073781\n",
            "Iteration 123, loss = 0.43758767\n",
            "Iteration 124, loss = 0.43395733\n",
            "Iteration 125, loss = 0.43618067\n",
            "Iteration 126, loss = 0.44092761\n",
            "Iteration 127, loss = 0.44061005\n",
            "Iteration 128, loss = 0.43970486\n",
            "Iteration 129, loss = 0.43423663\n",
            "Iteration 130, loss = 0.43730267\n",
            "Iteration 131, loss = 0.43944881\n",
            "Iteration 132, loss = 0.43414165\n",
            "Iteration 133, loss = 0.43911078\n",
            "Iteration 134, loss = 0.43964610\n",
            "Iteration 135, loss = 0.43899461\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.17232581\n",
            "Iteration 2, loss = 0.68571124\n",
            "Iteration 3, loss = 0.67888710\n",
            "Iteration 4, loss = 0.64673825\n",
            "Iteration 5, loss = 0.59794892\n",
            "Iteration 6, loss = 0.57312271\n",
            "Iteration 7, loss = 0.56466572\n",
            "Iteration 8, loss = 0.56229608\n",
            "Iteration 9, loss = 0.55561310\n",
            "Iteration 10, loss = 0.54912067\n",
            "Iteration 11, loss = 0.54482135\n",
            "Iteration 12, loss = 0.54181354\n",
            "Iteration 13, loss = 0.53790741\n",
            "Iteration 14, loss = 0.53433278\n",
            "Iteration 15, loss = 0.52898001\n",
            "Iteration 16, loss = 0.52559670\n",
            "Iteration 17, loss = 0.52245148\n",
            "Iteration 18, loss = 0.52076866\n",
            "Iteration 19, loss = 0.51855342\n",
            "Iteration 20, loss = 0.51651247\n",
            "Iteration 21, loss = 0.51437681\n",
            "Iteration 22, loss = 0.51293226\n",
            "Iteration 23, loss = 0.51158435\n",
            "Iteration 24, loss = 0.51007435\n",
            "Iteration 25, loss = 0.50918096\n",
            "Iteration 26, loss = 0.50769243\n",
            "Iteration 27, loss = 0.50590206\n",
            "Iteration 28, loss = 0.50344794\n",
            "Iteration 29, loss = 0.50376017\n",
            "Iteration 30, loss = 0.50259131\n",
            "Iteration 31, loss = 0.49779779\n",
            "Iteration 32, loss = 0.50038877\n",
            "Iteration 33, loss = 0.49986861\n",
            "Iteration 34, loss = 0.49659424\n",
            "Iteration 35, loss = 0.49537589\n",
            "Iteration 36, loss = 0.49442566\n",
            "Iteration 37, loss = 0.49355529\n",
            "Iteration 38, loss = 0.49079018\n",
            "Iteration 39, loss = 0.49320428\n",
            "Iteration 40, loss = 0.49091291\n",
            "Iteration 41, loss = 0.49177786\n",
            "Iteration 42, loss = 0.48780397\n",
            "Iteration 43, loss = 0.48707865\n",
            "Iteration 44, loss = 0.48403883\n",
            "Iteration 45, loss = 0.48323445\n",
            "Iteration 46, loss = 0.48137168\n",
            "Iteration 47, loss = 0.48031872\n",
            "Iteration 48, loss = 0.47925227\n",
            "Iteration 49, loss = 0.47750569\n",
            "Iteration 50, loss = 0.47614650\n",
            "Iteration 51, loss = 0.47494394\n",
            "Iteration 52, loss = 0.47613063\n",
            "Iteration 53, loss = 0.47203151\n",
            "Iteration 54, loss = 0.47099025\n",
            "Iteration 55, loss = 0.47014201\n",
            "Iteration 56, loss = 0.46862588\n",
            "Iteration 57, loss = 0.46814231\n",
            "Iteration 58, loss = 0.46903360\n",
            "Iteration 59, loss = 0.46719988\n",
            "Iteration 60, loss = 0.46525691\n",
            "Iteration 61, loss = 0.46857972\n",
            "Iteration 62, loss = 0.46629160\n",
            "Iteration 63, loss = 0.46516734\n",
            "Iteration 64, loss = 0.46558111\n",
            "Iteration 65, loss = 0.46518520\n",
            "Iteration 66, loss = 0.46280131\n",
            "Iteration 67, loss = 0.46380183\n",
            "Iteration 68, loss = 0.46591917\n",
            "Iteration 69, loss = 0.46315412\n",
            "Iteration 70, loss = 0.46036406\n",
            "Iteration 71, loss = 0.46176288\n",
            "Iteration 72, loss = 0.45721844\n",
            "Iteration 73, loss = 0.46086504\n",
            "Iteration 74, loss = 0.45675920\n",
            "Iteration 75, loss = 0.45976963\n",
            "Iteration 76, loss = 0.45768641\n",
            "Iteration 77, loss = 0.45566741\n",
            "Iteration 78, loss = 0.45668412\n",
            "Iteration 79, loss = 0.45701608\n",
            "Iteration 80, loss = 0.45535166\n",
            "Iteration 81, loss = 0.45516166\n",
            "Iteration 82, loss = 0.45334298\n",
            "Iteration 83, loss = 0.45488815\n",
            "Iteration 84, loss = 0.45266330\n",
            "Iteration 85, loss = 0.45515735\n",
            "Iteration 86, loss = 0.45433561\n",
            "Iteration 87, loss = 0.45214455\n",
            "Iteration 88, loss = 0.45610070\n",
            "Iteration 89, loss = 0.45281270\n",
            "Iteration 90, loss = 0.45580547\n",
            "Iteration 91, loss = 0.45791238\n",
            "Iteration 92, loss = 0.45517448\n",
            "Iteration 93, loss = 0.45607128\n",
            "Iteration 94, loss = 0.45501342\n",
            "Iteration 95, loss = 0.45437550\n",
            "Iteration 96, loss = 0.45452545\n",
            "Iteration 97, loss = 0.45300895\n",
            "Iteration 98, loss = 0.45116718\n",
            "Iteration 99, loss = 0.45147916\n",
            "Iteration 100, loss = 0.45130028\n",
            "Iteration 101, loss = 0.45260655\n",
            "Iteration 102, loss = 0.45125205\n",
            "Iteration 103, loss = 0.45082029\n",
            "Iteration 104, loss = 0.45389515\n",
            "Iteration 105, loss = 0.45264845\n",
            "Iteration 106, loss = 0.45003545\n",
            "Iteration 107, loss = 0.45175451\n",
            "Iteration 108, loss = 0.45140976\n",
            "Iteration 109, loss = 0.45340609\n",
            "Iteration 110, loss = 0.45144869\n",
            "Iteration 111, loss = 0.45279922\n",
            "Iteration 112, loss = 0.45128596\n",
            "Iteration 113, loss = 0.44817011\n",
            "Iteration 114, loss = 0.44694340\n",
            "Iteration 115, loss = 0.44726506\n",
            "Iteration 116, loss = 0.44720818\n",
            "Iteration 117, loss = 0.44637300\n",
            "Iteration 118, loss = 0.44430881\n",
            "Iteration 119, loss = 0.44673647\n",
            "Iteration 120, loss = 0.44947592\n",
            "Iteration 121, loss = 0.44690117\n",
            "Iteration 122, loss = 0.44659092\n",
            "Iteration 123, loss = 0.44429407\n",
            "Iteration 124, loss = 0.44387008\n",
            "Iteration 125, loss = 0.44271516\n",
            "Iteration 126, loss = 0.44786302\n",
            "Iteration 127, loss = 0.44939677\n",
            "Iteration 128, loss = 0.44589560\n",
            "Iteration 129, loss = 0.44260942\n",
            "Iteration 130, loss = 0.44243635\n",
            "Iteration 131, loss = 0.44216317\n",
            "Iteration 132, loss = 0.44120818\n",
            "Iteration 133, loss = 0.44414177\n",
            "Iteration 134, loss = 0.44233022\n",
            "Iteration 135, loss = 0.44012408\n",
            "Iteration 136, loss = 0.44117582\n",
            "Iteration 137, loss = 0.43969211\n",
            "Iteration 138, loss = 0.44123941\n",
            "Iteration 139, loss = 0.43827430\n",
            "Iteration 140, loss = 0.43912199\n",
            "Iteration 141, loss = 0.44022162\n",
            "Iteration 142, loss = 0.43625789\n",
            "Iteration 143, loss = 0.44038959\n",
            "Iteration 144, loss = 0.44088420\n",
            "Iteration 145, loss = 0.43830682\n",
            "Iteration 146, loss = 0.43847359\n",
            "Iteration 147, loss = 0.43895210\n",
            "Iteration 148, loss = 0.43877769\n",
            "Iteration 149, loss = 0.44392436\n",
            "Iteration 150, loss = 0.44263141\n",
            "Iteration 1, loss = 1.16332810\n",
            "Iteration 2, loss = 0.68618232\n",
            "Iteration 3, loss = 0.68913900\n",
            "Iteration 4, loss = 0.65398233\n",
            "Iteration 5, loss = 0.60390842\n",
            "Iteration 6, loss = 0.57885852\n",
            "Iteration 7, loss = 0.57025920\n",
            "Iteration 8, loss = 0.56663455\n",
            "Iteration 9, loss = 0.55823330\n",
            "Iteration 10, loss = 0.55248266\n",
            "Iteration 11, loss = 0.54848967\n",
            "Iteration 12, loss = 0.54592930\n",
            "Iteration 13, loss = 0.54329974\n",
            "Iteration 14, loss = 0.54033888\n",
            "Iteration 15, loss = 0.53572275\n",
            "Iteration 16, loss = 0.53346657\n",
            "Iteration 17, loss = 0.53074143\n",
            "Iteration 18, loss = 0.52813994\n",
            "Iteration 19, loss = 0.52519975\n",
            "Iteration 20, loss = 0.52278328\n",
            "Iteration 21, loss = 0.52152420\n",
            "Iteration 22, loss = 0.52051844\n",
            "Iteration 23, loss = 0.51838123\n",
            "Iteration 24, loss = 0.51528032\n",
            "Iteration 25, loss = 0.51406820\n",
            "Iteration 26, loss = 0.51326190\n",
            "Iteration 27, loss = 0.51152712\n",
            "Iteration 28, loss = 0.50905238\n",
            "Iteration 29, loss = 0.50776352\n",
            "Iteration 30, loss = 0.50738792\n",
            "Iteration 31, loss = 0.50674779\n",
            "Iteration 32, loss = 0.50443804\n",
            "Iteration 33, loss = 0.50413038\n",
            "Iteration 34, loss = 0.50267196\n",
            "Iteration 35, loss = 0.50345086\n",
            "Iteration 36, loss = 0.50351132\n",
            "Iteration 37, loss = 0.50228991\n",
            "Iteration 38, loss = 0.50019602\n",
            "Iteration 39, loss = 0.50010008\n",
            "Iteration 40, loss = 0.49891127\n",
            "Iteration 41, loss = 0.49782462\n",
            "Iteration 42, loss = 0.49613568\n",
            "Iteration 43, loss = 0.49638432\n",
            "Iteration 44, loss = 0.49150995\n",
            "Iteration 45, loss = 0.49590998\n",
            "Iteration 46, loss = 0.49315333\n",
            "Iteration 47, loss = 0.49098981\n",
            "Iteration 48, loss = 0.48868236\n",
            "Iteration 49, loss = 0.48880587\n",
            "Iteration 50, loss = 0.48782580\n",
            "Iteration 51, loss = 0.48398444\n",
            "Iteration 52, loss = 0.48677887\n",
            "Iteration 53, loss = 0.48429541\n",
            "Iteration 54, loss = 0.48259965\n",
            "Iteration 55, loss = 0.48282400\n",
            "Iteration 56, loss = 0.48193661\n",
            "Iteration 57, loss = 0.48142843\n",
            "Iteration 58, loss = 0.48238175\n",
            "Iteration 59, loss = 0.48125050\n",
            "Iteration 60, loss = 0.48075630\n",
            "Iteration 61, loss = 0.48005422\n",
            "Iteration 62, loss = 0.47693954\n",
            "Iteration 63, loss = 0.47850162\n",
            "Iteration 64, loss = 0.47706809\n",
            "Iteration 65, loss = 0.47560735\n",
            "Iteration 66, loss = 0.47619349\n",
            "Iteration 67, loss = 0.47423727\n",
            "Iteration 68, loss = 0.47714908\n",
            "Iteration 69, loss = 0.47224118\n",
            "Iteration 70, loss = 0.47332061\n",
            "Iteration 71, loss = 0.47321793\n",
            "Iteration 72, loss = 0.47258821\n",
            "Iteration 73, loss = 0.47189953\n",
            "Iteration 74, loss = 0.47093795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 75, loss = 0.47350687\n",
            "Iteration 76, loss = 0.47030472\n",
            "Iteration 77, loss = 0.46734921\n",
            "Iteration 78, loss = 0.46952657\n",
            "Iteration 79, loss = 0.47024198\n",
            "Iteration 80, loss = 0.46871231\n",
            "Iteration 81, loss = 0.46987299\n",
            "Iteration 82, loss = 0.46711412\n",
            "Iteration 83, loss = 0.46694432\n",
            "Iteration 84, loss = 0.46505605\n",
            "Iteration 85, loss = 0.46497067\n",
            "Iteration 86, loss = 0.46634769\n",
            "Iteration 87, loss = 0.46120770\n",
            "Iteration 88, loss = 0.46237763\n",
            "Iteration 89, loss = 0.46308429\n",
            "Iteration 90, loss = 0.46505227\n",
            "Iteration 91, loss = 0.46288208\n",
            "Iteration 92, loss = 0.45916148\n",
            "Iteration 93, loss = 0.45643054\n",
            "Iteration 94, loss = 0.45848188\n",
            "Iteration 95, loss = 0.45912885\n",
            "Iteration 96, loss = 0.45837258\n",
            "Iteration 97, loss = 0.45821094\n",
            "Iteration 98, loss = 0.45545213\n",
            "Iteration 99, loss = 0.45392220\n",
            "Iteration 100, loss = 0.45374146\n",
            "Iteration 101, loss = 0.45642120\n",
            "Iteration 102, loss = 0.45451473\n",
            "Iteration 103, loss = 0.45299290\n",
            "Iteration 104, loss = 0.45309323\n",
            "Iteration 105, loss = 0.45222596\n",
            "Iteration 106, loss = 0.45629947\n",
            "Iteration 107, loss = 0.45483009\n",
            "Iteration 108, loss = 0.45190912\n",
            "Iteration 109, loss = 0.45302203\n",
            "Iteration 110, loss = 0.45154488\n",
            "Iteration 111, loss = 0.45372197\n",
            "Iteration 112, loss = 0.45328828\n",
            "Iteration 113, loss = 0.45380653\n",
            "Iteration 114, loss = 0.44976554\n",
            "Iteration 115, loss = 0.45205339\n",
            "Iteration 116, loss = 0.44973564\n",
            "Iteration 117, loss = 0.44918149\n",
            "Iteration 118, loss = 0.44854880\n",
            "Iteration 119, loss = 0.45234790\n",
            "Iteration 120, loss = 0.45286082\n",
            "Iteration 121, loss = 0.45114024\n",
            "Iteration 122, loss = 0.44876960\n",
            "Iteration 123, loss = 0.44987772\n",
            "Iteration 124, loss = 0.44799226\n",
            "Iteration 125, loss = 0.45073528\n",
            "Iteration 126, loss = 0.45691647\n",
            "Iteration 127, loss = 0.44963993\n",
            "Iteration 128, loss = 0.44748313\n",
            "Iteration 129, loss = 0.45213793\n",
            "Iteration 130, loss = 0.44700938\n",
            "Iteration 131, loss = 0.45288740\n",
            "Iteration 132, loss = 0.44719226\n",
            "Iteration 133, loss = 0.44984257\n",
            "Iteration 134, loss = 0.45191281\n",
            "Iteration 135, loss = 0.44697611\n",
            "Iteration 136, loss = 0.44616846\n",
            "Iteration 137, loss = 0.45207069\n",
            "Iteration 138, loss = 0.44800316\n",
            "Iteration 139, loss = 0.44725680\n",
            "Iteration 140, loss = 0.44956945\n",
            "Iteration 141, loss = 0.45020556\n",
            "Iteration 142, loss = 0.44446584\n",
            "Iteration 143, loss = 0.44366399\n",
            "Iteration 144, loss = 0.44138145\n",
            "Iteration 145, loss = 0.44428545\n",
            "Iteration 146, loss = 0.44786137\n",
            "Iteration 147, loss = 0.44710731\n",
            "Iteration 148, loss = 0.44413009\n",
            "Iteration 149, loss = 0.45815286\n",
            "Iteration 150, loss = 0.44439585\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 150 and for layer number 6 : 0.6924999999999999\n",
            "Iteration 1, loss = 0.61616344\n",
            "Iteration 2, loss = 0.61047446\n",
            "Iteration 3, loss = 0.60478519\n",
            "Iteration 4, loss = 0.60040729\n",
            "Iteration 5, loss = 0.59683886\n",
            "Iteration 6, loss = 0.59352251\n",
            "Iteration 7, loss = 0.59032056\n",
            "Iteration 8, loss = 0.58709569\n",
            "Iteration 9, loss = 0.58414076\n",
            "Iteration 10, loss = 0.58165635\n",
            "Iteration 11, loss = 0.57911060\n",
            "Iteration 12, loss = 0.57790214\n",
            "Iteration 13, loss = 0.57579006\n",
            "Iteration 14, loss = 0.57293731\n",
            "Iteration 15, loss = 0.56984478\n",
            "Iteration 16, loss = 0.56633085\n",
            "Iteration 17, loss = 0.56403178\n",
            "Iteration 18, loss = 0.56081149\n",
            "Iteration 19, loss = 0.55656498\n",
            "Iteration 20, loss = 0.55483738\n",
            "Iteration 21, loss = 0.55165056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mGörüntülenen çıkış son 5000 satıra kısaltıldı.\u001b[0m\n",
            "Iteration 79, loss = 0.50981380\n",
            "Iteration 80, loss = 0.51025917\n",
            "Iteration 81, loss = 0.51557245\n",
            "Iteration 82, loss = 0.51404262\n",
            "Iteration 83, loss = 0.50811045\n",
            "Iteration 84, loss = 0.51117944\n",
            "Iteration 85, loss = 0.51408823\n",
            "Iteration 86, loss = 0.50714546\n",
            "Iteration 87, loss = 0.50648920\n",
            "Iteration 88, loss = 0.50683883\n",
            "Iteration 89, loss = 0.50483630\n",
            "Iteration 90, loss = 0.50519714\n",
            "Iteration 91, loss = 0.50637432\n",
            "Iteration 92, loss = 0.50509134\n",
            "Iteration 93, loss = 0.50572160\n",
            "Iteration 94, loss = 0.50471110\n",
            "Iteration 95, loss = 0.50551280\n",
            "Iteration 96, loss = 0.50375059\n",
            "Iteration 97, loss = 0.50444488\n",
            "Iteration 98, loss = 0.50495500\n",
            "Iteration 99, loss = 0.50372446\n",
            "Iteration 100, loss = 0.50397249\n",
            "Iteration 101, loss = 0.50584103\n",
            "Iteration 102, loss = 0.50538688\n",
            "Iteration 103, loss = 0.50533859\n",
            "Iteration 104, loss = 0.50522541\n",
            "Iteration 105, loss = 0.50313477\n",
            "Iteration 106, loss = 0.50375229\n",
            "Iteration 107, loss = 0.50157035\n",
            "Iteration 108, loss = 0.50024633\n",
            "Iteration 109, loss = 0.50053768\n",
            "Iteration 110, loss = 0.50180636\n",
            "Iteration 111, loss = 0.50104015\n",
            "Iteration 112, loss = 0.49996236\n",
            "Iteration 113, loss = 0.49994954\n",
            "Iteration 114, loss = 0.50009725\n",
            "Iteration 115, loss = 0.49886425\n",
            "Iteration 116, loss = 0.49794556\n",
            "Iteration 117, loss = 0.49915618\n",
            "Iteration 118, loss = 0.49714965\n",
            "Iteration 119, loss = 0.49676612\n",
            "Iteration 120, loss = 0.49755745\n",
            "Iteration 121, loss = 0.49675014\n",
            "Iteration 122, loss = 0.49640613\n",
            "Iteration 123, loss = 0.49767010\n",
            "Iteration 124, loss = 0.49764428\n",
            "Iteration 125, loss = 0.49774683\n",
            "Iteration 126, loss = 0.49739006\n",
            "Iteration 127, loss = 0.49544826\n",
            "Iteration 128, loss = 0.49784077\n",
            "Iteration 129, loss = 0.49644203\n",
            "Iteration 130, loss = 0.49657157\n",
            "Iteration 131, loss = 0.49466084\n",
            "Iteration 132, loss = 0.49522061\n",
            "Iteration 133, loss = 0.49614322\n",
            "Iteration 134, loss = 0.49233119\n",
            "Iteration 135, loss = 0.49259802\n",
            "Iteration 136, loss = 0.49261920\n",
            "Iteration 137, loss = 0.49548365\n",
            "Iteration 138, loss = 0.49462814\n",
            "Iteration 139, loss = 0.49515159\n",
            "Iteration 140, loss = 0.49499592\n",
            "Iteration 141, loss = 0.49543857\n",
            "Iteration 142, loss = 0.49233858\n",
            "Iteration 143, loss = 0.49309902\n",
            "Iteration 144, loss = 0.49020281\n",
            "Iteration 145, loss = 0.49189385\n",
            "Iteration 146, loss = 0.49148586\n",
            "Iteration 147, loss = 0.49266871\n",
            "Iteration 148, loss = 0.49050547\n",
            "Iteration 149, loss = 0.49223937\n",
            "Iteration 150, loss = 0.49102742\n",
            "Iteration 151, loss = 0.49205071\n",
            "Iteration 152, loss = 0.49455023\n",
            "Iteration 153, loss = 0.49425244\n",
            "Iteration 154, loss = 0.48841071\n",
            "Iteration 155, loss = 0.49090468\n",
            "Iteration 156, loss = 0.49119644\n",
            "Iteration 157, loss = 0.49289922\n",
            "Iteration 158, loss = 0.49107514\n",
            "Iteration 159, loss = 0.48968597\n",
            "Iteration 160, loss = 0.49341045\n",
            "Iteration 161, loss = 0.49176106\n",
            "Iteration 162, loss = 0.48935169\n",
            "Iteration 163, loss = 0.48944420\n",
            "Iteration 164, loss = 0.48941782\n",
            "Iteration 165, loss = 0.48892615\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.84211562\n",
            "Iteration 2, loss = 0.67390273\n",
            "Iteration 3, loss = 0.61479397\n",
            "Iteration 4, loss = 0.59817770\n",
            "Iteration 5, loss = 0.58643524\n",
            "Iteration 6, loss = 0.57940279\n",
            "Iteration 7, loss = 0.57108334\n",
            "Iteration 8, loss = 0.56345206\n",
            "Iteration 9, loss = 0.55760032\n",
            "Iteration 10, loss = 0.55137476\n",
            "Iteration 11, loss = 0.54982593\n",
            "Iteration 12, loss = 0.54759444\n",
            "Iteration 13, loss = 0.54480473\n",
            "Iteration 14, loss = 0.54105592\n",
            "Iteration 15, loss = 0.53799031\n",
            "Iteration 16, loss = 0.53554759\n",
            "Iteration 17, loss = 0.53296703\n",
            "Iteration 18, loss = 0.53051284\n",
            "Iteration 19, loss = 0.52790683\n",
            "Iteration 20, loss = 0.52677164\n",
            "Iteration 21, loss = 0.52485005\n",
            "Iteration 22, loss = 0.52480127\n",
            "Iteration 23, loss = 0.52143077\n",
            "Iteration 24, loss = 0.52039570\n",
            "Iteration 25, loss = 0.51926551\n",
            "Iteration 26, loss = 0.51855653\n",
            "Iteration 27, loss = 0.51683205\n",
            "Iteration 28, loss = 0.51569551\n",
            "Iteration 29, loss = 0.51493485\n",
            "Iteration 30, loss = 0.51421020\n",
            "Iteration 31, loss = 0.51674064\n",
            "Iteration 32, loss = 0.51260548\n",
            "Iteration 33, loss = 0.51040642\n",
            "Iteration 34, loss = 0.50939175\n",
            "Iteration 35, loss = 0.50888292\n",
            "Iteration 36, loss = 0.50876223\n",
            "Iteration 37, loss = 0.50628136\n",
            "Iteration 38, loss = 0.50496776\n",
            "Iteration 39, loss = 0.50289192\n",
            "Iteration 40, loss = 0.50200486\n",
            "Iteration 41, loss = 0.50190053\n",
            "Iteration 42, loss = 0.50099544\n",
            "Iteration 43, loss = 0.50033700\n",
            "Iteration 44, loss = 0.49990491\n",
            "Iteration 45, loss = 0.49703635\n",
            "Iteration 46, loss = 0.49645873\n",
            "Iteration 47, loss = 0.49625000\n",
            "Iteration 48, loss = 0.49583677\n",
            "Iteration 49, loss = 0.49334537\n",
            "Iteration 50, loss = 0.49351033\n",
            "Iteration 51, loss = 0.49339818\n",
            "Iteration 52, loss = 0.49169668\n",
            "Iteration 53, loss = 0.49150701\n",
            "Iteration 54, loss = 0.49566927\n",
            "Iteration 55, loss = 0.49062902\n",
            "Iteration 56, loss = 0.48908149\n",
            "Iteration 57, loss = 0.48730963\n",
            "Iteration 58, loss = 0.48737917\n",
            "Iteration 59, loss = 0.48786171\n",
            "Iteration 60, loss = 0.48735379\n",
            "Iteration 61, loss = 0.48919848\n",
            "Iteration 62, loss = 0.48604695\n",
            "Iteration 63, loss = 0.48791279\n",
            "Iteration 64, loss = 0.48447282\n",
            "Iteration 65, loss = 0.48392292\n",
            "Iteration 66, loss = 0.48710397\n",
            "Iteration 67, loss = 0.48497721\n",
            "Iteration 68, loss = 0.48189699\n",
            "Iteration 69, loss = 0.48215848\n",
            "Iteration 70, loss = 0.48396642\n",
            "Iteration 71, loss = 0.48472021\n",
            "Iteration 72, loss = 0.48269523\n",
            "Iteration 73, loss = 0.48006770\n",
            "Iteration 74, loss = 0.47949046\n",
            "Iteration 75, loss = 0.47917436\n",
            "Iteration 76, loss = 0.47986172\n",
            "Iteration 77, loss = 0.47652529\n",
            "Iteration 78, loss = 0.47572204\n",
            "Iteration 79, loss = 0.47605137\n",
            "Iteration 80, loss = 0.47486116\n",
            "Iteration 81, loss = 0.47661630\n",
            "Iteration 82, loss = 0.47626359\n",
            "Iteration 83, loss = 0.47833453\n",
            "Iteration 84, loss = 0.47666945\n",
            "Iteration 85, loss = 0.48468544\n",
            "Iteration 86, loss = 0.47513421\n",
            "Iteration 87, loss = 0.47577963\n",
            "Iteration 88, loss = 0.47356521\n",
            "Iteration 89, loss = 0.47354569\n",
            "Iteration 90, loss = 0.47358204\n",
            "Iteration 91, loss = 0.47377450\n",
            "Iteration 92, loss = 0.47338047\n",
            "Iteration 93, loss = 0.47247782\n",
            "Iteration 94, loss = 0.47194345\n",
            "Iteration 95, loss = 0.47182012\n",
            "Iteration 96, loss = 0.47099111\n",
            "Iteration 97, loss = 0.47485566\n",
            "Iteration 98, loss = 0.47394550\n",
            "Iteration 99, loss = 0.47321010\n",
            "Iteration 100, loss = 0.47317758\n",
            "Iteration 101, loss = 0.47147078\n",
            "Iteration 102, loss = 0.46973482\n",
            "Iteration 103, loss = 0.47088729\n",
            "Iteration 104, loss = 0.47040228\n",
            "Iteration 105, loss = 0.47415798\n",
            "Iteration 106, loss = 0.47081625\n",
            "Iteration 107, loss = 0.47069119\n",
            "Iteration 108, loss = 0.46858736\n",
            "Iteration 109, loss = 0.46535988\n",
            "Iteration 110, loss = 0.46921133\n",
            "Iteration 111, loss = 0.47624427\n",
            "Iteration 112, loss = 0.46987930\n",
            "Iteration 113, loss = 0.46703870\n",
            "Iteration 114, loss = 0.46861779\n",
            "Iteration 115, loss = 0.47201319\n",
            "Iteration 116, loss = 0.46866252\n",
            "Iteration 117, loss = 0.46893663\n",
            "Iteration 118, loss = 0.46594707\n",
            "Iteration 119, loss = 0.46587544\n",
            "Iteration 120, loss = 0.46586999\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.84194648\n",
            "Iteration 2, loss = 0.66554851\n",
            "Iteration 3, loss = 0.61225024\n",
            "Iteration 4, loss = 0.59987886\n",
            "Iteration 5, loss = 0.59202196\n",
            "Iteration 6, loss = 0.58325873\n",
            "Iteration 7, loss = 0.57298718\n",
            "Iteration 8, loss = 0.56206000\n",
            "Iteration 9, loss = 0.55315409\n",
            "Iteration 10, loss = 0.54698521\n",
            "Iteration 11, loss = 0.54442773\n",
            "Iteration 12, loss = 0.54226291\n",
            "Iteration 13, loss = 0.53980072\n",
            "Iteration 14, loss = 0.53599988\n",
            "Iteration 15, loss = 0.53313323\n",
            "Iteration 16, loss = 0.52934416\n",
            "Iteration 17, loss = 0.52638109\n",
            "Iteration 18, loss = 0.52580879\n",
            "Iteration 19, loss = 0.52426243\n",
            "Iteration 20, loss = 0.52290946\n",
            "Iteration 21, loss = 0.52268451\n",
            "Iteration 22, loss = 0.52343882\n",
            "Iteration 23, loss = 0.52150082\n",
            "Iteration 24, loss = 0.52105498\n",
            "Iteration 25, loss = 0.51995877\n",
            "Iteration 26, loss = 0.51906444\n",
            "Iteration 27, loss = 0.51760723\n",
            "Iteration 28, loss = 0.51746923\n",
            "Iteration 29, loss = 0.51659429\n",
            "Iteration 30, loss = 0.51628310\n",
            "Iteration 31, loss = 0.51696056\n",
            "Iteration 32, loss = 0.51532952\n",
            "Iteration 33, loss = 0.51492982\n",
            "Iteration 34, loss = 0.51395807\n",
            "Iteration 35, loss = 0.51293487\n",
            "Iteration 36, loss = 0.51311644\n",
            "Iteration 37, loss = 0.51206223\n",
            "Iteration 38, loss = 0.51187460\n",
            "Iteration 39, loss = 0.51152712\n",
            "Iteration 40, loss = 0.51025039\n",
            "Iteration 41, loss = 0.51022046\n",
            "Iteration 42, loss = 0.51081281\n",
            "Iteration 43, loss = 0.51040966\n",
            "Iteration 44, loss = 0.51026804\n",
            "Iteration 45, loss = 0.50922244\n",
            "Iteration 46, loss = 0.50916438\n",
            "Iteration 47, loss = 0.50888024\n",
            "Iteration 48, loss = 0.50907711\n",
            "Iteration 49, loss = 0.50774179\n",
            "Iteration 50, loss = 0.50710401\n",
            "Iteration 51, loss = 0.50764802\n",
            "Iteration 52, loss = 0.50624387\n",
            "Iteration 53, loss = 0.50559071\n",
            "Iteration 54, loss = 0.50518696\n",
            "Iteration 55, loss = 0.50350782\n",
            "Iteration 56, loss = 0.50403498\n",
            "Iteration 57, loss = 0.50308545\n",
            "Iteration 58, loss = 0.50276083\n",
            "Iteration 59, loss = 0.50331295\n",
            "Iteration 60, loss = 0.50236108\n",
            "Iteration 61, loss = 0.50171369\n",
            "Iteration 62, loss = 0.50260973\n",
            "Iteration 63, loss = 0.50167126\n",
            "Iteration 64, loss = 0.50123775\n",
            "Iteration 65, loss = 0.49866415\n",
            "Iteration 66, loss = 0.50089686\n",
            "Iteration 67, loss = 0.49977304\n",
            "Iteration 68, loss = 0.49736768\n",
            "Iteration 69, loss = 0.49733328\n",
            "Iteration 70, loss = 0.49723094\n",
            "Iteration 71, loss = 0.49939928\n",
            "Iteration 72, loss = 0.49655591\n",
            "Iteration 73, loss = 0.49371061\n",
            "Iteration 74, loss = 0.49284754\n",
            "Iteration 75, loss = 0.49211623\n",
            "Iteration 76, loss = 0.49079801\n",
            "Iteration 77, loss = 0.48971083\n",
            "Iteration 78, loss = 0.48842846\n",
            "Iteration 79, loss = 0.48863192\n",
            "Iteration 80, loss = 0.48769454\n",
            "Iteration 81, loss = 0.48869401\n",
            "Iteration 82, loss = 0.48656822\n",
            "Iteration 83, loss = 0.48627403\n",
            "Iteration 84, loss = 0.48614048\n",
            "Iteration 85, loss = 0.48758305\n",
            "Iteration 86, loss = 0.48417598\n",
            "Iteration 87, loss = 0.48175374\n",
            "Iteration 88, loss = 0.48032217\n",
            "Iteration 89, loss = 0.48057079\n",
            "Iteration 90, loss = 0.48163173\n",
            "Iteration 91, loss = 0.48031648\n",
            "Iteration 92, loss = 0.47936347\n",
            "Iteration 93, loss = 0.47888769\n",
            "Iteration 94, loss = 0.47807031\n",
            "Iteration 95, loss = 0.47792276\n",
            "Iteration 96, loss = 0.47669075\n",
            "Iteration 97, loss = 0.47703885\n",
            "Iteration 98, loss = 0.47652764\n",
            "Iteration 99, loss = 0.47698218\n",
            "Iteration 100, loss = 0.47609304\n",
            "Iteration 101, loss = 0.47529402\n",
            "Iteration 102, loss = 0.47466444\n",
            "Iteration 103, loss = 0.47422635\n",
            "Iteration 104, loss = 0.47371837\n",
            "Iteration 105, loss = 0.47493566\n",
            "Iteration 106, loss = 0.47400548\n",
            "Iteration 107, loss = 0.47320379\n",
            "Iteration 108, loss = 0.47309130\n",
            "Iteration 109, loss = 0.47056961\n",
            "Iteration 110, loss = 0.47351695\n",
            "Iteration 111, loss = 0.47489974\n",
            "Iteration 112, loss = 0.47332189\n",
            "Iteration 113, loss = 0.47146561\n",
            "Iteration 114, loss = 0.47004990\n",
            "Iteration 115, loss = 0.47355515\n",
            "Iteration 116, loss = 0.47397915\n",
            "Iteration 117, loss = 0.47466726\n",
            "Iteration 118, loss = 0.47146515\n",
            "Iteration 119, loss = 0.46944354\n",
            "Iteration 120, loss = 0.47083722\n",
            "Iteration 121, loss = 0.46829298\n",
            "Iteration 122, loss = 0.47049314\n",
            "Iteration 123, loss = 0.46855504\n",
            "Iteration 124, loss = 0.47046074\n",
            "Iteration 125, loss = 0.46915822\n",
            "Iteration 126, loss = 0.46790139\n",
            "Iteration 127, loss = 0.46876398\n",
            "Iteration 128, loss = 0.46816952\n",
            "Iteration 129, loss = 0.46819922\n",
            "Iteration 130, loss = 0.46776867\n",
            "Iteration 131, loss = 0.46596833\n",
            "Iteration 132, loss = 0.46531227\n",
            "Iteration 133, loss = 0.46661815\n",
            "Iteration 134, loss = 0.46529771\n",
            "Iteration 135, loss = 0.46387486\n",
            "Iteration 136, loss = 0.46544262\n",
            "Iteration 137, loss = 0.46535451\n",
            "Iteration 138, loss = 0.46541958\n",
            "Iteration 139, loss = 0.46533499\n",
            "Iteration 140, loss = 0.46329613\n",
            "Iteration 141, loss = 0.46399936\n",
            "Iteration 142, loss = 0.46287439\n",
            "Iteration 143, loss = 0.46369668\n",
            "Iteration 144, loss = 0.46330816\n",
            "Iteration 145, loss = 0.46335661\n",
            "Iteration 146, loss = 0.46403232\n",
            "Iteration 147, loss = 0.46531830\n",
            "Iteration 148, loss = 0.46581724\n",
            "Iteration 149, loss = 0.46551850\n",
            "Iteration 150, loss = 0.46611122\n",
            "Iteration 151, loss = 0.46445827\n",
            "Iteration 152, loss = 0.46276198\n",
            "Iteration 153, loss = 0.46472329\n",
            "Iteration 154, loss = 0.46378864\n",
            "Iteration 155, loss = 0.46445555\n",
            "Iteration 156, loss = 0.46578407\n",
            "Iteration 157, loss = 0.46654125\n",
            "Iteration 158, loss = 0.47034106\n",
            "Iteration 159, loss = 0.46466263\n",
            "Iteration 160, loss = 0.46142475\n",
            "Iteration 161, loss = 0.46337598\n",
            "Iteration 162, loss = 0.46279314\n",
            "Iteration 163, loss = 0.46455164\n",
            "Iteration 164, loss = 0.46482939\n",
            "Iteration 165, loss = 0.46279006\n",
            "Iteration 166, loss = 0.46260688\n",
            "Iteration 167, loss = 0.46530952\n",
            "Iteration 168, loss = 0.46265043\n",
            "Iteration 169, loss = 0.46444094\n",
            "Iteration 170, loss = 0.46360634\n",
            "Iteration 171, loss = 0.46131717\n",
            "Iteration 172, loss = 0.46156849\n",
            "Iteration 173, loss = 0.46282120\n",
            "Iteration 174, loss = 0.46094399\n",
            "Iteration 175, loss = 0.46435120\n",
            "Iteration 176, loss = 0.46232158\n",
            "Iteration 177, loss = 0.46161511\n",
            "Iteration 178, loss = 0.46469640\n",
            "Iteration 179, loss = 0.46303747\n",
            "Iteration 180, loss = 0.46344337\n",
            "Iteration 181, loss = 0.46181827\n",
            "Iteration 182, loss = 0.46169673\n",
            "Iteration 183, loss = 0.46612811\n",
            "Iteration 184, loss = 0.46381456\n",
            "Iteration 185, loss = 0.46101203\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.83713456\n",
            "Iteration 2, loss = 0.66083566\n",
            "Iteration 3, loss = 0.62354131\n",
            "Iteration 4, loss = 0.60812662\n",
            "Iteration 5, loss = 0.59948788\n",
            "Iteration 6, loss = 0.59562370\n",
            "Iteration 7, loss = 0.58895751\n",
            "Iteration 8, loss = 0.57947492\n",
            "Iteration 9, loss = 0.57067064\n",
            "Iteration 10, loss = 0.56219342\n",
            "Iteration 11, loss = 0.55698371\n",
            "Iteration 12, loss = 0.55391316\n",
            "Iteration 13, loss = 0.55129977\n",
            "Iteration 14, loss = 0.54890671\n",
            "Iteration 15, loss = 0.54643885\n",
            "Iteration 16, loss = 0.54297314\n",
            "Iteration 17, loss = 0.54102246\n",
            "Iteration 18, loss = 0.54129445\n",
            "Iteration 19, loss = 0.53813457\n",
            "Iteration 20, loss = 0.53584825\n",
            "Iteration 21, loss = 0.53505185\n",
            "Iteration 22, loss = 0.53380029\n",
            "Iteration 23, loss = 0.53263117\n",
            "Iteration 24, loss = 0.53059466\n",
            "Iteration 25, loss = 0.53043742\n",
            "Iteration 26, loss = 0.52878689\n",
            "Iteration 27, loss = 0.52721957\n",
            "Iteration 28, loss = 0.52509053\n",
            "Iteration 29, loss = 0.52463802\n",
            "Iteration 30, loss = 0.52600622\n",
            "Iteration 31, loss = 0.52533686\n",
            "Iteration 32, loss = 0.52282068\n",
            "Iteration 33, loss = 0.52207370\n",
            "Iteration 34, loss = 0.52197151\n",
            "Iteration 35, loss = 0.52050410\n",
            "Iteration 36, loss = 0.51890510\n",
            "Iteration 37, loss = 0.51812398\n",
            "Iteration 38, loss = 0.51735240\n",
            "Iteration 39, loss = 0.51699532\n",
            "Iteration 40, loss = 0.51686667\n",
            "Iteration 41, loss = 0.51610695\n",
            "Iteration 42, loss = 0.51669172\n",
            "Iteration 43, loss = 0.51322902\n",
            "Iteration 44, loss = 0.51318415\n",
            "Iteration 45, loss = 0.51185272\n",
            "Iteration 46, loss = 0.51094082\n",
            "Iteration 47, loss = 0.51111908\n",
            "Iteration 48, loss = 0.50958323\n",
            "Iteration 49, loss = 0.51071506\n",
            "Iteration 50, loss = 0.50797224\n",
            "Iteration 51, loss = 0.50855548\n",
            "Iteration 52, loss = 0.50619339\n",
            "Iteration 53, loss = 0.50607627\n",
            "Iteration 54, loss = 0.50619348\n",
            "Iteration 55, loss = 0.50297138\n",
            "Iteration 56, loss = 0.50363331\n",
            "Iteration 57, loss = 0.50120507\n",
            "Iteration 58, loss = 0.50208289\n",
            "Iteration 59, loss = 0.50219130\n",
            "Iteration 60, loss = 0.50081060\n",
            "Iteration 61, loss = 0.50034864\n",
            "Iteration 62, loss = 0.49877738\n",
            "Iteration 63, loss = 0.49889385\n",
            "Iteration 64, loss = 0.49910741\n",
            "Iteration 65, loss = 0.49653541\n",
            "Iteration 66, loss = 0.49809058\n",
            "Iteration 67, loss = 0.49694000\n",
            "Iteration 68, loss = 0.49540907\n",
            "Iteration 69, loss = 0.49503097\n",
            "Iteration 70, loss = 0.49322977\n",
            "Iteration 71, loss = 0.49398712\n",
            "Iteration 72, loss = 0.49298884\n",
            "Iteration 73, loss = 0.49221089\n",
            "Iteration 74, loss = 0.49056898\n",
            "Iteration 75, loss = 0.49090439\n",
            "Iteration 76, loss = 0.49178349\n",
            "Iteration 77, loss = 0.48995355\n",
            "Iteration 78, loss = 0.48955926\n",
            "Iteration 79, loss = 0.49071445\n",
            "Iteration 80, loss = 0.48789645\n",
            "Iteration 81, loss = 0.49334601\n",
            "Iteration 82, loss = 0.48766543\n",
            "Iteration 83, loss = 0.49117912\n",
            "Iteration 84, loss = 0.49090990\n",
            "Iteration 85, loss = 0.49150852\n",
            "Iteration 86, loss = 0.48896881\n",
            "Iteration 87, loss = 0.48385274\n",
            "Iteration 88, loss = 0.48921794\n",
            "Iteration 89, loss = 0.48589666\n",
            "Iteration 90, loss = 0.48460451\n",
            "Iteration 91, loss = 0.48482953\n",
            "Iteration 92, loss = 0.48133251\n",
            "Iteration 93, loss = 0.47927352\n",
            "Iteration 94, loss = 0.47865817\n",
            "Iteration 95, loss = 0.47757052\n",
            "Iteration 96, loss = 0.47917336\n",
            "Iteration 97, loss = 0.47724566\n",
            "Iteration 98, loss = 0.47747433\n",
            "Iteration 99, loss = 0.47479210\n",
            "Iteration 100, loss = 0.47134891\n",
            "Iteration 101, loss = 0.47284035\n",
            "Iteration 102, loss = 0.47352995\n",
            "Iteration 103, loss = 0.47224160\n",
            "Iteration 104, loss = 0.47212225\n",
            "Iteration 105, loss = 0.47184532\n",
            "Iteration 106, loss = 0.46958548\n",
            "Iteration 107, loss = 0.47249782\n",
            "Iteration 108, loss = 0.47339093\n",
            "Iteration 109, loss = 0.46895803\n",
            "Iteration 110, loss = 0.47348572\n",
            "Iteration 111, loss = 0.47387400\n",
            "Iteration 112, loss = 0.47071031\n",
            "Iteration 113, loss = 0.47299048\n",
            "Iteration 114, loss = 0.47027790\n",
            "Iteration 115, loss = 0.47575480\n",
            "Iteration 116, loss = 0.47479950\n",
            "Iteration 117, loss = 0.47291502\n",
            "Iteration 118, loss = 0.46806154\n",
            "Iteration 119, loss = 0.46810702\n",
            "Iteration 120, loss = 0.46852562\n",
            "Iteration 121, loss = 0.46785207\n",
            "Iteration 122, loss = 0.46712060\n",
            "Iteration 123, loss = 0.46379537\n",
            "Iteration 124, loss = 0.46692458\n",
            "Iteration 125, loss = 0.46876335\n",
            "Iteration 126, loss = 0.46553169\n",
            "Iteration 127, loss = 0.46911223\n",
            "Iteration 128, loss = 0.46725812\n",
            "Iteration 129, loss = 0.46746697\n",
            "Iteration 130, loss = 0.47360943\n",
            "Iteration 131, loss = 0.47004586\n",
            "Iteration 132, loss = 0.46820468\n",
            "Iteration 133, loss = 0.47028128\n",
            "Iteration 134, loss = 0.46296306\n",
            "Iteration 135, loss = 0.46389413\n",
            "Iteration 136, loss = 0.46786654\n",
            "Iteration 137, loss = 0.46407171\n",
            "Iteration 138, loss = 0.46591593\n",
            "Iteration 139, loss = 0.46129456\n",
            "Iteration 140, loss = 0.45925773\n",
            "Iteration 141, loss = 0.46237936\n",
            "Iteration 142, loss = 0.46106316\n",
            "Iteration 143, loss = 0.46256290\n",
            "Iteration 144, loss = 0.45925252\n",
            "Iteration 145, loss = 0.45847400\n",
            "Iteration 146, loss = 0.46023258\n",
            "Iteration 147, loss = 0.46050539\n",
            "Iteration 148, loss = 0.46045107\n",
            "Iteration 149, loss = 0.45892109\n",
            "Iteration 150, loss = 0.45637400\n",
            "Iteration 151, loss = 0.45738062\n",
            "Iteration 152, loss = 0.45673926\n",
            "Iteration 153, loss = 0.45432851\n",
            "Iteration 154, loss = 0.45561840\n",
            "Iteration 155, loss = 0.45573120\n",
            "Iteration 156, loss = 0.45511602\n",
            "Iteration 157, loss = 0.45396266\n",
            "Iteration 158, loss = 0.45703000\n",
            "Iteration 159, loss = 0.45205032\n",
            "Iteration 160, loss = 0.45181513\n",
            "Iteration 161, loss = 0.45337970\n",
            "Iteration 162, loss = 0.45210888\n",
            "Iteration 163, loss = 0.45246126\n",
            "Iteration 164, loss = 0.45982796\n",
            "Iteration 165, loss = 0.45671538\n",
            "Iteration 166, loss = 0.45619953\n",
            "Iteration 167, loss = 0.46491576\n",
            "Iteration 168, loss = 0.45230790\n",
            "Iteration 169, loss = 0.45124629\n",
            "Iteration 170, loss = 0.45200775\n",
            "Iteration 171, loss = 0.44979065\n",
            "Iteration 172, loss = 0.44880209\n",
            "Iteration 173, loss = 0.45179913\n",
            "Iteration 174, loss = 0.44910856\n",
            "Iteration 175, loss = 0.45113636\n",
            "Iteration 176, loss = 0.45087167\n",
            "Iteration 177, loss = 0.45073614\n",
            "Iteration 178, loss = 0.45007733\n",
            "Iteration 179, loss = 0.44798217\n",
            "Iteration 180, loss = 0.45466122\n",
            "Iteration 181, loss = 0.45132158\n",
            "Iteration 182, loss = 0.45200276\n",
            "Iteration 183, loss = 0.46005232\n",
            "Iteration 184, loss = 0.45618634\n",
            "Iteration 185, loss = 0.44957141\n",
            "Iteration 186, loss = 0.45491288\n",
            "Iteration 187, loss = 0.45059765\n",
            "Iteration 188, loss = 0.45323102\n",
            "Iteration 189, loss = 0.45098230\n",
            "Iteration 190, loss = 0.45073028\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.84068350\n",
            "Iteration 2, loss = 0.66163454\n",
            "Iteration 3, loss = 0.62089339\n",
            "Iteration 4, loss = 0.60367884\n",
            "Iteration 5, loss = 0.59758809\n",
            "Iteration 6, loss = 0.59053744\n",
            "Iteration 7, loss = 0.58355859\n",
            "Iteration 8, loss = 0.57575257\n",
            "Iteration 9, loss = 0.56752153\n",
            "Iteration 10, loss = 0.56250510\n",
            "Iteration 11, loss = 0.55940582\n",
            "Iteration 12, loss = 0.55725922\n",
            "Iteration 13, loss = 0.55539372\n",
            "Iteration 14, loss = 0.55261931\n",
            "Iteration 15, loss = 0.55125222\n",
            "Iteration 16, loss = 0.54938656\n",
            "Iteration 17, loss = 0.54723075\n",
            "Iteration 18, loss = 0.54611201\n",
            "Iteration 19, loss = 0.54445852\n",
            "Iteration 20, loss = 0.54260693\n",
            "Iteration 21, loss = 0.54191735\n",
            "Iteration 22, loss = 0.54076356\n",
            "Iteration 23, loss = 0.54072044\n",
            "Iteration 24, loss = 0.53816883\n",
            "Iteration 25, loss = 0.53735542\n",
            "Iteration 26, loss = 0.53648829\n",
            "Iteration 27, loss = 0.53660585\n",
            "Iteration 28, loss = 0.53461130\n",
            "Iteration 29, loss = 0.53387902\n",
            "Iteration 30, loss = 0.53469368\n",
            "Iteration 31, loss = 0.53202572\n",
            "Iteration 32, loss = 0.52926070\n",
            "Iteration 33, loss = 0.52892232\n",
            "Iteration 34, loss = 0.52792346\n",
            "Iteration 35, loss = 0.52631842\n",
            "Iteration 36, loss = 0.52525013\n",
            "Iteration 37, loss = 0.52366796\n",
            "Iteration 38, loss = 0.52165717\n",
            "Iteration 39, loss = 0.52017580\n",
            "Iteration 40, loss = 0.52051896\n",
            "Iteration 41, loss = 0.51962111\n",
            "Iteration 42, loss = 0.51778834\n",
            "Iteration 43, loss = 0.51575968\n",
            "Iteration 44, loss = 0.51410563\n",
            "Iteration 45, loss = 0.51679264\n",
            "Iteration 46, loss = 0.51113185\n",
            "Iteration 47, loss = 0.51272641\n",
            "Iteration 48, loss = 0.51033363\n",
            "Iteration 49, loss = 0.51149875\n",
            "Iteration 50, loss = 0.50972279\n",
            "Iteration 51, loss = 0.50659960\n",
            "Iteration 52, loss = 0.50812025\n",
            "Iteration 53, loss = 0.50583664\n",
            "Iteration 54, loss = 0.50413478\n",
            "Iteration 55, loss = 0.50309056\n",
            "Iteration 56, loss = 0.50207817\n",
            "Iteration 57, loss = 0.50064145\n",
            "Iteration 58, loss = 0.50182169\n",
            "Iteration 59, loss = 0.50027190\n",
            "Iteration 60, loss = 0.50034093\n",
            "Iteration 61, loss = 0.50228740\n",
            "Iteration 62, loss = 0.49936254\n",
            "Iteration 63, loss = 0.49841749\n",
            "Iteration 64, loss = 0.49726227\n",
            "Iteration 65, loss = 0.49574820\n",
            "Iteration 66, loss = 0.49597583\n",
            "Iteration 67, loss = 0.49666907\n",
            "Iteration 68, loss = 0.49781259\n",
            "Iteration 69, loss = 0.49598924\n",
            "Iteration 70, loss = 0.49504260\n",
            "Iteration 71, loss = 0.49511654\n",
            "Iteration 72, loss = 0.49587326\n",
            "Iteration 73, loss = 0.49325396\n",
            "Iteration 74, loss = 0.49171675\n",
            "Iteration 75, loss = 0.49379349\n",
            "Iteration 76, loss = 0.49232947\n",
            "Iteration 77, loss = 0.49154613\n",
            "Iteration 78, loss = 0.48956218\n",
            "Iteration 79, loss = 0.49085117\n",
            "Iteration 80, loss = 0.48951285\n",
            "Iteration 81, loss = 0.48964164\n",
            "Iteration 82, loss = 0.48776003\n",
            "Iteration 83, loss = 0.48866045\n",
            "Iteration 84, loss = 0.48743867\n",
            "Iteration 85, loss = 0.48959427\n",
            "Iteration 86, loss = 0.48893899\n",
            "Iteration 87, loss = 0.48581596\n",
            "Iteration 88, loss = 0.48958197\n",
            "Iteration 89, loss = 0.48734166\n",
            "Iteration 90, loss = 0.48549654\n",
            "Iteration 91, loss = 0.48475242\n",
            "Iteration 92, loss = 0.48860945\n",
            "Iteration 93, loss = 0.48724563\n",
            "Iteration 94, loss = 0.48767697\n",
            "Iteration 95, loss = 0.48842445\n",
            "Iteration 96, loss = 0.48431945\n",
            "Iteration 97, loss = 0.48893577\n",
            "Iteration 98, loss = 0.48836483\n",
            "Iteration 99, loss = 0.48372760\n",
            "Iteration 100, loss = 0.48331204\n",
            "Iteration 101, loss = 0.48457879\n",
            "Iteration 102, loss = 0.48378776\n",
            "Iteration 103, loss = 0.48212422\n",
            "Iteration 104, loss = 0.48130812\n",
            "Iteration 105, loss = 0.48237457\n",
            "Iteration 106, loss = 0.48402408\n",
            "Iteration 107, loss = 0.48117475\n",
            "Iteration 108, loss = 0.48229236\n",
            "Iteration 109, loss = 0.48249911\n",
            "Iteration 110, loss = 0.48217632\n",
            "Iteration 111, loss = 0.48300399\n",
            "Iteration 112, loss = 0.48305660\n",
            "Iteration 113, loss = 0.48623112\n",
            "Iteration 114, loss = 0.48033446\n",
            "Iteration 115, loss = 0.48255182\n",
            "Iteration 116, loss = 0.48329862\n",
            "Iteration 117, loss = 0.48245303\n",
            "Iteration 118, loss = 0.47916846\n",
            "Iteration 119, loss = 0.48122489\n",
            "Iteration 120, loss = 0.48029741\n",
            "Iteration 121, loss = 0.48173266\n",
            "Iteration 122, loss = 0.48009463\n",
            "Iteration 123, loss = 0.48139827\n",
            "Iteration 124, loss = 0.48397352\n",
            "Iteration 125, loss = 0.48144800\n",
            "Iteration 126, loss = 0.47902017\n",
            "Iteration 127, loss = 0.48260405\n",
            "Iteration 128, loss = 0.48030403\n",
            "Iteration 129, loss = 0.48113867\n",
            "Iteration 130, loss = 0.48312732\n",
            "Iteration 131, loss = 0.48231551\n",
            "Iteration 132, loss = 0.48290192\n",
            "Iteration 133, loss = 0.48060189\n",
            "Iteration 134, loss = 0.47852087\n",
            "Iteration 135, loss = 0.47857566\n",
            "Iteration 136, loss = 0.47896620\n",
            "Iteration 137, loss = 0.48044220\n",
            "Iteration 138, loss = 0.48120303\n",
            "Iteration 139, loss = 0.48065450\n",
            "Iteration 140, loss = 0.48012169\n",
            "Iteration 141, loss = 0.48184580\n",
            "Iteration 142, loss = 0.47990577\n",
            "Iteration 143, loss = 0.47887212\n",
            "Iteration 144, loss = 0.47922702\n",
            "Iteration 145, loss = 0.47890527\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 200 and for layer number 5 : 0.68\n",
            "Iteration 1, loss = 1.17918922\n",
            "Iteration 2, loss = 0.68685959\n",
            "Iteration 3, loss = 0.66908384\n",
            "Iteration 4, loss = 0.63908765\n",
            "Iteration 5, loss = 0.59631417\n",
            "Iteration 6, loss = 0.57411322\n",
            "Iteration 7, loss = 0.56515132\n",
            "Iteration 8, loss = 0.56235259\n",
            "Iteration 9, loss = 0.55879602\n",
            "Iteration 10, loss = 0.55343311\n",
            "Iteration 11, loss = 0.54937387\n",
            "Iteration 12, loss = 0.54565812\n",
            "Iteration 13, loss = 0.54212281\n",
            "Iteration 14, loss = 0.53924739\n",
            "Iteration 15, loss = 0.53679909\n",
            "Iteration 16, loss = 0.53377760\n",
            "Iteration 17, loss = 0.53062494\n",
            "Iteration 18, loss = 0.52882499\n",
            "Iteration 19, loss = 0.52691208\n",
            "Iteration 20, loss = 0.52429166\n",
            "Iteration 21, loss = 0.52287004\n",
            "Iteration 22, loss = 0.52153317\n",
            "Iteration 23, loss = 0.52034916\n",
            "Iteration 24, loss = 0.51827497\n",
            "Iteration 25, loss = 0.51660905\n",
            "Iteration 26, loss = 0.51371597\n",
            "Iteration 27, loss = 0.51319660\n",
            "Iteration 28, loss = 0.51266851\n",
            "Iteration 29, loss = 0.51153150\n",
            "Iteration 30, loss = 0.51012592\n",
            "Iteration 31, loss = 0.50761414\n",
            "Iteration 32, loss = 0.50575164\n",
            "Iteration 33, loss = 0.50502220\n",
            "Iteration 34, loss = 0.50257502\n",
            "Iteration 35, loss = 0.50116088\n",
            "Iteration 36, loss = 0.50085492\n",
            "Iteration 37, loss = 0.50006507\n",
            "Iteration 38, loss = 0.50045593\n",
            "Iteration 39, loss = 0.49817018\n",
            "Iteration 40, loss = 0.49616517\n",
            "Iteration 41, loss = 0.49493665\n",
            "Iteration 42, loss = 0.49382371\n",
            "Iteration 43, loss = 0.49116192\n",
            "Iteration 44, loss = 0.49137707\n",
            "Iteration 45, loss = 0.49019324\n",
            "Iteration 46, loss = 0.48951993\n",
            "Iteration 47, loss = 0.48910536\n",
            "Iteration 48, loss = 0.48741551\n",
            "Iteration 49, loss = 0.48718603\n",
            "Iteration 50, loss = 0.48788111\n",
            "Iteration 51, loss = 0.48658319\n",
            "Iteration 52, loss = 0.48428395\n",
            "Iteration 53, loss = 0.48301730\n",
            "Iteration 54, loss = 0.48312539\n",
            "Iteration 55, loss = 0.48457488\n",
            "Iteration 56, loss = 0.48226929\n",
            "Iteration 57, loss = 0.48317217\n",
            "Iteration 58, loss = 0.48192920\n",
            "Iteration 59, loss = 0.48146113\n",
            "Iteration 60, loss = 0.47980896\n",
            "Iteration 61, loss = 0.48194676\n",
            "Iteration 62, loss = 0.48177424\n",
            "Iteration 63, loss = 0.47940223\n",
            "Iteration 64, loss = 0.47986086\n",
            "Iteration 65, loss = 0.47906337\n",
            "Iteration 66, loss = 0.48176154\n",
            "Iteration 67, loss = 0.47980551\n",
            "Iteration 68, loss = 0.47421164\n",
            "Iteration 69, loss = 0.47866929\n",
            "Iteration 70, loss = 0.47818193\n",
            "Iteration 71, loss = 0.47648211\n",
            "Iteration 72, loss = 0.47809994\n",
            "Iteration 73, loss = 0.47671165\n",
            "Iteration 74, loss = 0.47807209\n",
            "Iteration 75, loss = 0.47386657\n",
            "Iteration 76, loss = 0.47979068\n",
            "Iteration 77, loss = 0.47641987\n",
            "Iteration 78, loss = 0.47519644\n",
            "Iteration 79, loss = 0.47597674\n",
            "Iteration 80, loss = 0.47752150\n",
            "Iteration 81, loss = 0.47264753\n",
            "Iteration 82, loss = 0.47679573\n",
            "Iteration 83, loss = 0.47137165\n",
            "Iteration 84, loss = 0.47241433\n",
            "Iteration 85, loss = 0.47296123\n",
            "Iteration 86, loss = 0.47121904\n",
            "Iteration 87, loss = 0.46978595\n",
            "Iteration 88, loss = 0.46902987\n",
            "Iteration 89, loss = 0.47068566\n",
            "Iteration 90, loss = 0.47041782\n",
            "Iteration 91, loss = 0.46913431\n",
            "Iteration 92, loss = 0.46712164\n",
            "Iteration 93, loss = 0.46786863\n",
            "Iteration 94, loss = 0.47072284\n",
            "Iteration 95, loss = 0.46650386\n",
            "Iteration 96, loss = 0.46759921\n",
            "Iteration 97, loss = 0.46723491\n",
            "Iteration 98, loss = 0.46798436\n",
            "Iteration 99, loss = 0.46617021\n",
            "Iteration 100, loss = 0.46677471\n",
            "Iteration 101, loss = 0.46883795\n",
            "Iteration 102, loss = 0.46501524\n",
            "Iteration 103, loss = 0.46521559\n",
            "Iteration 104, loss = 0.46640698\n",
            "Iteration 105, loss = 0.46541906\n",
            "Iteration 106, loss = 0.46309067\n",
            "Iteration 107, loss = 0.46109671\n",
            "Iteration 108, loss = 0.46360672\n",
            "Iteration 109, loss = 0.46457365\n",
            "Iteration 110, loss = 0.46823808\n",
            "Iteration 111, loss = 0.46414644\n",
            "Iteration 112, loss = 0.46096768\n",
            "Iteration 113, loss = 0.46169197\n",
            "Iteration 114, loss = 0.46468778\n",
            "Iteration 115, loss = 0.46661842\n",
            "Iteration 116, loss = 0.46006286\n",
            "Iteration 117, loss = 0.45994884\n",
            "Iteration 118, loss = 0.46063935\n",
            "Iteration 119, loss = 0.46146452\n",
            "Iteration 120, loss = 0.46024749\n",
            "Iteration 121, loss = 0.46181429\n",
            "Iteration 122, loss = 0.46016368\n",
            "Iteration 123, loss = 0.45925512\n",
            "Iteration 124, loss = 0.45811744\n",
            "Iteration 125, loss = 0.45703953\n",
            "Iteration 126, loss = 0.45884028\n",
            "Iteration 127, loss = 0.45823976\n",
            "Iteration 128, loss = 0.45926850\n",
            "Iteration 129, loss = 0.45980924\n",
            "Iteration 130, loss = 0.46215638\n",
            "Iteration 131, loss = 0.46053598\n",
            "Iteration 132, loss = 0.45790395\n",
            "Iteration 133, loss = 0.46100678\n",
            "Iteration 134, loss = 0.46189882\n",
            "Iteration 135, loss = 0.45768441\n",
            "Iteration 136, loss = 0.45600856\n",
            "Iteration 137, loss = 0.45665909\n",
            "Iteration 138, loss = 0.45685970\n",
            "Iteration 139, loss = 0.45704522\n",
            "Iteration 140, loss = 0.45598413\n",
            "Iteration 141, loss = 0.45634036\n",
            "Iteration 142, loss = 0.46182268\n",
            "Iteration 143, loss = 0.46050592\n",
            "Iteration 144, loss = 0.45835694\n",
            "Iteration 145, loss = 0.45599864\n",
            "Iteration 146, loss = 0.45600910\n",
            "Iteration 147, loss = 0.45545299\n",
            "Iteration 148, loss = 0.45865224\n",
            "Iteration 149, loss = 0.45750648\n",
            "Iteration 150, loss = 0.45908949\n",
            "Iteration 151, loss = 0.45858047\n",
            "Iteration 152, loss = 0.46115714\n",
            "Iteration 153, loss = 0.45968438\n",
            "Iteration 154, loss = 0.45898212\n",
            "Iteration 155, loss = 0.45578507\n",
            "Iteration 156, loss = 0.46019638\n",
            "Iteration 157, loss = 0.45825232\n",
            "Iteration 158, loss = 0.45621158\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.16748585\n",
            "Iteration 2, loss = 0.68435580\n",
            "Iteration 3, loss = 0.66513531\n",
            "Iteration 4, loss = 0.63465831\n",
            "Iteration 5, loss = 0.59114244\n",
            "Iteration 6, loss = 0.57003818\n",
            "Iteration 7, loss = 0.56114980\n",
            "Iteration 8, loss = 0.55536206\n",
            "Iteration 9, loss = 0.55068060\n",
            "Iteration 10, loss = 0.54421841\n",
            "Iteration 11, loss = 0.53917315\n",
            "Iteration 12, loss = 0.53756731\n",
            "Iteration 13, loss = 0.53401793\n",
            "Iteration 14, loss = 0.53102906\n",
            "Iteration 15, loss = 0.52923240\n",
            "Iteration 16, loss = 0.52842161\n",
            "Iteration 17, loss = 0.52571400\n",
            "Iteration 18, loss = 0.52352226\n",
            "Iteration 19, loss = 0.52236147\n",
            "Iteration 20, loss = 0.52094060\n",
            "Iteration 21, loss = 0.51926264\n",
            "Iteration 22, loss = 0.51803912\n",
            "Iteration 23, loss = 0.51743550\n",
            "Iteration 24, loss = 0.51611098\n",
            "Iteration 25, loss = 0.51484374\n",
            "Iteration 26, loss = 0.51366008\n",
            "Iteration 27, loss = 0.51252271\n",
            "Iteration 28, loss = 0.51171310\n",
            "Iteration 29, loss = 0.51033110\n",
            "Iteration 30, loss = 0.51004992\n",
            "Iteration 31, loss = 0.50854952\n",
            "Iteration 32, loss = 0.50724227\n",
            "Iteration 33, loss = 0.50680377\n",
            "Iteration 34, loss = 0.50552425\n",
            "Iteration 35, loss = 0.50445364\n",
            "Iteration 36, loss = 0.50348245\n",
            "Iteration 37, loss = 0.50306294\n",
            "Iteration 38, loss = 0.50302057\n",
            "Iteration 39, loss = 0.50034962\n",
            "Iteration 40, loss = 0.49685972\n",
            "Iteration 41, loss = 0.49725617\n",
            "Iteration 42, loss = 0.49689740\n",
            "Iteration 43, loss = 0.49322015\n",
            "Iteration 44, loss = 0.49091816\n",
            "Iteration 45, loss = 0.49002955\n",
            "Iteration 46, loss = 0.48930378\n",
            "Iteration 47, loss = 0.48750406\n",
            "Iteration 48, loss = 0.48563755\n",
            "Iteration 49, loss = 0.48510987\n",
            "Iteration 50, loss = 0.48263117\n",
            "Iteration 51, loss = 0.48163564\n",
            "Iteration 52, loss = 0.48019692\n",
            "Iteration 53, loss = 0.47899467\n",
            "Iteration 54, loss = 0.47797759\n",
            "Iteration 55, loss = 0.47806081\n",
            "Iteration 56, loss = 0.47472758\n",
            "Iteration 57, loss = 0.47475328\n",
            "Iteration 58, loss = 0.48037708\n",
            "Iteration 59, loss = 0.47454856\n",
            "Iteration 60, loss = 0.47150624\n",
            "Iteration 61, loss = 0.47368171\n",
            "Iteration 62, loss = 0.47159185\n",
            "Iteration 63, loss = 0.47018719\n",
            "Iteration 64, loss = 0.47013049\n",
            "Iteration 65, loss = 0.46900835\n",
            "Iteration 66, loss = 0.47222578\n",
            "Iteration 67, loss = 0.46624930\n",
            "Iteration 68, loss = 0.46616882\n",
            "Iteration 69, loss = 0.46744872\n",
            "Iteration 70, loss = 0.46542103\n",
            "Iteration 71, loss = 0.46563050\n",
            "Iteration 72, loss = 0.46410833\n",
            "Iteration 73, loss = 0.46574532\n",
            "Iteration 74, loss = 0.46582344\n",
            "Iteration 75, loss = 0.46387258\n",
            "Iteration 76, loss = 0.46665653\n",
            "Iteration 77, loss = 0.46445674\n",
            "Iteration 78, loss = 0.45919444\n",
            "Iteration 79, loss = 0.46157350\n",
            "Iteration 80, loss = 0.45949043\n",
            "Iteration 81, loss = 0.45832037\n",
            "Iteration 82, loss = 0.46105550\n",
            "Iteration 83, loss = 0.45602345\n",
            "Iteration 84, loss = 0.45494166\n",
            "Iteration 85, loss = 0.45595838\n",
            "Iteration 86, loss = 0.45507863\n",
            "Iteration 87, loss = 0.45255795\n",
            "Iteration 88, loss = 0.45389850\n",
            "Iteration 89, loss = 0.45556347\n",
            "Iteration 90, loss = 0.45402081\n",
            "Iteration 91, loss = 0.45350112\n",
            "Iteration 92, loss = 0.45323924\n",
            "Iteration 93, loss = 0.45354713\n",
            "Iteration 94, loss = 0.45469343\n",
            "Iteration 95, loss = 0.45434926\n",
            "Iteration 96, loss = 0.45160621\n",
            "Iteration 97, loss = 0.45766711\n",
            "Iteration 98, loss = 0.45407384\n",
            "Iteration 99, loss = 0.45107557\n",
            "Iteration 100, loss = 0.45150522\n",
            "Iteration 101, loss = 0.45337602\n",
            "Iteration 102, loss = 0.45051003\n",
            "Iteration 103, loss = 0.45141026\n",
            "Iteration 104, loss = 0.45372546\n",
            "Iteration 105, loss = 0.45051399\n",
            "Iteration 106, loss = 0.45271310\n",
            "Iteration 107, loss = 0.45451704\n",
            "Iteration 108, loss = 0.45499414\n",
            "Iteration 109, loss = 0.45433798\n",
            "Iteration 110, loss = 0.45173734\n",
            "Iteration 111, loss = 0.45229799\n",
            "Iteration 112, loss = 0.45241619\n",
            "Iteration 113, loss = 0.44668247\n",
            "Iteration 114, loss = 0.45128533\n",
            "Iteration 115, loss = 0.45399400\n",
            "Iteration 116, loss = 0.45011937\n",
            "Iteration 117, loss = 0.44890083\n",
            "Iteration 118, loss = 0.45017289\n",
            "Iteration 119, loss = 0.44728390\n",
            "Iteration 120, loss = 0.45071777\n",
            "Iteration 121, loss = 0.44625580\n",
            "Iteration 122, loss = 0.44767060\n",
            "Iteration 123, loss = 0.44902646\n",
            "Iteration 124, loss = 0.44515681\n",
            "Iteration 125, loss = 0.44702235\n",
            "Iteration 126, loss = 0.44793449\n",
            "Iteration 127, loss = 0.45023562\n",
            "Iteration 128, loss = 0.45075530\n",
            "Iteration 129, loss = 0.45404378\n",
            "Iteration 130, loss = 0.44971471\n",
            "Iteration 131, loss = 0.44851051\n",
            "Iteration 132, loss = 0.44745229\n",
            "Iteration 133, loss = 0.45351603\n",
            "Iteration 134, loss = 0.45378934\n",
            "Iteration 135, loss = 0.44528727\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.16546862\n",
            "Iteration 2, loss = 0.68633174\n",
            "Iteration 3, loss = 0.66932821\n",
            "Iteration 4, loss = 0.63316451\n",
            "Iteration 5, loss = 0.58313492\n",
            "Iteration 6, loss = 0.56010652\n",
            "Iteration 7, loss = 0.55169390\n",
            "Iteration 8, loss = 0.54727909\n",
            "Iteration 9, loss = 0.54056381\n",
            "Iteration 10, loss = 0.53462264\n",
            "Iteration 11, loss = 0.53074940\n",
            "Iteration 12, loss = 0.52810966\n",
            "Iteration 13, loss = 0.52335327\n",
            "Iteration 14, loss = 0.51907926\n",
            "Iteration 15, loss = 0.51620450\n",
            "Iteration 16, loss = 0.51215303\n",
            "Iteration 17, loss = 0.50909448\n",
            "Iteration 18, loss = 0.50678571\n",
            "Iteration 19, loss = 0.50463134\n",
            "Iteration 20, loss = 0.50275679\n",
            "Iteration 21, loss = 0.50193707\n",
            "Iteration 22, loss = 0.50067535\n",
            "Iteration 23, loss = 0.49933591\n",
            "Iteration 24, loss = 0.49650228\n",
            "Iteration 25, loss = 0.49555558\n",
            "Iteration 26, loss = 0.49408503\n",
            "Iteration 27, loss = 0.49212077\n",
            "Iteration 28, loss = 0.49039763\n",
            "Iteration 29, loss = 0.49091874\n",
            "Iteration 30, loss = 0.48948791\n",
            "Iteration 31, loss = 0.48581534\n",
            "Iteration 32, loss = 0.48869327\n",
            "Iteration 33, loss = 0.48897904\n",
            "Iteration 34, loss = 0.48535231\n",
            "Iteration 35, loss = 0.48291656\n",
            "Iteration 36, loss = 0.48277159\n",
            "Iteration 37, loss = 0.48203690\n",
            "Iteration 38, loss = 0.48011133\n",
            "Iteration 39, loss = 0.47814519\n",
            "Iteration 40, loss = 0.47752528\n",
            "Iteration 41, loss = 0.47808257\n",
            "Iteration 42, loss = 0.47548653\n",
            "Iteration 43, loss = 0.47488039\n",
            "Iteration 44, loss = 0.47215938\n",
            "Iteration 45, loss = 0.47241277\n",
            "Iteration 46, loss = 0.47041245\n",
            "Iteration 47, loss = 0.46855368\n",
            "Iteration 48, loss = 0.46766108\n",
            "Iteration 49, loss = 0.46726617\n",
            "Iteration 50, loss = 0.46619583\n",
            "Iteration 51, loss = 0.46808418\n",
            "Iteration 52, loss = 0.46650417\n",
            "Iteration 53, loss = 0.46421970\n",
            "Iteration 54, loss = 0.46349735\n",
            "Iteration 55, loss = 0.46203259\n",
            "Iteration 56, loss = 0.45977442\n",
            "Iteration 57, loss = 0.46027057\n",
            "Iteration 58, loss = 0.46988778\n",
            "Iteration 59, loss = 0.46233902\n",
            "Iteration 60, loss = 0.45988533\n",
            "Iteration 61, loss = 0.46002719\n",
            "Iteration 62, loss = 0.45684772\n",
            "Iteration 63, loss = 0.45614815\n",
            "Iteration 64, loss = 0.45659778\n",
            "Iteration 65, loss = 0.45477147\n",
            "Iteration 66, loss = 0.45365227\n",
            "Iteration 67, loss = 0.45417324\n",
            "Iteration 68, loss = 0.45558637\n",
            "Iteration 69, loss = 0.45327549\n",
            "Iteration 70, loss = 0.45172777\n",
            "Iteration 71, loss = 0.45299898\n",
            "Iteration 72, loss = 0.44959190\n",
            "Iteration 73, loss = 0.45170402\n",
            "Iteration 74, loss = 0.44775769\n",
            "Iteration 75, loss = 0.45124258\n",
            "Iteration 76, loss = 0.45279056\n",
            "Iteration 77, loss = 0.45367221\n",
            "Iteration 78, loss = 0.45209065\n",
            "Iteration 79, loss = 0.44550968\n",
            "Iteration 80, loss = 0.44876667\n",
            "Iteration 81, loss = 0.45109419\n",
            "Iteration 82, loss = 0.44660740\n",
            "Iteration 83, loss = 0.44514269\n",
            "Iteration 84, loss = 0.44699843\n",
            "Iteration 85, loss = 0.44512059\n",
            "Iteration 86, loss = 0.44512139\n",
            "Iteration 87, loss = 0.44485383\n",
            "Iteration 88, loss = 0.44676667\n",
            "Iteration 89, loss = 0.44270317\n",
            "Iteration 90, loss = 0.44603391\n",
            "Iteration 91, loss = 0.44541570\n",
            "Iteration 92, loss = 0.44201438\n",
            "Iteration 93, loss = 0.44779921\n",
            "Iteration 94, loss = 0.44237476\n",
            "Iteration 95, loss = 0.44077190\n",
            "Iteration 96, loss = 0.44335455\n",
            "Iteration 97, loss = 0.44102656\n",
            "Iteration 98, loss = 0.44041186\n",
            "Iteration 99, loss = 0.44373161\n",
            "Iteration 100, loss = 0.44043808\n",
            "Iteration 101, loss = 0.44140721\n",
            "Iteration 102, loss = 0.44211328\n",
            "Iteration 103, loss = 0.44206878\n",
            "Iteration 104, loss = 0.44635574\n",
            "Iteration 105, loss = 0.44078835\n",
            "Iteration 106, loss = 0.43766046\n",
            "Iteration 107, loss = 0.44037017\n",
            "Iteration 108, loss = 0.44718351\n",
            "Iteration 109, loss = 0.44289678\n",
            "Iteration 110, loss = 0.43875981\n",
            "Iteration 111, loss = 0.44048935\n",
            "Iteration 112, loss = 0.44324779\n",
            "Iteration 113, loss = 0.43909179\n",
            "Iteration 114, loss = 0.44425033\n",
            "Iteration 115, loss = 0.43701218\n",
            "Iteration 116, loss = 0.44133170\n",
            "Iteration 117, loss = 0.43576260\n",
            "Iteration 118, loss = 0.43681952\n",
            "Iteration 119, loss = 0.44194694\n",
            "Iteration 120, loss = 0.44437692\n",
            "Iteration 121, loss = 0.43976692\n",
            "Iteration 122, loss = 0.44073781\n",
            "Iteration 123, loss = 0.43758767\n",
            "Iteration 124, loss = 0.43395733\n",
            "Iteration 125, loss = 0.43618067\n",
            "Iteration 126, loss = 0.44092761\n",
            "Iteration 127, loss = 0.44061005\n",
            "Iteration 128, loss = 0.43970486\n",
            "Iteration 129, loss = 0.43423663\n",
            "Iteration 130, loss = 0.43730267\n",
            "Iteration 131, loss = 0.43944881\n",
            "Iteration 132, loss = 0.43414165\n",
            "Iteration 133, loss = 0.43911078\n",
            "Iteration 134, loss = 0.43964610\n",
            "Iteration 135, loss = 0.43899461\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.17232581\n",
            "Iteration 2, loss = 0.68571124\n",
            "Iteration 3, loss = 0.67888710\n",
            "Iteration 4, loss = 0.64673825\n",
            "Iteration 5, loss = 0.59794892\n",
            "Iteration 6, loss = 0.57312271\n",
            "Iteration 7, loss = 0.56466572\n",
            "Iteration 8, loss = 0.56229608\n",
            "Iteration 9, loss = 0.55561310\n",
            "Iteration 10, loss = 0.54912067\n",
            "Iteration 11, loss = 0.54482135\n",
            "Iteration 12, loss = 0.54181354\n",
            "Iteration 13, loss = 0.53790741\n",
            "Iteration 14, loss = 0.53433278\n",
            "Iteration 15, loss = 0.52898001\n",
            "Iteration 16, loss = 0.52559670\n",
            "Iteration 17, loss = 0.52245148\n",
            "Iteration 18, loss = 0.52076866\n",
            "Iteration 19, loss = 0.51855342\n",
            "Iteration 20, loss = 0.51651247\n",
            "Iteration 21, loss = 0.51437681\n",
            "Iteration 22, loss = 0.51293226\n",
            "Iteration 23, loss = 0.51158435\n",
            "Iteration 24, loss = 0.51007435\n",
            "Iteration 25, loss = 0.50918096\n",
            "Iteration 26, loss = 0.50769243\n",
            "Iteration 27, loss = 0.50590206\n",
            "Iteration 28, loss = 0.50344794\n",
            "Iteration 29, loss = 0.50376017\n",
            "Iteration 30, loss = 0.50259131\n",
            "Iteration 31, loss = 0.49779779\n",
            "Iteration 32, loss = 0.50038877\n",
            "Iteration 33, loss = 0.49986861\n",
            "Iteration 34, loss = 0.49659424\n",
            "Iteration 35, loss = 0.49537589\n",
            "Iteration 36, loss = 0.49442566\n",
            "Iteration 37, loss = 0.49355529\n",
            "Iteration 38, loss = 0.49079018\n",
            "Iteration 39, loss = 0.49320428\n",
            "Iteration 40, loss = 0.49091291\n",
            "Iteration 41, loss = 0.49177786\n",
            "Iteration 42, loss = 0.48780397\n",
            "Iteration 43, loss = 0.48707865\n",
            "Iteration 44, loss = 0.48403883\n",
            "Iteration 45, loss = 0.48323445\n",
            "Iteration 46, loss = 0.48137168\n",
            "Iteration 47, loss = 0.48031872\n",
            "Iteration 48, loss = 0.47925227\n",
            "Iteration 49, loss = 0.47750569\n",
            "Iteration 50, loss = 0.47614650\n",
            "Iteration 51, loss = 0.47494394\n",
            "Iteration 52, loss = 0.47613063\n",
            "Iteration 53, loss = 0.47203151\n",
            "Iteration 54, loss = 0.47099025\n",
            "Iteration 55, loss = 0.47014201\n",
            "Iteration 56, loss = 0.46862588\n",
            "Iteration 57, loss = 0.46814231\n",
            "Iteration 58, loss = 0.46903360\n",
            "Iteration 59, loss = 0.46719988\n",
            "Iteration 60, loss = 0.46525691\n",
            "Iteration 61, loss = 0.46857972\n",
            "Iteration 62, loss = 0.46629160\n",
            "Iteration 63, loss = 0.46516734\n",
            "Iteration 64, loss = 0.46558111\n",
            "Iteration 65, loss = 0.46518520\n",
            "Iteration 66, loss = 0.46280131\n",
            "Iteration 67, loss = 0.46380183\n",
            "Iteration 68, loss = 0.46591917\n",
            "Iteration 69, loss = 0.46315412\n",
            "Iteration 70, loss = 0.46036406\n",
            "Iteration 71, loss = 0.46176288\n",
            "Iteration 72, loss = 0.45721844\n",
            "Iteration 73, loss = 0.46086504\n",
            "Iteration 74, loss = 0.45675920\n",
            "Iteration 75, loss = 0.45976963\n",
            "Iteration 76, loss = 0.45768641\n",
            "Iteration 77, loss = 0.45566741\n",
            "Iteration 78, loss = 0.45668412\n",
            "Iteration 79, loss = 0.45701608\n",
            "Iteration 80, loss = 0.45535166\n",
            "Iteration 81, loss = 0.45516166\n",
            "Iteration 82, loss = 0.45334298\n",
            "Iteration 83, loss = 0.45488815\n",
            "Iteration 84, loss = 0.45266330\n",
            "Iteration 85, loss = 0.45515735\n",
            "Iteration 86, loss = 0.45433561\n",
            "Iteration 87, loss = 0.45214455\n",
            "Iteration 88, loss = 0.45610070\n",
            "Iteration 89, loss = 0.45281270\n",
            "Iteration 90, loss = 0.45580547\n",
            "Iteration 91, loss = 0.45791238\n",
            "Iteration 92, loss = 0.45517448\n",
            "Iteration 93, loss = 0.45607128\n",
            "Iteration 94, loss = 0.45501342\n",
            "Iteration 95, loss = 0.45437550\n",
            "Iteration 96, loss = 0.45452545\n",
            "Iteration 97, loss = 0.45300895\n",
            "Iteration 98, loss = 0.45116718\n",
            "Iteration 99, loss = 0.45147916\n",
            "Iteration 100, loss = 0.45130028\n",
            "Iteration 101, loss = 0.45260655\n",
            "Iteration 102, loss = 0.45125205\n",
            "Iteration 103, loss = 0.45082029\n",
            "Iteration 104, loss = 0.45389515\n",
            "Iteration 105, loss = 0.45264845\n",
            "Iteration 106, loss = 0.45003545\n",
            "Iteration 107, loss = 0.45175451\n",
            "Iteration 108, loss = 0.45140976\n",
            "Iteration 109, loss = 0.45340609\n",
            "Iteration 110, loss = 0.45144869\n",
            "Iteration 111, loss = 0.45279922\n",
            "Iteration 112, loss = 0.45128596\n",
            "Iteration 113, loss = 0.44817011\n",
            "Iteration 114, loss = 0.44694340\n",
            "Iteration 115, loss = 0.44726506\n",
            "Iteration 116, loss = 0.44720818\n",
            "Iteration 117, loss = 0.44637300\n",
            "Iteration 118, loss = 0.44430881\n",
            "Iteration 119, loss = 0.44673647\n",
            "Iteration 120, loss = 0.44947592\n",
            "Iteration 121, loss = 0.44690117\n",
            "Iteration 122, loss = 0.44659092\n",
            "Iteration 123, loss = 0.44429407\n",
            "Iteration 124, loss = 0.44387008\n",
            "Iteration 125, loss = 0.44271516\n",
            "Iteration 126, loss = 0.44786302\n",
            "Iteration 127, loss = 0.44939677\n",
            "Iteration 128, loss = 0.44589560\n",
            "Iteration 129, loss = 0.44260942\n",
            "Iteration 130, loss = 0.44243635\n",
            "Iteration 131, loss = 0.44216317\n",
            "Iteration 132, loss = 0.44120818\n",
            "Iteration 133, loss = 0.44414177\n",
            "Iteration 134, loss = 0.44233022\n",
            "Iteration 135, loss = 0.44012408\n",
            "Iteration 136, loss = 0.44117582\n",
            "Iteration 137, loss = 0.43969211\n",
            "Iteration 138, loss = 0.44123941\n",
            "Iteration 139, loss = 0.43827430\n",
            "Iteration 140, loss = 0.43912199\n",
            "Iteration 141, loss = 0.44022162\n",
            "Iteration 142, loss = 0.43625789\n",
            "Iteration 143, loss = 0.44038959\n",
            "Iteration 144, loss = 0.44088420\n",
            "Iteration 145, loss = 0.43830682\n",
            "Iteration 146, loss = 0.43847359\n",
            "Iteration 147, loss = 0.43895210\n",
            "Iteration 148, loss = 0.43877769\n",
            "Iteration 149, loss = 0.44392436\n",
            "Iteration 150, loss = 0.44263141\n",
            "Iteration 151, loss = 0.43916214\n",
            "Iteration 152, loss = 0.43607532\n",
            "Iteration 153, loss = 0.43667055\n",
            "Iteration 154, loss = 0.43676254\n",
            "Iteration 155, loss = 0.43831448\n",
            "Iteration 156, loss = 0.44010960\n",
            "Iteration 157, loss = 0.43901476\n",
            "Iteration 158, loss = 0.43763188\n",
            "Iteration 159, loss = 0.43693828\n",
            "Iteration 160, loss = 0.43797491\n",
            "Iteration 161, loss = 0.43509970\n",
            "Iteration 162, loss = 0.43955267\n",
            "Iteration 163, loss = 0.43812215\n",
            "Iteration 164, loss = 0.43918429\n",
            "Iteration 165, loss = 0.43590246\n",
            "Iteration 166, loss = 0.43934137\n",
            "Iteration 167, loss = 0.44069563\n",
            "Iteration 168, loss = 0.43681832\n",
            "Iteration 169, loss = 0.43923612\n",
            "Iteration 170, loss = 0.43613118\n",
            "Iteration 171, loss = 0.44360665\n",
            "Iteration 172, loss = 0.43694688\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.16332810\n",
            "Iteration 2, loss = 0.68618232\n",
            "Iteration 3, loss = 0.68913900\n",
            "Iteration 4, loss = 0.65398233\n",
            "Iteration 5, loss = 0.60390842\n",
            "Iteration 6, loss = 0.57885852\n",
            "Iteration 7, loss = 0.57025920\n",
            "Iteration 8, loss = 0.56663455\n",
            "Iteration 9, loss = 0.55823330\n",
            "Iteration 10, loss = 0.55248266\n",
            "Iteration 11, loss = 0.54848967\n",
            "Iteration 12, loss = 0.54592930\n",
            "Iteration 13, loss = 0.54329974\n",
            "Iteration 14, loss = 0.54033888\n",
            "Iteration 15, loss = 0.53572275\n",
            "Iteration 16, loss = 0.53346657\n",
            "Iteration 17, loss = 0.53074143\n",
            "Iteration 18, loss = 0.52813994\n",
            "Iteration 19, loss = 0.52519975\n",
            "Iteration 20, loss = 0.52278328\n",
            "Iteration 21, loss = 0.52152420\n",
            "Iteration 22, loss = 0.52051844\n",
            "Iteration 23, loss = 0.51838123\n",
            "Iteration 24, loss = 0.51528032\n",
            "Iteration 25, loss = 0.51406820\n",
            "Iteration 26, loss = 0.51326190\n",
            "Iteration 27, loss = 0.51152712\n",
            "Iteration 28, loss = 0.50905238\n",
            "Iteration 29, loss = 0.50776352\n",
            "Iteration 30, loss = 0.50738792\n",
            "Iteration 31, loss = 0.50674779\n",
            "Iteration 32, loss = 0.50443804\n",
            "Iteration 33, loss = 0.50413038\n",
            "Iteration 34, loss = 0.50267196\n",
            "Iteration 35, loss = 0.50345086\n",
            "Iteration 36, loss = 0.50351132\n",
            "Iteration 37, loss = 0.50228991\n",
            "Iteration 38, loss = 0.50019602\n",
            "Iteration 39, loss = 0.50010008\n",
            "Iteration 40, loss = 0.49891127\n",
            "Iteration 41, loss = 0.49782462\n",
            "Iteration 42, loss = 0.49613568\n",
            "Iteration 43, loss = 0.49638432\n",
            "Iteration 44, loss = 0.49150995\n",
            "Iteration 45, loss = 0.49590998\n",
            "Iteration 46, loss = 0.49315333\n",
            "Iteration 47, loss = 0.49098981\n",
            "Iteration 48, loss = 0.48868236\n",
            "Iteration 49, loss = 0.48880587\n",
            "Iteration 50, loss = 0.48782580\n",
            "Iteration 51, loss = 0.48398444\n",
            "Iteration 52, loss = 0.48677887\n",
            "Iteration 53, loss = 0.48429541\n",
            "Iteration 54, loss = 0.48259965\n",
            "Iteration 55, loss = 0.48282400\n",
            "Iteration 56, loss = 0.48193661\n",
            "Iteration 57, loss = 0.48142843\n",
            "Iteration 58, loss = 0.48238175\n",
            "Iteration 59, loss = 0.48125050\n",
            "Iteration 60, loss = 0.48075630\n",
            "Iteration 61, loss = 0.48005422\n",
            "Iteration 62, loss = 0.47693954\n",
            "Iteration 63, loss = 0.47850162\n",
            "Iteration 64, loss = 0.47706809\n",
            "Iteration 65, loss = 0.47560735\n",
            "Iteration 66, loss = 0.47619349\n",
            "Iteration 67, loss = 0.47423727\n",
            "Iteration 68, loss = 0.47714908\n",
            "Iteration 69, loss = 0.47224118\n",
            "Iteration 70, loss = 0.47332061\n",
            "Iteration 71, loss = 0.47321793\n",
            "Iteration 72, loss = 0.47258821\n",
            "Iteration 73, loss = 0.47189953\n",
            "Iteration 74, loss = 0.47093795\n",
            "Iteration 75, loss = 0.47350687\n",
            "Iteration 76, loss = 0.47030472\n",
            "Iteration 77, loss = 0.46734921\n",
            "Iteration 78, loss = 0.46952657\n",
            "Iteration 79, loss = 0.47024198\n",
            "Iteration 80, loss = 0.46871231\n",
            "Iteration 81, loss = 0.46987299\n",
            "Iteration 82, loss = 0.46711412\n",
            "Iteration 83, loss = 0.46694432\n",
            "Iteration 84, loss = 0.46505605\n",
            "Iteration 85, loss = 0.46497067\n",
            "Iteration 86, loss = 0.46634769\n",
            "Iteration 87, loss = 0.46120770\n",
            "Iteration 88, loss = 0.46237763\n",
            "Iteration 89, loss = 0.46308429\n",
            "Iteration 90, loss = 0.46505227\n",
            "Iteration 91, loss = 0.46288208\n",
            "Iteration 92, loss = 0.45916148\n",
            "Iteration 93, loss = 0.45643054\n",
            "Iteration 94, loss = 0.45848188\n",
            "Iteration 95, loss = 0.45912885\n",
            "Iteration 96, loss = 0.45837258\n",
            "Iteration 97, loss = 0.45821094\n",
            "Iteration 98, loss = 0.45545213\n",
            "Iteration 99, loss = 0.45392220\n",
            "Iteration 100, loss = 0.45374146\n",
            "Iteration 101, loss = 0.45642120\n",
            "Iteration 102, loss = 0.45451473\n",
            "Iteration 103, loss = 0.45299290\n",
            "Iteration 104, loss = 0.45309323\n",
            "Iteration 105, loss = 0.45222596\n",
            "Iteration 106, loss = 0.45629947\n",
            "Iteration 107, loss = 0.45483009\n",
            "Iteration 108, loss = 0.45190912\n",
            "Iteration 109, loss = 0.45302203\n",
            "Iteration 110, loss = 0.45154488\n",
            "Iteration 111, loss = 0.45372197\n",
            "Iteration 112, loss = 0.45328828\n",
            "Iteration 113, loss = 0.45380653\n",
            "Iteration 114, loss = 0.44976554\n",
            "Iteration 115, loss = 0.45205339\n",
            "Iteration 116, loss = 0.44973564\n",
            "Iteration 117, loss = 0.44918149\n",
            "Iteration 118, loss = 0.44854880\n",
            "Iteration 119, loss = 0.45234790\n",
            "Iteration 120, loss = 0.45286082\n",
            "Iteration 121, loss = 0.45114024\n",
            "Iteration 122, loss = 0.44876960\n",
            "Iteration 123, loss = 0.44987772\n",
            "Iteration 124, loss = 0.44799226\n",
            "Iteration 125, loss = 0.45073528\n",
            "Iteration 126, loss = 0.45691647\n",
            "Iteration 127, loss = 0.44963993\n",
            "Iteration 128, loss = 0.44748313\n",
            "Iteration 129, loss = 0.45213793\n",
            "Iteration 130, loss = 0.44700938\n",
            "Iteration 131, loss = 0.45288740\n",
            "Iteration 132, loss = 0.44719226\n",
            "Iteration 133, loss = 0.44984257\n",
            "Iteration 134, loss = 0.45191281\n",
            "Iteration 135, loss = 0.44697611\n",
            "Iteration 136, loss = 0.44616846\n",
            "Iteration 137, loss = 0.45207069\n",
            "Iteration 138, loss = 0.44800316\n",
            "Iteration 139, loss = 0.44725680\n",
            "Iteration 140, loss = 0.44956945\n",
            "Iteration 141, loss = 0.45020556\n",
            "Iteration 142, loss = 0.44446584\n",
            "Iteration 143, loss = 0.44366399\n",
            "Iteration 144, loss = 0.44138145\n",
            "Iteration 145, loss = 0.44428545\n",
            "Iteration 146, loss = 0.44786137\n",
            "Iteration 147, loss = 0.44710731\n",
            "Iteration 148, loss = 0.44413009\n",
            "Iteration 149, loss = 0.45815286\n",
            "Iteration 150, loss = 0.44439585\n",
            "Iteration 151, loss = 0.44270242\n",
            "Iteration 152, loss = 0.44372411\n",
            "Iteration 153, loss = 0.44328977\n",
            "Iteration 154, loss = 0.44452222\n",
            "Iteration 155, loss = 0.45832714\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 200 and for layer number 6 : 0.6875\n",
            "Iteration 1, loss = 0.61616344\n",
            "Iteration 2, loss = 0.61047446\n",
            "Iteration 3, loss = 0.60478519\n",
            "Iteration 4, loss = 0.60040729\n",
            "Iteration 5, loss = 0.59683886\n",
            "Iteration 6, loss = 0.59352251\n",
            "Iteration 7, loss = 0.59032056\n",
            "Iteration 8, loss = 0.58709569\n",
            "Iteration 9, loss = 0.58414076\n",
            "Iteration 10, loss = 0.58165635\n",
            "Iteration 11, loss = 0.57911060\n",
            "Iteration 12, loss = 0.57790214\n",
            "Iteration 13, loss = 0.57579006\n",
            "Iteration 14, loss = 0.57293731\n",
            "Iteration 15, loss = 0.56984478\n",
            "Iteration 16, loss = 0.56633085\n",
            "Iteration 17, loss = 0.56403178\n",
            "Iteration 18, loss = 0.56081149\n",
            "Iteration 19, loss = 0.55656498\n",
            "Iteration 20, loss = 0.55483738\n",
            "Iteration 21, loss = 0.55165056\n",
            "Iteration 22, loss = 0.55122710\n",
            "Iteration 23, loss = 0.54983455\n",
            "Iteration 24, loss = 0.54824201\n",
            "Iteration 25, loss = 0.54789085\n",
            "Iteration 26, loss = 0.54627285\n",
            "Iteration 27, loss = 0.54523166\n",
            "Iteration 28, loss = 0.54485553\n",
            "Iteration 29, loss = 0.54476942\n",
            "Iteration 30, loss = 0.54341941\n",
            "Iteration 31, loss = 0.54275300\n",
            "Iteration 32, loss = 0.54166800\n",
            "Iteration 33, loss = 0.54111902\n",
            "Iteration 34, loss = 0.54465660\n",
            "Iteration 35, loss = 0.54087738\n",
            "Iteration 36, loss = 0.53954129\n",
            "Iteration 37, loss = 0.54069694\n",
            "Iteration 38, loss = 0.53983298\n",
            "Iteration 39, loss = 0.53907627\n",
            "Iteration 40, loss = 0.53860343\n",
            "Iteration 41, loss = 0.53932428\n",
            "Iteration 42, loss = 0.53915650\n",
            "Iteration 43, loss = 0.53861646\n",
            "Iteration 44, loss = 0.53889930\n",
            "Iteration 45, loss = 0.53773883\n",
            "Iteration 46, loss = 0.53837864\n",
            "Iteration 47, loss = 0.53835545\n",
            "Iteration 48, loss = 0.53696691\n",
            "Iteration 49, loss = 0.53629861\n",
            "Iteration 50, loss = 0.53652009\n",
            "Iteration 51, loss = 0.53844088\n",
            "Iteration 52, loss = 0.53710225\n",
            "Iteration 53, loss = 0.53678021\n",
            "Iteration 54, loss = 0.53589026\n",
            "Iteration 55, loss = 0.53500914\n",
            "Iteration 56, loss = 0.53512312\n",
            "Iteration 57, loss = 0.53536117\n",
            "Iteration 58, loss = 0.53590119\n",
            "Iteration 59, loss = 0.53510110\n",
            "Iteration 60, loss = 0.53468041\n",
            "Iteration 61, loss = 0.53525025\n",
            "Iteration 62, loss = 0.53545549\n",
            "Iteration 63, loss = 0.53428015\n",
            "Iteration 64, loss = 0.53534948\n",
            "Iteration 65, loss = 0.53517115\n",
            "Iteration 66, loss = 0.53455759\n",
            "Iteration 67, loss = 0.53467990\n",
            "Iteration 68, loss = 0.53436042\n",
            "Iteration 69, loss = 0.53448972\n",
            "Iteration 70, loss = 0.53444176\n",
            "Iteration 71, loss = 0.53474912\n",
            "Iteration 72, loss = 0.53471431\n",
            "Iteration 73, loss = 0.53404600\n",
            "Iteration 74, loss = 0.53413797\n",
            "Iteration 75, loss = 0.53474503\n",
            "Iteration 76, loss = 0.53460398\n",
            "Iteration 77, loss = 0.53496211\n",
            "Iteration 78, loss = 0.53504312\n",
            "Iteration 79, loss = 0.53361651\n",
            "Iteration 80, loss = 0.53294444\n",
            "Iteration 81, loss = 0.53551544\n",
            "Iteration 82, loss = 0.53583933\n",
            "Iteration 83, loss = 0.53495348\n",
            "Iteration 84, loss = 0.53577048\n",
            "Iteration 85, loss = 0.53327554\n",
            "Iteration 86, loss = 0.53315024\n",
            "Iteration 87, loss = 0.53348090\n",
            "Iteration 88, loss = 0.53214641\n",
            "Iteration 89, loss = 0.53452553\n",
            "Iteration 90, loss = 0.53339749\n",
            "Iteration 91, loss = 0.53408545\n",
            "Iteration 92, loss = 0.53402317\n",
            "Iteration 93, loss = 0.53249531\n",
            "Iteration 94, loss = 0.53232751\n",
            "Iteration 95, loss = 0.53441131\n",
            "Iteration 96, loss = 0.53359358\n",
            "Iteration 97, loss = 0.53233717\n",
            "Iteration 98, loss = 0.53477685\n",
            "Iteration 99, loss = 0.53576123\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61685270\n",
            "Iteration 2, loss = 0.61073358\n",
            "Iteration 3, loss = 0.60482747\n",
            "Iteration 4, loss = 0.60021840\n",
            "Iteration 5, loss = 0.59678875\n",
            "Iteration 6, loss = 0.59294638\n",
            "Iteration 7, loss = 0.58982852\n",
            "Iteration 8, loss = 0.58727008\n",
            "Iteration 9, loss = 0.58561090\n",
            "Iteration 10, loss = 0.58412723\n",
            "Iteration 11, loss = 0.58439637\n",
            "Iteration 12, loss = 0.58235275\n",
            "Iteration 13, loss = 0.58096798\n",
            "Iteration 14, loss = 0.57891433\n",
            "Iteration 15, loss = 0.57650903\n",
            "Iteration 16, loss = 0.57419914\n",
            "Iteration 17, loss = 0.57181285\n",
            "Iteration 18, loss = 0.56990192\n",
            "Iteration 19, loss = 0.56742489\n",
            "Iteration 20, loss = 0.56484876\n",
            "Iteration 21, loss = 0.56171823\n",
            "Iteration 22, loss = 0.55955647\n",
            "Iteration 23, loss = 0.55886560\n",
            "Iteration 24, loss = 0.55706339\n",
            "Iteration 25, loss = 0.55553422\n",
            "Iteration 26, loss = 0.55262330\n",
            "Iteration 27, loss = 0.55047921\n",
            "Iteration 28, loss = 0.54972788\n",
            "Iteration 29, loss = 0.54861285\n",
            "Iteration 30, loss = 0.54843663\n",
            "Iteration 31, loss = 0.54698559\n",
            "Iteration 32, loss = 0.54512202\n",
            "Iteration 33, loss = 0.54496236\n",
            "Iteration 34, loss = 0.54537399\n",
            "Iteration 35, loss = 0.54621125\n",
            "Iteration 36, loss = 0.54307091\n",
            "Iteration 37, loss = 0.54376705\n",
            "Iteration 38, loss = 0.54240030\n",
            "Iteration 39, loss = 0.54055163\n",
            "Iteration 40, loss = 0.53948219\n",
            "Iteration 41, loss = 0.53937518\n",
            "Iteration 42, loss = 0.53894503\n",
            "Iteration 43, loss = 0.53814838\n",
            "Iteration 44, loss = 0.53717987\n",
            "Iteration 45, loss = 0.53577462\n",
            "Iteration 46, loss = 0.53457548\n",
            "Iteration 47, loss = 0.53370548\n",
            "Iteration 48, loss = 0.53374536\n",
            "Iteration 49, loss = 0.53227638\n",
            "Iteration 50, loss = 0.53107787\n",
            "Iteration 51, loss = 0.53314206\n",
            "Iteration 52, loss = 0.53196477\n",
            "Iteration 53, loss = 0.53088992\n",
            "Iteration 54, loss = 0.53029162\n",
            "Iteration 55, loss = 0.52988151\n",
            "Iteration 56, loss = 0.52888568\n",
            "Iteration 57, loss = 0.52890005\n",
            "Iteration 58, loss = 0.52924797\n",
            "Iteration 59, loss = 0.52921454\n",
            "Iteration 60, loss = 0.52868950\n",
            "Iteration 61, loss = 0.52862123\n",
            "Iteration 62, loss = 0.52993570\n",
            "Iteration 63, loss = 0.52877129\n",
            "Iteration 64, loss = 0.52946871\n",
            "Iteration 65, loss = 0.52981092\n",
            "Iteration 66, loss = 0.53015241\n",
            "Iteration 67, loss = 0.52990268\n",
            "Iteration 68, loss = 0.52903022\n",
            "Iteration 69, loss = 0.52935976\n",
            "Iteration 70, loss = 0.52849976\n",
            "Iteration 71, loss = 0.52799905\n",
            "Iteration 72, loss = 0.52879280\n",
            "Iteration 73, loss = 0.52821299\n",
            "Iteration 74, loss = 0.52730704\n",
            "Iteration 75, loss = 0.52866449\n",
            "Iteration 76, loss = 0.52869782\n",
            "Iteration 77, loss = 0.52738765\n",
            "Iteration 78, loss = 0.52743448\n",
            "Iteration 79, loss = 0.52749131\n",
            "Iteration 80, loss = 0.52713528\n",
            "Iteration 81, loss = 0.52811449\n",
            "Iteration 82, loss = 0.52784675\n",
            "Iteration 83, loss = 0.52592258\n",
            "Iteration 84, loss = 0.52574880\n",
            "Iteration 85, loss = 0.52562074\n",
            "Iteration 86, loss = 0.52562702\n",
            "Iteration 87, loss = 0.52489044\n",
            "Iteration 88, loss = 0.52673234\n",
            "Iteration 89, loss = 0.52780336\n",
            "Iteration 90, loss = 0.52606327\n",
            "Iteration 91, loss = 0.52617697\n",
            "Iteration 92, loss = 0.52557126\n",
            "Iteration 93, loss = 0.52481340\n",
            "Iteration 94, loss = 0.52480812\n",
            "Iteration 95, loss = 0.52540942\n",
            "Iteration 96, loss = 0.52637237\n",
            "Iteration 97, loss = 0.52433287\n",
            "Iteration 98, loss = 0.52539716\n",
            "Iteration 99, loss = 0.52543487\n",
            "Iteration 100, loss = 0.52402267\n",
            "Iteration 101, loss = 0.52394746\n",
            "Iteration 102, loss = 0.52557838\n",
            "Iteration 103, loss = 0.52511001\n",
            "Iteration 104, loss = 0.52432925\n",
            "Iteration 105, loss = 0.52443864\n",
            "Iteration 106, loss = 0.52406957\n",
            "Iteration 107, loss = 0.52459945\n",
            "Iteration 108, loss = 0.52394326\n",
            "Iteration 109, loss = 0.52307189\n",
            "Iteration 110, loss = 0.52400625\n",
            "Iteration 111, loss = 0.52296824\n",
            "Iteration 112, loss = 0.52321968\n",
            "Iteration 113, loss = 0.52327495\n",
            "Iteration 114, loss = 0.52374982\n",
            "Iteration 115, loss = 0.52267798\n",
            "Iteration 116, loss = 0.52300414\n",
            "Iteration 117, loss = 0.52305182\n",
            "Iteration 118, loss = 0.52330732\n",
            "Iteration 119, loss = 0.52477222\n",
            "Iteration 120, loss = 0.52362954\n",
            "Iteration 121, loss = 0.52294899\n",
            "Iteration 122, loss = 0.52279810\n",
            "Iteration 123, loss = 0.52346034\n",
            "Iteration 124, loss = 0.52288044\n",
            "Iteration 125, loss = 0.52290763\n",
            "Iteration 126, loss = 0.52227996\n",
            "Iteration 127, loss = 0.52311677\n",
            "Iteration 128, loss = 0.52422479\n",
            "Iteration 129, loss = 0.52620033\n",
            "Iteration 130, loss = 0.52289523\n",
            "Iteration 131, loss = 0.52251810\n",
            "Iteration 132, loss = 0.52295944\n",
            "Iteration 133, loss = 0.52369208\n",
            "Iteration 134, loss = 0.52439265\n",
            "Iteration 135, loss = 0.52252922\n",
            "Iteration 136, loss = 0.52396035\n",
            "Iteration 137, loss = 0.52261474\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61686379\n",
            "Iteration 2, loss = 0.61078213\n",
            "Iteration 3, loss = 0.60373906\n",
            "Iteration 4, loss = 0.59835244\n",
            "Iteration 5, loss = 0.59457664\n",
            "Iteration 6, loss = 0.59038982\n",
            "Iteration 7, loss = 0.58694864\n",
            "Iteration 8, loss = 0.58122019\n",
            "Iteration 9, loss = 0.57559384\n",
            "Iteration 10, loss = 0.57111279\n",
            "Iteration 11, loss = 0.56767019\n",
            "Iteration 12, loss = 0.56484322\n",
            "Iteration 13, loss = 0.55949772\n",
            "Iteration 14, loss = 0.55649967\n",
            "Iteration 15, loss = 0.55290617\n",
            "Iteration 16, loss = 0.54970478\n",
            "Iteration 17, loss = 0.54607332\n",
            "Iteration 18, loss = 0.54403178\n",
            "Iteration 19, loss = 0.54132068\n",
            "Iteration 20, loss = 0.53854210\n",
            "Iteration 21, loss = 0.53710357\n",
            "Iteration 22, loss = 0.53429746\n",
            "Iteration 23, loss = 0.53375008\n",
            "Iteration 24, loss = 0.53094852\n",
            "Iteration 25, loss = 0.52965028\n",
            "Iteration 26, loss = 0.52815839\n",
            "Iteration 27, loss = 0.52688496\n",
            "Iteration 28, loss = 0.52553353\n",
            "Iteration 29, loss = 0.52394223\n",
            "Iteration 30, loss = 0.52356500\n",
            "Iteration 31, loss = 0.52294680\n",
            "Iteration 32, loss = 0.52183150\n",
            "Iteration 33, loss = 0.52255860\n",
            "Iteration 34, loss = 0.52142071\n",
            "Iteration 35, loss = 0.52125496\n",
            "Iteration 36, loss = 0.52111537\n",
            "Iteration 37, loss = 0.52026383\n",
            "Iteration 38, loss = 0.52045544\n",
            "Iteration 39, loss = 0.52091092\n",
            "Iteration 40, loss = 0.52008196\n",
            "Iteration 41, loss = 0.51990365\n",
            "Iteration 42, loss = 0.51974732\n",
            "Iteration 43, loss = 0.51904939\n",
            "Iteration 44, loss = 0.51941507\n",
            "Iteration 45, loss = 0.51917314\n",
            "Iteration 46, loss = 0.51924421\n",
            "Iteration 47, loss = 0.51928018\n",
            "Iteration 48, loss = 0.51909289\n",
            "Iteration 49, loss = 0.51842013\n",
            "Iteration 50, loss = 0.51802283\n",
            "Iteration 51, loss = 0.51819928\n",
            "Iteration 52, loss = 0.51951413\n",
            "Iteration 53, loss = 0.51949918\n",
            "Iteration 54, loss = 0.52004138\n",
            "Iteration 55, loss = 0.51851564\n",
            "Iteration 56, loss = 0.51716944\n",
            "Iteration 57, loss = 0.51662275\n",
            "Iteration 58, loss = 0.51660941\n",
            "Iteration 59, loss = 0.51655928\n",
            "Iteration 60, loss = 0.51630371\n",
            "Iteration 61, loss = 0.51683368\n",
            "Iteration 62, loss = 0.51674972\n",
            "Iteration 63, loss = 0.51545783\n",
            "Iteration 64, loss = 0.51608693\n",
            "Iteration 65, loss = 0.51590192\n",
            "Iteration 66, loss = 0.51553887\n",
            "Iteration 67, loss = 0.51569551\n",
            "Iteration 68, loss = 0.51562204\n",
            "Iteration 69, loss = 0.51538478\n",
            "Iteration 70, loss = 0.51492148\n",
            "Iteration 71, loss = 0.51418214\n",
            "Iteration 72, loss = 0.51644749\n",
            "Iteration 73, loss = 0.51636934\n",
            "Iteration 74, loss = 0.51514015\n",
            "Iteration 75, loss = 0.51694397\n",
            "Iteration 76, loss = 0.51490108\n",
            "Iteration 77, loss = 0.51324531\n",
            "Iteration 78, loss = 0.51423000\n",
            "Iteration 79, loss = 0.51396088\n",
            "Iteration 80, loss = 0.51433935\n",
            "Iteration 81, loss = 0.51317206\n",
            "Iteration 82, loss = 0.51380235\n",
            "Iteration 83, loss = 0.51273531\n",
            "Iteration 84, loss = 0.51314830\n",
            "Iteration 85, loss = 0.51374298\n",
            "Iteration 86, loss = 0.51415859\n",
            "Iteration 87, loss = 0.51239317\n",
            "Iteration 88, loss = 0.51355738\n",
            "Iteration 89, loss = 0.51420369\n",
            "Iteration 90, loss = 0.51490013\n",
            "Iteration 91, loss = 0.51391767\n",
            "Iteration 92, loss = 0.51283594\n",
            "Iteration 93, loss = 0.51340462\n",
            "Iteration 94, loss = 0.51341917\n",
            "Iteration 95, loss = 0.51351797\n",
            "Iteration 96, loss = 0.51398162\n",
            "Iteration 97, loss = 0.51293432\n",
            "Iteration 98, loss = 0.51275042\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61997020\n",
            "Iteration 2, loss = 0.61412378\n",
            "Iteration 3, loss = 0.60919162\n",
            "Iteration 4, loss = 0.60490434\n",
            "Iteration 5, loss = 0.60251389\n",
            "Iteration 6, loss = 0.59997429\n",
            "Iteration 7, loss = 0.59738037\n",
            "Iteration 8, loss = 0.59269736\n",
            "Iteration 9, loss = 0.58767828\n",
            "Iteration 10, loss = 0.58446028\n",
            "Iteration 11, loss = 0.58104889\n",
            "Iteration 12, loss = 0.57867536\n",
            "Iteration 13, loss = 0.57465469\n",
            "Iteration 14, loss = 0.57118039\n",
            "Iteration 15, loss = 0.56831045\n",
            "Iteration 16, loss = 0.56705962\n",
            "Iteration 17, loss = 0.56455372\n",
            "Iteration 18, loss = 0.56249116\n",
            "Iteration 19, loss = 0.56029054\n",
            "Iteration 20, loss = 0.55870059\n",
            "Iteration 21, loss = 0.55790136\n",
            "Iteration 22, loss = 0.55542813\n",
            "Iteration 23, loss = 0.55448658\n",
            "Iteration 24, loss = 0.55341077\n",
            "Iteration 25, loss = 0.55234914\n",
            "Iteration 26, loss = 0.55099291\n",
            "Iteration 27, loss = 0.54992709\n",
            "Iteration 28, loss = 0.54865229\n",
            "Iteration 29, loss = 0.54789818\n",
            "Iteration 30, loss = 0.54655713\n",
            "Iteration 31, loss = 0.54464837\n",
            "Iteration 32, loss = 0.54318115\n",
            "Iteration 33, loss = 0.54176155\n",
            "Iteration 34, loss = 0.53991255\n",
            "Iteration 35, loss = 0.53939289\n",
            "Iteration 36, loss = 0.53903124\n",
            "Iteration 37, loss = 0.53655695\n",
            "Iteration 38, loss = 0.53469995\n",
            "Iteration 39, loss = 0.53484729\n",
            "Iteration 40, loss = 0.53341719\n",
            "Iteration 41, loss = 0.53159598\n",
            "Iteration 42, loss = 0.53132522\n",
            "Iteration 43, loss = 0.53072165\n",
            "Iteration 44, loss = 0.53090682\n",
            "Iteration 45, loss = 0.53074961\n",
            "Iteration 46, loss = 0.53065336\n",
            "Iteration 47, loss = 0.53033300\n",
            "Iteration 48, loss = 0.52958211\n",
            "Iteration 49, loss = 0.52900039\n",
            "Iteration 50, loss = 0.52795815\n",
            "Iteration 51, loss = 0.52848891\n",
            "Iteration 52, loss = 0.53032016\n",
            "Iteration 53, loss = 0.52827347\n",
            "Iteration 54, loss = 0.52872219\n",
            "Iteration 55, loss = 0.52744500\n",
            "Iteration 56, loss = 0.52682941\n",
            "Iteration 57, loss = 0.52662720\n",
            "Iteration 58, loss = 0.52671375\n",
            "Iteration 59, loss = 0.52668943\n",
            "Iteration 60, loss = 0.52675084\n",
            "Iteration 61, loss = 0.52698153\n",
            "Iteration 62, loss = 0.52731884\n",
            "Iteration 63, loss = 0.52657748\n",
            "Iteration 64, loss = 0.52577505\n",
            "Iteration 65, loss = 0.52717356\n",
            "Iteration 66, loss = 0.52654840\n",
            "Iteration 67, loss = 0.52671599\n",
            "Iteration 68, loss = 0.52702098\n",
            "Iteration 69, loss = 0.52778156\n",
            "Iteration 70, loss = 0.52659366\n",
            "Iteration 71, loss = 0.52593393\n",
            "Iteration 72, loss = 0.52768568\n",
            "Iteration 73, loss = 0.52725121\n",
            "Iteration 74, loss = 0.52653828\n",
            "Iteration 75, loss = 0.52815880\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61941224\n",
            "Iteration 2, loss = 0.61318364\n",
            "Iteration 3, loss = 0.60864231\n",
            "Iteration 4, loss = 0.60454530\n",
            "Iteration 5, loss = 0.60243915\n",
            "Iteration 6, loss = 0.60246689\n",
            "Iteration 7, loss = 0.60083027\n",
            "Iteration 8, loss = 0.59640653\n",
            "Iteration 9, loss = 0.59331881\n",
            "Iteration 10, loss = 0.59175851\n",
            "Iteration 11, loss = 0.58960825\n",
            "Iteration 12, loss = 0.58941496\n",
            "Iteration 13, loss = 0.58594276\n",
            "Iteration 14, loss = 0.58312039\n",
            "Iteration 15, loss = 0.58096844\n",
            "Iteration 16, loss = 0.57971808\n",
            "Iteration 17, loss = 0.58006510\n",
            "Iteration 18, loss = 0.57982617\n",
            "Iteration 19, loss = 0.57840057\n",
            "Iteration 20, loss = 0.57747882\n",
            "Iteration 21, loss = 0.57701133\n",
            "Iteration 22, loss = 0.57534329\n",
            "Iteration 23, loss = 0.57568807\n",
            "Iteration 24, loss = 0.57479372\n",
            "Iteration 25, loss = 0.57414903\n",
            "Iteration 26, loss = 0.57320683\n",
            "Iteration 27, loss = 0.57351214\n",
            "Iteration 28, loss = 0.57212556\n",
            "Iteration 29, loss = 0.57182505\n",
            "Iteration 30, loss = 0.57175012\n",
            "Iteration 31, loss = 0.57021913\n",
            "Iteration 32, loss = 0.56935480\n",
            "Iteration 33, loss = 0.56839796\n",
            "Iteration 34, loss = 0.56693242\n",
            "Iteration 35, loss = 0.56572092\n",
            "Iteration 36, loss = 0.56395992\n",
            "Iteration 37, loss = 0.56338466\n",
            "Iteration 38, loss = 0.56170087\n",
            "Iteration 39, loss = 0.55877045\n",
            "Iteration 40, loss = 0.55793019\n",
            "Iteration 41, loss = 0.55614417\n",
            "Iteration 42, loss = 0.55506357\n",
            "Iteration 43, loss = 0.55343058\n",
            "Iteration 44, loss = 0.55340796\n",
            "Iteration 45, loss = 0.55087697\n",
            "Iteration 46, loss = 0.54966880\n",
            "Iteration 47, loss = 0.54774331\n",
            "Iteration 48, loss = 0.54593504\n",
            "Iteration 49, loss = 0.54453803\n",
            "Iteration 50, loss = 0.54326147\n",
            "Iteration 51, loss = 0.54251671\n",
            "Iteration 52, loss = 0.54193067\n",
            "Iteration 53, loss = 0.54062192\n",
            "Iteration 54, loss = 0.54211250\n",
            "Iteration 55, loss = 0.54021127\n",
            "Iteration 56, loss = 0.53903735\n",
            "Iteration 57, loss = 0.53750337\n",
            "Iteration 58, loss = 0.53657233\n",
            "Iteration 59, loss = 0.53494783\n",
            "Iteration 60, loss = 0.53609512\n",
            "Iteration 61, loss = 0.53605135\n",
            "Iteration 62, loss = 0.53444912\n",
            "Iteration 63, loss = 0.53426413\n",
            "Iteration 64, loss = 0.53256619\n",
            "Iteration 65, loss = 0.53272366\n",
            "Iteration 66, loss = 0.53213028\n",
            "Iteration 67, loss = 0.53294015\n",
            "Iteration 68, loss = 0.53405626\n",
            "Iteration 69, loss = 0.53502634\n",
            "Iteration 70, loss = 0.53220216\n",
            "Iteration 71, loss = 0.53183938\n",
            "Iteration 72, loss = 0.53266801\n",
            "Iteration 73, loss = 0.53181568\n",
            "Iteration 74, loss = 0.53103228\n",
            "Iteration 75, loss = 0.53109650\n",
            "Iteration 76, loss = 0.53120562\n",
            "Iteration 77, loss = 0.53139712\n",
            "Iteration 78, loss = 0.53105354\n",
            "Iteration 79, loss = 0.53115344\n",
            "Iteration 80, loss = 0.53351865\n",
            "Iteration 81, loss = 0.53029266\n",
            "Iteration 82, loss = 0.53141297\n",
            "Iteration 83, loss = 0.53030700\n",
            "Iteration 84, loss = 0.53077603\n",
            "Iteration 85, loss = 0.53117501\n",
            "Iteration 86, loss = 0.53017275\n",
            "Iteration 87, loss = 0.53167091\n",
            "Iteration 88, loss = 0.53110833\n",
            "Iteration 89, loss = 0.53082480\n",
            "Iteration 90, loss = 0.53097012\n",
            "Iteration 91, loss = 0.52953903\n",
            "Iteration 92, loss = 0.53041845\n",
            "Iteration 93, loss = 0.53062090\n",
            "Iteration 94, loss = 0.53060511\n",
            "Iteration 95, loss = 0.52943256\n",
            "Iteration 96, loss = 0.52966999\n",
            "Iteration 97, loss = 0.53005740\n",
            "Iteration 98, loss = 0.53237745\n",
            "Iteration 99, loss = 0.53421484\n",
            "Iteration 100, loss = 0.53217815\n",
            "Iteration 101, loss = 0.53011092\n",
            "Iteration 102, loss = 0.53037191\n",
            "Iteration 103, loss = 0.52964197\n",
            "Iteration 104, loss = 0.53177603\n",
            "Iteration 105, loss = 0.53014765\n",
            "Iteration 106, loss = 0.53351194\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 250 and for layer number 2 : 0.70625\n",
            "Iteration 1, loss = 0.81149998\n",
            "Iteration 2, loss = 0.65873604\n",
            "Iteration 3, loss = 0.59987267\n",
            "Iteration 4, loss = 0.58584785\n",
            "Iteration 5, loss = 0.57524774\n",
            "Iteration 6, loss = 0.57117799\n",
            "Iteration 7, loss = 0.56778662\n",
            "Iteration 8, loss = 0.56432353\n",
            "Iteration 9, loss = 0.56078602\n",
            "Iteration 10, loss = 0.55775868\n",
            "Iteration 11, loss = 0.55650784\n",
            "Iteration 12, loss = 0.55523370\n",
            "Iteration 13, loss = 0.55379450\n",
            "Iteration 14, loss = 0.55218434\n",
            "Iteration 15, loss = 0.54947622\n",
            "Iteration 16, loss = 0.54665060\n",
            "Iteration 17, loss = 0.54742043\n",
            "Iteration 18, loss = 0.54599748\n",
            "Iteration 19, loss = 0.54336523\n",
            "Iteration 20, loss = 0.54192285\n",
            "Iteration 21, loss = 0.54095123\n",
            "Iteration 22, loss = 0.53904375\n",
            "Iteration 23, loss = 0.53750011\n",
            "Iteration 24, loss = 0.53633557\n",
            "Iteration 25, loss = 0.53686758\n",
            "Iteration 26, loss = 0.53616775\n",
            "Iteration 27, loss = 0.53438193\n",
            "Iteration 28, loss = 0.53371599\n",
            "Iteration 29, loss = 0.53315282\n",
            "Iteration 30, loss = 0.53305221\n",
            "Iteration 31, loss = 0.53267854\n",
            "Iteration 32, loss = 0.53127495\n",
            "Iteration 33, loss = 0.53042640\n",
            "Iteration 34, loss = 0.52970462\n",
            "Iteration 35, loss = 0.52915225\n",
            "Iteration 36, loss = 0.52950701\n",
            "Iteration 37, loss = 0.53004409\n",
            "Iteration 38, loss = 0.52794971\n",
            "Iteration 39, loss = 0.52762295\n",
            "Iteration 40, loss = 0.52731455\n",
            "Iteration 41, loss = 0.52624037\n",
            "Iteration 42, loss = 0.52665690\n",
            "Iteration 43, loss = 0.52605307\n",
            "Iteration 44, loss = 0.52585806\n",
            "Iteration 45, loss = 0.52614887\n",
            "Iteration 46, loss = 0.52391730\n",
            "Iteration 47, loss = 0.52342006\n",
            "Iteration 48, loss = 0.52281511\n",
            "Iteration 49, loss = 0.52151818\n",
            "Iteration 50, loss = 0.52013343\n",
            "Iteration 51, loss = 0.52039141\n",
            "Iteration 52, loss = 0.52155114\n",
            "Iteration 53, loss = 0.51973222\n",
            "Iteration 54, loss = 0.51937204\n",
            "Iteration 55, loss = 0.52073481\n",
            "Iteration 56, loss = 0.52026603\n",
            "Iteration 57, loss = 0.51854102\n",
            "Iteration 58, loss = 0.51726925\n",
            "Iteration 59, loss = 0.51684776\n",
            "Iteration 60, loss = 0.51709917\n",
            "Iteration 61, loss = 0.51740898\n",
            "Iteration 62, loss = 0.51849608\n",
            "Iteration 63, loss = 0.51816904\n",
            "Iteration 64, loss = 0.51797494\n",
            "Iteration 65, loss = 0.51884720\n",
            "Iteration 66, loss = 0.51748484\n",
            "Iteration 67, loss = 0.51679103\n",
            "Iteration 68, loss = 0.51567568\n",
            "Iteration 69, loss = 0.51513003\n",
            "Iteration 70, loss = 0.51663152\n",
            "Iteration 71, loss = 0.51592509\n",
            "Iteration 72, loss = 0.51354275\n",
            "Iteration 73, loss = 0.51737624\n",
            "Iteration 74, loss = 0.51699973\n",
            "Iteration 75, loss = 0.51609717\n",
            "Iteration 76, loss = 0.51411590\n",
            "Iteration 77, loss = 0.51467742\n",
            "Iteration 78, loss = 0.51476699\n",
            "Iteration 79, loss = 0.51310993\n",
            "Iteration 80, loss = 0.51284290\n",
            "Iteration 81, loss = 0.51462620\n",
            "Iteration 82, loss = 0.51531153\n",
            "Iteration 83, loss = 0.51651873\n",
            "Iteration 84, loss = 0.51583456\n",
            "Iteration 85, loss = 0.51574633\n",
            "Iteration 86, loss = 0.51492935\n",
            "Iteration 87, loss = 0.51491277\n",
            "Iteration 88, loss = 0.51469364\n",
            "Iteration 89, loss = 0.51383952\n",
            "Iteration 90, loss = 0.51359792\n",
            "Iteration 91, loss = 0.51379622\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.81331306\n",
            "Iteration 2, loss = 0.66033007\n",
            "Iteration 3, loss = 0.60394338\n",
            "Iteration 4, loss = 0.58347678\n",
            "Iteration 5, loss = 0.57325470\n",
            "Iteration 6, loss = 0.56846700\n",
            "Iteration 7, loss = 0.56686121\n",
            "Iteration 8, loss = 0.56389750\n",
            "Iteration 9, loss = 0.55854005\n",
            "Iteration 10, loss = 0.55686836\n",
            "Iteration 11, loss = 0.55570604\n",
            "Iteration 12, loss = 0.55406456\n",
            "Iteration 13, loss = 0.55150024\n",
            "Iteration 14, loss = 0.54875073\n",
            "Iteration 15, loss = 0.54638142\n",
            "Iteration 16, loss = 0.54388926\n",
            "Iteration 17, loss = 0.54225501\n",
            "Iteration 18, loss = 0.54000220\n",
            "Iteration 19, loss = 0.53809810\n",
            "Iteration 20, loss = 0.53710679\n",
            "Iteration 21, loss = 0.53602823\n",
            "Iteration 22, loss = 0.53495535\n",
            "Iteration 23, loss = 0.53323611\n",
            "Iteration 24, loss = 0.53158803\n",
            "Iteration 25, loss = 0.53142116\n",
            "Iteration 26, loss = 0.53039578\n",
            "Iteration 27, loss = 0.52896039\n",
            "Iteration 28, loss = 0.52812962\n",
            "Iteration 29, loss = 0.52760837\n",
            "Iteration 30, loss = 0.52750333\n",
            "Iteration 31, loss = 0.52844395\n",
            "Iteration 32, loss = 0.52805030\n",
            "Iteration 33, loss = 0.52654364\n",
            "Iteration 34, loss = 0.52559491\n",
            "Iteration 35, loss = 0.52472209\n",
            "Iteration 36, loss = 0.52462304\n",
            "Iteration 37, loss = 0.52484838\n",
            "Iteration 38, loss = 0.52243952\n",
            "Iteration 39, loss = 0.52135493\n",
            "Iteration 40, loss = 0.52089779\n",
            "Iteration 41, loss = 0.52040099\n",
            "Iteration 42, loss = 0.52008310\n",
            "Iteration 43, loss = 0.51987458\n",
            "Iteration 44, loss = 0.51911580\n",
            "Iteration 45, loss = 0.51961977\n",
            "Iteration 46, loss = 0.51973649\n",
            "Iteration 47, loss = 0.51772961\n",
            "Iteration 48, loss = 0.51829609\n",
            "Iteration 49, loss = 0.51755155\n",
            "Iteration 50, loss = 0.51591976\n",
            "Iteration 51, loss = 0.51644725\n",
            "Iteration 52, loss = 0.51663831\n",
            "Iteration 53, loss = 0.51686230\n",
            "Iteration 54, loss = 0.51725454\n",
            "Iteration 55, loss = 0.51699703\n",
            "Iteration 56, loss = 0.51687145\n",
            "Iteration 57, loss = 0.51589051\n",
            "Iteration 58, loss = 0.51582670\n",
            "Iteration 59, loss = 0.51564801\n",
            "Iteration 60, loss = 0.51650497\n",
            "Iteration 61, loss = 0.51722020\n",
            "Iteration 62, loss = 0.51686858\n",
            "Iteration 63, loss = 0.51537984\n",
            "Iteration 64, loss = 0.51598666\n",
            "Iteration 65, loss = 0.51758055\n",
            "Iteration 66, loss = 0.51641849\n",
            "Iteration 67, loss = 0.51479609\n",
            "Iteration 68, loss = 0.51482381\n",
            "Iteration 69, loss = 0.51634873\n",
            "Iteration 70, loss = 0.51664928\n",
            "Iteration 71, loss = 0.51580978\n",
            "Iteration 72, loss = 0.51464183\n",
            "Iteration 73, loss = 0.51606701\n",
            "Iteration 74, loss = 0.51718934\n",
            "Iteration 75, loss = 0.51586765\n",
            "Iteration 76, loss = 0.51353713\n",
            "Iteration 77, loss = 0.51510832\n",
            "Iteration 78, loss = 0.51452434\n",
            "Iteration 79, loss = 0.51383952\n",
            "Iteration 80, loss = 0.51460585\n",
            "Iteration 81, loss = 0.51428523\n",
            "Iteration 82, loss = 0.51447232\n",
            "Iteration 83, loss = 0.51480264\n",
            "Iteration 84, loss = 0.51529507\n",
            "Iteration 85, loss = 0.51354914\n",
            "Iteration 86, loss = 0.51193067\n",
            "Iteration 87, loss = 0.51160359\n",
            "Iteration 88, loss = 0.51109447\n",
            "Iteration 89, loss = 0.51041741\n",
            "Iteration 90, loss = 0.51053244\n",
            "Iteration 91, loss = 0.51303529\n",
            "Iteration 92, loss = 0.51296644\n",
            "Iteration 93, loss = 0.50982758\n",
            "Iteration 94, loss = 0.51173122\n",
            "Iteration 95, loss = 0.51226543\n",
            "Iteration 96, loss = 0.51108616\n",
            "Iteration 97, loss = 0.51200241\n",
            "Iteration 98, loss = 0.51100175\n",
            "Iteration 99, loss = 0.51246029\n",
            "Iteration 100, loss = 0.51000404\n",
            "Iteration 101, loss = 0.51138436\n",
            "Iteration 102, loss = 0.51236465\n",
            "Iteration 103, loss = 0.51172918\n",
            "Iteration 104, loss = 0.50855530\n",
            "Iteration 105, loss = 0.50935817\n",
            "Iteration 106, loss = 0.51052172\n",
            "Iteration 107, loss = 0.50964702\n",
            "Iteration 108, loss = 0.51046143\n",
            "Iteration 109, loss = 0.50950127\n",
            "Iteration 110, loss = 0.50842225\n",
            "Iteration 111, loss = 0.50861293\n",
            "Iteration 112, loss = 0.50905976\n",
            "Iteration 113, loss = 0.50864322\n",
            "Iteration 114, loss = 0.50808864\n",
            "Iteration 115, loss = 0.50867589\n",
            "Iteration 116, loss = 0.50841214\n",
            "Iteration 117, loss = 0.50897808\n",
            "Iteration 118, loss = 0.51029751\n",
            "Iteration 119, loss = 0.51017563\n",
            "Iteration 120, loss = 0.51101536\n",
            "Iteration 121, loss = 0.51041526\n",
            "Iteration 122, loss = 0.50764798\n",
            "Iteration 123, loss = 0.50879865\n",
            "Iteration 124, loss = 0.50951965\n",
            "Iteration 125, loss = 0.50867811\n",
            "Iteration 126, loss = 0.50849240\n",
            "Iteration 127, loss = 0.50884949\n",
            "Iteration 128, loss = 0.50952545\n",
            "Iteration 129, loss = 0.50750653\n",
            "Iteration 130, loss = 0.50831559\n",
            "Iteration 131, loss = 0.50812065\n",
            "Iteration 132, loss = 0.50747709\n",
            "Iteration 133, loss = 0.50883508\n",
            "Iteration 134, loss = 0.50905708\n",
            "Iteration 135, loss = 0.50842161\n",
            "Iteration 136, loss = 0.50808287\n",
            "Iteration 137, loss = 0.50961933\n",
            "Iteration 138, loss = 0.51003489\n",
            "Iteration 139, loss = 0.50941765\n",
            "Iteration 140, loss = 0.50742270\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.82932651\n",
            "Iteration 2, loss = 0.66164239\n",
            "Iteration 3, loss = 0.60009028\n",
            "Iteration 4, loss = 0.57580262\n",
            "Iteration 5, loss = 0.56470176\n",
            "Iteration 6, loss = 0.55748571\n",
            "Iteration 7, loss = 0.55627910\n",
            "Iteration 8, loss = 0.55132255\n",
            "Iteration 9, loss = 0.54679100\n",
            "Iteration 10, loss = 0.54440088\n",
            "Iteration 11, loss = 0.54314934\n",
            "Iteration 12, loss = 0.54115322\n",
            "Iteration 13, loss = 0.53896820\n",
            "Iteration 14, loss = 0.53738517\n",
            "Iteration 15, loss = 0.53610209\n",
            "Iteration 16, loss = 0.53548329\n",
            "Iteration 17, loss = 0.53479088\n",
            "Iteration 18, loss = 0.53420721\n",
            "Iteration 19, loss = 0.53365687\n",
            "Iteration 20, loss = 0.53360199\n",
            "Iteration 21, loss = 0.53262289\n",
            "Iteration 22, loss = 0.53272172\n",
            "Iteration 23, loss = 0.53160057\n",
            "Iteration 24, loss = 0.53020057\n",
            "Iteration 25, loss = 0.53032962\n",
            "Iteration 26, loss = 0.52946469\n",
            "Iteration 27, loss = 0.52945571\n",
            "Iteration 28, loss = 0.52875327\n",
            "Iteration 29, loss = 0.52877389\n",
            "Iteration 30, loss = 0.52682750\n",
            "Iteration 31, loss = 0.52605181\n",
            "Iteration 32, loss = 0.52571340\n",
            "Iteration 33, loss = 0.52546378\n",
            "Iteration 34, loss = 0.52562253\n",
            "Iteration 35, loss = 0.52363862\n",
            "Iteration 36, loss = 0.52212134\n",
            "Iteration 37, loss = 0.52251926\n",
            "Iteration 38, loss = 0.52218800\n",
            "Iteration 39, loss = 0.52165898\n",
            "Iteration 40, loss = 0.52026508\n",
            "Iteration 41, loss = 0.51934207\n",
            "Iteration 42, loss = 0.51863317\n",
            "Iteration 43, loss = 0.51868689\n",
            "Iteration 44, loss = 0.51721053\n",
            "Iteration 45, loss = 0.51841107\n",
            "Iteration 46, loss = 0.51880717\n",
            "Iteration 47, loss = 0.51778831\n",
            "Iteration 48, loss = 0.51753928\n",
            "Iteration 49, loss = 0.51562444\n",
            "Iteration 50, loss = 0.51508967\n",
            "Iteration 51, loss = 0.51559385\n",
            "Iteration 52, loss = 0.51571383\n",
            "Iteration 53, loss = 0.51504503\n",
            "Iteration 54, loss = 0.51481022\n",
            "Iteration 55, loss = 0.51387321\n",
            "Iteration 56, loss = 0.51488602\n",
            "Iteration 57, loss = 0.51372919\n",
            "Iteration 58, loss = 0.51402763\n",
            "Iteration 59, loss = 0.51448928\n",
            "Iteration 60, loss = 0.51337426\n",
            "Iteration 61, loss = 0.51427479\n",
            "Iteration 62, loss = 0.51474731\n",
            "Iteration 63, loss = 0.51357626\n",
            "Iteration 64, loss = 0.51252725\n",
            "Iteration 65, loss = 0.51231619\n",
            "Iteration 66, loss = 0.51255882\n",
            "Iteration 67, loss = 0.51204635\n",
            "Iteration 68, loss = 0.51242474\n",
            "Iteration 69, loss = 0.51219609\n",
            "Iteration 70, loss = 0.51366912\n",
            "Iteration 71, loss = 0.51423931\n",
            "Iteration 72, loss = 0.51245169\n",
            "Iteration 73, loss = 0.51357885\n",
            "Iteration 74, loss = 0.51272285\n",
            "Iteration 75, loss = 0.51279332\n",
            "Iteration 76, loss = 0.51178815\n",
            "Iteration 77, loss = 0.51023404\n",
            "Iteration 78, loss = 0.51053386\n",
            "Iteration 79, loss = 0.51100373\n",
            "Iteration 80, loss = 0.50993851\n",
            "Iteration 81, loss = 0.50974919\n",
            "Iteration 82, loss = 0.50959666\n",
            "Iteration 83, loss = 0.51106230\n",
            "Iteration 84, loss = 0.51063345\n",
            "Iteration 85, loss = 0.51076724\n",
            "Iteration 86, loss = 0.50930014\n",
            "Iteration 87, loss = 0.50845813\n",
            "Iteration 88, loss = 0.50816720\n",
            "Iteration 89, loss = 0.50865641\n",
            "Iteration 90, loss = 0.50926082\n",
            "Iteration 91, loss = 0.51053776\n",
            "Iteration 92, loss = 0.51101996\n",
            "Iteration 93, loss = 0.50967905\n",
            "Iteration 94, loss = 0.51493859\n",
            "Iteration 95, loss = 0.51080848\n",
            "Iteration 96, loss = 0.51116814\n",
            "Iteration 97, loss = 0.51019227\n",
            "Iteration 98, loss = 0.51022928\n",
            "Iteration 99, loss = 0.50991010\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.80972489\n",
            "Iteration 2, loss = 0.65904043\n",
            "Iteration 3, loss = 0.60553834\n",
            "Iteration 4, loss = 0.58737349\n",
            "Iteration 5, loss = 0.57959275\n",
            "Iteration 6, loss = 0.57310858\n",
            "Iteration 7, loss = 0.57025941\n",
            "Iteration 8, loss = 0.56762812\n",
            "Iteration 9, loss = 0.56369167\n",
            "Iteration 10, loss = 0.56030709\n",
            "Iteration 11, loss = 0.55715604\n",
            "Iteration 12, loss = 0.55424368\n",
            "Iteration 13, loss = 0.55086767\n",
            "Iteration 14, loss = 0.54968789\n",
            "Iteration 15, loss = 0.54804813\n",
            "Iteration 16, loss = 0.54716522\n",
            "Iteration 17, loss = 0.54589839\n",
            "Iteration 18, loss = 0.54464670\n",
            "Iteration 19, loss = 0.54270658\n",
            "Iteration 20, loss = 0.54195373\n",
            "Iteration 21, loss = 0.54050459\n",
            "Iteration 22, loss = 0.54002333\n",
            "Iteration 23, loss = 0.53829247\n",
            "Iteration 24, loss = 0.53714600\n",
            "Iteration 25, loss = 0.53752867\n",
            "Iteration 26, loss = 0.53664465\n",
            "Iteration 27, loss = 0.53524365\n",
            "Iteration 28, loss = 0.53398566\n",
            "Iteration 29, loss = 0.53388843\n",
            "Iteration 30, loss = 0.53308004\n",
            "Iteration 31, loss = 0.53183431\n",
            "Iteration 32, loss = 0.53173172\n",
            "Iteration 33, loss = 0.53088996\n",
            "Iteration 34, loss = 0.53055194\n",
            "Iteration 35, loss = 0.52922236\n",
            "Iteration 36, loss = 0.52768203\n",
            "Iteration 37, loss = 0.52816267\n",
            "Iteration 38, loss = 0.52732724\n",
            "Iteration 39, loss = 0.52567655\n",
            "Iteration 40, loss = 0.52483423\n",
            "Iteration 41, loss = 0.52393807\n",
            "Iteration 42, loss = 0.52282236\n",
            "Iteration 43, loss = 0.52273856\n",
            "Iteration 44, loss = 0.52322500\n",
            "Iteration 45, loss = 0.52272876\n",
            "Iteration 46, loss = 0.52240351\n",
            "Iteration 47, loss = 0.52226690\n",
            "Iteration 48, loss = 0.52143150\n",
            "Iteration 49, loss = 0.51896197\n",
            "Iteration 50, loss = 0.51832271\n",
            "Iteration 51, loss = 0.51789919\n",
            "Iteration 52, loss = 0.51599498\n",
            "Iteration 53, loss = 0.51552360\n",
            "Iteration 54, loss = 0.51513925\n",
            "Iteration 55, loss = 0.51481940\n",
            "Iteration 56, loss = 0.51382834\n",
            "Iteration 57, loss = 0.51223555\n",
            "Iteration 58, loss = 0.51167038\n",
            "Iteration 59, loss = 0.51239758\n",
            "Iteration 60, loss = 0.51254387\n",
            "Iteration 61, loss = 0.51088149\n",
            "Iteration 62, loss = 0.51097122\n",
            "Iteration 63, loss = 0.50975030\n",
            "Iteration 64, loss = 0.51038171\n",
            "Iteration 65, loss = 0.51104626\n",
            "Iteration 66, loss = 0.51060992\n",
            "Iteration 67, loss = 0.50930488\n",
            "Iteration 68, loss = 0.50885791\n",
            "Iteration 69, loss = 0.50957961\n",
            "Iteration 70, loss = 0.50996656\n",
            "Iteration 71, loss = 0.51323337\n",
            "Iteration 72, loss = 0.51190285\n",
            "Iteration 73, loss = 0.51171478\n",
            "Iteration 74, loss = 0.51071170\n",
            "Iteration 75, loss = 0.51021603\n",
            "Iteration 76, loss = 0.51167047\n",
            "Iteration 77, loss = 0.50976451\n",
            "Iteration 78, loss = 0.50861749\n",
            "Iteration 79, loss = 0.51106608\n",
            "Iteration 80, loss = 0.50849303\n",
            "Iteration 81, loss = 0.50817405\n",
            "Iteration 82, loss = 0.50984304\n",
            "Iteration 83, loss = 0.50968674\n",
            "Iteration 84, loss = 0.50810151\n",
            "Iteration 85, loss = 0.50825494\n",
            "Iteration 86, loss = 0.50784555\n",
            "Iteration 87, loss = 0.50798596\n",
            "Iteration 88, loss = 0.50880394\n",
            "Iteration 89, loss = 0.50803305\n",
            "Iteration 90, loss = 0.50877266\n",
            "Iteration 91, loss = 0.50864130\n",
            "Iteration 92, loss = 0.50836387\n",
            "Iteration 93, loss = 0.50758278\n",
            "Iteration 94, loss = 0.50909866\n",
            "Iteration 95, loss = 0.50893600\n",
            "Iteration 96, loss = 0.51058805\n",
            "Iteration 97, loss = 0.50825085\n",
            "Iteration 98, loss = 0.50973095\n",
            "Iteration 99, loss = 0.50818597\n",
            "Iteration 100, loss = 0.50770833\n",
            "Iteration 101, loss = 0.50783601\n",
            "Iteration 102, loss = 0.50693994\n",
            "Iteration 103, loss = 0.50820330\n",
            "Iteration 104, loss = 0.50955333\n",
            "Iteration 105, loss = 0.50794062\n",
            "Iteration 106, loss = 0.50728308\n",
            "Iteration 107, loss = 0.50720195\n",
            "Iteration 108, loss = 0.50774986\n",
            "Iteration 109, loss = 0.50752158\n",
            "Iteration 110, loss = 0.50663107\n",
            "Iteration 111, loss = 0.50605649\n",
            "Iteration 112, loss = 0.50666184\n",
            "Iteration 113, loss = 0.50683693\n",
            "Iteration 114, loss = 0.50618463\n",
            "Iteration 115, loss = 0.50648116\n",
            "Iteration 116, loss = 0.50631940\n",
            "Iteration 117, loss = 0.50758764\n",
            "Iteration 118, loss = 0.50727017\n",
            "Iteration 119, loss = 0.50610174\n",
            "Iteration 120, loss = 0.50615163\n",
            "Iteration 121, loss = 0.50730000\n",
            "Iteration 122, loss = 0.50583915\n",
            "Iteration 123, loss = 0.50618300\n",
            "Iteration 124, loss = 0.50521044\n",
            "Iteration 125, loss = 0.50601022\n",
            "Iteration 126, loss = 0.50563019\n",
            "Iteration 127, loss = 0.50482379\n",
            "Iteration 128, loss = 0.50540935\n",
            "Iteration 129, loss = 0.50458830\n",
            "Iteration 130, loss = 0.50497663\n",
            "Iteration 131, loss = 0.50588508\n",
            "Iteration 132, loss = 0.50589222\n",
            "Iteration 133, loss = 0.50604027\n",
            "Iteration 134, loss = 0.50603218\n",
            "Iteration 135, loss = 0.50531004\n",
            "Iteration 136, loss = 0.50503691\n",
            "Iteration 137, loss = 0.50465889\n",
            "Iteration 138, loss = 0.50606272\n",
            "Iteration 139, loss = 0.50520856\n",
            "Iteration 140, loss = 0.50477656\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.80475080\n",
            "Iteration 2, loss = 0.65487955\n",
            "Iteration 3, loss = 0.60390614\n",
            "Iteration 4, loss = 0.58819751\n",
            "Iteration 5, loss = 0.57865468\n",
            "Iteration 6, loss = 0.57206376\n",
            "Iteration 7, loss = 0.57135814\n",
            "Iteration 8, loss = 0.56979090\n",
            "Iteration 9, loss = 0.56702828\n",
            "Iteration 10, loss = 0.56515424\n",
            "Iteration 11, loss = 0.56408998\n",
            "Iteration 12, loss = 0.56223931\n",
            "Iteration 13, loss = 0.55712257\n",
            "Iteration 14, loss = 0.55543343\n",
            "Iteration 15, loss = 0.55460630\n",
            "Iteration 16, loss = 0.55343467\n",
            "Iteration 17, loss = 0.55204421\n",
            "Iteration 18, loss = 0.55203563\n",
            "Iteration 19, loss = 0.55038470\n",
            "Iteration 20, loss = 0.54916114\n",
            "Iteration 21, loss = 0.54790691\n",
            "Iteration 22, loss = 0.54823634\n",
            "Iteration 23, loss = 0.54770996\n",
            "Iteration 24, loss = 0.54647616\n",
            "Iteration 25, loss = 0.54569847\n",
            "Iteration 26, loss = 0.54635954\n",
            "Iteration 27, loss = 0.54486154\n",
            "Iteration 28, loss = 0.54269137\n",
            "Iteration 29, loss = 0.54393359\n",
            "Iteration 30, loss = 0.54525630\n",
            "Iteration 31, loss = 0.54050479\n",
            "Iteration 32, loss = 0.54251410\n",
            "Iteration 33, loss = 0.54337415\n",
            "Iteration 34, loss = 0.54084148\n",
            "Iteration 35, loss = 0.53936802\n",
            "Iteration 36, loss = 0.53834473\n",
            "Iteration 37, loss = 0.53647467\n",
            "Iteration 38, loss = 0.53587492\n",
            "Iteration 39, loss = 0.53480966\n",
            "Iteration 40, loss = 0.53401460\n",
            "Iteration 41, loss = 0.53291611\n",
            "Iteration 42, loss = 0.53377357\n",
            "Iteration 43, loss = 0.53302631\n",
            "Iteration 44, loss = 0.53221832\n",
            "Iteration 45, loss = 0.53264785\n",
            "Iteration 46, loss = 0.52988870\n",
            "Iteration 47, loss = 0.52953298\n",
            "Iteration 48, loss = 0.52807396\n",
            "Iteration 49, loss = 0.52674531\n",
            "Iteration 50, loss = 0.52573679\n",
            "Iteration 51, loss = 0.52553847\n",
            "Iteration 52, loss = 0.52517267\n",
            "Iteration 53, loss = 0.52597777\n",
            "Iteration 54, loss = 0.52580652\n",
            "Iteration 55, loss = 0.52542780\n",
            "Iteration 56, loss = 0.52500573\n",
            "Iteration 57, loss = 0.52407180\n",
            "Iteration 58, loss = 0.52276023\n",
            "Iteration 59, loss = 0.52280461\n",
            "Iteration 60, loss = 0.52352107\n",
            "Iteration 61, loss = 0.52285380\n",
            "Iteration 62, loss = 0.52188201\n",
            "Iteration 63, loss = 0.52059995\n",
            "Iteration 64, loss = 0.52139367\n",
            "Iteration 65, loss = 0.52289242\n",
            "Iteration 66, loss = 0.52192389\n",
            "Iteration 67, loss = 0.52091656\n",
            "Iteration 68, loss = 0.52019784\n",
            "Iteration 69, loss = 0.51985503\n",
            "Iteration 70, loss = 0.51990398\n",
            "Iteration 71, loss = 0.52013587\n",
            "Iteration 72, loss = 0.52161612\n",
            "Iteration 73, loss = 0.52083912\n",
            "Iteration 74, loss = 0.51885686\n",
            "Iteration 75, loss = 0.51883100\n",
            "Iteration 76, loss = 0.52128410\n",
            "Iteration 77, loss = 0.52155454\n",
            "Iteration 78, loss = 0.51890984\n",
            "Iteration 79, loss = 0.52133313\n",
            "Iteration 80, loss = 0.51892092\n",
            "Iteration 81, loss = 0.51817384\n",
            "Iteration 82, loss = 0.52172643\n",
            "Iteration 83, loss = 0.52130464\n",
            "Iteration 84, loss = 0.52020023\n",
            "Iteration 85, loss = 0.51936990\n",
            "Iteration 86, loss = 0.51910924\n",
            "Iteration 87, loss = 0.51918844\n",
            "Iteration 88, loss = 0.51849875\n",
            "Iteration 89, loss = 0.51964345\n",
            "Iteration 90, loss = 0.51967115\n",
            "Iteration 91, loss = 0.51912139\n",
            "Iteration 92, loss = 0.51886701\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 250 and for layer number 3 : 0.6950000000000001\n",
            "Iteration 1, loss = 0.89843010\n",
            "Iteration 2, loss = 0.76513132\n",
            "Iteration 3, loss = 0.65384082\n",
            "Iteration 4, loss = 0.60270237\n",
            "Iteration 5, loss = 0.58704637\n",
            "Iteration 6, loss = 0.57794274\n",
            "Iteration 7, loss = 0.57124497\n",
            "Iteration 8, loss = 0.56673704\n",
            "Iteration 9, loss = 0.56287505\n",
            "Iteration 10, loss = 0.55892982\n",
            "Iteration 11, loss = 0.55698535\n",
            "Iteration 12, loss = 0.55516473\n",
            "Iteration 13, loss = 0.55305959\n",
            "Iteration 14, loss = 0.55269439\n",
            "Iteration 15, loss = 0.55268796\n",
            "Iteration 16, loss = 0.55141608\n",
            "Iteration 17, loss = 0.55198928\n",
            "Iteration 18, loss = 0.55193711\n",
            "Iteration 19, loss = 0.55199247\n",
            "Iteration 20, loss = 0.55087278\n",
            "Iteration 21, loss = 0.54920550\n",
            "Iteration 22, loss = 0.54957461\n",
            "Iteration 23, loss = 0.55002924\n",
            "Iteration 24, loss = 0.54902125\n",
            "Iteration 25, loss = 0.54833007\n",
            "Iteration 26, loss = 0.54641390\n",
            "Iteration 27, loss = 0.54527628\n",
            "Iteration 28, loss = 0.54547347\n",
            "Iteration 29, loss = 0.54497476\n",
            "Iteration 30, loss = 0.54315303\n",
            "Iteration 31, loss = 0.54269913\n",
            "Iteration 32, loss = 0.54345058\n",
            "Iteration 33, loss = 0.54133958\n",
            "Iteration 34, loss = 0.54006651\n",
            "Iteration 35, loss = 0.54181417\n",
            "Iteration 36, loss = 0.54064432\n",
            "Iteration 37, loss = 0.54176667\n",
            "Iteration 38, loss = 0.53892320\n",
            "Iteration 39, loss = 0.53749626\n",
            "Iteration 40, loss = 0.53643073\n",
            "Iteration 41, loss = 0.53749149\n",
            "Iteration 42, loss = 0.53688622\n",
            "Iteration 43, loss = 0.53409458\n",
            "Iteration 44, loss = 0.53520926\n",
            "Iteration 45, loss = 0.53517666\n",
            "Iteration 46, loss = 0.53266025\n",
            "Iteration 47, loss = 0.53317645\n",
            "Iteration 48, loss = 0.53170110\n",
            "Iteration 49, loss = 0.53091427\n",
            "Iteration 50, loss = 0.52962578\n",
            "Iteration 51, loss = 0.52946461\n",
            "Iteration 52, loss = 0.52939386\n",
            "Iteration 53, loss = 0.52910654\n",
            "Iteration 54, loss = 0.52878266\n",
            "Iteration 55, loss = 0.52787557\n",
            "Iteration 56, loss = 0.52630304\n",
            "Iteration 57, loss = 0.52688899\n",
            "Iteration 58, loss = 0.52679798\n",
            "Iteration 59, loss = 0.52907269\n",
            "Iteration 60, loss = 0.52540211\n",
            "Iteration 61, loss = 0.52761757\n",
            "Iteration 62, loss = 0.52778592\n",
            "Iteration 63, loss = 0.52798004\n",
            "Iteration 64, loss = 0.52820011\n",
            "Iteration 65, loss = 0.52612079\n",
            "Iteration 66, loss = 0.52629661\n",
            "Iteration 67, loss = 0.52709228\n",
            "Iteration 68, loss = 0.52815700\n",
            "Iteration 69, loss = 0.52708967\n",
            "Iteration 70, loss = 0.52691176\n",
            "Iteration 71, loss = 0.52816892\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.89647974\n",
            "Iteration 2, loss = 0.76503390\n",
            "Iteration 3, loss = 0.65738862\n",
            "Iteration 4, loss = 0.60451657\n",
            "Iteration 5, loss = 0.58466759\n",
            "Iteration 6, loss = 0.57504316\n",
            "Iteration 7, loss = 0.56767168\n",
            "Iteration 8, loss = 0.56133735\n",
            "Iteration 9, loss = 0.55764613\n",
            "Iteration 10, loss = 0.55376055\n",
            "Iteration 11, loss = 0.55274203\n",
            "Iteration 12, loss = 0.55149150\n",
            "Iteration 13, loss = 0.54874116\n",
            "Iteration 14, loss = 0.54757801\n",
            "Iteration 15, loss = 0.54687336\n",
            "Iteration 16, loss = 0.54592392\n",
            "Iteration 17, loss = 0.54463412\n",
            "Iteration 18, loss = 0.54412666\n",
            "Iteration 19, loss = 0.54401242\n",
            "Iteration 20, loss = 0.54346391\n",
            "Iteration 21, loss = 0.54321807\n",
            "Iteration 22, loss = 0.54308250\n",
            "Iteration 23, loss = 0.54393627\n",
            "Iteration 24, loss = 0.54504573\n",
            "Iteration 25, loss = 0.54398395\n",
            "Iteration 26, loss = 0.54195910\n",
            "Iteration 27, loss = 0.54102551\n",
            "Iteration 28, loss = 0.54000856\n",
            "Iteration 29, loss = 0.53864889\n",
            "Iteration 30, loss = 0.53780552\n",
            "Iteration 31, loss = 0.53879175\n",
            "Iteration 32, loss = 0.53899122\n",
            "Iteration 33, loss = 0.53803797\n",
            "Iteration 34, loss = 0.53777915\n",
            "Iteration 35, loss = 0.53787486\n",
            "Iteration 36, loss = 0.53457417\n",
            "Iteration 37, loss = 0.53692346\n",
            "Iteration 38, loss = 0.53428080\n",
            "Iteration 39, loss = 0.53379512\n",
            "Iteration 40, loss = 0.53249173\n",
            "Iteration 41, loss = 0.53436636\n",
            "Iteration 42, loss = 0.53136372\n",
            "Iteration 43, loss = 0.52905241\n",
            "Iteration 44, loss = 0.52976079\n",
            "Iteration 45, loss = 0.52792724\n",
            "Iteration 46, loss = 0.52571268\n",
            "Iteration 47, loss = 0.52518925\n",
            "Iteration 48, loss = 0.52391380\n",
            "Iteration 49, loss = 0.52326992\n",
            "Iteration 50, loss = 0.52120426\n",
            "Iteration 51, loss = 0.52140533\n",
            "Iteration 52, loss = 0.52240227\n",
            "Iteration 53, loss = 0.52119785\n",
            "Iteration 54, loss = 0.52163119\n",
            "Iteration 55, loss = 0.52027640\n",
            "Iteration 56, loss = 0.52069217\n",
            "Iteration 57, loss = 0.52106924\n",
            "Iteration 58, loss = 0.51928000\n",
            "Iteration 59, loss = 0.51916456\n",
            "Iteration 60, loss = 0.52327257\n",
            "Iteration 61, loss = 0.51895711\n",
            "Iteration 62, loss = 0.51532216\n",
            "Iteration 63, loss = 0.51514669\n",
            "Iteration 64, loss = 0.51379276\n",
            "Iteration 65, loss = 0.51532715\n",
            "Iteration 66, loss = 0.51897733\n",
            "Iteration 67, loss = 0.51749566\n",
            "Iteration 68, loss = 0.51430419\n",
            "Iteration 69, loss = 0.51448492\n",
            "Iteration 70, loss = 0.51420734\n",
            "Iteration 71, loss = 0.51086393\n",
            "Iteration 72, loss = 0.50569609\n",
            "Iteration 73, loss = 0.50630985\n",
            "Iteration 74, loss = 0.50555591\n",
            "Iteration 75, loss = 0.50792682\n",
            "Iteration 76, loss = 0.50417952\n",
            "Iteration 77, loss = 0.50254183\n",
            "Iteration 78, loss = 0.50139324\n",
            "Iteration 79, loss = 0.50452224\n",
            "Iteration 80, loss = 0.50252965\n",
            "Iteration 81, loss = 0.50458088\n",
            "Iteration 82, loss = 0.49910814\n",
            "Iteration 83, loss = 0.50903244\n",
            "Iteration 84, loss = 0.49569914\n",
            "Iteration 85, loss = 0.50412909\n",
            "Iteration 86, loss = 0.49595235\n",
            "Iteration 87, loss = 0.49687348\n",
            "Iteration 88, loss = 0.49134680\n",
            "Iteration 89, loss = 0.49834615\n",
            "Iteration 90, loss = 0.48980261\n",
            "Iteration 91, loss = 0.49338704\n",
            "Iteration 92, loss = 0.48902344\n",
            "Iteration 93, loss = 0.49145853\n",
            "Iteration 94, loss = 0.48747800\n",
            "Iteration 95, loss = 0.48887107\n",
            "Iteration 96, loss = 0.49037425\n",
            "Iteration 97, loss = 0.49859268\n",
            "Iteration 98, loss = 0.48675766\n",
            "Iteration 99, loss = 0.49052318\n",
            "Iteration 100, loss = 0.48894740\n",
            "Iteration 101, loss = 0.48919468\n",
            "Iteration 102, loss = 0.49201846\n",
            "Iteration 103, loss = 0.48345248\n",
            "Iteration 104, loss = 0.49059289\n",
            "Iteration 105, loss = 0.49260947\n",
            "Iteration 106, loss = 0.49523811\n",
            "Iteration 107, loss = 0.49297273\n",
            "Iteration 108, loss = 0.49140348\n",
            "Iteration 109, loss = 0.48191850\n",
            "Iteration 110, loss = 0.48719341\n",
            "Iteration 111, loss = 0.48443655\n",
            "Iteration 112, loss = 0.48192928\n",
            "Iteration 113, loss = 0.48586406\n",
            "Iteration 114, loss = 0.48068653\n",
            "Iteration 115, loss = 0.47871787\n",
            "Iteration 116, loss = 0.48071761\n",
            "Iteration 117, loss = 0.48186814\n",
            "Iteration 118, loss = 0.48641580\n",
            "Iteration 119, loss = 0.48315740\n",
            "Iteration 120, loss = 0.48302756\n",
            "Iteration 121, loss = 0.48043503\n",
            "Iteration 122, loss = 0.48211741\n",
            "Iteration 123, loss = 0.48912972\n",
            "Iteration 124, loss = 0.48290244\n",
            "Iteration 125, loss = 0.48100373\n",
            "Iteration 126, loss = 0.47845257\n",
            "Iteration 127, loss = 0.48121488\n",
            "Iteration 128, loss = 0.48829914\n",
            "Iteration 129, loss = 0.48272910\n",
            "Iteration 130, loss = 0.48383524\n",
            "Iteration 131, loss = 0.48492663\n",
            "Iteration 132, loss = 0.48394394\n",
            "Iteration 133, loss = 0.48218977\n",
            "Iteration 134, loss = 0.48130342\n",
            "Iteration 135, loss = 0.47719077\n",
            "Iteration 136, loss = 0.48500437\n",
            "Iteration 137, loss = 0.47948613\n",
            "Iteration 138, loss = 0.47840764\n",
            "Iteration 139, loss = 0.48790227\n",
            "Iteration 140, loss = 0.48435334\n",
            "Iteration 141, loss = 0.48877738\n",
            "Iteration 142, loss = 0.48881379\n",
            "Iteration 143, loss = 0.48103120\n",
            "Iteration 144, loss = 0.48172600\n",
            "Iteration 145, loss = 0.48632543\n",
            "Iteration 146, loss = 0.48128421\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.90447953\n",
            "Iteration 2, loss = 0.76691485\n",
            "Iteration 3, loss = 0.65636497\n",
            "Iteration 4, loss = 0.59577920\n",
            "Iteration 5, loss = 0.57707171\n",
            "Iteration 6, loss = 0.56749853\n",
            "Iteration 7, loss = 0.56265587\n",
            "Iteration 8, loss = 0.55936371\n",
            "Iteration 9, loss = 0.55478568\n",
            "Iteration 10, loss = 0.55004024\n",
            "Iteration 11, loss = 0.54769876\n",
            "Iteration 12, loss = 0.54630747\n",
            "Iteration 13, loss = 0.54357485\n",
            "Iteration 14, loss = 0.54209368\n",
            "Iteration 15, loss = 0.54201538\n",
            "Iteration 16, loss = 0.54178238\n",
            "Iteration 17, loss = 0.54056641\n",
            "Iteration 18, loss = 0.53921933\n",
            "Iteration 19, loss = 0.53823331\n",
            "Iteration 20, loss = 0.53717933\n",
            "Iteration 21, loss = 0.53771539\n",
            "Iteration 22, loss = 0.53632534\n",
            "Iteration 23, loss = 0.53647887\n",
            "Iteration 24, loss = 0.53782796\n",
            "Iteration 25, loss = 0.53758771\n",
            "Iteration 26, loss = 0.53530753\n",
            "Iteration 27, loss = 0.53393838\n",
            "Iteration 28, loss = 0.53190523\n",
            "Iteration 29, loss = 0.53044664\n",
            "Iteration 30, loss = 0.52968979\n",
            "Iteration 31, loss = 0.52941003\n",
            "Iteration 32, loss = 0.52925026\n",
            "Iteration 33, loss = 0.52945278\n",
            "Iteration 34, loss = 0.52754751\n",
            "Iteration 35, loss = 0.52513913\n",
            "Iteration 36, loss = 0.52465087\n",
            "Iteration 37, loss = 0.52490959\n",
            "Iteration 38, loss = 0.52384790\n",
            "Iteration 39, loss = 0.52392522\n",
            "Iteration 40, loss = 0.52349030\n",
            "Iteration 41, loss = 0.52296316\n",
            "Iteration 42, loss = 0.52142275\n",
            "Iteration 43, loss = 0.52025813\n",
            "Iteration 44, loss = 0.52140690\n",
            "Iteration 45, loss = 0.51916030\n",
            "Iteration 46, loss = 0.51798468\n",
            "Iteration 47, loss = 0.51756444\n",
            "Iteration 48, loss = 0.51911537\n",
            "Iteration 49, loss = 0.51761755\n",
            "Iteration 50, loss = 0.51629200\n",
            "Iteration 51, loss = 0.51601996\n",
            "Iteration 52, loss = 0.51569139\n",
            "Iteration 53, loss = 0.51522243\n",
            "Iteration 54, loss = 0.51449337\n",
            "Iteration 55, loss = 0.51487080\n",
            "Iteration 56, loss = 0.51443166\n",
            "Iteration 57, loss = 0.51406125\n",
            "Iteration 58, loss = 0.51403510\n",
            "Iteration 59, loss = 0.51353227\n",
            "Iteration 60, loss = 0.51573735\n",
            "Iteration 61, loss = 0.51559794\n",
            "Iteration 62, loss = 0.51492730\n",
            "Iteration 63, loss = 0.51295094\n",
            "Iteration 64, loss = 0.51173244\n",
            "Iteration 65, loss = 0.51256462\n",
            "Iteration 66, loss = 0.51539152\n",
            "Iteration 67, loss = 0.51606858\n",
            "Iteration 68, loss = 0.51419822\n",
            "Iteration 69, loss = 0.51133250\n",
            "Iteration 70, loss = 0.51035396\n",
            "Iteration 71, loss = 0.51073227\n",
            "Iteration 72, loss = 0.51138410\n",
            "Iteration 73, loss = 0.50991133\n",
            "Iteration 74, loss = 0.50943337\n",
            "Iteration 75, loss = 0.50905278\n",
            "Iteration 76, loss = 0.50930612\n",
            "Iteration 77, loss = 0.50761673\n",
            "Iteration 78, loss = 0.50834262\n",
            "Iteration 79, loss = 0.50950490\n",
            "Iteration 80, loss = 0.50716846\n",
            "Iteration 81, loss = 0.50880714\n",
            "Iteration 82, loss = 0.50480841\n",
            "Iteration 83, loss = 0.50816916\n",
            "Iteration 84, loss = 0.50715524\n",
            "Iteration 85, loss = 0.51112409\n",
            "Iteration 86, loss = 0.50938590\n",
            "Iteration 87, loss = 0.50451063\n",
            "Iteration 88, loss = 0.50303698\n",
            "Iteration 89, loss = 0.50204349\n",
            "Iteration 90, loss = 0.50549412\n",
            "Iteration 91, loss = 0.50266367\n",
            "Iteration 92, loss = 0.50142561\n",
            "Iteration 93, loss = 0.50091745\n",
            "Iteration 94, loss = 0.50182045\n",
            "Iteration 95, loss = 0.50059640\n",
            "Iteration 96, loss = 0.50122117\n",
            "Iteration 97, loss = 0.50394290\n",
            "Iteration 98, loss = 0.50454859\n",
            "Iteration 99, loss = 0.50382952\n",
            "Iteration 100, loss = 0.50259390\n",
            "Iteration 101, loss = 0.50723207\n",
            "Iteration 102, loss = 0.50419079\n",
            "Iteration 103, loss = 0.50751390\n",
            "Iteration 104, loss = 0.50523052\n",
            "Iteration 105, loss = 0.50583591\n",
            "Iteration 106, loss = 0.50369211\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.90198656\n",
            "Iteration 2, loss = 0.76291623\n",
            "Iteration 3, loss = 0.65742388\n",
            "Iteration 4, loss = 0.60221160\n",
            "Iteration 5, loss = 0.58317327\n",
            "Iteration 6, loss = 0.57433136\n",
            "Iteration 7, loss = 0.56872138\n",
            "Iteration 8, loss = 0.56549704\n",
            "Iteration 9, loss = 0.56136635\n",
            "Iteration 10, loss = 0.55708719\n",
            "Iteration 11, loss = 0.55448963\n",
            "Iteration 12, loss = 0.55290078\n",
            "Iteration 13, loss = 0.55099528\n",
            "Iteration 14, loss = 0.54931756\n",
            "Iteration 15, loss = 0.54756538\n",
            "Iteration 16, loss = 0.54790434\n",
            "Iteration 17, loss = 0.54674437\n",
            "Iteration 18, loss = 0.54610576\n",
            "Iteration 19, loss = 0.54541517\n",
            "Iteration 20, loss = 0.54421269\n",
            "Iteration 21, loss = 0.54342487\n",
            "Iteration 22, loss = 0.54277383\n",
            "Iteration 23, loss = 0.54073149\n",
            "Iteration 24, loss = 0.53939535\n",
            "Iteration 25, loss = 0.53830286\n",
            "Iteration 26, loss = 0.53745869\n",
            "Iteration 27, loss = 0.53549679\n",
            "Iteration 28, loss = 0.53399954\n",
            "Iteration 29, loss = 0.53321457\n",
            "Iteration 30, loss = 0.53192810\n",
            "Iteration 31, loss = 0.53203935\n",
            "Iteration 32, loss = 0.53023525\n",
            "Iteration 33, loss = 0.53114364\n",
            "Iteration 34, loss = 0.52945194\n",
            "Iteration 35, loss = 0.52907989\n",
            "Iteration 36, loss = 0.52834601\n",
            "Iteration 37, loss = 0.52729509\n",
            "Iteration 38, loss = 0.52699016\n",
            "Iteration 39, loss = 0.52728567\n",
            "Iteration 40, loss = 0.52614239\n",
            "Iteration 41, loss = 0.52530819\n",
            "Iteration 42, loss = 0.52660269\n",
            "Iteration 43, loss = 0.52394303\n",
            "Iteration 44, loss = 0.52572657\n",
            "Iteration 45, loss = 0.52671551\n",
            "Iteration 46, loss = 0.52209890\n",
            "Iteration 47, loss = 0.52253360\n",
            "Iteration 48, loss = 0.52161146\n",
            "Iteration 49, loss = 0.51878445\n",
            "Iteration 50, loss = 0.51725599\n",
            "Iteration 51, loss = 0.51697770\n",
            "Iteration 52, loss = 0.51832703\n",
            "Iteration 53, loss = 0.51684925\n",
            "Iteration 54, loss = 0.51487529\n",
            "Iteration 55, loss = 0.51478777\n",
            "Iteration 56, loss = 0.51717655\n",
            "Iteration 57, loss = 0.51394420\n",
            "Iteration 58, loss = 0.51565967\n",
            "Iteration 59, loss = 0.51330813\n",
            "Iteration 60, loss = 0.51544560\n",
            "Iteration 61, loss = 0.51703005\n",
            "Iteration 62, loss = 0.51640741\n",
            "Iteration 63, loss = 0.51268030\n",
            "Iteration 64, loss = 0.51199019\n",
            "Iteration 65, loss = 0.51365689\n",
            "Iteration 66, loss = 0.51556396\n",
            "Iteration 67, loss = 0.51719521\n",
            "Iteration 68, loss = 0.51272104\n",
            "Iteration 69, loss = 0.50991236\n",
            "Iteration 70, loss = 0.51058489\n",
            "Iteration 71, loss = 0.50994532\n",
            "Iteration 72, loss = 0.51339412\n",
            "Iteration 73, loss = 0.51098616\n",
            "Iteration 74, loss = 0.51041455\n",
            "Iteration 75, loss = 0.51109968\n",
            "Iteration 76, loss = 0.51151649\n",
            "Iteration 77, loss = 0.51007244\n",
            "Iteration 78, loss = 0.50933724\n",
            "Iteration 79, loss = 0.50888062\n",
            "Iteration 80, loss = 0.50800370\n",
            "Iteration 81, loss = 0.50708513\n",
            "Iteration 82, loss = 0.50945677\n",
            "Iteration 83, loss = 0.50837970\n",
            "Iteration 84, loss = 0.50710538\n",
            "Iteration 85, loss = 0.50540758\n",
            "Iteration 86, loss = 0.50423081\n",
            "Iteration 87, loss = 0.50390843\n",
            "Iteration 88, loss = 0.50198227\n",
            "Iteration 89, loss = 0.50078540\n",
            "Iteration 90, loss = 0.49806554\n",
            "Iteration 91, loss = 0.49996821\n",
            "Iteration 92, loss = 0.49663444\n",
            "Iteration 93, loss = 0.49797597\n",
            "Iteration 94, loss = 0.49714516\n",
            "Iteration 95, loss = 0.49746944\n",
            "Iteration 96, loss = 0.49608519\n",
            "Iteration 97, loss = 0.49377143\n",
            "Iteration 98, loss = 0.49313557\n",
            "Iteration 99, loss = 0.49273804\n",
            "Iteration 100, loss = 0.49246199\n",
            "Iteration 101, loss = 0.49582359\n",
            "Iteration 102, loss = 0.49465084\n",
            "Iteration 103, loss = 0.49043097\n",
            "Iteration 104, loss = 0.49075977\n",
            "Iteration 105, loss = 0.48804810\n",
            "Iteration 106, loss = 0.49138036\n",
            "Iteration 107, loss = 0.48862655\n",
            "Iteration 108, loss = 0.48664654\n",
            "Iteration 109, loss = 0.48721615\n",
            "Iteration 110, loss = 0.48491582\n",
            "Iteration 111, loss = 0.48582198\n",
            "Iteration 112, loss = 0.48424068\n",
            "Iteration 113, loss = 0.48887209\n",
            "Iteration 114, loss = 0.48474359\n",
            "Iteration 115, loss = 0.48135174\n",
            "Iteration 116, loss = 0.47917012\n",
            "Iteration 117, loss = 0.47815764\n",
            "Iteration 118, loss = 0.47774196\n",
            "Iteration 119, loss = 0.47968992\n",
            "Iteration 120, loss = 0.48299151\n",
            "Iteration 121, loss = 0.47731419\n",
            "Iteration 122, loss = 0.48110555\n",
            "Iteration 123, loss = 0.47994359\n",
            "Iteration 124, loss = 0.47881270\n",
            "Iteration 125, loss = 0.47609209\n",
            "Iteration 126, loss = 0.47652508\n",
            "Iteration 127, loss = 0.47701695\n",
            "Iteration 128, loss = 0.47673891\n",
            "Iteration 129, loss = 0.47503491\n",
            "Iteration 130, loss = 0.47374311\n",
            "Iteration 131, loss = 0.47586992\n",
            "Iteration 132, loss = 0.47706351\n",
            "Iteration 133, loss = 0.48030158\n",
            "Iteration 134, loss = 0.47672322\n",
            "Iteration 135, loss = 0.48001233\n",
            "Iteration 136, loss = 0.47768191\n",
            "Iteration 137, loss = 0.47533839\n",
            "Iteration 138, loss = 0.47186347\n",
            "Iteration 139, loss = 0.47333103\n",
            "Iteration 140, loss = 0.47154149\n",
            "Iteration 141, loss = 0.47557482\n",
            "Iteration 142, loss = 0.47545775\n",
            "Iteration 143, loss = 0.47501401\n",
            "Iteration 144, loss = 0.47262526\n",
            "Iteration 145, loss = 0.47461588\n",
            "Iteration 146, loss = 0.47290281\n",
            "Iteration 147, loss = 0.47154879\n",
            "Iteration 148, loss = 0.46919477\n",
            "Iteration 149, loss = 0.47140531\n",
            "Iteration 150, loss = 0.47070103\n",
            "Iteration 151, loss = 0.47659178\n",
            "Iteration 152, loss = 0.47466762\n",
            "Iteration 153, loss = 0.46960639\n",
            "Iteration 154, loss = 0.46899566\n",
            "Iteration 155, loss = 0.47188915\n",
            "Iteration 156, loss = 0.47411691\n",
            "Iteration 157, loss = 0.47241134\n",
            "Iteration 158, loss = 0.47012577\n",
            "Iteration 159, loss = 0.47744351\n",
            "Iteration 160, loss = 0.47177071\n",
            "Iteration 161, loss = 0.46970732\n",
            "Iteration 162, loss = 0.47211926\n",
            "Iteration 163, loss = 0.46944076\n",
            "Iteration 164, loss = 0.46968047\n",
            "Iteration 165, loss = 0.46861415\n",
            "Iteration 166, loss = 0.46708734\n",
            "Iteration 167, loss = 0.46877470\n",
            "Iteration 168, loss = 0.46641345\n",
            "Iteration 169, loss = 0.46956026\n",
            "Iteration 170, loss = 0.46805191\n",
            "Iteration 171, loss = 0.47380264\n",
            "Iteration 172, loss = 0.47268876\n",
            "Iteration 173, loss = 0.46740413\n",
            "Iteration 174, loss = 0.46753689\n",
            "Iteration 175, loss = 0.46915021\n",
            "Iteration 176, loss = 0.46724044\n",
            "Iteration 177, loss = 0.46547354\n",
            "Iteration 178, loss = 0.46714400\n",
            "Iteration 179, loss = 0.47143759\n",
            "Iteration 180, loss = 0.47498056\n",
            "Iteration 181, loss = 0.46649279\n",
            "Iteration 182, loss = 0.46682178\n",
            "Iteration 183, loss = 0.46271192\n",
            "Iteration 184, loss = 0.46462280\n",
            "Iteration 185, loss = 0.46493534\n",
            "Iteration 186, loss = 0.46349947\n",
            "Iteration 187, loss = 0.46532047\n",
            "Iteration 188, loss = 0.46524817\n",
            "Iteration 189, loss = 0.46377423\n",
            "Iteration 190, loss = 0.46576224\n",
            "Iteration 191, loss = 0.46457976\n",
            "Iteration 192, loss = 0.46370629\n",
            "Iteration 193, loss = 0.46327290\n",
            "Iteration 194, loss = 0.46458187\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.90952303\n",
            "Iteration 2, loss = 0.77155333\n",
            "Iteration 3, loss = 0.67094423\n",
            "Iteration 4, loss = 0.62256267\n",
            "Iteration 5, loss = 0.60199531\n",
            "Iteration 6, loss = 0.59268166\n",
            "Iteration 7, loss = 0.58716016\n",
            "Iteration 8, loss = 0.58316253\n",
            "Iteration 9, loss = 0.58015802\n",
            "Iteration 10, loss = 0.57589625\n",
            "Iteration 11, loss = 0.57238103\n",
            "Iteration 12, loss = 0.56950780\n",
            "Iteration 13, loss = 0.56551747\n",
            "Iteration 14, loss = 0.56266632\n",
            "Iteration 15, loss = 0.56045276\n",
            "Iteration 16, loss = 0.55996391\n",
            "Iteration 17, loss = 0.56033396\n",
            "Iteration 18, loss = 0.55859220\n",
            "Iteration 19, loss = 0.55699684\n",
            "Iteration 20, loss = 0.55853159\n",
            "Iteration 21, loss = 0.55800404\n",
            "Iteration 22, loss = 0.55612568\n",
            "Iteration 23, loss = 0.55443691\n",
            "Iteration 24, loss = 0.55519688\n",
            "Iteration 25, loss = 0.55540400\n",
            "Iteration 26, loss = 0.55363007\n",
            "Iteration 27, loss = 0.55216340\n",
            "Iteration 28, loss = 0.55081333\n",
            "Iteration 29, loss = 0.55203330\n",
            "Iteration 30, loss = 0.55126833\n",
            "Iteration 31, loss = 0.55193759\n",
            "Iteration 32, loss = 0.54991332\n",
            "Iteration 33, loss = 0.54961661\n",
            "Iteration 34, loss = 0.54899348\n",
            "Iteration 35, loss = 0.54876516\n",
            "Iteration 36, loss = 0.54808754\n",
            "Iteration 37, loss = 0.54874480\n",
            "Iteration 38, loss = 0.54786161\n",
            "Iteration 39, loss = 0.54812496\n",
            "Iteration 40, loss = 0.54753323\n",
            "Iteration 41, loss = 0.54695736\n",
            "Iteration 42, loss = 0.54787232\n",
            "Iteration 43, loss = 0.54571606\n",
            "Iteration 44, loss = 0.54937040\n",
            "Iteration 45, loss = 0.54782186\n",
            "Iteration 46, loss = 0.54661798\n",
            "Iteration 47, loss = 0.54675632\n",
            "Iteration 48, loss = 0.54689029\n",
            "Iteration 49, loss = 0.54523948\n",
            "Iteration 50, loss = 0.54503063\n",
            "Iteration 51, loss = 0.54525127\n",
            "Iteration 52, loss = 0.54468624\n",
            "Iteration 53, loss = 0.54400302\n",
            "Iteration 54, loss = 0.54391423\n",
            "Iteration 55, loss = 0.54280670\n",
            "Iteration 56, loss = 0.54259832\n",
            "Iteration 57, loss = 0.54213790\n",
            "Iteration 58, loss = 0.54282466\n",
            "Iteration 59, loss = 0.54178980\n",
            "Iteration 60, loss = 0.54152020\n",
            "Iteration 61, loss = 0.54155992\n",
            "Iteration 62, loss = 0.54119488\n",
            "Iteration 63, loss = 0.54071023\n",
            "Iteration 64, loss = 0.54030722\n",
            "Iteration 65, loss = 0.53913105\n",
            "Iteration 66, loss = 0.53989860\n",
            "Iteration 67, loss = 0.54074834\n",
            "Iteration 68, loss = 0.54150445\n",
            "Iteration 69, loss = 0.53777836\n",
            "Iteration 70, loss = 0.53800642\n",
            "Iteration 71, loss = 0.53700912\n",
            "Iteration 72, loss = 0.53829864\n",
            "Iteration 73, loss = 0.53702260\n",
            "Iteration 74, loss = 0.53586493\n",
            "Iteration 75, loss = 0.54016969\n",
            "Iteration 76, loss = 0.53650803\n",
            "Iteration 77, loss = 0.53598665\n",
            "Iteration 78, loss = 0.53578596\n",
            "Iteration 79, loss = 0.53630111\n",
            "Iteration 80, loss = 0.53602261\n",
            "Iteration 81, loss = 0.53346023\n",
            "Iteration 82, loss = 0.53441862\n",
            "Iteration 83, loss = 0.53432345\n",
            "Iteration 84, loss = 0.53342583\n",
            "Iteration 85, loss = 0.53437527\n",
            "Iteration 86, loss = 0.53154339\n",
            "Iteration 87, loss = 0.53295855\n",
            "Iteration 88, loss = 0.53567582\n",
            "Iteration 89, loss = 0.53415989\n",
            "Iteration 90, loss = 0.52991798\n",
            "Iteration 91, loss = 0.53273745\n",
            "Iteration 92, loss = 0.52870751\n",
            "Iteration 93, loss = 0.52784626\n",
            "Iteration 94, loss = 0.52855230\n",
            "Iteration 95, loss = 0.53029506\n",
            "Iteration 96, loss = 0.52741036\n",
            "Iteration 97, loss = 0.52717025\n",
            "Iteration 98, loss = 0.52456752\n",
            "Iteration 99, loss = 0.52656043\n",
            "Iteration 100, loss = 0.52491723\n",
            "Iteration 101, loss = 0.52483333\n",
            "Iteration 102, loss = 0.52385748\n",
            "Iteration 103, loss = 0.52248737\n",
            "Iteration 104, loss = 0.52100438\n",
            "Iteration 105, loss = 0.52075252\n",
            "Iteration 106, loss = 0.52322728\n",
            "Iteration 107, loss = 0.52289920\n",
            "Iteration 108, loss = 0.51981161\n",
            "Iteration 109, loss = 0.52062543\n",
            "Iteration 110, loss = 0.51794407\n",
            "Iteration 111, loss = 0.52074644\n",
            "Iteration 112, loss = 0.51821047\n",
            "Iteration 113, loss = 0.51730661\n",
            "Iteration 114, loss = 0.51684496\n",
            "Iteration 115, loss = 0.51611415\n",
            "Iteration 116, loss = 0.51614487\n",
            "Iteration 117, loss = 0.51452083\n",
            "Iteration 118, loss = 0.51238568\n",
            "Iteration 119, loss = 0.51315305\n",
            "Iteration 120, loss = 0.51433388\n",
            "Iteration 121, loss = 0.51170358\n",
            "Iteration 122, loss = 0.51384342\n",
            "Iteration 123, loss = 0.51519640\n",
            "Iteration 124, loss = 0.51469395\n",
            "Iteration 125, loss = 0.51220491\n",
            "Iteration 126, loss = 0.50915291\n",
            "Iteration 127, loss = 0.50821678\n",
            "Iteration 128, loss = 0.50649950\n",
            "Iteration 129, loss = 0.50748697\n",
            "Iteration 130, loss = 0.50801146\n",
            "Iteration 131, loss = 0.50718197\n",
            "Iteration 132, loss = 0.50999426\n",
            "Iteration 133, loss = 0.50587073\n",
            "Iteration 134, loss = 0.50716233\n",
            "Iteration 135, loss = 0.50597473\n",
            "Iteration 136, loss = 0.50702109\n",
            "Iteration 137, loss = 0.50638497\n",
            "Iteration 138, loss = 0.50359542\n",
            "Iteration 139, loss = 0.50938738\n",
            "Iteration 140, loss = 0.50860421\n",
            "Iteration 141, loss = 0.50834190\n",
            "Iteration 142, loss = 0.50732540\n",
            "Iteration 143, loss = 0.50718784\n",
            "Iteration 144, loss = 0.50687248\n",
            "Iteration 145, loss = 0.50995060\n",
            "Iteration 146, loss = 0.50964205\n",
            "Iteration 147, loss = 0.50568991\n",
            "Iteration 148, loss = 0.50419144\n",
            "Iteration 149, loss = 0.50265334\n",
            "Iteration 150, loss = 0.50231836\n",
            "Iteration 151, loss = 0.50307620\n",
            "Iteration 152, loss = 0.50239274\n",
            "Iteration 153, loss = 0.50215520\n",
            "Iteration 154, loss = 0.50020164\n",
            "Iteration 155, loss = 0.50050225\n",
            "Iteration 156, loss = 0.50352768\n",
            "Iteration 157, loss = 0.49939456\n",
            "Iteration 158, loss = 0.50586997\n",
            "Iteration 159, loss = 0.50367848\n",
            "Iteration 160, loss = 0.50742324\n",
            "Iteration 161, loss = 0.51676017\n",
            "Iteration 162, loss = 0.50538113\n",
            "Iteration 163, loss = 0.50249294\n",
            "Iteration 164, loss = 0.50397304\n",
            "Iteration 165, loss = 0.50251003\n",
            "Iteration 166, loss = 0.50217175\n",
            "Iteration 167, loss = 0.50207735\n",
            "Iteration 168, loss = 0.50218548\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 250 and for layer number 4 : 0.6762499999999999\n",
            "Iteration 1, loss = 0.84283593\n",
            "Iteration 2, loss = 0.67832748\n",
            "Iteration 3, loss = 0.61850827\n",
            "Iteration 4, loss = 0.60382734\n",
            "Iteration 5, loss = 0.59477653\n",
            "Iteration 6, loss = 0.59056898\n",
            "Iteration 7, loss = 0.58162134\n",
            "Iteration 8, loss = 0.57169835\n",
            "Iteration 9, loss = 0.56715602\n",
            "Iteration 10, loss = 0.56250128\n",
            "Iteration 11, loss = 0.56056030\n",
            "Iteration 12, loss = 0.55803894\n",
            "Iteration 13, loss = 0.55555691\n",
            "Iteration 14, loss = 0.55356033\n",
            "Iteration 15, loss = 0.54894640\n",
            "Iteration 16, loss = 0.54914943\n",
            "Iteration 17, loss = 0.54874113\n",
            "Iteration 18, loss = 0.54701632\n",
            "Iteration 19, loss = 0.54482747\n",
            "Iteration 20, loss = 0.54364637\n",
            "Iteration 21, loss = 0.54290350\n",
            "Iteration 22, loss = 0.54365119\n",
            "Iteration 23, loss = 0.54035673\n",
            "Iteration 24, loss = 0.53845438\n",
            "Iteration 25, loss = 0.53723289\n",
            "Iteration 26, loss = 0.53636142\n",
            "Iteration 27, loss = 0.53484490\n",
            "Iteration 28, loss = 0.53512642\n",
            "Iteration 29, loss = 0.53544812\n",
            "Iteration 30, loss = 0.53257999\n",
            "Iteration 31, loss = 0.53591516\n",
            "Iteration 32, loss = 0.53339687\n",
            "Iteration 33, loss = 0.53051295\n",
            "Iteration 34, loss = 0.52991251\n",
            "Iteration 35, loss = 0.52986860\n",
            "Iteration 36, loss = 0.52797787\n",
            "Iteration 37, loss = 0.52785962\n",
            "Iteration 38, loss = 0.52742171\n",
            "Iteration 39, loss = 0.52639113\n",
            "Iteration 40, loss = 0.52615978\n",
            "Iteration 41, loss = 0.52616046\n",
            "Iteration 42, loss = 0.52438233\n",
            "Iteration 43, loss = 0.52450988\n",
            "Iteration 44, loss = 0.52431472\n",
            "Iteration 45, loss = 0.52376981\n",
            "Iteration 46, loss = 0.52195023\n",
            "Iteration 47, loss = 0.52206280\n",
            "Iteration 48, loss = 0.52291998\n",
            "Iteration 49, loss = 0.52195095\n",
            "Iteration 50, loss = 0.52108269\n",
            "Iteration 51, loss = 0.52130216\n",
            "Iteration 52, loss = 0.52001568\n",
            "Iteration 53, loss = 0.52004530\n",
            "Iteration 54, loss = 0.52191695\n",
            "Iteration 55, loss = 0.52080266\n",
            "Iteration 56, loss = 0.51808299\n",
            "Iteration 57, loss = 0.51748947\n",
            "Iteration 58, loss = 0.51716362\n",
            "Iteration 59, loss = 0.51694316\n",
            "Iteration 60, loss = 0.51658528\n",
            "Iteration 61, loss = 0.51648336\n",
            "Iteration 62, loss = 0.51510877\n",
            "Iteration 63, loss = 0.51749582\n",
            "Iteration 64, loss = 0.51449864\n",
            "Iteration 65, loss = 0.51468475\n",
            "Iteration 66, loss = 0.51606903\n",
            "Iteration 67, loss = 0.51518097\n",
            "Iteration 68, loss = 0.51455031\n",
            "Iteration 69, loss = 0.51350630\n",
            "Iteration 70, loss = 0.51362597\n",
            "Iteration 71, loss = 0.51534284\n",
            "Iteration 72, loss = 0.51294395\n",
            "Iteration 73, loss = 0.51181823\n",
            "Iteration 74, loss = 0.51195315\n",
            "Iteration 75, loss = 0.51020922\n",
            "Iteration 76, loss = 0.50961620\n",
            "Iteration 77, loss = 0.50872978\n",
            "Iteration 78, loss = 0.50980573\n",
            "Iteration 79, loss = 0.50981380\n",
            "Iteration 80, loss = 0.51025917\n",
            "Iteration 81, loss = 0.51557245\n",
            "Iteration 82, loss = 0.51404262\n",
            "Iteration 83, loss = 0.50811045\n",
            "Iteration 84, loss = 0.51117944\n",
            "Iteration 85, loss = 0.51408823\n",
            "Iteration 86, loss = 0.50714546\n",
            "Iteration 87, loss = 0.50648920\n",
            "Iteration 88, loss = 0.50683883\n",
            "Iteration 89, loss = 0.50483630\n",
            "Iteration 90, loss = 0.50519714\n",
            "Iteration 91, loss = 0.50637432\n",
            "Iteration 92, loss = 0.50509134\n",
            "Iteration 93, loss = 0.50572160\n",
            "Iteration 94, loss = 0.50471110\n",
            "Iteration 95, loss = 0.50551280\n",
            "Iteration 96, loss = 0.50375059\n",
            "Iteration 97, loss = 0.50444488\n",
            "Iteration 98, loss = 0.50495500\n",
            "Iteration 99, loss = 0.50372446\n",
            "Iteration 100, loss = 0.50397249\n",
            "Iteration 101, loss = 0.50584103\n",
            "Iteration 102, loss = 0.50538688\n",
            "Iteration 103, loss = 0.50533859\n",
            "Iteration 104, loss = 0.50522541\n",
            "Iteration 105, loss = 0.50313477\n",
            "Iteration 106, loss = 0.50375229\n",
            "Iteration 107, loss = 0.50157035\n",
            "Iteration 108, loss = 0.50024633\n",
            "Iteration 109, loss = 0.50053768\n",
            "Iteration 110, loss = 0.50180636\n",
            "Iteration 111, loss = 0.50104015\n",
            "Iteration 112, loss = 0.49996236\n",
            "Iteration 113, loss = 0.49994954\n",
            "Iteration 114, loss = 0.50009725\n",
            "Iteration 115, loss = 0.49886425\n",
            "Iteration 116, loss = 0.49794556\n",
            "Iteration 117, loss = 0.49915618\n",
            "Iteration 118, loss = 0.49714965\n",
            "Iteration 119, loss = 0.49676612\n",
            "Iteration 120, loss = 0.49755745\n",
            "Iteration 121, loss = 0.49675014\n",
            "Iteration 122, loss = 0.49640613\n",
            "Iteration 123, loss = 0.49767010\n",
            "Iteration 124, loss = 0.49764428\n",
            "Iteration 125, loss = 0.49774683\n",
            "Iteration 126, loss = 0.49739006\n",
            "Iteration 127, loss = 0.49544826\n",
            "Iteration 128, loss = 0.49784077\n",
            "Iteration 129, loss = 0.49644203\n",
            "Iteration 130, loss = 0.49657157\n",
            "Iteration 131, loss = 0.49466084\n",
            "Iteration 132, loss = 0.49522061\n",
            "Iteration 133, loss = 0.49614322\n",
            "Iteration 134, loss = 0.49233119\n",
            "Iteration 135, loss = 0.49259802\n",
            "Iteration 136, loss = 0.49261920\n",
            "Iteration 137, loss = 0.49548365\n",
            "Iteration 138, loss = 0.49462814\n",
            "Iteration 139, loss = 0.49515159\n",
            "Iteration 140, loss = 0.49499592\n",
            "Iteration 141, loss = 0.49543857\n",
            "Iteration 142, loss = 0.49233858\n",
            "Iteration 143, loss = 0.49309902\n",
            "Iteration 144, loss = 0.49020281\n",
            "Iteration 145, loss = 0.49189385\n",
            "Iteration 146, loss = 0.49148586\n",
            "Iteration 147, loss = 0.49266871\n",
            "Iteration 148, loss = 0.49050547\n",
            "Iteration 149, loss = 0.49223937\n",
            "Iteration 150, loss = 0.49102742\n",
            "Iteration 151, loss = 0.49205071\n",
            "Iteration 152, loss = 0.49455023\n",
            "Iteration 153, loss = 0.49425244\n",
            "Iteration 154, loss = 0.48841071\n",
            "Iteration 155, loss = 0.49090468\n",
            "Iteration 156, loss = 0.49119644\n",
            "Iteration 157, loss = 0.49289922\n",
            "Iteration 158, loss = 0.49107514\n",
            "Iteration 159, loss = 0.48968597\n",
            "Iteration 160, loss = 0.49341045\n",
            "Iteration 161, loss = 0.49176106\n",
            "Iteration 162, loss = 0.48935169\n",
            "Iteration 163, loss = 0.48944420\n",
            "Iteration 164, loss = 0.48941782\n",
            "Iteration 165, loss = 0.48892615\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.84211562\n",
            "Iteration 2, loss = 0.67390273\n",
            "Iteration 3, loss = 0.61479397\n",
            "Iteration 4, loss = 0.59817770\n",
            "Iteration 5, loss = 0.58643524\n",
            "Iteration 6, loss = 0.57940279\n",
            "Iteration 7, loss = 0.57108334\n",
            "Iteration 8, loss = 0.56345206\n",
            "Iteration 9, loss = 0.55760032\n",
            "Iteration 10, loss = 0.55137476\n",
            "Iteration 11, loss = 0.54982593\n",
            "Iteration 12, loss = 0.54759444\n",
            "Iteration 13, loss = 0.54480473\n",
            "Iteration 14, loss = 0.54105592\n",
            "Iteration 15, loss = 0.53799031\n",
            "Iteration 16, loss = 0.53554759\n",
            "Iteration 17, loss = 0.53296703\n",
            "Iteration 18, loss = 0.53051284\n",
            "Iteration 19, loss = 0.52790683\n",
            "Iteration 20, loss = 0.52677164\n",
            "Iteration 21, loss = 0.52485005\n",
            "Iteration 22, loss = 0.52480127\n",
            "Iteration 23, loss = 0.52143077\n",
            "Iteration 24, loss = 0.52039570\n",
            "Iteration 25, loss = 0.51926551\n",
            "Iteration 26, loss = 0.51855653\n",
            "Iteration 27, loss = 0.51683205\n",
            "Iteration 28, loss = 0.51569551\n",
            "Iteration 29, loss = 0.51493485\n",
            "Iteration 30, loss = 0.51421020\n",
            "Iteration 31, loss = 0.51674064\n",
            "Iteration 32, loss = 0.51260548\n",
            "Iteration 33, loss = 0.51040642\n",
            "Iteration 34, loss = 0.50939175\n",
            "Iteration 35, loss = 0.50888292\n",
            "Iteration 36, loss = 0.50876223\n",
            "Iteration 37, loss = 0.50628136\n",
            "Iteration 38, loss = 0.50496776\n",
            "Iteration 39, loss = 0.50289192\n",
            "Iteration 40, loss = 0.50200486\n",
            "Iteration 41, loss = 0.50190053\n",
            "Iteration 42, loss = 0.50099544\n",
            "Iteration 43, loss = 0.50033700\n",
            "Iteration 44, loss = 0.49990491\n",
            "Iteration 45, loss = 0.49703635\n",
            "Iteration 46, loss = 0.49645873\n",
            "Iteration 47, loss = 0.49625000\n",
            "Iteration 48, loss = 0.49583677\n",
            "Iteration 49, loss = 0.49334537\n",
            "Iteration 50, loss = 0.49351033\n",
            "Iteration 51, loss = 0.49339818\n",
            "Iteration 52, loss = 0.49169668\n",
            "Iteration 53, loss = 0.49150701\n",
            "Iteration 54, loss = 0.49566927\n",
            "Iteration 55, loss = 0.49062902\n",
            "Iteration 56, loss = 0.48908149\n",
            "Iteration 57, loss = 0.48730963\n",
            "Iteration 58, loss = 0.48737917\n",
            "Iteration 59, loss = 0.48786171\n",
            "Iteration 60, loss = 0.48735379\n",
            "Iteration 61, loss = 0.48919848\n",
            "Iteration 62, loss = 0.48604695\n",
            "Iteration 63, loss = 0.48791279\n",
            "Iteration 64, loss = 0.48447282\n",
            "Iteration 65, loss = 0.48392292\n",
            "Iteration 66, loss = 0.48710397\n",
            "Iteration 67, loss = 0.48497721\n",
            "Iteration 68, loss = 0.48189699\n",
            "Iteration 69, loss = 0.48215848\n",
            "Iteration 70, loss = 0.48396642\n",
            "Iteration 71, loss = 0.48472021\n",
            "Iteration 72, loss = 0.48269523\n",
            "Iteration 73, loss = 0.48006770\n",
            "Iteration 74, loss = 0.47949046\n",
            "Iteration 75, loss = 0.47917436\n",
            "Iteration 76, loss = 0.47986172\n",
            "Iteration 77, loss = 0.47652529\n",
            "Iteration 78, loss = 0.47572204\n",
            "Iteration 79, loss = 0.47605137\n",
            "Iteration 80, loss = 0.47486116\n",
            "Iteration 81, loss = 0.47661630\n",
            "Iteration 82, loss = 0.47626359\n",
            "Iteration 83, loss = 0.47833453\n",
            "Iteration 84, loss = 0.47666945\n",
            "Iteration 85, loss = 0.48468544\n",
            "Iteration 86, loss = 0.47513421\n",
            "Iteration 87, loss = 0.47577963\n",
            "Iteration 88, loss = 0.47356521\n",
            "Iteration 89, loss = 0.47354569\n",
            "Iteration 90, loss = 0.47358204\n",
            "Iteration 91, loss = 0.47377450\n",
            "Iteration 92, loss = 0.47338047\n",
            "Iteration 93, loss = 0.47247782\n",
            "Iteration 94, loss = 0.47194345\n",
            "Iteration 95, loss = 0.47182012\n",
            "Iteration 96, loss = 0.47099111\n",
            "Iteration 97, loss = 0.47485566\n",
            "Iteration 98, loss = 0.47394550\n",
            "Iteration 99, loss = 0.47321010\n",
            "Iteration 100, loss = 0.47317758\n",
            "Iteration 101, loss = 0.47147078\n",
            "Iteration 102, loss = 0.46973482\n",
            "Iteration 103, loss = 0.47088729\n",
            "Iteration 104, loss = 0.47040228\n",
            "Iteration 105, loss = 0.47415798\n",
            "Iteration 106, loss = 0.47081625\n",
            "Iteration 107, loss = 0.47069119\n",
            "Iteration 108, loss = 0.46858736\n",
            "Iteration 109, loss = 0.46535988\n",
            "Iteration 110, loss = 0.46921133\n",
            "Iteration 111, loss = 0.47624427\n",
            "Iteration 112, loss = 0.46987930\n",
            "Iteration 113, loss = 0.46703870\n",
            "Iteration 114, loss = 0.46861779\n",
            "Iteration 115, loss = 0.47201319\n",
            "Iteration 116, loss = 0.46866252\n",
            "Iteration 117, loss = 0.46893663\n",
            "Iteration 118, loss = 0.46594707\n",
            "Iteration 119, loss = 0.46587544\n",
            "Iteration 120, loss = 0.46586999\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.84194648\n",
            "Iteration 2, loss = 0.66554851\n",
            "Iteration 3, loss = 0.61225024\n",
            "Iteration 4, loss = 0.59987886\n",
            "Iteration 5, loss = 0.59202196\n",
            "Iteration 6, loss = 0.58325873\n",
            "Iteration 7, loss = 0.57298718\n",
            "Iteration 8, loss = 0.56206000\n",
            "Iteration 9, loss = 0.55315409\n",
            "Iteration 10, loss = 0.54698521\n",
            "Iteration 11, loss = 0.54442773\n",
            "Iteration 12, loss = 0.54226291\n",
            "Iteration 13, loss = 0.53980072\n",
            "Iteration 14, loss = 0.53599988\n",
            "Iteration 15, loss = 0.53313323\n",
            "Iteration 16, loss = 0.52934416\n",
            "Iteration 17, loss = 0.52638109\n",
            "Iteration 18, loss = 0.52580879\n",
            "Iteration 19, loss = 0.52426243\n",
            "Iteration 20, loss = 0.52290946\n",
            "Iteration 21, loss = 0.52268451\n",
            "Iteration 22, loss = 0.52343882\n",
            "Iteration 23, loss = 0.52150082\n",
            "Iteration 24, loss = 0.52105498\n",
            "Iteration 25, loss = 0.51995877\n",
            "Iteration 26, loss = 0.51906444\n",
            "Iteration 27, loss = 0.51760723\n",
            "Iteration 28, loss = 0.51746923\n",
            "Iteration 29, loss = 0.51659429\n",
            "Iteration 30, loss = 0.51628310\n",
            "Iteration 31, loss = 0.51696056\n",
            "Iteration 32, loss = 0.51532952\n",
            "Iteration 33, loss = 0.51492982\n",
            "Iteration 34, loss = 0.51395807\n",
            "Iteration 35, loss = 0.51293487\n",
            "Iteration 36, loss = 0.51311644\n",
            "Iteration 37, loss = 0.51206223\n",
            "Iteration 38, loss = 0.51187460\n",
            "Iteration 39, loss = 0.51152712\n",
            "Iteration 40, loss = 0.51025039\n",
            "Iteration 41, loss = 0.51022046\n",
            "Iteration 42, loss = 0.51081281\n",
            "Iteration 43, loss = 0.51040966\n",
            "Iteration 44, loss = 0.51026804\n",
            "Iteration 45, loss = 0.50922244\n",
            "Iteration 46, loss = 0.50916438\n",
            "Iteration 47, loss = 0.50888024\n",
            "Iteration 48, loss = 0.50907711\n",
            "Iteration 49, loss = 0.50774179\n",
            "Iteration 50, loss = 0.50710401\n",
            "Iteration 51, loss = 0.50764802\n",
            "Iteration 52, loss = 0.50624387\n",
            "Iteration 53, loss = 0.50559071\n",
            "Iteration 54, loss = 0.50518696\n",
            "Iteration 55, loss = 0.50350782\n",
            "Iteration 56, loss = 0.50403498\n",
            "Iteration 57, loss = 0.50308545\n",
            "Iteration 58, loss = 0.50276083\n",
            "Iteration 59, loss = 0.50331295\n",
            "Iteration 60, loss = 0.50236108\n",
            "Iteration 61, loss = 0.50171369\n",
            "Iteration 62, loss = 0.50260973\n",
            "Iteration 63, loss = 0.50167126\n",
            "Iteration 64, loss = 0.50123775\n",
            "Iteration 65, loss = 0.49866415\n",
            "Iteration 66, loss = 0.50089686\n",
            "Iteration 67, loss = 0.49977304\n",
            "Iteration 68, loss = 0.49736768\n",
            "Iteration 69, loss = 0.49733328\n",
            "Iteration 70, loss = 0.49723094\n",
            "Iteration 71, loss = 0.49939928\n",
            "Iteration 72, loss = 0.49655591\n",
            "Iteration 73, loss = 0.49371061\n",
            "Iteration 74, loss = 0.49284754\n",
            "Iteration 75, loss = 0.49211623\n",
            "Iteration 76, loss = 0.49079801\n",
            "Iteration 77, loss = 0.48971083\n",
            "Iteration 78, loss = 0.48842846\n",
            "Iteration 79, loss = 0.48863192\n",
            "Iteration 80, loss = 0.48769454\n",
            "Iteration 81, loss = 0.48869401\n",
            "Iteration 82, loss = 0.48656822\n",
            "Iteration 83, loss = 0.48627403\n",
            "Iteration 84, loss = 0.48614048\n",
            "Iteration 85, loss = 0.48758305\n",
            "Iteration 86, loss = 0.48417598\n",
            "Iteration 87, loss = 0.48175374\n",
            "Iteration 88, loss = 0.48032217\n",
            "Iteration 89, loss = 0.48057079\n",
            "Iteration 90, loss = 0.48163173\n",
            "Iteration 91, loss = 0.48031648\n",
            "Iteration 92, loss = 0.47936347\n",
            "Iteration 93, loss = 0.47888769\n",
            "Iteration 94, loss = 0.47807031\n",
            "Iteration 95, loss = 0.47792276\n",
            "Iteration 96, loss = 0.47669075\n",
            "Iteration 97, loss = 0.47703885\n",
            "Iteration 98, loss = 0.47652764\n",
            "Iteration 99, loss = 0.47698218\n",
            "Iteration 100, loss = 0.47609304\n",
            "Iteration 101, loss = 0.47529402\n",
            "Iteration 102, loss = 0.47466444\n",
            "Iteration 103, loss = 0.47422635\n",
            "Iteration 104, loss = 0.47371837\n",
            "Iteration 105, loss = 0.47493566\n",
            "Iteration 106, loss = 0.47400548\n",
            "Iteration 107, loss = 0.47320379\n",
            "Iteration 108, loss = 0.47309130\n",
            "Iteration 109, loss = 0.47056961\n",
            "Iteration 110, loss = 0.47351695\n",
            "Iteration 111, loss = 0.47489974\n",
            "Iteration 112, loss = 0.47332189\n",
            "Iteration 113, loss = 0.47146561\n",
            "Iteration 114, loss = 0.47004990\n",
            "Iteration 115, loss = 0.47355515\n",
            "Iteration 116, loss = 0.47397915\n",
            "Iteration 117, loss = 0.47466726\n",
            "Iteration 118, loss = 0.47146515\n",
            "Iteration 119, loss = 0.46944354\n",
            "Iteration 120, loss = 0.47083722\n",
            "Iteration 121, loss = 0.46829298\n",
            "Iteration 122, loss = 0.47049314\n",
            "Iteration 123, loss = 0.46855504\n",
            "Iteration 124, loss = 0.47046074\n",
            "Iteration 125, loss = 0.46915822\n",
            "Iteration 126, loss = 0.46790139\n",
            "Iteration 127, loss = 0.46876398\n",
            "Iteration 128, loss = 0.46816952\n",
            "Iteration 129, loss = 0.46819922\n",
            "Iteration 130, loss = 0.46776867\n",
            "Iteration 131, loss = 0.46596833\n",
            "Iteration 132, loss = 0.46531227\n",
            "Iteration 133, loss = 0.46661815\n",
            "Iteration 134, loss = 0.46529771\n",
            "Iteration 135, loss = 0.46387486\n",
            "Iteration 136, loss = 0.46544262\n",
            "Iteration 137, loss = 0.46535451\n",
            "Iteration 138, loss = 0.46541958\n",
            "Iteration 139, loss = 0.46533499\n",
            "Iteration 140, loss = 0.46329613\n",
            "Iteration 141, loss = 0.46399936\n",
            "Iteration 142, loss = 0.46287439\n",
            "Iteration 143, loss = 0.46369668\n",
            "Iteration 144, loss = 0.46330816\n",
            "Iteration 145, loss = 0.46335661\n",
            "Iteration 146, loss = 0.46403232\n",
            "Iteration 147, loss = 0.46531830\n",
            "Iteration 148, loss = 0.46581724\n",
            "Iteration 149, loss = 0.46551850\n",
            "Iteration 150, loss = 0.46611122\n",
            "Iteration 151, loss = 0.46445827\n",
            "Iteration 152, loss = 0.46276198\n",
            "Iteration 153, loss = 0.46472329\n",
            "Iteration 154, loss = 0.46378864\n",
            "Iteration 155, loss = 0.46445555\n",
            "Iteration 156, loss = 0.46578407\n",
            "Iteration 157, loss = 0.46654125\n",
            "Iteration 158, loss = 0.47034106\n",
            "Iteration 159, loss = 0.46466263\n",
            "Iteration 160, loss = 0.46142475\n",
            "Iteration 161, loss = 0.46337598\n",
            "Iteration 162, loss = 0.46279314\n",
            "Iteration 163, loss = 0.46455164\n",
            "Iteration 164, loss = 0.46482939\n",
            "Iteration 165, loss = 0.46279006\n",
            "Iteration 166, loss = 0.46260688\n",
            "Iteration 167, loss = 0.46530952\n",
            "Iteration 168, loss = 0.46265043\n",
            "Iteration 169, loss = 0.46444094\n",
            "Iteration 170, loss = 0.46360634\n",
            "Iteration 171, loss = 0.46131717\n",
            "Iteration 172, loss = 0.46156849\n",
            "Iteration 173, loss = 0.46282120\n",
            "Iteration 174, loss = 0.46094399\n",
            "Iteration 175, loss = 0.46435120\n",
            "Iteration 176, loss = 0.46232158\n",
            "Iteration 177, loss = 0.46161511\n",
            "Iteration 178, loss = 0.46469640\n",
            "Iteration 179, loss = 0.46303747\n",
            "Iteration 180, loss = 0.46344337\n",
            "Iteration 181, loss = 0.46181827\n",
            "Iteration 182, loss = 0.46169673\n",
            "Iteration 183, loss = 0.46612811\n",
            "Iteration 184, loss = 0.46381456\n",
            "Iteration 185, loss = 0.46101203\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.83713456\n",
            "Iteration 2, loss = 0.66083566\n",
            "Iteration 3, loss = 0.62354131\n",
            "Iteration 4, loss = 0.60812662\n",
            "Iteration 5, loss = 0.59948788\n",
            "Iteration 6, loss = 0.59562370\n",
            "Iteration 7, loss = 0.58895751\n",
            "Iteration 8, loss = 0.57947492\n",
            "Iteration 9, loss = 0.57067064\n",
            "Iteration 10, loss = 0.56219342\n",
            "Iteration 11, loss = 0.55698371\n",
            "Iteration 12, loss = 0.55391316\n",
            "Iteration 13, loss = 0.55129977\n",
            "Iteration 14, loss = 0.54890671\n",
            "Iteration 15, loss = 0.54643885\n",
            "Iteration 16, loss = 0.54297314\n",
            "Iteration 17, loss = 0.54102246\n",
            "Iteration 18, loss = 0.54129445\n",
            "Iteration 19, loss = 0.53813457\n",
            "Iteration 20, loss = 0.53584825\n",
            "Iteration 21, loss = 0.53505185\n",
            "Iteration 22, loss = 0.53380029\n",
            "Iteration 23, loss = 0.53263117\n",
            "Iteration 24, loss = 0.53059466\n",
            "Iteration 25, loss = 0.53043742\n",
            "Iteration 26, loss = 0.52878689\n",
            "Iteration 27, loss = 0.52721957\n",
            "Iteration 28, loss = 0.52509053\n",
            "Iteration 29, loss = 0.52463802\n",
            "Iteration 30, loss = 0.52600622\n",
            "Iteration 31, loss = 0.52533686\n",
            "Iteration 32, loss = 0.52282068\n",
            "Iteration 33, loss = 0.52207370\n",
            "Iteration 34, loss = 0.52197151\n",
            "Iteration 35, loss = 0.52050410\n",
            "Iteration 36, loss = 0.51890510\n",
            "Iteration 37, loss = 0.51812398\n",
            "Iteration 38, loss = 0.51735240\n",
            "Iteration 39, loss = 0.51699532\n",
            "Iteration 40, loss = 0.51686667\n",
            "Iteration 41, loss = 0.51610695\n",
            "Iteration 42, loss = 0.51669172\n",
            "Iteration 43, loss = 0.51322902\n",
            "Iteration 44, loss = 0.51318415\n",
            "Iteration 45, loss = 0.51185272\n",
            "Iteration 46, loss = 0.51094082\n",
            "Iteration 47, loss = 0.51111908\n",
            "Iteration 48, loss = 0.50958323\n",
            "Iteration 49, loss = 0.51071506\n",
            "Iteration 50, loss = 0.50797224\n",
            "Iteration 51, loss = 0.50855548\n",
            "Iteration 52, loss = 0.50619339\n",
            "Iteration 53, loss = 0.50607627\n",
            "Iteration 54, loss = 0.50619348\n",
            "Iteration 55, loss = 0.50297138\n",
            "Iteration 56, loss = 0.50363331\n",
            "Iteration 57, loss = 0.50120507\n",
            "Iteration 58, loss = 0.50208289\n",
            "Iteration 59, loss = 0.50219130\n",
            "Iteration 60, loss = 0.50081060\n",
            "Iteration 61, loss = 0.50034864\n",
            "Iteration 62, loss = 0.49877738\n",
            "Iteration 63, loss = 0.49889385\n",
            "Iteration 64, loss = 0.49910741\n",
            "Iteration 65, loss = 0.49653541\n",
            "Iteration 66, loss = 0.49809058\n",
            "Iteration 67, loss = 0.49694000\n",
            "Iteration 68, loss = 0.49540907\n",
            "Iteration 69, loss = 0.49503097\n",
            "Iteration 70, loss = 0.49322977\n",
            "Iteration 71, loss = 0.49398712\n",
            "Iteration 72, loss = 0.49298884\n",
            "Iteration 73, loss = 0.49221089\n",
            "Iteration 74, loss = 0.49056898\n",
            "Iteration 75, loss = 0.49090439\n",
            "Iteration 76, loss = 0.49178349\n",
            "Iteration 77, loss = 0.48995355\n",
            "Iteration 78, loss = 0.48955926\n",
            "Iteration 79, loss = 0.49071445\n",
            "Iteration 80, loss = 0.48789645\n",
            "Iteration 81, loss = 0.49334601\n",
            "Iteration 82, loss = 0.48766543\n",
            "Iteration 83, loss = 0.49117912\n",
            "Iteration 84, loss = 0.49090990\n",
            "Iteration 85, loss = 0.49150852\n",
            "Iteration 86, loss = 0.48896881\n",
            "Iteration 87, loss = 0.48385274\n",
            "Iteration 88, loss = 0.48921794\n",
            "Iteration 89, loss = 0.48589666\n",
            "Iteration 90, loss = 0.48460451\n",
            "Iteration 91, loss = 0.48482953\n",
            "Iteration 92, loss = 0.48133251\n",
            "Iteration 93, loss = 0.47927352\n",
            "Iteration 94, loss = 0.47865817\n",
            "Iteration 95, loss = 0.47757052\n",
            "Iteration 96, loss = 0.47917336\n",
            "Iteration 97, loss = 0.47724566\n",
            "Iteration 98, loss = 0.47747433\n",
            "Iteration 99, loss = 0.47479210\n",
            "Iteration 100, loss = 0.47134891\n",
            "Iteration 101, loss = 0.47284035\n",
            "Iteration 102, loss = 0.47352995\n",
            "Iteration 103, loss = 0.47224160\n",
            "Iteration 104, loss = 0.47212225\n",
            "Iteration 105, loss = 0.47184532\n",
            "Iteration 106, loss = 0.46958548\n",
            "Iteration 107, loss = 0.47249782\n",
            "Iteration 108, loss = 0.47339093\n",
            "Iteration 109, loss = 0.46895803\n",
            "Iteration 110, loss = 0.47348572\n",
            "Iteration 111, loss = 0.47387400\n",
            "Iteration 112, loss = 0.47071031\n",
            "Iteration 113, loss = 0.47299048\n",
            "Iteration 114, loss = 0.47027790\n",
            "Iteration 115, loss = 0.47575480\n",
            "Iteration 116, loss = 0.47479950\n",
            "Iteration 117, loss = 0.47291502\n",
            "Iteration 118, loss = 0.46806154\n",
            "Iteration 119, loss = 0.46810702\n",
            "Iteration 120, loss = 0.46852562\n",
            "Iteration 121, loss = 0.46785207\n",
            "Iteration 122, loss = 0.46712060\n",
            "Iteration 123, loss = 0.46379537\n",
            "Iteration 124, loss = 0.46692458\n",
            "Iteration 125, loss = 0.46876335\n",
            "Iteration 126, loss = 0.46553169\n",
            "Iteration 127, loss = 0.46911223\n",
            "Iteration 128, loss = 0.46725812\n",
            "Iteration 129, loss = 0.46746697\n",
            "Iteration 130, loss = 0.47360943\n",
            "Iteration 131, loss = 0.47004586\n",
            "Iteration 132, loss = 0.46820468\n",
            "Iteration 133, loss = 0.47028128\n",
            "Iteration 134, loss = 0.46296306\n",
            "Iteration 135, loss = 0.46389413\n",
            "Iteration 136, loss = 0.46786654\n",
            "Iteration 137, loss = 0.46407171\n",
            "Iteration 138, loss = 0.46591593\n",
            "Iteration 139, loss = 0.46129456\n",
            "Iteration 140, loss = 0.45925773\n",
            "Iteration 141, loss = 0.46237936\n",
            "Iteration 142, loss = 0.46106316\n",
            "Iteration 143, loss = 0.46256290\n",
            "Iteration 144, loss = 0.45925252\n",
            "Iteration 145, loss = 0.45847400\n",
            "Iteration 146, loss = 0.46023258\n",
            "Iteration 147, loss = 0.46050539\n",
            "Iteration 148, loss = 0.46045107\n",
            "Iteration 149, loss = 0.45892109\n",
            "Iteration 150, loss = 0.45637400\n",
            "Iteration 151, loss = 0.45738062\n",
            "Iteration 152, loss = 0.45673926\n",
            "Iteration 153, loss = 0.45432851\n",
            "Iteration 154, loss = 0.45561840\n",
            "Iteration 155, loss = 0.45573120\n",
            "Iteration 156, loss = 0.45511602\n",
            "Iteration 157, loss = 0.45396266\n",
            "Iteration 158, loss = 0.45703000\n",
            "Iteration 159, loss = 0.45205032\n",
            "Iteration 160, loss = 0.45181513\n",
            "Iteration 161, loss = 0.45337970\n",
            "Iteration 162, loss = 0.45210888\n",
            "Iteration 163, loss = 0.45246126\n",
            "Iteration 164, loss = 0.45982796\n",
            "Iteration 165, loss = 0.45671538\n",
            "Iteration 166, loss = 0.45619953\n",
            "Iteration 167, loss = 0.46491576\n",
            "Iteration 168, loss = 0.45230790\n",
            "Iteration 169, loss = 0.45124629\n",
            "Iteration 170, loss = 0.45200775\n",
            "Iteration 171, loss = 0.44979065\n",
            "Iteration 172, loss = 0.44880209\n",
            "Iteration 173, loss = 0.45179913\n",
            "Iteration 174, loss = 0.44910856\n",
            "Iteration 175, loss = 0.45113636\n",
            "Iteration 176, loss = 0.45087167\n",
            "Iteration 177, loss = 0.45073614\n",
            "Iteration 178, loss = 0.45007733\n",
            "Iteration 179, loss = 0.44798217\n",
            "Iteration 180, loss = 0.45466122\n",
            "Iteration 181, loss = 0.45132158\n",
            "Iteration 182, loss = 0.45200276\n",
            "Iteration 183, loss = 0.46005232\n",
            "Iteration 184, loss = 0.45618634\n",
            "Iteration 185, loss = 0.44957141\n",
            "Iteration 186, loss = 0.45491288\n",
            "Iteration 187, loss = 0.45059765\n",
            "Iteration 188, loss = 0.45323102\n",
            "Iteration 189, loss = 0.45098230\n",
            "Iteration 190, loss = 0.45073028\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.84068350\n",
            "Iteration 2, loss = 0.66163454\n",
            "Iteration 3, loss = 0.62089339\n",
            "Iteration 4, loss = 0.60367884\n",
            "Iteration 5, loss = 0.59758809\n",
            "Iteration 6, loss = 0.59053744\n",
            "Iteration 7, loss = 0.58355859\n",
            "Iteration 8, loss = 0.57575257\n",
            "Iteration 9, loss = 0.56752153\n",
            "Iteration 10, loss = 0.56250510\n",
            "Iteration 11, loss = 0.55940582\n",
            "Iteration 12, loss = 0.55725922\n",
            "Iteration 13, loss = 0.55539372\n",
            "Iteration 14, loss = 0.55261931\n",
            "Iteration 15, loss = 0.55125222\n",
            "Iteration 16, loss = 0.54938656\n",
            "Iteration 17, loss = 0.54723075\n",
            "Iteration 18, loss = 0.54611201\n",
            "Iteration 19, loss = 0.54445852\n",
            "Iteration 20, loss = 0.54260693\n",
            "Iteration 21, loss = 0.54191735\n",
            "Iteration 22, loss = 0.54076356\n",
            "Iteration 23, loss = 0.54072044\n",
            "Iteration 24, loss = 0.53816883\n",
            "Iteration 25, loss = 0.53735542\n",
            "Iteration 26, loss = 0.53648829\n",
            "Iteration 27, loss = 0.53660585\n",
            "Iteration 28, loss = 0.53461130\n",
            "Iteration 29, loss = 0.53387902\n",
            "Iteration 30, loss = 0.53469368\n",
            "Iteration 31, loss = 0.53202572\n",
            "Iteration 32, loss = 0.52926070\n",
            "Iteration 33, loss = 0.52892232\n",
            "Iteration 34, loss = 0.52792346\n",
            "Iteration 35, loss = 0.52631842\n",
            "Iteration 36, loss = 0.52525013\n",
            "Iteration 37, loss = 0.52366796\n",
            "Iteration 38, loss = 0.52165717\n",
            "Iteration 39, loss = 0.52017580\n",
            "Iteration 40, loss = 0.52051896\n",
            "Iteration 41, loss = 0.51962111\n",
            "Iteration 42, loss = 0.51778834\n",
            "Iteration 43, loss = 0.51575968\n",
            "Iteration 44, loss = 0.51410563\n",
            "Iteration 45, loss = 0.51679264\n",
            "Iteration 46, loss = 0.51113185\n",
            "Iteration 47, loss = 0.51272641\n",
            "Iteration 48, loss = 0.51033363\n",
            "Iteration 49, loss = 0.51149875\n",
            "Iteration 50, loss = 0.50972279\n",
            "Iteration 51, loss = 0.50659960\n",
            "Iteration 52, loss = 0.50812025\n",
            "Iteration 53, loss = 0.50583664\n",
            "Iteration 54, loss = 0.50413478\n",
            "Iteration 55, loss = 0.50309056\n",
            "Iteration 56, loss = 0.50207817\n",
            "Iteration 57, loss = 0.50064145\n",
            "Iteration 58, loss = 0.50182169\n",
            "Iteration 59, loss = 0.50027190\n",
            "Iteration 60, loss = 0.50034093\n",
            "Iteration 61, loss = 0.50228740\n",
            "Iteration 62, loss = 0.49936254\n",
            "Iteration 63, loss = 0.49841749\n",
            "Iteration 64, loss = 0.49726227\n",
            "Iteration 65, loss = 0.49574820\n",
            "Iteration 66, loss = 0.49597583\n",
            "Iteration 67, loss = 0.49666907\n",
            "Iteration 68, loss = 0.49781259\n",
            "Iteration 69, loss = 0.49598924\n",
            "Iteration 70, loss = 0.49504260\n",
            "Iteration 71, loss = 0.49511654\n",
            "Iteration 72, loss = 0.49587326\n",
            "Iteration 73, loss = 0.49325396\n",
            "Iteration 74, loss = 0.49171675\n",
            "Iteration 75, loss = 0.49379349\n",
            "Iteration 76, loss = 0.49232947\n",
            "Iteration 77, loss = 0.49154613\n",
            "Iteration 78, loss = 0.48956218\n",
            "Iteration 79, loss = 0.49085117\n",
            "Iteration 80, loss = 0.48951285\n",
            "Iteration 81, loss = 0.48964164\n",
            "Iteration 82, loss = 0.48776003\n",
            "Iteration 83, loss = 0.48866045\n",
            "Iteration 84, loss = 0.48743867\n",
            "Iteration 85, loss = 0.48959427\n",
            "Iteration 86, loss = 0.48893899\n",
            "Iteration 87, loss = 0.48581596\n",
            "Iteration 88, loss = 0.48958197\n",
            "Iteration 89, loss = 0.48734166\n",
            "Iteration 90, loss = 0.48549654\n",
            "Iteration 91, loss = 0.48475242\n",
            "Iteration 92, loss = 0.48860945\n",
            "Iteration 93, loss = 0.48724563\n",
            "Iteration 94, loss = 0.48767697\n",
            "Iteration 95, loss = 0.48842445\n",
            "Iteration 96, loss = 0.48431945\n",
            "Iteration 97, loss = 0.48893577\n",
            "Iteration 98, loss = 0.48836483\n",
            "Iteration 99, loss = 0.48372760\n",
            "Iteration 100, loss = 0.48331204\n",
            "Iteration 101, loss = 0.48457879\n",
            "Iteration 102, loss = 0.48378776\n",
            "Iteration 103, loss = 0.48212422\n",
            "Iteration 104, loss = 0.48130812\n",
            "Iteration 105, loss = 0.48237457\n",
            "Iteration 106, loss = 0.48402408\n",
            "Iteration 107, loss = 0.48117475\n",
            "Iteration 108, loss = 0.48229236\n",
            "Iteration 109, loss = 0.48249911\n",
            "Iteration 110, loss = 0.48217632\n",
            "Iteration 111, loss = 0.48300399\n",
            "Iteration 112, loss = 0.48305660\n",
            "Iteration 113, loss = 0.48623112\n",
            "Iteration 114, loss = 0.48033446\n",
            "Iteration 115, loss = 0.48255182\n",
            "Iteration 116, loss = 0.48329862\n",
            "Iteration 117, loss = 0.48245303\n",
            "Iteration 118, loss = 0.47916846\n",
            "Iteration 119, loss = 0.48122489\n",
            "Iteration 120, loss = 0.48029741\n",
            "Iteration 121, loss = 0.48173266\n",
            "Iteration 122, loss = 0.48009463\n",
            "Iteration 123, loss = 0.48139827\n",
            "Iteration 124, loss = 0.48397352\n",
            "Iteration 125, loss = 0.48144800\n",
            "Iteration 126, loss = 0.47902017\n",
            "Iteration 127, loss = 0.48260405\n",
            "Iteration 128, loss = 0.48030403\n",
            "Iteration 129, loss = 0.48113867\n",
            "Iteration 130, loss = 0.48312732\n",
            "Iteration 131, loss = 0.48231551\n",
            "Iteration 132, loss = 0.48290192\n",
            "Iteration 133, loss = 0.48060189\n",
            "Iteration 134, loss = 0.47852087\n",
            "Iteration 135, loss = 0.47857566\n",
            "Iteration 136, loss = 0.47896620\n",
            "Iteration 137, loss = 0.48044220\n",
            "Iteration 138, loss = 0.48120303\n",
            "Iteration 139, loss = 0.48065450\n",
            "Iteration 140, loss = 0.48012169\n",
            "Iteration 141, loss = 0.48184580\n",
            "Iteration 142, loss = 0.47990577\n",
            "Iteration 143, loss = 0.47887212\n",
            "Iteration 144, loss = 0.47922702\n",
            "Iteration 145, loss = 0.47890527\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 250 and for layer number 5 : 0.68\n",
            "Iteration 1, loss = 1.17918922\n",
            "Iteration 2, loss = 0.68685959\n",
            "Iteration 3, loss = 0.66908384\n",
            "Iteration 4, loss = 0.63908765\n",
            "Iteration 5, loss = 0.59631417\n",
            "Iteration 6, loss = 0.57411322\n",
            "Iteration 7, loss = 0.56515132\n",
            "Iteration 8, loss = 0.56235259\n",
            "Iteration 9, loss = 0.55879602\n",
            "Iteration 10, loss = 0.55343311\n",
            "Iteration 11, loss = 0.54937387\n",
            "Iteration 12, loss = 0.54565812\n",
            "Iteration 13, loss = 0.54212281\n",
            "Iteration 14, loss = 0.53924739\n",
            "Iteration 15, loss = 0.53679909\n",
            "Iteration 16, loss = 0.53377760\n",
            "Iteration 17, loss = 0.53062494\n",
            "Iteration 18, loss = 0.52882499\n",
            "Iteration 19, loss = 0.52691208\n",
            "Iteration 20, loss = 0.52429166\n",
            "Iteration 21, loss = 0.52287004\n",
            "Iteration 22, loss = 0.52153317\n",
            "Iteration 23, loss = 0.52034916\n",
            "Iteration 24, loss = 0.51827497\n",
            "Iteration 25, loss = 0.51660905\n",
            "Iteration 26, loss = 0.51371597\n",
            "Iteration 27, loss = 0.51319660\n",
            "Iteration 28, loss = 0.51266851\n",
            "Iteration 29, loss = 0.51153150\n",
            "Iteration 30, loss = 0.51012592\n",
            "Iteration 31, loss = 0.50761414\n",
            "Iteration 32, loss = 0.50575164\n",
            "Iteration 33, loss = 0.50502220\n",
            "Iteration 34, loss = 0.50257502\n",
            "Iteration 35, loss = 0.50116088\n",
            "Iteration 36, loss = 0.50085492\n",
            "Iteration 37, loss = 0.50006507\n",
            "Iteration 38, loss = 0.50045593\n",
            "Iteration 39, loss = 0.49817018\n",
            "Iteration 40, loss = 0.49616517\n",
            "Iteration 41, loss = 0.49493665\n",
            "Iteration 42, loss = 0.49382371\n",
            "Iteration 43, loss = 0.49116192\n",
            "Iteration 44, loss = 0.49137707\n",
            "Iteration 45, loss = 0.49019324\n",
            "Iteration 46, loss = 0.48951993\n",
            "Iteration 47, loss = 0.48910536\n",
            "Iteration 48, loss = 0.48741551\n",
            "Iteration 49, loss = 0.48718603\n",
            "Iteration 50, loss = 0.48788111\n",
            "Iteration 51, loss = 0.48658319\n",
            "Iteration 52, loss = 0.48428395\n",
            "Iteration 53, loss = 0.48301730\n",
            "Iteration 54, loss = 0.48312539\n",
            "Iteration 55, loss = 0.48457488\n",
            "Iteration 56, loss = 0.48226929\n",
            "Iteration 57, loss = 0.48317217\n",
            "Iteration 58, loss = 0.48192920\n",
            "Iteration 59, loss = 0.48146113\n",
            "Iteration 60, loss = 0.47980896\n",
            "Iteration 61, loss = 0.48194676\n",
            "Iteration 62, loss = 0.48177424\n",
            "Iteration 63, loss = 0.47940223\n",
            "Iteration 64, loss = 0.47986086\n",
            "Iteration 65, loss = 0.47906337\n",
            "Iteration 66, loss = 0.48176154\n",
            "Iteration 67, loss = 0.47980551\n",
            "Iteration 68, loss = 0.47421164\n",
            "Iteration 69, loss = 0.47866929\n",
            "Iteration 70, loss = 0.47818193\n",
            "Iteration 71, loss = 0.47648211\n",
            "Iteration 72, loss = 0.47809994\n",
            "Iteration 73, loss = 0.47671165\n",
            "Iteration 74, loss = 0.47807209\n",
            "Iteration 75, loss = 0.47386657\n",
            "Iteration 76, loss = 0.47979068\n",
            "Iteration 77, loss = 0.47641987\n",
            "Iteration 78, loss = 0.47519644\n",
            "Iteration 79, loss = 0.47597674\n",
            "Iteration 80, loss = 0.47752150\n",
            "Iteration 81, loss = 0.47264753\n",
            "Iteration 82, loss = 0.47679573\n",
            "Iteration 83, loss = 0.47137165\n",
            "Iteration 84, loss = 0.47241433\n",
            "Iteration 85, loss = 0.47296123\n",
            "Iteration 86, loss = 0.47121904\n",
            "Iteration 87, loss = 0.46978595\n",
            "Iteration 88, loss = 0.46902987\n",
            "Iteration 89, loss = 0.47068566\n",
            "Iteration 90, loss = 0.47041782\n",
            "Iteration 91, loss = 0.46913431\n",
            "Iteration 92, loss = 0.46712164\n",
            "Iteration 93, loss = 0.46786863\n",
            "Iteration 94, loss = 0.47072284\n",
            "Iteration 95, loss = 0.46650386\n",
            "Iteration 96, loss = 0.46759921\n",
            "Iteration 97, loss = 0.46723491\n",
            "Iteration 98, loss = 0.46798436\n",
            "Iteration 99, loss = 0.46617021\n",
            "Iteration 100, loss = 0.46677471\n",
            "Iteration 101, loss = 0.46883795\n",
            "Iteration 102, loss = 0.46501524\n",
            "Iteration 103, loss = 0.46521559\n",
            "Iteration 104, loss = 0.46640698\n",
            "Iteration 105, loss = 0.46541906\n",
            "Iteration 106, loss = 0.46309067\n",
            "Iteration 107, loss = 0.46109671\n",
            "Iteration 108, loss = 0.46360672\n",
            "Iteration 109, loss = 0.46457365\n",
            "Iteration 110, loss = 0.46823808\n",
            "Iteration 111, loss = 0.46414644\n",
            "Iteration 112, loss = 0.46096768\n",
            "Iteration 113, loss = 0.46169197\n",
            "Iteration 114, loss = 0.46468778\n",
            "Iteration 115, loss = 0.46661842\n",
            "Iteration 116, loss = 0.46006286\n",
            "Iteration 117, loss = 0.45994884\n",
            "Iteration 118, loss = 0.46063935\n",
            "Iteration 119, loss = 0.46146452\n",
            "Iteration 120, loss = 0.46024749\n",
            "Iteration 121, loss = 0.46181429\n",
            "Iteration 122, loss = 0.46016368\n",
            "Iteration 123, loss = 0.45925512\n",
            "Iteration 124, loss = 0.45811744\n",
            "Iteration 125, loss = 0.45703953\n",
            "Iteration 126, loss = 0.45884028\n",
            "Iteration 127, loss = 0.45823976\n",
            "Iteration 128, loss = 0.45926850\n",
            "Iteration 129, loss = 0.45980924\n",
            "Iteration 130, loss = 0.46215638\n",
            "Iteration 131, loss = 0.46053598\n",
            "Iteration 132, loss = 0.45790395\n",
            "Iteration 133, loss = 0.46100678\n",
            "Iteration 134, loss = 0.46189882\n",
            "Iteration 135, loss = 0.45768441\n",
            "Iteration 136, loss = 0.45600856\n",
            "Iteration 137, loss = 0.45665909\n",
            "Iteration 138, loss = 0.45685970\n",
            "Iteration 139, loss = 0.45704522\n",
            "Iteration 140, loss = 0.45598413\n",
            "Iteration 141, loss = 0.45634036\n",
            "Iteration 142, loss = 0.46182268\n",
            "Iteration 143, loss = 0.46050592\n",
            "Iteration 144, loss = 0.45835694\n",
            "Iteration 145, loss = 0.45599864\n",
            "Iteration 146, loss = 0.45600910\n",
            "Iteration 147, loss = 0.45545299\n",
            "Iteration 148, loss = 0.45865224\n",
            "Iteration 149, loss = 0.45750648\n",
            "Iteration 150, loss = 0.45908949\n",
            "Iteration 151, loss = 0.45858047\n",
            "Iteration 152, loss = 0.46115714\n",
            "Iteration 153, loss = 0.45968438\n",
            "Iteration 154, loss = 0.45898212\n",
            "Iteration 155, loss = 0.45578507\n",
            "Iteration 156, loss = 0.46019638\n",
            "Iteration 157, loss = 0.45825232\n",
            "Iteration 158, loss = 0.45621158\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.16748585\n",
            "Iteration 2, loss = 0.68435580\n",
            "Iteration 3, loss = 0.66513531\n",
            "Iteration 4, loss = 0.63465831\n",
            "Iteration 5, loss = 0.59114244\n",
            "Iteration 6, loss = 0.57003818\n",
            "Iteration 7, loss = 0.56114980\n",
            "Iteration 8, loss = 0.55536206\n",
            "Iteration 9, loss = 0.55068060\n",
            "Iteration 10, loss = 0.54421841\n",
            "Iteration 11, loss = 0.53917315\n",
            "Iteration 12, loss = 0.53756731\n",
            "Iteration 13, loss = 0.53401793\n",
            "Iteration 14, loss = 0.53102906\n",
            "Iteration 15, loss = 0.52923240\n",
            "Iteration 16, loss = 0.52842161\n",
            "Iteration 17, loss = 0.52571400\n",
            "Iteration 18, loss = 0.52352226\n",
            "Iteration 19, loss = 0.52236147\n",
            "Iteration 20, loss = 0.52094060\n",
            "Iteration 21, loss = 0.51926264\n",
            "Iteration 22, loss = 0.51803912\n",
            "Iteration 23, loss = 0.51743550\n",
            "Iteration 24, loss = 0.51611098\n",
            "Iteration 25, loss = 0.51484374\n",
            "Iteration 26, loss = 0.51366008\n",
            "Iteration 27, loss = 0.51252271\n",
            "Iteration 28, loss = 0.51171310\n",
            "Iteration 29, loss = 0.51033110\n",
            "Iteration 30, loss = 0.51004992\n",
            "Iteration 31, loss = 0.50854952\n",
            "Iteration 32, loss = 0.50724227\n",
            "Iteration 33, loss = 0.50680377\n",
            "Iteration 34, loss = 0.50552425\n",
            "Iteration 35, loss = 0.50445364\n",
            "Iteration 36, loss = 0.50348245\n",
            "Iteration 37, loss = 0.50306294\n",
            "Iteration 38, loss = 0.50302057\n",
            "Iteration 39, loss = 0.50034962\n",
            "Iteration 40, loss = 0.49685972\n",
            "Iteration 41, loss = 0.49725617\n",
            "Iteration 42, loss = 0.49689740\n",
            "Iteration 43, loss = 0.49322015\n",
            "Iteration 44, loss = 0.49091816\n",
            "Iteration 45, loss = 0.49002955\n",
            "Iteration 46, loss = 0.48930378\n",
            "Iteration 47, loss = 0.48750406\n",
            "Iteration 48, loss = 0.48563755\n",
            "Iteration 49, loss = 0.48510987\n",
            "Iteration 50, loss = 0.48263117\n",
            "Iteration 51, loss = 0.48163564\n",
            "Iteration 52, loss = 0.48019692\n",
            "Iteration 53, loss = 0.47899467\n",
            "Iteration 54, loss = 0.47797759\n",
            "Iteration 55, loss = 0.47806081\n",
            "Iteration 56, loss = 0.47472758\n",
            "Iteration 57, loss = 0.47475328\n",
            "Iteration 58, loss = 0.48037708\n",
            "Iteration 59, loss = 0.47454856\n",
            "Iteration 60, loss = 0.47150624\n",
            "Iteration 61, loss = 0.47368171\n",
            "Iteration 62, loss = 0.47159185\n",
            "Iteration 63, loss = 0.47018719\n",
            "Iteration 64, loss = 0.47013049\n",
            "Iteration 65, loss = 0.46900835\n",
            "Iteration 66, loss = 0.47222578\n",
            "Iteration 67, loss = 0.46624930\n",
            "Iteration 68, loss = 0.46616882\n",
            "Iteration 69, loss = 0.46744872\n",
            "Iteration 70, loss = 0.46542103\n",
            "Iteration 71, loss = 0.46563050\n",
            "Iteration 72, loss = 0.46410833\n",
            "Iteration 73, loss = 0.46574532\n",
            "Iteration 74, loss = 0.46582344\n",
            "Iteration 75, loss = 0.46387258\n",
            "Iteration 76, loss = 0.46665653\n",
            "Iteration 77, loss = 0.46445674\n",
            "Iteration 78, loss = 0.45919444\n",
            "Iteration 79, loss = 0.46157350\n",
            "Iteration 80, loss = 0.45949043\n",
            "Iteration 81, loss = 0.45832037\n",
            "Iteration 82, loss = 0.46105550\n",
            "Iteration 83, loss = 0.45602345\n",
            "Iteration 84, loss = 0.45494166\n",
            "Iteration 85, loss = 0.45595838\n",
            "Iteration 86, loss = 0.45507863\n",
            "Iteration 87, loss = 0.45255795\n",
            "Iteration 88, loss = 0.45389850\n",
            "Iteration 89, loss = 0.45556347\n",
            "Iteration 90, loss = 0.45402081\n",
            "Iteration 91, loss = 0.45350112\n",
            "Iteration 92, loss = 0.45323924\n",
            "Iteration 93, loss = 0.45354713\n",
            "Iteration 94, loss = 0.45469343\n",
            "Iteration 95, loss = 0.45434926\n",
            "Iteration 96, loss = 0.45160621\n",
            "Iteration 97, loss = 0.45766711\n",
            "Iteration 98, loss = 0.45407384\n",
            "Iteration 99, loss = 0.45107557\n",
            "Iteration 100, loss = 0.45150522\n",
            "Iteration 101, loss = 0.45337602\n",
            "Iteration 102, loss = 0.45051003\n",
            "Iteration 103, loss = 0.45141026\n",
            "Iteration 104, loss = 0.45372546\n",
            "Iteration 105, loss = 0.45051399\n",
            "Iteration 106, loss = 0.45271310\n",
            "Iteration 107, loss = 0.45451704\n",
            "Iteration 108, loss = 0.45499414\n",
            "Iteration 109, loss = 0.45433798\n",
            "Iteration 110, loss = 0.45173734\n",
            "Iteration 111, loss = 0.45229799\n",
            "Iteration 112, loss = 0.45241619\n",
            "Iteration 113, loss = 0.44668247\n",
            "Iteration 114, loss = 0.45128533\n",
            "Iteration 115, loss = 0.45399400\n",
            "Iteration 116, loss = 0.45011937\n",
            "Iteration 117, loss = 0.44890083\n",
            "Iteration 118, loss = 0.45017289\n",
            "Iteration 119, loss = 0.44728390\n",
            "Iteration 120, loss = 0.45071777\n",
            "Iteration 121, loss = 0.44625580\n",
            "Iteration 122, loss = 0.44767060\n",
            "Iteration 123, loss = 0.44902646\n",
            "Iteration 124, loss = 0.44515681\n",
            "Iteration 125, loss = 0.44702235\n",
            "Iteration 126, loss = 0.44793449\n",
            "Iteration 127, loss = 0.45023562\n",
            "Iteration 128, loss = 0.45075530\n",
            "Iteration 129, loss = 0.45404378\n",
            "Iteration 130, loss = 0.44971471\n",
            "Iteration 131, loss = 0.44851051\n",
            "Iteration 132, loss = 0.44745229\n",
            "Iteration 133, loss = 0.45351603\n",
            "Iteration 134, loss = 0.45378934\n",
            "Iteration 135, loss = 0.44528727\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.16546862\n",
            "Iteration 2, loss = 0.68633174\n",
            "Iteration 3, loss = 0.66932821\n",
            "Iteration 4, loss = 0.63316451\n",
            "Iteration 5, loss = 0.58313492\n",
            "Iteration 6, loss = 0.56010652\n",
            "Iteration 7, loss = 0.55169390\n",
            "Iteration 8, loss = 0.54727909\n",
            "Iteration 9, loss = 0.54056381\n",
            "Iteration 10, loss = 0.53462264\n",
            "Iteration 11, loss = 0.53074940\n",
            "Iteration 12, loss = 0.52810966\n",
            "Iteration 13, loss = 0.52335327\n",
            "Iteration 14, loss = 0.51907926\n",
            "Iteration 15, loss = 0.51620450\n",
            "Iteration 16, loss = 0.51215303\n",
            "Iteration 17, loss = 0.50909448\n",
            "Iteration 18, loss = 0.50678571\n",
            "Iteration 19, loss = 0.50463134\n",
            "Iteration 20, loss = 0.50275679\n",
            "Iteration 21, loss = 0.50193707\n",
            "Iteration 22, loss = 0.50067535\n",
            "Iteration 23, loss = 0.49933591\n",
            "Iteration 24, loss = 0.49650228\n",
            "Iteration 25, loss = 0.49555558\n",
            "Iteration 26, loss = 0.49408503\n",
            "Iteration 27, loss = 0.49212077\n",
            "Iteration 28, loss = 0.49039763\n",
            "Iteration 29, loss = 0.49091874\n",
            "Iteration 30, loss = 0.48948791\n",
            "Iteration 31, loss = 0.48581534\n",
            "Iteration 32, loss = 0.48869327\n",
            "Iteration 33, loss = 0.48897904\n",
            "Iteration 34, loss = 0.48535231\n",
            "Iteration 35, loss = 0.48291656\n",
            "Iteration 36, loss = 0.48277159\n",
            "Iteration 37, loss = 0.48203690\n",
            "Iteration 38, loss = 0.48011133\n",
            "Iteration 39, loss = 0.47814519\n",
            "Iteration 40, loss = 0.47752528\n",
            "Iteration 41, loss = 0.47808257\n",
            "Iteration 42, loss = 0.47548653\n",
            "Iteration 43, loss = 0.47488039\n",
            "Iteration 44, loss = 0.47215938\n",
            "Iteration 45, loss = 0.47241277\n",
            "Iteration 46, loss = 0.47041245\n",
            "Iteration 47, loss = 0.46855368\n",
            "Iteration 48, loss = 0.46766108\n",
            "Iteration 49, loss = 0.46726617\n",
            "Iteration 50, loss = 0.46619583\n",
            "Iteration 51, loss = 0.46808418\n",
            "Iteration 52, loss = 0.46650417\n",
            "Iteration 53, loss = 0.46421970\n",
            "Iteration 54, loss = 0.46349735\n",
            "Iteration 55, loss = 0.46203259\n",
            "Iteration 56, loss = 0.45977442\n",
            "Iteration 57, loss = 0.46027057\n",
            "Iteration 58, loss = 0.46988778\n",
            "Iteration 59, loss = 0.46233902\n",
            "Iteration 60, loss = 0.45988533\n",
            "Iteration 61, loss = 0.46002719\n",
            "Iteration 62, loss = 0.45684772\n",
            "Iteration 63, loss = 0.45614815\n",
            "Iteration 64, loss = 0.45659778\n",
            "Iteration 65, loss = 0.45477147\n",
            "Iteration 66, loss = 0.45365227\n",
            "Iteration 67, loss = 0.45417324\n",
            "Iteration 68, loss = 0.45558637\n",
            "Iteration 69, loss = 0.45327549\n",
            "Iteration 70, loss = 0.45172777\n",
            "Iteration 71, loss = 0.45299898\n",
            "Iteration 72, loss = 0.44959190\n",
            "Iteration 73, loss = 0.45170402\n",
            "Iteration 74, loss = 0.44775769\n",
            "Iteration 75, loss = 0.45124258\n",
            "Iteration 76, loss = 0.45279056\n",
            "Iteration 77, loss = 0.45367221\n",
            "Iteration 78, loss = 0.45209065\n",
            "Iteration 79, loss = 0.44550968\n",
            "Iteration 80, loss = 0.44876667\n",
            "Iteration 81, loss = 0.45109419\n",
            "Iteration 82, loss = 0.44660740\n",
            "Iteration 83, loss = 0.44514269\n",
            "Iteration 84, loss = 0.44699843\n",
            "Iteration 85, loss = 0.44512059\n",
            "Iteration 86, loss = 0.44512139\n",
            "Iteration 87, loss = 0.44485383\n",
            "Iteration 88, loss = 0.44676667\n",
            "Iteration 89, loss = 0.44270317\n",
            "Iteration 90, loss = 0.44603391\n",
            "Iteration 91, loss = 0.44541570\n",
            "Iteration 92, loss = 0.44201438\n",
            "Iteration 93, loss = 0.44779921\n",
            "Iteration 94, loss = 0.44237476\n",
            "Iteration 95, loss = 0.44077190\n",
            "Iteration 96, loss = 0.44335455\n",
            "Iteration 97, loss = 0.44102656\n",
            "Iteration 98, loss = 0.44041186\n",
            "Iteration 99, loss = 0.44373161\n",
            "Iteration 100, loss = 0.44043808\n",
            "Iteration 101, loss = 0.44140721\n",
            "Iteration 102, loss = 0.44211328\n",
            "Iteration 103, loss = 0.44206878\n",
            "Iteration 104, loss = 0.44635574\n",
            "Iteration 105, loss = 0.44078835\n",
            "Iteration 106, loss = 0.43766046\n",
            "Iteration 107, loss = 0.44037017\n",
            "Iteration 108, loss = 0.44718351\n",
            "Iteration 109, loss = 0.44289678\n",
            "Iteration 110, loss = 0.43875981\n",
            "Iteration 111, loss = 0.44048935\n",
            "Iteration 112, loss = 0.44324779\n",
            "Iteration 113, loss = 0.43909179\n",
            "Iteration 114, loss = 0.44425033\n",
            "Iteration 115, loss = 0.43701218\n",
            "Iteration 116, loss = 0.44133170\n",
            "Iteration 117, loss = 0.43576260\n",
            "Iteration 118, loss = 0.43681952\n",
            "Iteration 119, loss = 0.44194694\n",
            "Iteration 120, loss = 0.44437692\n",
            "Iteration 121, loss = 0.43976692\n",
            "Iteration 122, loss = 0.44073781\n",
            "Iteration 123, loss = 0.43758767\n",
            "Iteration 124, loss = 0.43395733\n",
            "Iteration 125, loss = 0.43618067\n",
            "Iteration 126, loss = 0.44092761\n",
            "Iteration 127, loss = 0.44061005\n",
            "Iteration 128, loss = 0.43970486\n",
            "Iteration 129, loss = 0.43423663\n",
            "Iteration 130, loss = 0.43730267\n",
            "Iteration 131, loss = 0.43944881\n",
            "Iteration 132, loss = 0.43414165\n",
            "Iteration 133, loss = 0.43911078\n",
            "Iteration 134, loss = 0.43964610\n",
            "Iteration 135, loss = 0.43899461\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.17232581\n",
            "Iteration 2, loss = 0.68571124\n",
            "Iteration 3, loss = 0.67888710\n",
            "Iteration 4, loss = 0.64673825\n",
            "Iteration 5, loss = 0.59794892\n",
            "Iteration 6, loss = 0.57312271\n",
            "Iteration 7, loss = 0.56466572\n",
            "Iteration 8, loss = 0.56229608\n",
            "Iteration 9, loss = 0.55561310\n",
            "Iteration 10, loss = 0.54912067\n",
            "Iteration 11, loss = 0.54482135\n",
            "Iteration 12, loss = 0.54181354\n",
            "Iteration 13, loss = 0.53790741\n",
            "Iteration 14, loss = 0.53433278\n",
            "Iteration 15, loss = 0.52898001\n",
            "Iteration 16, loss = 0.52559670\n",
            "Iteration 17, loss = 0.52245148\n",
            "Iteration 18, loss = 0.52076866\n",
            "Iteration 19, loss = 0.51855342\n",
            "Iteration 20, loss = 0.51651247\n",
            "Iteration 21, loss = 0.51437681\n",
            "Iteration 22, loss = 0.51293226\n",
            "Iteration 23, loss = 0.51158435\n",
            "Iteration 24, loss = 0.51007435\n",
            "Iteration 25, loss = 0.50918096\n",
            "Iteration 26, loss = 0.50769243\n",
            "Iteration 27, loss = 0.50590206\n",
            "Iteration 28, loss = 0.50344794\n",
            "Iteration 29, loss = 0.50376017\n",
            "Iteration 30, loss = 0.50259131\n",
            "Iteration 31, loss = 0.49779779\n",
            "Iteration 32, loss = 0.50038877\n",
            "Iteration 33, loss = 0.49986861\n",
            "Iteration 34, loss = 0.49659424\n",
            "Iteration 35, loss = 0.49537589\n",
            "Iteration 36, loss = 0.49442566\n",
            "Iteration 37, loss = 0.49355529\n",
            "Iteration 38, loss = 0.49079018\n",
            "Iteration 39, loss = 0.49320428\n",
            "Iteration 40, loss = 0.49091291\n",
            "Iteration 41, loss = 0.49177786\n",
            "Iteration 42, loss = 0.48780397\n",
            "Iteration 43, loss = 0.48707865\n",
            "Iteration 44, loss = 0.48403883\n",
            "Iteration 45, loss = 0.48323445\n",
            "Iteration 46, loss = 0.48137168\n",
            "Iteration 47, loss = 0.48031872\n",
            "Iteration 48, loss = 0.47925227\n",
            "Iteration 49, loss = 0.47750569\n",
            "Iteration 50, loss = 0.47614650\n",
            "Iteration 51, loss = 0.47494394\n",
            "Iteration 52, loss = 0.47613063\n",
            "Iteration 53, loss = 0.47203151\n",
            "Iteration 54, loss = 0.47099025\n",
            "Iteration 55, loss = 0.47014201\n",
            "Iteration 56, loss = 0.46862588\n",
            "Iteration 57, loss = 0.46814231\n",
            "Iteration 58, loss = 0.46903360\n",
            "Iteration 59, loss = 0.46719988\n",
            "Iteration 60, loss = 0.46525691\n",
            "Iteration 61, loss = 0.46857972\n",
            "Iteration 62, loss = 0.46629160\n",
            "Iteration 63, loss = 0.46516734\n",
            "Iteration 64, loss = 0.46558111\n",
            "Iteration 65, loss = 0.46518520\n",
            "Iteration 66, loss = 0.46280131\n",
            "Iteration 67, loss = 0.46380183\n",
            "Iteration 68, loss = 0.46591917\n",
            "Iteration 69, loss = 0.46315412\n",
            "Iteration 70, loss = 0.46036406\n",
            "Iteration 71, loss = 0.46176288\n",
            "Iteration 72, loss = 0.45721844\n",
            "Iteration 73, loss = 0.46086504\n",
            "Iteration 74, loss = 0.45675920\n",
            "Iteration 75, loss = 0.45976963\n",
            "Iteration 76, loss = 0.45768641\n",
            "Iteration 77, loss = 0.45566741\n",
            "Iteration 78, loss = 0.45668412\n",
            "Iteration 79, loss = 0.45701608\n",
            "Iteration 80, loss = 0.45535166\n",
            "Iteration 81, loss = 0.45516166\n",
            "Iteration 82, loss = 0.45334298\n",
            "Iteration 83, loss = 0.45488815\n",
            "Iteration 84, loss = 0.45266330\n",
            "Iteration 85, loss = 0.45515735\n",
            "Iteration 86, loss = 0.45433561\n",
            "Iteration 87, loss = 0.45214455\n",
            "Iteration 88, loss = 0.45610070\n",
            "Iteration 89, loss = 0.45281270\n",
            "Iteration 90, loss = 0.45580547\n",
            "Iteration 91, loss = 0.45791238\n",
            "Iteration 92, loss = 0.45517448\n",
            "Iteration 93, loss = 0.45607128\n",
            "Iteration 94, loss = 0.45501342\n",
            "Iteration 95, loss = 0.45437550\n",
            "Iteration 96, loss = 0.45452545\n",
            "Iteration 97, loss = 0.45300895\n",
            "Iteration 98, loss = 0.45116718\n",
            "Iteration 99, loss = 0.45147916\n",
            "Iteration 100, loss = 0.45130028\n",
            "Iteration 101, loss = 0.45260655\n",
            "Iteration 102, loss = 0.45125205\n",
            "Iteration 103, loss = 0.45082029\n",
            "Iteration 104, loss = 0.45389515\n",
            "Iteration 105, loss = 0.45264845\n",
            "Iteration 106, loss = 0.45003545\n",
            "Iteration 107, loss = 0.45175451\n",
            "Iteration 108, loss = 0.45140976\n",
            "Iteration 109, loss = 0.45340609\n",
            "Iteration 110, loss = 0.45144869\n",
            "Iteration 111, loss = 0.45279922\n",
            "Iteration 112, loss = 0.45128596\n",
            "Iteration 113, loss = 0.44817011\n",
            "Iteration 114, loss = 0.44694340\n",
            "Iteration 115, loss = 0.44726506\n",
            "Iteration 116, loss = 0.44720818\n",
            "Iteration 117, loss = 0.44637300\n",
            "Iteration 118, loss = 0.44430881\n",
            "Iteration 119, loss = 0.44673647\n",
            "Iteration 120, loss = 0.44947592\n",
            "Iteration 121, loss = 0.44690117\n",
            "Iteration 122, loss = 0.44659092\n",
            "Iteration 123, loss = 0.44429407\n",
            "Iteration 124, loss = 0.44387008\n",
            "Iteration 125, loss = 0.44271516\n",
            "Iteration 126, loss = 0.44786302\n",
            "Iteration 127, loss = 0.44939677\n",
            "Iteration 128, loss = 0.44589560\n",
            "Iteration 129, loss = 0.44260942\n",
            "Iteration 130, loss = 0.44243635\n",
            "Iteration 131, loss = 0.44216317\n",
            "Iteration 132, loss = 0.44120818\n",
            "Iteration 133, loss = 0.44414177\n",
            "Iteration 134, loss = 0.44233022\n",
            "Iteration 135, loss = 0.44012408\n",
            "Iteration 136, loss = 0.44117582\n",
            "Iteration 137, loss = 0.43969211\n",
            "Iteration 138, loss = 0.44123941\n",
            "Iteration 139, loss = 0.43827430\n",
            "Iteration 140, loss = 0.43912199\n",
            "Iteration 141, loss = 0.44022162\n",
            "Iteration 142, loss = 0.43625789\n",
            "Iteration 143, loss = 0.44038959\n",
            "Iteration 144, loss = 0.44088420\n",
            "Iteration 145, loss = 0.43830682\n",
            "Iteration 146, loss = 0.43847359\n",
            "Iteration 147, loss = 0.43895210\n",
            "Iteration 148, loss = 0.43877769\n",
            "Iteration 149, loss = 0.44392436\n",
            "Iteration 150, loss = 0.44263141\n",
            "Iteration 151, loss = 0.43916214\n",
            "Iteration 152, loss = 0.43607532\n",
            "Iteration 153, loss = 0.43667055\n",
            "Iteration 154, loss = 0.43676254\n",
            "Iteration 155, loss = 0.43831448\n",
            "Iteration 156, loss = 0.44010960\n",
            "Iteration 157, loss = 0.43901476\n",
            "Iteration 158, loss = 0.43763188\n",
            "Iteration 159, loss = 0.43693828\n",
            "Iteration 160, loss = 0.43797491\n",
            "Iteration 161, loss = 0.43509970\n",
            "Iteration 162, loss = 0.43955267\n",
            "Iteration 163, loss = 0.43812215\n",
            "Iteration 164, loss = 0.43918429\n",
            "Iteration 165, loss = 0.43590246\n",
            "Iteration 166, loss = 0.43934137\n",
            "Iteration 167, loss = 0.44069563\n",
            "Iteration 168, loss = 0.43681832\n",
            "Iteration 169, loss = 0.43923612\n",
            "Iteration 170, loss = 0.43613118\n",
            "Iteration 171, loss = 0.44360665\n",
            "Iteration 172, loss = 0.43694688\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.16332810\n",
            "Iteration 2, loss = 0.68618232\n",
            "Iteration 3, loss = 0.68913900\n",
            "Iteration 4, loss = 0.65398233\n",
            "Iteration 5, loss = 0.60390842\n",
            "Iteration 6, loss = 0.57885852\n",
            "Iteration 7, loss = 0.57025920\n",
            "Iteration 8, loss = 0.56663455\n",
            "Iteration 9, loss = 0.55823330\n",
            "Iteration 10, loss = 0.55248266\n",
            "Iteration 11, loss = 0.54848967\n",
            "Iteration 12, loss = 0.54592930\n",
            "Iteration 13, loss = 0.54329974\n",
            "Iteration 14, loss = 0.54033888\n",
            "Iteration 15, loss = 0.53572275\n",
            "Iteration 16, loss = 0.53346657\n",
            "Iteration 17, loss = 0.53074143\n",
            "Iteration 18, loss = 0.52813994\n",
            "Iteration 19, loss = 0.52519975\n",
            "Iteration 20, loss = 0.52278328\n",
            "Iteration 21, loss = 0.52152420\n",
            "Iteration 22, loss = 0.52051844\n",
            "Iteration 23, loss = 0.51838123\n",
            "Iteration 24, loss = 0.51528032\n",
            "Iteration 25, loss = 0.51406820\n",
            "Iteration 26, loss = 0.51326190\n",
            "Iteration 27, loss = 0.51152712\n",
            "Iteration 28, loss = 0.50905238\n",
            "Iteration 29, loss = 0.50776352\n",
            "Iteration 30, loss = 0.50738792\n",
            "Iteration 31, loss = 0.50674779\n",
            "Iteration 32, loss = 0.50443804\n",
            "Iteration 33, loss = 0.50413038\n",
            "Iteration 34, loss = 0.50267196\n",
            "Iteration 35, loss = 0.50345086\n",
            "Iteration 36, loss = 0.50351132\n",
            "Iteration 37, loss = 0.50228991\n",
            "Iteration 38, loss = 0.50019602\n",
            "Iteration 39, loss = 0.50010008\n",
            "Iteration 40, loss = 0.49891127\n",
            "Iteration 41, loss = 0.49782462\n",
            "Iteration 42, loss = 0.49613568\n",
            "Iteration 43, loss = 0.49638432\n",
            "Iteration 44, loss = 0.49150995\n",
            "Iteration 45, loss = 0.49590998\n",
            "Iteration 46, loss = 0.49315333\n",
            "Iteration 47, loss = 0.49098981\n",
            "Iteration 48, loss = 0.48868236\n",
            "Iteration 49, loss = 0.48880587\n",
            "Iteration 50, loss = 0.48782580\n",
            "Iteration 51, loss = 0.48398444\n",
            "Iteration 52, loss = 0.48677887\n",
            "Iteration 53, loss = 0.48429541\n",
            "Iteration 54, loss = 0.48259965\n",
            "Iteration 55, loss = 0.48282400\n",
            "Iteration 56, loss = 0.48193661\n",
            "Iteration 57, loss = 0.48142843\n",
            "Iteration 58, loss = 0.48238175\n",
            "Iteration 59, loss = 0.48125050\n",
            "Iteration 60, loss = 0.48075630\n",
            "Iteration 61, loss = 0.48005422\n",
            "Iteration 62, loss = 0.47693954\n",
            "Iteration 63, loss = 0.47850162\n",
            "Iteration 64, loss = 0.47706809\n",
            "Iteration 65, loss = 0.47560735\n",
            "Iteration 66, loss = 0.47619349\n",
            "Iteration 67, loss = 0.47423727\n",
            "Iteration 68, loss = 0.47714908\n",
            "Iteration 69, loss = 0.47224118\n",
            "Iteration 70, loss = 0.47332061\n",
            "Iteration 71, loss = 0.47321793\n",
            "Iteration 72, loss = 0.47258821\n",
            "Iteration 73, loss = 0.47189953\n",
            "Iteration 74, loss = 0.47093795\n",
            "Iteration 75, loss = 0.47350687\n",
            "Iteration 76, loss = 0.47030472\n",
            "Iteration 77, loss = 0.46734921\n",
            "Iteration 78, loss = 0.46952657\n",
            "Iteration 79, loss = 0.47024198\n",
            "Iteration 80, loss = 0.46871231\n",
            "Iteration 81, loss = 0.46987299\n",
            "Iteration 82, loss = 0.46711412\n",
            "Iteration 83, loss = 0.46694432\n",
            "Iteration 84, loss = 0.46505605\n",
            "Iteration 85, loss = 0.46497067\n",
            "Iteration 86, loss = 0.46634769\n",
            "Iteration 87, loss = 0.46120770\n",
            "Iteration 88, loss = 0.46237763\n",
            "Iteration 89, loss = 0.46308429\n",
            "Iteration 90, loss = 0.46505227\n",
            "Iteration 91, loss = 0.46288208\n",
            "Iteration 92, loss = 0.45916148\n",
            "Iteration 93, loss = 0.45643054\n",
            "Iteration 94, loss = 0.45848188\n",
            "Iteration 95, loss = 0.45912885\n",
            "Iteration 96, loss = 0.45837258\n",
            "Iteration 97, loss = 0.45821094\n",
            "Iteration 98, loss = 0.45545213\n",
            "Iteration 99, loss = 0.45392220\n",
            "Iteration 100, loss = 0.45374146\n",
            "Iteration 101, loss = 0.45642120\n",
            "Iteration 102, loss = 0.45451473\n",
            "Iteration 103, loss = 0.45299290\n",
            "Iteration 104, loss = 0.45309323\n",
            "Iteration 105, loss = 0.45222596\n",
            "Iteration 106, loss = 0.45629947\n",
            "Iteration 107, loss = 0.45483009\n",
            "Iteration 108, loss = 0.45190912\n",
            "Iteration 109, loss = 0.45302203\n",
            "Iteration 110, loss = 0.45154488\n",
            "Iteration 111, loss = 0.45372197\n",
            "Iteration 112, loss = 0.45328828\n",
            "Iteration 113, loss = 0.45380653\n",
            "Iteration 114, loss = 0.44976554\n",
            "Iteration 115, loss = 0.45205339\n",
            "Iteration 116, loss = 0.44973564\n",
            "Iteration 117, loss = 0.44918149\n",
            "Iteration 118, loss = 0.44854880\n",
            "Iteration 119, loss = 0.45234790\n",
            "Iteration 120, loss = 0.45286082\n",
            "Iteration 121, loss = 0.45114024\n",
            "Iteration 122, loss = 0.44876960\n",
            "Iteration 123, loss = 0.44987772\n",
            "Iteration 124, loss = 0.44799226\n",
            "Iteration 125, loss = 0.45073528\n",
            "Iteration 126, loss = 0.45691647\n",
            "Iteration 127, loss = 0.44963993\n",
            "Iteration 128, loss = 0.44748313\n",
            "Iteration 129, loss = 0.45213793\n",
            "Iteration 130, loss = 0.44700938\n",
            "Iteration 131, loss = 0.45288740\n",
            "Iteration 132, loss = 0.44719226\n",
            "Iteration 133, loss = 0.44984257\n",
            "Iteration 134, loss = 0.45191281\n",
            "Iteration 135, loss = 0.44697611\n",
            "Iteration 136, loss = 0.44616846\n",
            "Iteration 137, loss = 0.45207069\n",
            "Iteration 138, loss = 0.44800316\n",
            "Iteration 139, loss = 0.44725680\n",
            "Iteration 140, loss = 0.44956945\n",
            "Iteration 141, loss = 0.45020556\n",
            "Iteration 142, loss = 0.44446584\n",
            "Iteration 143, loss = 0.44366399\n",
            "Iteration 144, loss = 0.44138145\n",
            "Iteration 145, loss = 0.44428545\n",
            "Iteration 146, loss = 0.44786137\n",
            "Iteration 147, loss = 0.44710731\n",
            "Iteration 148, loss = 0.44413009\n",
            "Iteration 149, loss = 0.45815286\n",
            "Iteration 150, loss = 0.44439585\n",
            "Iteration 151, loss = 0.44270242\n",
            "Iteration 152, loss = 0.44372411\n",
            "Iteration 153, loss = 0.44328977\n",
            "Iteration 154, loss = 0.44452222\n",
            "Iteration 155, loss = 0.45832714\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.1  and epoaches= 250 and for layer number 6 : 0.6875\n",
            "Iteration 1, loss = 0.61686348\n",
            "Iteration 2, loss = 0.61615212\n",
            "Iteration 3, loss = 0.61509889\n",
            "Iteration 4, loss = 0.61393355\n",
            "Iteration 5, loss = 0.61267100\n",
            "Iteration 6, loss = 0.61156262\n",
            "Iteration 7, loss = 0.61039793\n",
            "Iteration 8, loss = 0.60920464\n",
            "Iteration 9, loss = 0.60819776\n",
            "Iteration 10, loss = 0.60727368\n",
            "Iteration 11, loss = 0.60643854\n",
            "Iteration 12, loss = 0.60574605\n",
            "Iteration 13, loss = 0.60503921\n",
            "Iteration 14, loss = 0.60439293\n",
            "Iteration 15, loss = 0.60372900\n",
            "Iteration 16, loss = 0.60312998\n",
            "Iteration 17, loss = 0.60257188\n",
            "Iteration 18, loss = 0.60207421\n",
            "Iteration 19, loss = 0.60156745\n",
            "Iteration 20, loss = 0.60103417\n",
            "Iteration 21, loss = 0.60046005\n",
            "Iteration 22, loss = 0.60001837\n",
            "Iteration 23, loss = 0.59949961\n",
            "Iteration 24, loss = 0.59907076\n",
            "Iteration 25, loss = 0.59862261\n",
            "Iteration 26, loss = 0.59821262\n",
            "Iteration 27, loss = 0.59768043\n",
            "Iteration 28, loss = 0.59727575\n",
            "Iteration 29, loss = 0.59686577\n",
            "Iteration 30, loss = 0.59634456\n",
            "Iteration 31, loss = 0.59592913\n",
            "Iteration 32, loss = 0.59551543\n",
            "Iteration 33, loss = 0.59511982\n",
            "Iteration 34, loss = 0.59488057\n",
            "Iteration 35, loss = 0.59437990\n",
            "Iteration 36, loss = 0.59393766\n",
            "Iteration 37, loss = 0.59343460\n",
            "Iteration 38, loss = 0.59299992\n",
            "Iteration 39, loss = 0.59251347\n",
            "Iteration 40, loss = 0.59210013\n",
            "Iteration 41, loss = 0.59158753\n",
            "Iteration 42, loss = 0.59116433\n",
            "Iteration 43, loss = 0.59067977\n",
            "Iteration 44, loss = 0.59030424\n",
            "Iteration 45, loss = 0.58985025\n",
            "Iteration 46, loss = 0.58934126\n",
            "Iteration 47, loss = 0.58893644\n",
            "Iteration 48, loss = 0.58838509\n",
            "Iteration 49, loss = 0.58789781\n",
            "Iteration 50, loss = 0.58745034\n",
            "Iteration 51, loss = 0.58705463\n",
            "Iteration 52, loss = 0.58659520\n",
            "Iteration 53, loss = 0.58621788\n",
            "Iteration 54, loss = 0.58582979\n",
            "Iteration 55, loss = 0.58542994\n",
            "Iteration 56, loss = 0.58504137\n",
            "Iteration 57, loss = 0.58463168\n",
            "Iteration 58, loss = 0.58429345\n",
            "Iteration 59, loss = 0.58388301\n",
            "Iteration 60, loss = 0.58354039\n",
            "Iteration 61, loss = 0.58310095\n",
            "Iteration 62, loss = 0.58279069\n",
            "Iteration 63, loss = 0.58237879\n",
            "Iteration 64, loss = 0.58204179\n",
            "Iteration 65, loss = 0.58167540\n",
            "Iteration 66, loss = 0.58131958\n",
            "Iteration 67, loss = 0.58107527\n",
            "Iteration 68, loss = 0.58075012\n",
            "Iteration 69, loss = 0.58043709\n",
            "Iteration 70, loss = 0.58009698\n",
            "Iteration 71, loss = 0.57979144\n",
            "Iteration 72, loss = 0.57952207\n",
            "Iteration 73, loss = 0.57918802\n",
            "Iteration 74, loss = 0.57892494\n",
            "Iteration 75, loss = 0.57862554\n",
            "Iteration 76, loss = 0.57835511\n",
            "Iteration 77, loss = 0.57808078\n",
            "Iteration 78, loss = 0.57780786\n",
            "Iteration 79, loss = 0.57753624\n",
            "Iteration 80, loss = 0.57722696\n",
            "Iteration 81, loss = 0.57695431\n",
            "Iteration 82, loss = 0.57660842\n",
            "Iteration 83, loss = 0.57639990\n",
            "Iteration 84, loss = 0.57623494\n",
            "Iteration 85, loss = 0.57594787\n",
            "Iteration 86, loss = 0.57557150\n",
            "Iteration 87, loss = 0.57527755\n",
            "Iteration 88, loss = 0.57490136\n",
            "Iteration 89, loss = 0.57463870\n",
            "Iteration 90, loss = 0.57420350\n",
            "Iteration 91, loss = 0.57382071\n",
            "Iteration 92, loss = 0.57349973\n",
            "Iteration 93, loss = 0.57316240\n",
            "Iteration 94, loss = 0.57277331\n",
            "Iteration 95, loss = 0.57224303\n",
            "Iteration 96, loss = 0.57170654\n",
            "Iteration 97, loss = 0.57127444\n",
            "Iteration 98, loss = 0.57075803\n",
            "Iteration 99, loss = 0.57034183\n",
            "Iteration 100, loss = 0.56991355\n",
            "Iteration 1, loss = 0.61762838\n",
            "Iteration 2, loss = 0.61685222\n",
            "Iteration 3, loss = 0.61571214\n",
            "Iteration 4, loss = 0.61441758\n",
            "Iteration 5, loss = 0.61318349\n",
            "Iteration 6, loss = 0.61195746\n",
            "Iteration 7, loss = 0.61086455\n",
            "Iteration 8, loss = 0.60959350\n",
            "Iteration 9, loss = 0.60852999\n",
            "Iteration 10, loss = 0.60759281\n",
            "Iteration 11, loss = 0.60667038\n",
            "Iteration 12, loss = 0.60588256\n",
            "Iteration 13, loss = 0.60500642\n",
            "Iteration 14, loss = 0.60429505\n",
            "Iteration 15, loss = 0.60355287\n",
            "Iteration 16, loss = 0.60296533\n",
            "Iteration 17, loss = 0.60238875\n",
            "Iteration 18, loss = 0.60183941\n",
            "Iteration 19, loss = 0.60124485\n",
            "Iteration 20, loss = 0.60063271\n",
            "Iteration 21, loss = 0.60004317\n",
            "Iteration 22, loss = 0.59951396\n",
            "Iteration 23, loss = 0.59887438\n",
            "Iteration 24, loss = 0.59834187\n",
            "Iteration 25, loss = 0.59783710\n",
            "Iteration 26, loss = 0.59731217\n",
            "Iteration 27, loss = 0.59680236\n",
            "Iteration 28, loss = 0.59634036\n",
            "Iteration 29, loss = 0.59585225\n",
            "Iteration 30, loss = 0.59540926\n",
            "Iteration 31, loss = 0.59494784\n",
            "Iteration 32, loss = 0.59442520\n",
            "Iteration 33, loss = 0.59397329\n",
            "Iteration 34, loss = 0.59352410\n",
            "Iteration 35, loss = 0.59310715\n",
            "Iteration 36, loss = 0.59259080\n",
            "Iteration 37, loss = 0.59198806\n",
            "Iteration 38, loss = 0.59141069\n",
            "Iteration 39, loss = 0.59091519\n",
            "Iteration 40, loss = 0.59042012\n",
            "Iteration 41, loss = 0.58996507\n",
            "Iteration 42, loss = 0.58947078\n",
            "Iteration 43, loss = 0.58903801\n",
            "Iteration 44, loss = 0.58869976\n",
            "Iteration 45, loss = 0.58832512\n",
            "Iteration 46, loss = 0.58801003\n",
            "Iteration 47, loss = 0.58772939\n",
            "Iteration 48, loss = 0.58749791\n",
            "Iteration 49, loss = 0.58718487\n",
            "Iteration 50, loss = 0.58696384\n",
            "Iteration 51, loss = 0.58679751\n",
            "Iteration 52, loss = 0.58653756\n",
            "Iteration 53, loss = 0.58627720\n",
            "Iteration 54, loss = 0.58609678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 55, loss = 0.58587111\n",
            "Iteration 56, loss = 0.58563655\n",
            "Iteration 57, loss = 0.58540147\n",
            "Iteration 58, loss = 0.58515985\n",
            "Iteration 59, loss = 0.58495083\n",
            "Iteration 60, loss = 0.58467493\n",
            "Iteration 61, loss = 0.58436402\n",
            "Iteration 62, loss = 0.58419131\n",
            "Iteration 63, loss = 0.58386634\n",
            "Iteration 64, loss = 0.58367166\n",
            "Iteration 65, loss = 0.58340516\n",
            "Iteration 66, loss = 0.58311369\n",
            "Iteration 67, loss = 0.58294822\n",
            "Iteration 68, loss = 0.58272389\n",
            "Iteration 69, loss = 0.58248177\n",
            "Iteration 70, loss = 0.58222376\n",
            "Iteration 71, loss = 0.58199437\n",
            "Iteration 72, loss = 0.58183608\n",
            "Iteration 73, loss = 0.58158739\n",
            "Iteration 74, loss = 0.58128332\n",
            "Iteration 75, loss = 0.58109528\n",
            "Iteration 76, loss = 0.58079797\n",
            "Iteration 77, loss = 0.58055032\n",
            "Iteration 78, loss = 0.58038713\n",
            "Iteration 79, loss = 0.58011096\n",
            "Iteration 80, loss = 0.57987169\n",
            "Iteration 81, loss = 0.57960002\n",
            "Iteration 82, loss = 0.57928697\n",
            "Iteration 83, loss = 0.57902631\n",
            "Iteration 84, loss = 0.57873116\n",
            "Iteration 85, loss = 0.57846126\n",
            "Iteration 86, loss = 0.57816501\n",
            "Iteration 87, loss = 0.57791457\n",
            "Iteration 88, loss = 0.57758301\n",
            "Iteration 89, loss = 0.57729328\n",
            "Iteration 90, loss = 0.57700737\n",
            "Iteration 91, loss = 0.57667723\n",
            "Iteration 92, loss = 0.57637588\n",
            "Iteration 93, loss = 0.57608463\n",
            "Iteration 94, loss = 0.57589851\n",
            "Iteration 95, loss = 0.57547867\n",
            "Iteration 96, loss = 0.57515434\n",
            "Iteration 97, loss = 0.57484887\n",
            "Iteration 98, loss = 0.57450644\n",
            "Iteration 99, loss = 0.57423971\n",
            "Iteration 100, loss = 0.57394845\n",
            "Iteration 1, loss = 0.61756281\n",
            "Iteration 2, loss = 0.61678746\n",
            "Iteration 3, loss = 0.61551433\n",
            "Iteration 4, loss = 0.61406466\n",
            "Iteration 5, loss = 0.61275832\n",
            "Iteration 6, loss = 0.61139014\n",
            "Iteration 7, loss = 0.61018275\n",
            "Iteration 8, loss = 0.60882083\n",
            "Iteration 9, loss = 0.60770150\n",
            "Iteration 10, loss = 0.60664596\n",
            "Iteration 11, loss = 0.60580536\n",
            "Iteration 12, loss = 0.60503210\n",
            "Iteration 13, loss = 0.60414541\n",
            "Iteration 14, loss = 0.60345742\n",
            "Iteration 15, loss = 0.60273866\n",
            "Iteration 16, loss = 0.60214876\n",
            "Iteration 17, loss = 0.60155645\n",
            "Iteration 18, loss = 0.60098285\n",
            "Iteration 19, loss = 0.60041376\n",
            "Iteration 20, loss = 0.59981786\n",
            "Iteration 21, loss = 0.59929265\n",
            "Iteration 22, loss = 0.59865761\n",
            "Iteration 23, loss = 0.59789106\n",
            "Iteration 24, loss = 0.59730522\n",
            "Iteration 25, loss = 0.59667276\n",
            "Iteration 26, loss = 0.59602085\n",
            "Iteration 27, loss = 0.59538918\n",
            "Iteration 28, loss = 0.59476517\n",
            "Iteration 29, loss = 0.59420415\n",
            "Iteration 30, loss = 0.59360792\n",
            "Iteration 31, loss = 0.59298919\n",
            "Iteration 32, loss = 0.59233785\n",
            "Iteration 33, loss = 0.59173787\n",
            "Iteration 34, loss = 0.59105348\n",
            "Iteration 35, loss = 0.59039394\n",
            "Iteration 36, loss = 0.58983637\n",
            "Iteration 37, loss = 0.58907064\n",
            "Iteration 38, loss = 0.58833953\n",
            "Iteration 39, loss = 0.58772116\n",
            "Iteration 40, loss = 0.58701155\n",
            "Iteration 41, loss = 0.58632754\n",
            "Iteration 42, loss = 0.58559078\n",
            "Iteration 43, loss = 0.58496621\n",
            "Iteration 44, loss = 0.58428972\n",
            "Iteration 45, loss = 0.58358511\n",
            "Iteration 46, loss = 0.58293101\n",
            "Iteration 47, loss = 0.58227798\n",
            "Iteration 48, loss = 0.58170222\n",
            "Iteration 49, loss = 0.58101512\n",
            "Iteration 50, loss = 0.58043029\n",
            "Iteration 51, loss = 0.57991960\n",
            "Iteration 52, loss = 0.57933529\n",
            "Iteration 53, loss = 0.57870244\n",
            "Iteration 54, loss = 0.57829782\n",
            "Iteration 55, loss = 0.57767445\n",
            "Iteration 56, loss = 0.57715199\n",
            "Iteration 57, loss = 0.57663478\n",
            "Iteration 58, loss = 0.57604727\n",
            "Iteration 59, loss = 0.57548153\n",
            "Iteration 60, loss = 0.57475842\n",
            "Iteration 61, loss = 0.57416184\n",
            "Iteration 62, loss = 0.57353961\n",
            "Iteration 63, loss = 0.57287798\n",
            "Iteration 64, loss = 0.57223335\n",
            "Iteration 65, loss = 0.57144876\n",
            "Iteration 66, loss = 0.57074095\n",
            "Iteration 67, loss = 0.57018441\n",
            "Iteration 68, loss = 0.56959535\n",
            "Iteration 69, loss = 0.56896543\n",
            "Iteration 70, loss = 0.56840160\n",
            "Iteration 71, loss = 0.56782256\n",
            "Iteration 72, loss = 0.56726268\n",
            "Iteration 73, loss = 0.56657180\n",
            "Iteration 74, loss = 0.56591685\n",
            "Iteration 75, loss = 0.56546255\n",
            "Iteration 76, loss = 0.56487449\n",
            "Iteration 77, loss = 0.56432787\n",
            "Iteration 78, loss = 0.56379671\n",
            "Iteration 79, loss = 0.56323004\n",
            "Iteration 80, loss = 0.56263426\n",
            "Iteration 81, loss = 0.56212573\n",
            "Iteration 82, loss = 0.56151629\n",
            "Iteration 83, loss = 0.56097539\n",
            "Iteration 84, loss = 0.56051914\n",
            "Iteration 85, loss = 0.56006355\n",
            "Iteration 86, loss = 0.55956176\n",
            "Iteration 87, loss = 0.55907687\n",
            "Iteration 88, loss = 0.55861192\n",
            "Iteration 89, loss = 0.55803563\n",
            "Iteration 90, loss = 0.55760848\n",
            "Iteration 91, loss = 0.55701483\n",
            "Iteration 92, loss = 0.55655069\n",
            "Iteration 93, loss = 0.55609322\n",
            "Iteration 94, loss = 0.55567123\n",
            "Iteration 95, loss = 0.55502534\n",
            "Iteration 96, loss = 0.55460817\n",
            "Iteration 97, loss = 0.55402209\n",
            "Iteration 98, loss = 0.55355900\n",
            "Iteration 99, loss = 0.55318405\n",
            "Iteration 100, loss = 0.55277358\n",
            "Iteration 1, loss = 0.62079721\n",
            "Iteration 2, loss = 0.62005324\n",
            "Iteration 3, loss = 0.61900678\n",
            "Iteration 4, loss = 0.61766780\n",
            "Iteration 5, loss = 0.61643559\n",
            "Iteration 6, loss = 0.61518085\n",
            "Iteration 7, loss = 0.61403740\n",
            "Iteration 8, loss = 0.61288976\n",
            "Iteration 9, loss = 0.61190780\n",
            "Iteration 10, loss = 0.61098604\n",
            "Iteration 11, loss = 0.61018985\n",
            "Iteration 12, loss = 0.60952147\n",
            "Iteration 13, loss = 0.60886466\n",
            "Iteration 14, loss = 0.60818565\n",
            "Iteration 15, loss = 0.60758049\n",
            "Iteration 16, loss = 0.60707209\n",
            "Iteration 17, loss = 0.60648497\n",
            "Iteration 18, loss = 0.60597777\n",
            "Iteration 19, loss = 0.60548418\n",
            "Iteration 20, loss = 0.60501951\n",
            "Iteration 21, loss = 0.60460897\n",
            "Iteration 22, loss = 0.60410824\n",
            "Iteration 23, loss = 0.60353209\n",
            "Iteration 24, loss = 0.60308578\n",
            "Iteration 25, loss = 0.60261600\n",
            "Iteration 26, loss = 0.60213958\n",
            "Iteration 27, loss = 0.60173715\n",
            "Iteration 28, loss = 0.60121898\n",
            "Iteration 29, loss = 0.60074744\n",
            "Iteration 30, loss = 0.60026547\n",
            "Iteration 31, loss = 0.59976581\n",
            "Iteration 32, loss = 0.59927205\n",
            "Iteration 33, loss = 0.59878003\n",
            "Iteration 34, loss = 0.59832639\n",
            "Iteration 35, loss = 0.59786446\n",
            "Iteration 36, loss = 0.59746968\n",
            "Iteration 37, loss = 0.59693135\n",
            "Iteration 38, loss = 0.59639720\n",
            "Iteration 39, loss = 0.59583464\n",
            "Iteration 40, loss = 0.59533087\n",
            "Iteration 41, loss = 0.59475384\n",
            "Iteration 42, loss = 0.59425957\n",
            "Iteration 43, loss = 0.59373095\n",
            "Iteration 44, loss = 0.59323666\n",
            "Iteration 45, loss = 0.59264518\n",
            "Iteration 46, loss = 0.59213675\n",
            "Iteration 47, loss = 0.59166031\n",
            "Iteration 48, loss = 0.59110663\n",
            "Iteration 49, loss = 0.59053353\n",
            "Iteration 50, loss = 0.58998709\n",
            "Iteration 51, loss = 0.58950475\n",
            "Iteration 52, loss = 0.58904938\n",
            "Iteration 53, loss = 0.58852797\n",
            "Iteration 54, loss = 0.58812035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 55, loss = 0.58762289\n",
            "Iteration 56, loss = 0.58712541\n",
            "Iteration 57, loss = 0.58666104\n",
            "Iteration 58, loss = 0.58615629\n",
            "Iteration 59, loss = 0.58561809\n",
            "Iteration 60, loss = 0.58512577\n",
            "Iteration 61, loss = 0.58466198\n",
            "Iteration 62, loss = 0.58411778\n",
            "Iteration 63, loss = 0.58359615\n",
            "Iteration 64, loss = 0.58300139\n",
            "Iteration 65, loss = 0.58244389\n",
            "Iteration 66, loss = 0.58190597\n",
            "Iteration 67, loss = 0.58141029\n",
            "Iteration 68, loss = 0.58085057\n",
            "Iteration 69, loss = 0.58035273\n",
            "Iteration 70, loss = 0.57970416\n",
            "Iteration 71, loss = 0.57915712\n",
            "Iteration 72, loss = 0.57860550\n",
            "Iteration 73, loss = 0.57797929\n",
            "Iteration 74, loss = 0.57738233\n",
            "Iteration 75, loss = 0.57704390\n",
            "Iteration 76, loss = 0.57639729\n",
            "Iteration 77, loss = 0.57581682\n",
            "Iteration 78, loss = 0.57543752\n",
            "Iteration 79, loss = 0.57485995\n",
            "Iteration 80, loss = 0.57445793\n",
            "Iteration 81, loss = 0.57398971\n",
            "Iteration 82, loss = 0.57361636\n",
            "Iteration 83, loss = 0.57318059\n",
            "Iteration 84, loss = 0.57282423\n",
            "Iteration 85, loss = 0.57252176\n",
            "Iteration 86, loss = 0.57218478\n",
            "Iteration 87, loss = 0.57185258\n",
            "Iteration 88, loss = 0.57147761\n",
            "Iteration 89, loss = 0.57112446\n",
            "Iteration 90, loss = 0.57071716\n",
            "Iteration 91, loss = 0.57028615\n",
            "Iteration 92, loss = 0.56993985\n",
            "Iteration 93, loss = 0.56954471\n",
            "Iteration 94, loss = 0.56924308\n",
            "Iteration 95, loss = 0.56872118\n",
            "Iteration 96, loss = 0.56844243\n",
            "Iteration 97, loss = 0.56803433\n",
            "Iteration 98, loss = 0.56776972\n",
            "Iteration 99, loss = 0.56749363\n",
            "Iteration 100, loss = 0.56718606\n",
            "Iteration 1, loss = 0.62010582\n",
            "Iteration 2, loss = 0.61930514\n",
            "Iteration 3, loss = 0.61826319\n",
            "Iteration 4, loss = 0.61682911\n",
            "Iteration 5, loss = 0.61543360\n",
            "Iteration 6, loss = 0.61413706\n",
            "Iteration 7, loss = 0.61295141\n",
            "Iteration 8, loss = 0.61172441\n",
            "Iteration 9, loss = 0.61081242\n",
            "Iteration 10, loss = 0.60991460\n",
            "Iteration 11, loss = 0.60917637\n",
            "Iteration 12, loss = 0.60858965\n",
            "Iteration 13, loss = 0.60792078\n",
            "Iteration 14, loss = 0.60730029\n",
            "Iteration 15, loss = 0.60678248\n",
            "Iteration 16, loss = 0.60632478\n",
            "Iteration 17, loss = 0.60582791\n",
            "Iteration 18, loss = 0.60531306\n",
            "Iteration 19, loss = 0.60480910\n",
            "Iteration 20, loss = 0.60441098\n",
            "Iteration 21, loss = 0.60409300\n",
            "Iteration 22, loss = 0.60359792\n",
            "Iteration 23, loss = 0.60325513\n",
            "Iteration 24, loss = 0.60288068\n",
            "Iteration 25, loss = 0.60251982\n",
            "Iteration 26, loss = 0.60216641\n",
            "Iteration 27, loss = 0.60186024\n",
            "Iteration 28, loss = 0.60148385\n",
            "Iteration 29, loss = 0.60117667\n",
            "Iteration 30, loss = 0.60087078\n",
            "Iteration 31, loss = 0.60049222\n",
            "Iteration 32, loss = 0.60013302\n",
            "Iteration 33, loss = 0.59970023\n",
            "Iteration 34, loss = 0.59933655\n",
            "Iteration 35, loss = 0.59892240\n",
            "Iteration 36, loss = 0.59855545\n",
            "Iteration 37, loss = 0.59817110\n",
            "Iteration 38, loss = 0.59773733\n",
            "Iteration 39, loss = 0.59731738\n",
            "Iteration 40, loss = 0.59701270\n",
            "Iteration 41, loss = 0.59659154\n",
            "Iteration 42, loss = 0.59622730\n",
            "Iteration 43, loss = 0.59577996\n",
            "Iteration 44, loss = 0.59559981\n",
            "Iteration 45, loss = 0.59513820\n",
            "Iteration 46, loss = 0.59480242\n",
            "Iteration 47, loss = 0.59452497\n",
            "Iteration 48, loss = 0.59421464\n",
            "Iteration 49, loss = 0.59389969\n",
            "Iteration 50, loss = 0.59360376\n",
            "Iteration 51, loss = 0.59338566\n",
            "Iteration 52, loss = 0.59307514\n",
            "Iteration 53, loss = 0.59278326\n",
            "Iteration 54, loss = 0.59252864\n",
            "Iteration 55, loss = 0.59218267\n",
            "Iteration 56, loss = 0.59179838\n",
            "Iteration 57, loss = 0.59144292\n",
            "Iteration 58, loss = 0.59107432\n",
            "Iteration 59, loss = 0.59072772\n",
            "Iteration 60, loss = 0.59033379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 61, loss = 0.58997606\n",
            "Iteration 62, loss = 0.58954634\n",
            "Iteration 63, loss = 0.58924779\n",
            "Iteration 64, loss = 0.58890475\n",
            "Iteration 65, loss = 0.58864831\n",
            "Iteration 66, loss = 0.58836352\n",
            "Iteration 67, loss = 0.58815782\n",
            "Iteration 68, loss = 0.58783305\n",
            "Iteration 69, loss = 0.58756811\n",
            "Iteration 70, loss = 0.58721380\n",
            "Iteration 71, loss = 0.58694958\n",
            "Iteration 72, loss = 0.58664618\n",
            "Iteration 73, loss = 0.58633497\n",
            "Iteration 74, loss = 0.58601464\n",
            "Iteration 75, loss = 0.58576382\n",
            "Iteration 76, loss = 0.58549933\n",
            "Iteration 77, loss = 0.58529067\n",
            "Iteration 78, loss = 0.58500034\n",
            "Iteration 79, loss = 0.58465738\n",
            "Iteration 80, loss = 0.58453297\n",
            "Iteration 81, loss = 0.58413051\n",
            "Iteration 82, loss = 0.58400797\n",
            "Iteration 83, loss = 0.58362854\n",
            "Iteration 84, loss = 0.58342692\n",
            "Iteration 85, loss = 0.58325088\n",
            "Iteration 86, loss = 0.58308466\n",
            "Iteration 87, loss = 0.58297834\n",
            "Iteration 88, loss = 0.58282060\n",
            "Iteration 89, loss = 0.58260746\n",
            "Iteration 90, loss = 0.58236208\n",
            "Iteration 91, loss = 0.58209636\n",
            "Iteration 92, loss = 0.58198327\n",
            "Iteration 93, loss = 0.58176341\n",
            "Iteration 94, loss = 0.58164160\n",
            "Iteration 95, loss = 0.58140872\n",
            "Iteration 96, loss = 0.58129105\n",
            "Iteration 97, loss = 0.58108386\n",
            "Iteration 98, loss = 0.58097283\n",
            "Iteration 99, loss = 0.58070632\n",
            "Iteration 100, loss = 0.58053302\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 100 and for layer number 2 : 0.7012499999999999\n",
            "Iteration 1, loss = 0.84424359\n",
            "Iteration 2, loss = 0.81824608\n",
            "Iteration 3, loss = 0.78301818\n",
            "Iteration 4, loss = 0.74369898\n",
            "Iteration 5, loss = 0.70772940\n",
            "Iteration 6, loss = 0.67905372\n",
            "Iteration 7, loss = 0.65459215\n",
            "Iteration 8, loss = 0.63750703\n",
            "Iteration 9, loss = 0.62609078\n",
            "Iteration 10, loss = 0.61732840\n",
            "Iteration 11, loss = 0.61055362\n",
            "Iteration 12, loss = 0.60475642\n",
            "Iteration 13, loss = 0.60059365\n",
            "Iteration 14, loss = 0.59676833\n",
            "Iteration 15, loss = 0.59386373\n",
            "Iteration 16, loss = 0.59105742\n",
            "Iteration 17, loss = 0.58857593\n",
            "Iteration 18, loss = 0.58599616\n",
            "Iteration 19, loss = 0.58413312\n",
            "Iteration 20, loss = 0.58213654\n",
            "Iteration 21, loss = 0.58047540\n",
            "Iteration 22, loss = 0.57899956\n",
            "Iteration 23, loss = 0.57767412\n",
            "Iteration 24, loss = 0.57662470\n",
            "Iteration 25, loss = 0.57549965\n",
            "Iteration 26, loss = 0.57448619\n",
            "Iteration 27, loss = 0.57350458\n",
            "Iteration 28, loss = 0.57272338\n",
            "Iteration 29, loss = 0.57195199\n",
            "Iteration 30, loss = 0.57117292\n",
            "Iteration 31, loss = 0.57052970\n",
            "Iteration 32, loss = 0.56989653\n",
            "Iteration 33, loss = 0.56924336\n",
            "Iteration 34, loss = 0.56855370\n",
            "Iteration 35, loss = 0.56793890\n",
            "Iteration 36, loss = 0.56726158\n",
            "Iteration 37, loss = 0.56681235\n",
            "Iteration 38, loss = 0.56619017\n",
            "Iteration 39, loss = 0.56555914\n",
            "Iteration 40, loss = 0.56496310\n",
            "Iteration 41, loss = 0.56418147\n",
            "Iteration 42, loss = 0.56369303\n",
            "Iteration 43, loss = 0.56314826\n",
            "Iteration 44, loss = 0.56264112\n",
            "Iteration 45, loss = 0.56218156\n",
            "Iteration 46, loss = 0.56170246\n",
            "Iteration 47, loss = 0.56127065\n",
            "Iteration 48, loss = 0.56095972\n",
            "Iteration 49, loss = 0.56051664\n",
            "Iteration 50, loss = 0.56016687\n",
            "Iteration 51, loss = 0.55986462\n",
            "Iteration 52, loss = 0.55963340\n",
            "Iteration 53, loss = 0.55921424\n",
            "Iteration 54, loss = 0.55880487\n",
            "Iteration 55, loss = 0.55855142\n",
            "Iteration 56, loss = 0.55808873\n",
            "Iteration 57, loss = 0.55768277\n",
            "Iteration 58, loss = 0.55738430\n",
            "Iteration 59, loss = 0.55704771\n",
            "Iteration 60, loss = 0.55673806\n",
            "Iteration 61, loss = 0.55628107\n",
            "Iteration 62, loss = 0.55598312\n",
            "Iteration 63, loss = 0.55560140\n",
            "Iteration 64, loss = 0.55527998\n",
            "Iteration 65, loss = 0.55510876\n",
            "Iteration 66, loss = 0.55472229\n",
            "Iteration 67, loss = 0.55441299\n",
            "Iteration 68, loss = 0.55417873\n",
            "Iteration 69, loss = 0.55397098\n",
            "Iteration 70, loss = 0.55377076\n",
            "Iteration 71, loss = 0.55358584\n",
            "Iteration 72, loss = 0.55322471\n",
            "Iteration 73, loss = 0.55307920\n",
            "Iteration 74, loss = 0.55268158\n",
            "Iteration 75, loss = 0.55243838\n",
            "Iteration 76, loss = 0.55227362\n",
            "Iteration 77, loss = 0.55205947\n",
            "Iteration 78, loss = 0.55176691\n",
            "Iteration 79, loss = 0.55148431\n",
            "Iteration 80, loss = 0.55117819\n",
            "Iteration 81, loss = 0.55094568\n",
            "Iteration 82, loss = 0.55068506\n",
            "Iteration 83, loss = 0.55041758\n",
            "Iteration 84, loss = 0.55009782\n",
            "Iteration 85, loss = 0.54980800\n",
            "Iteration 86, loss = 0.54956132\n",
            "Iteration 87, loss = 0.54938696\n",
            "Iteration 88, loss = 0.54914225\n",
            "Iteration 89, loss = 0.54883945\n",
            "Iteration 90, loss = 0.54857565\n",
            "Iteration 91, loss = 0.54837773\n",
            "Iteration 92, loss = 0.54813146\n",
            "Iteration 93, loss = 0.54776457\n",
            "Iteration 94, loss = 0.54734763\n",
            "Iteration 95, loss = 0.54709515\n",
            "Iteration 96, loss = 0.54679073\n",
            "Iteration 97, loss = 0.54658268\n",
            "Iteration 98, loss = 0.54627656\n",
            "Iteration 99, loss = 0.54604382\n",
            "Iteration 100, loss = 0.54579330\n",
            "Iteration 1, loss = 0.84514500\n",
            "Iteration 2, loss = 0.81909328\n",
            "Iteration 3, loss = 0.78652477\n",
            "Iteration 4, loss = 0.74811660\n",
            "Iteration 5, loss = 0.71383507\n",
            "Iteration 6, loss = 0.68552273\n",
            "Iteration 7, loss = 0.66168581\n",
            "Iteration 8, loss = 0.64338982\n",
            "Iteration 9, loss = 0.63160645\n",
            "Iteration 10, loss = 0.62136865\n",
            "Iteration 11, loss = 0.61380923\n",
            "Iteration 12, loss = 0.60791210\n",
            "Iteration 13, loss = 0.60355558\n",
            "Iteration 14, loss = 0.59965695\n",
            "Iteration 15, loss = 0.59624464\n",
            "Iteration 16, loss = 0.59328627\n",
            "Iteration 17, loss = 0.59072275\n",
            "Iteration 18, loss = 0.58830866\n",
            "Iteration 19, loss = 0.58624201\n",
            "Iteration 20, loss = 0.58419678\n",
            "Iteration 21, loss = 0.58224472\n",
            "Iteration 22, loss = 0.58048793\n",
            "Iteration 23, loss = 0.57892935\n",
            "Iteration 24, loss = 0.57765823\n",
            "Iteration 25, loss = 0.57634431\n",
            "Iteration 26, loss = 0.57526519\n",
            "Iteration 27, loss = 0.57409528\n",
            "Iteration 28, loss = 0.57316406\n",
            "Iteration 29, loss = 0.57222788\n",
            "Iteration 30, loss = 0.57132622\n",
            "Iteration 31, loss = 0.57066822\n",
            "Iteration 32, loss = 0.56993909\n",
            "Iteration 33, loss = 0.56926119\n",
            "Iteration 34, loss = 0.56861361\n",
            "Iteration 35, loss = 0.56790836\n",
            "Iteration 36, loss = 0.56732057\n",
            "Iteration 37, loss = 0.56682809\n",
            "Iteration 38, loss = 0.56615271\n",
            "Iteration 39, loss = 0.56557409\n",
            "Iteration 40, loss = 0.56501093\n",
            "Iteration 41, loss = 0.56427012\n",
            "Iteration 42, loss = 0.56383162\n",
            "Iteration 43, loss = 0.56324564\n",
            "Iteration 44, loss = 0.56257550\n",
            "Iteration 45, loss = 0.56205907\n",
            "Iteration 46, loss = 0.56161548\n",
            "Iteration 47, loss = 0.56097687\n",
            "Iteration 48, loss = 0.56050221\n",
            "Iteration 49, loss = 0.55997645\n",
            "Iteration 50, loss = 0.55950729\n",
            "Iteration 51, loss = 0.55905100\n",
            "Iteration 52, loss = 0.55864909\n",
            "Iteration 53, loss = 0.55822691\n",
            "Iteration 54, loss = 0.55777537\n",
            "Iteration 55, loss = 0.55742865\n",
            "Iteration 56, loss = 0.55691692\n",
            "Iteration 57, loss = 0.55641760\n",
            "Iteration 58, loss = 0.55598568\n",
            "Iteration 59, loss = 0.55547413\n",
            "Iteration 60, loss = 0.55496972\n",
            "Iteration 61, loss = 0.55443302\n",
            "Iteration 62, loss = 0.55393228\n",
            "Iteration 63, loss = 0.55347585\n",
            "Iteration 64, loss = 0.55302202\n",
            "Iteration 65, loss = 0.55257241\n",
            "Iteration 66, loss = 0.55210691\n",
            "Iteration 67, loss = 0.55157379\n",
            "Iteration 68, loss = 0.55117839\n",
            "Iteration 69, loss = 0.55073343\n",
            "Iteration 70, loss = 0.55030951\n",
            "Iteration 71, loss = 0.54994572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 72, loss = 0.54945093\n",
            "Iteration 73, loss = 0.54910117\n",
            "Iteration 74, loss = 0.54861316\n",
            "Iteration 75, loss = 0.54822427\n",
            "Iteration 76, loss = 0.54796648\n",
            "Iteration 77, loss = 0.54761925\n",
            "Iteration 78, loss = 0.54726992\n",
            "Iteration 79, loss = 0.54702011\n",
            "Iteration 80, loss = 0.54682356\n",
            "Iteration 81, loss = 0.54654362\n",
            "Iteration 82, loss = 0.54640230\n",
            "Iteration 83, loss = 0.54610336\n",
            "Iteration 84, loss = 0.54585375\n",
            "Iteration 85, loss = 0.54554325\n",
            "Iteration 86, loss = 0.54522963\n",
            "Iteration 87, loss = 0.54487368\n",
            "Iteration 88, loss = 0.54453649\n",
            "Iteration 89, loss = 0.54424273\n",
            "Iteration 90, loss = 0.54393287\n",
            "Iteration 91, loss = 0.54370091\n",
            "Iteration 92, loss = 0.54354063\n",
            "Iteration 93, loss = 0.54315551\n",
            "Iteration 94, loss = 0.54284090\n",
            "Iteration 95, loss = 0.54260775\n",
            "Iteration 96, loss = 0.54226539\n",
            "Iteration 97, loss = 0.54218460\n",
            "Iteration 98, loss = 0.54188639\n",
            "Iteration 99, loss = 0.54171213\n",
            "Iteration 100, loss = 0.54137443\n",
            "Iteration 1, loss = 0.86511432\n",
            "Iteration 2, loss = 0.83659966\n",
            "Iteration 3, loss = 0.80055969\n",
            "Iteration 4, loss = 0.75853561\n",
            "Iteration 5, loss = 0.72086132\n",
            "Iteration 6, loss = 0.68969566\n",
            "Iteration 7, loss = 0.66421861\n",
            "Iteration 8, loss = 0.64434004\n",
            "Iteration 9, loss = 0.63160029\n",
            "Iteration 10, loss = 0.62011071\n",
            "Iteration 11, loss = 0.61155999\n",
            "Iteration 12, loss = 0.60497986\n",
            "Iteration 13, loss = 0.59971010\n",
            "Iteration 14, loss = 0.59516710\n",
            "Iteration 15, loss = 0.59089636\n",
            "Iteration 16, loss = 0.58736900\n",
            "Iteration 17, loss = 0.58433935\n",
            "Iteration 18, loss = 0.58137018\n",
            "Iteration 19, loss = 0.57880625\n",
            "Iteration 20, loss = 0.57652473\n",
            "Iteration 21, loss = 0.57419227\n",
            "Iteration 22, loss = 0.57221919\n",
            "Iteration 23, loss = 0.57043440\n",
            "Iteration 24, loss = 0.56876358\n",
            "Iteration 25, loss = 0.56716867\n",
            "Iteration 26, loss = 0.56584442\n",
            "Iteration 27, loss = 0.56433635\n",
            "Iteration 28, loss = 0.56311521\n",
            "Iteration 29, loss = 0.56189909\n",
            "Iteration 30, loss = 0.56081779\n",
            "Iteration 31, loss = 0.55979345\n",
            "Iteration 32, loss = 0.55889623\n",
            "Iteration 33, loss = 0.55803597\n",
            "Iteration 34, loss = 0.55720608\n",
            "Iteration 35, loss = 0.55645261\n",
            "Iteration 36, loss = 0.55572053\n",
            "Iteration 37, loss = 0.55504676\n",
            "Iteration 38, loss = 0.55442651\n",
            "Iteration 39, loss = 0.55373834\n",
            "Iteration 40, loss = 0.55307272\n",
            "Iteration 41, loss = 0.55223474\n",
            "Iteration 42, loss = 0.55169607\n",
            "Iteration 43, loss = 0.55108669\n",
            "Iteration 44, loss = 0.55038771\n",
            "Iteration 45, loss = 0.54992049\n",
            "Iteration 46, loss = 0.54944630\n",
            "Iteration 47, loss = 0.54894736\n",
            "Iteration 48, loss = 0.54847299\n",
            "Iteration 49, loss = 0.54798903\n",
            "Iteration 50, loss = 0.54744079\n",
            "Iteration 51, loss = 0.54699818\n",
            "Iteration 52, loss = 0.54644728\n",
            "Iteration 53, loss = 0.54582215\n",
            "Iteration 54, loss = 0.54523601\n",
            "Iteration 55, loss = 0.54486145\n",
            "Iteration 56, loss = 0.54441470\n",
            "Iteration 57, loss = 0.54396780\n",
            "Iteration 58, loss = 0.54354432\n",
            "Iteration 59, loss = 0.54317617\n",
            "Iteration 60, loss = 0.54271414\n",
            "Iteration 61, loss = 0.54228241\n",
            "Iteration 62, loss = 0.54187862\n",
            "Iteration 63, loss = 0.54145488\n",
            "Iteration 64, loss = 0.54105818\n",
            "Iteration 65, loss = 0.54070902\n",
            "Iteration 66, loss = 0.54033059\n",
            "Iteration 67, loss = 0.53986242\n",
            "Iteration 68, loss = 0.53947009\n",
            "Iteration 69, loss = 0.53907710\n",
            "Iteration 70, loss = 0.53872273\n",
            "Iteration 71, loss = 0.53852677\n",
            "Iteration 72, loss = 0.53825027\n",
            "Iteration 73, loss = 0.53802444\n",
            "Iteration 74, loss = 0.53759156\n",
            "Iteration 75, loss = 0.53719528\n",
            "Iteration 76, loss = 0.53695689\n",
            "Iteration 77, loss = 0.53658347\n",
            "Iteration 78, loss = 0.53635432\n",
            "Iteration 79, loss = 0.53605595\n",
            "Iteration 80, loss = 0.53582474\n",
            "Iteration 81, loss = 0.53554773\n",
            "Iteration 82, loss = 0.53535155\n",
            "Iteration 83, loss = 0.53515822\n",
            "Iteration 84, loss = 0.53487830\n",
            "Iteration 85, loss = 0.53471838\n",
            "Iteration 86, loss = 0.53452936\n",
            "Iteration 87, loss = 0.53428679\n",
            "Iteration 88, loss = 0.53411485\n",
            "Iteration 89, loss = 0.53393016\n",
            "Iteration 90, loss = 0.53375943\n",
            "Iteration 91, loss = 0.53365426\n",
            "Iteration 92, loss = 0.53351596\n",
            "Iteration 93, loss = 0.53327774\n",
            "Iteration 94, loss = 0.53314514\n",
            "Iteration 95, loss = 0.53281641\n",
            "Iteration 96, loss = 0.53262599\n",
            "Iteration 97, loss = 0.53239384\n",
            "Iteration 98, loss = 0.53232582\n",
            "Iteration 99, loss = 0.53211350\n",
            "Iteration 100, loss = 0.53190029\n",
            "Iteration 1, loss = 0.84459665\n",
            "Iteration 2, loss = 0.81870228\n",
            "Iteration 3, loss = 0.78595389\n",
            "Iteration 4, loss = 0.74703750\n",
            "Iteration 5, loss = 0.71258339\n",
            "Iteration 6, loss = 0.68531183\n",
            "Iteration 7, loss = 0.66229352\n",
            "Iteration 8, loss = 0.64435029\n",
            "Iteration 9, loss = 0.63245378\n",
            "Iteration 10, loss = 0.62203620\n",
            "Iteration 11, loss = 0.61422600\n",
            "Iteration 12, loss = 0.60819304\n",
            "Iteration 13, loss = 0.60394945\n",
            "Iteration 14, loss = 0.60001920\n",
            "Iteration 15, loss = 0.59682229\n",
            "Iteration 16, loss = 0.59409794\n",
            "Iteration 17, loss = 0.59187271\n",
            "Iteration 18, loss = 0.58971012\n",
            "Iteration 19, loss = 0.58782321\n",
            "Iteration 20, loss = 0.58622969\n",
            "Iteration 21, loss = 0.58456990\n",
            "Iteration 22, loss = 0.58308345\n",
            "Iteration 23, loss = 0.58175424\n",
            "Iteration 24, loss = 0.58053072\n",
            "Iteration 25, loss = 0.57943520\n",
            "Iteration 26, loss = 0.57847461\n",
            "Iteration 27, loss = 0.57744098\n",
            "Iteration 28, loss = 0.57649424\n",
            "Iteration 29, loss = 0.57561182\n",
            "Iteration 30, loss = 0.57472458\n",
            "Iteration 31, loss = 0.57386337\n",
            "Iteration 32, loss = 0.57320966\n",
            "Iteration 33, loss = 0.57256386\n",
            "Iteration 34, loss = 0.57190437\n",
            "Iteration 35, loss = 0.57129547\n",
            "Iteration 36, loss = 0.57075666\n",
            "Iteration 37, loss = 0.57013798\n",
            "Iteration 38, loss = 0.56964642\n",
            "Iteration 39, loss = 0.56913319\n",
            "Iteration 40, loss = 0.56867031\n",
            "Iteration 41, loss = 0.56805439\n",
            "Iteration 42, loss = 0.56757060\n",
            "Iteration 43, loss = 0.56705874\n",
            "Iteration 44, loss = 0.56652038\n",
            "Iteration 45, loss = 0.56613455\n",
            "Iteration 46, loss = 0.56564884\n",
            "Iteration 47, loss = 0.56532256\n",
            "Iteration 48, loss = 0.56486247\n",
            "Iteration 49, loss = 0.56442581\n",
            "Iteration 50, loss = 0.56400200\n",
            "Iteration 51, loss = 0.56363125\n",
            "Iteration 52, loss = 0.56319439\n",
            "Iteration 53, loss = 0.56270183\n",
            "Iteration 54, loss = 0.56225417\n",
            "Iteration 55, loss = 0.56195227\n",
            "Iteration 56, loss = 0.56154741\n",
            "Iteration 57, loss = 0.56111386\n",
            "Iteration 58, loss = 0.56077495\n",
            "Iteration 59, loss = 0.56040345\n",
            "Iteration 60, loss = 0.56005605\n",
            "Iteration 61, loss = 0.55961365\n",
            "Iteration 62, loss = 0.55924774\n",
            "Iteration 63, loss = 0.55888500\n",
            "Iteration 64, loss = 0.55855013\n",
            "Iteration 65, loss = 0.55820503\n",
            "Iteration 66, loss = 0.55783153\n",
            "Iteration 67, loss = 0.55740062\n",
            "Iteration 68, loss = 0.55713550\n",
            "Iteration 69, loss = 0.55676995\n",
            "Iteration 70, loss = 0.55640435\n",
            "Iteration 71, loss = 0.55623051\n",
            "Iteration 72, loss = 0.55586898\n",
            "Iteration 73, loss = 0.55551588\n",
            "Iteration 74, loss = 0.55517806\n",
            "Iteration 75, loss = 0.55474794\n",
            "Iteration 76, loss = 0.55447635\n",
            "Iteration 77, loss = 0.55405120\n",
            "Iteration 78, loss = 0.55367833\n",
            "Iteration 79, loss = 0.55337910\n",
            "Iteration 80, loss = 0.55296347\n",
            "Iteration 81, loss = 0.55258527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 82, loss = 0.55233845\n",
            "Iteration 83, loss = 0.55204456\n",
            "Iteration 84, loss = 0.55171987\n",
            "Iteration 85, loss = 0.55152066\n",
            "Iteration 86, loss = 0.55125547\n",
            "Iteration 87, loss = 0.55102926\n",
            "Iteration 88, loss = 0.55078327\n",
            "Iteration 89, loss = 0.55049191\n",
            "Iteration 90, loss = 0.55022934\n",
            "Iteration 91, loss = 0.55002248\n",
            "Iteration 92, loss = 0.54970664\n",
            "Iteration 93, loss = 0.54939765\n",
            "Iteration 94, loss = 0.54915329\n",
            "Iteration 95, loss = 0.54878256\n",
            "Iteration 96, loss = 0.54848819\n",
            "Iteration 97, loss = 0.54816994\n",
            "Iteration 98, loss = 0.54797776\n",
            "Iteration 99, loss = 0.54764892\n",
            "Iteration 100, loss = 0.54741962\n",
            "Iteration 1, loss = 0.84006331\n",
            "Iteration 2, loss = 0.81376413\n",
            "Iteration 3, loss = 0.77923652\n",
            "Iteration 4, loss = 0.74192691\n",
            "Iteration 5, loss = 0.71081488\n",
            "Iteration 6, loss = 0.68542628\n",
            "Iteration 7, loss = 0.66332206\n",
            "Iteration 8, loss = 0.64635824\n",
            "Iteration 9, loss = 0.63430855\n",
            "Iteration 10, loss = 0.62333389\n",
            "Iteration 11, loss = 0.61592732\n",
            "Iteration 12, loss = 0.61059120\n",
            "Iteration 13, loss = 0.60572314\n",
            "Iteration 14, loss = 0.60162257\n",
            "Iteration 15, loss = 0.59818823\n",
            "Iteration 16, loss = 0.59510392\n",
            "Iteration 17, loss = 0.59243361\n",
            "Iteration 18, loss = 0.58999220\n",
            "Iteration 19, loss = 0.58775910\n",
            "Iteration 20, loss = 0.58570466\n",
            "Iteration 21, loss = 0.58382153\n",
            "Iteration 22, loss = 0.58221027\n",
            "Iteration 23, loss = 0.58074782\n",
            "Iteration 24, loss = 0.57942583\n",
            "Iteration 25, loss = 0.57825592\n",
            "Iteration 26, loss = 0.57718551\n",
            "Iteration 27, loss = 0.57609237\n",
            "Iteration 28, loss = 0.57502039\n",
            "Iteration 29, loss = 0.57416546\n",
            "Iteration 30, loss = 0.57336706\n",
            "Iteration 31, loss = 0.57254110\n",
            "Iteration 32, loss = 0.57178136\n",
            "Iteration 33, loss = 0.57121735\n",
            "Iteration 34, loss = 0.57065672\n",
            "Iteration 35, loss = 0.57011026\n",
            "Iteration 36, loss = 0.56962549\n",
            "Iteration 37, loss = 0.56905940\n",
            "Iteration 38, loss = 0.56870017\n",
            "Iteration 39, loss = 0.56841004\n",
            "Iteration 40, loss = 0.56813621\n",
            "Iteration 41, loss = 0.56767469\n",
            "Iteration 42, loss = 0.56735452\n",
            "Iteration 43, loss = 0.56692246\n",
            "Iteration 44, loss = 0.56655300\n",
            "Iteration 45, loss = 0.56632262\n",
            "Iteration 46, loss = 0.56592314\n",
            "Iteration 47, loss = 0.56566446\n",
            "Iteration 48, loss = 0.56536787\n",
            "Iteration 49, loss = 0.56507362\n",
            "Iteration 50, loss = 0.56477701\n",
            "Iteration 51, loss = 0.56458465\n",
            "Iteration 52, loss = 0.56430101\n",
            "Iteration 53, loss = 0.56397691\n",
            "Iteration 54, loss = 0.56383676\n",
            "Iteration 55, loss = 0.56355069\n",
            "Iteration 56, loss = 0.56333377\n",
            "Iteration 57, loss = 0.56300657\n",
            "Iteration 58, loss = 0.56280934\n",
            "Iteration 59, loss = 0.56259266\n",
            "Iteration 60, loss = 0.56235040\n",
            "Iteration 61, loss = 0.56206429\n",
            "Iteration 62, loss = 0.56184369\n",
            "Iteration 63, loss = 0.56162229\n",
            "Iteration 64, loss = 0.56138660\n",
            "Iteration 65, loss = 0.56115882\n",
            "Iteration 66, loss = 0.56092963\n",
            "Iteration 67, loss = 0.56062588\n",
            "Iteration 68, loss = 0.56041223\n",
            "Iteration 69, loss = 0.56023079\n",
            "Iteration 70, loss = 0.55999148\n",
            "Iteration 71, loss = 0.55977984\n",
            "Iteration 72, loss = 0.55951684\n",
            "Iteration 73, loss = 0.55937821\n",
            "Iteration 74, loss = 0.55921986\n",
            "Iteration 75, loss = 0.55887053\n",
            "Iteration 76, loss = 0.55866763\n",
            "Iteration 77, loss = 0.55835573\n",
            "Iteration 78, loss = 0.55806711\n",
            "Iteration 79, loss = 0.55774697\n",
            "Iteration 80, loss = 0.55738107\n",
            "Iteration 81, loss = 0.55707906\n",
            "Iteration 82, loss = 0.55689875\n",
            "Iteration 83, loss = 0.55667676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 84, loss = 0.55639916\n",
            "Iteration 85, loss = 0.55622890\n",
            "Iteration 86, loss = 0.55600852\n",
            "Iteration 87, loss = 0.55583013\n",
            "Iteration 88, loss = 0.55565230\n",
            "Iteration 89, loss = 0.55543481\n",
            "Iteration 90, loss = 0.55528561\n",
            "Iteration 91, loss = 0.55506069\n",
            "Iteration 92, loss = 0.55477530\n",
            "Iteration 93, loss = 0.55451576\n",
            "Iteration 94, loss = 0.55430681\n",
            "Iteration 95, loss = 0.55400776\n",
            "Iteration 96, loss = 0.55369259\n",
            "Iteration 97, loss = 0.55338130\n",
            "Iteration 98, loss = 0.55315037\n",
            "Iteration 99, loss = 0.55291589\n",
            "Iteration 100, loss = 0.55264640\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 100 and for layer number 3 : 0.6987500000000001\n",
            "Iteration 1, loss = 0.92540177\n",
            "Iteration 2, loss = 0.90606988\n",
            "Iteration 3, loss = 0.87636521\n",
            "Iteration 4, loss = 0.84453499\n",
            "Iteration 5, loss = 0.81069894\n",
            "Iteration 6, loss = 0.78049825\n",
            "Iteration 7, loss = 0.75458834\n",
            "Iteration 8, loss = 0.73221444\n",
            "Iteration 9, loss = 0.71299243\n",
            "Iteration 10, loss = 0.69629293\n",
            "Iteration 11, loss = 0.68190019\n",
            "Iteration 12, loss = 0.66823391\n",
            "Iteration 13, loss = 0.65677101\n",
            "Iteration 14, loss = 0.64656579\n",
            "Iteration 15, loss = 0.63780899\n",
            "Iteration 16, loss = 0.63058347\n",
            "Iteration 17, loss = 0.62420683\n",
            "Iteration 18, loss = 0.61911339\n",
            "Iteration 19, loss = 0.61402184\n",
            "Iteration 20, loss = 0.60966985\n",
            "Iteration 21, loss = 0.60588786\n",
            "Iteration 22, loss = 0.60253512\n",
            "Iteration 23, loss = 0.59952335\n",
            "Iteration 24, loss = 0.59636324\n",
            "Iteration 25, loss = 0.59415604\n",
            "Iteration 26, loss = 0.59185568\n",
            "Iteration 27, loss = 0.58962051\n",
            "Iteration 28, loss = 0.58753568\n",
            "Iteration 29, loss = 0.58565227\n",
            "Iteration 30, loss = 0.58359418\n",
            "Iteration 31, loss = 0.58153387\n",
            "Iteration 32, loss = 0.57977999\n",
            "Iteration 33, loss = 0.57793385\n",
            "Iteration 34, loss = 0.57632165\n",
            "Iteration 35, loss = 0.57508868\n",
            "Iteration 36, loss = 0.57386065\n",
            "Iteration 37, loss = 0.57284075\n",
            "Iteration 38, loss = 0.57151683\n",
            "Iteration 39, loss = 0.57035367\n",
            "Iteration 40, loss = 0.56936141\n",
            "Iteration 41, loss = 0.56829218\n",
            "Iteration 42, loss = 0.56718683\n",
            "Iteration 43, loss = 0.56628285\n",
            "Iteration 44, loss = 0.56540172\n",
            "Iteration 45, loss = 0.56450610\n",
            "Iteration 46, loss = 0.56359359\n",
            "Iteration 47, loss = 0.56274113\n",
            "Iteration 48, loss = 0.56205347\n",
            "Iteration 49, loss = 0.56141325\n",
            "Iteration 50, loss = 0.56076824\n",
            "Iteration 51, loss = 0.56010553\n",
            "Iteration 52, loss = 0.55955061\n",
            "Iteration 53, loss = 0.55909395\n",
            "Iteration 54, loss = 0.55851126\n",
            "Iteration 55, loss = 0.55829418\n",
            "Iteration 56, loss = 0.55769846\n",
            "Iteration 57, loss = 0.55725754\n",
            "Iteration 58, loss = 0.55675838\n",
            "Iteration 59, loss = 0.55638045\n",
            "Iteration 60, loss = 0.55586590\n",
            "Iteration 61, loss = 0.55559102\n",
            "Iteration 62, loss = 0.55526240\n",
            "Iteration 63, loss = 0.55501197\n",
            "Iteration 64, loss = 0.55472593\n",
            "Iteration 65, loss = 0.55441042\n",
            "Iteration 66, loss = 0.55417765\n",
            "Iteration 67, loss = 0.55391604\n",
            "Iteration 68, loss = 0.55367443\n",
            "Iteration 69, loss = 0.55341227\n",
            "Iteration 70, loss = 0.55306736\n",
            "Iteration 71, loss = 0.55291328\n",
            "Iteration 72, loss = 0.55263420\n",
            "Iteration 73, loss = 0.55247432\n",
            "Iteration 74, loss = 0.55217557\n",
            "Iteration 75, loss = 0.55197460\n",
            "Iteration 76, loss = 0.55176851\n",
            "Iteration 77, loss = 0.55160822\n",
            "Iteration 78, loss = 0.55124273\n",
            "Iteration 79, loss = 0.55130751\n",
            "Iteration 80, loss = 0.55101272\n",
            "Iteration 81, loss = 0.55080159\n",
            "Iteration 82, loss = 0.55055801\n",
            "Iteration 83, loss = 0.55028475\n",
            "Iteration 84, loss = 0.55008004\n",
            "Iteration 85, loss = 0.54978130\n",
            "Iteration 86, loss = 0.54946212\n",
            "Iteration 87, loss = 0.54921940\n",
            "Iteration 88, loss = 0.54905529\n",
            "Iteration 89, loss = 0.54883849\n",
            "Iteration 90, loss = 0.54866519\n",
            "Iteration 91, loss = 0.54847662\n",
            "Iteration 92, loss = 0.54822737\n",
            "Iteration 93, loss = 0.54809133\n",
            "Iteration 94, loss = 0.54783013\n",
            "Iteration 95, loss = 0.54764196\n",
            "Iteration 96, loss = 0.54742739\n",
            "Iteration 97, loss = 0.54731608\n",
            "Iteration 98, loss = 0.54716179\n",
            "Iteration 99, loss = 0.54712924\n",
            "Iteration 100, loss = 0.54680985\n",
            "Iteration 1, loss = 0.92236300\n",
            "Iteration 2, loss = 0.90372668\n",
            "Iteration 3, loss = 0.87500833\n",
            "Iteration 4, loss = 0.84285035\n",
            "Iteration 5, loss = 0.80913380\n",
            "Iteration 6, loss = 0.77972252\n",
            "Iteration 7, loss = 0.75423862\n",
            "Iteration 8, loss = 0.73244226\n",
            "Iteration 9, loss = 0.71337957\n",
            "Iteration 10, loss = 0.69694930\n",
            "Iteration 11, loss = 0.68235517\n",
            "Iteration 12, loss = 0.66874001\n",
            "Iteration 13, loss = 0.65701897\n",
            "Iteration 14, loss = 0.64666486\n",
            "Iteration 15, loss = 0.63680422\n",
            "Iteration 16, loss = 0.62903974\n",
            "Iteration 17, loss = 0.62282581\n",
            "Iteration 18, loss = 0.61753655\n",
            "Iteration 19, loss = 0.61255961\n",
            "Iteration 20, loss = 0.60822250\n",
            "Iteration 21, loss = 0.60455223\n",
            "Iteration 22, loss = 0.60093557\n",
            "Iteration 23, loss = 0.59766462\n",
            "Iteration 24, loss = 0.59443533\n",
            "Iteration 25, loss = 0.59186429\n",
            "Iteration 26, loss = 0.58954458\n",
            "Iteration 27, loss = 0.58722957\n",
            "Iteration 28, loss = 0.58504645\n",
            "Iteration 29, loss = 0.58302508\n",
            "Iteration 30, loss = 0.58092002\n",
            "Iteration 31, loss = 0.57890737\n",
            "Iteration 32, loss = 0.57713204\n",
            "Iteration 33, loss = 0.57553550\n",
            "Iteration 34, loss = 0.57399110\n",
            "Iteration 35, loss = 0.57258257\n",
            "Iteration 36, loss = 0.57132345\n",
            "Iteration 37, loss = 0.57036989\n",
            "Iteration 38, loss = 0.56919804\n",
            "Iteration 39, loss = 0.56804072\n",
            "Iteration 40, loss = 0.56720233\n",
            "Iteration 41, loss = 0.56616219\n",
            "Iteration 42, loss = 0.56523123\n",
            "Iteration 43, loss = 0.56418669\n",
            "Iteration 44, loss = 0.56340104\n",
            "Iteration 45, loss = 0.56229242\n",
            "Iteration 46, loss = 0.56136004\n",
            "Iteration 47, loss = 0.56050574\n",
            "Iteration 48, loss = 0.55975555\n",
            "Iteration 49, loss = 0.55893402\n",
            "Iteration 50, loss = 0.55825810\n",
            "Iteration 51, loss = 0.55750539\n",
            "Iteration 52, loss = 0.55689701\n",
            "Iteration 53, loss = 0.55630534\n",
            "Iteration 54, loss = 0.55554960\n",
            "Iteration 55, loss = 0.55530925\n",
            "Iteration 56, loss = 0.55458557\n",
            "Iteration 57, loss = 0.55417055\n",
            "Iteration 58, loss = 0.55375697\n",
            "Iteration 59, loss = 0.55343024\n",
            "Iteration 60, loss = 0.55303828\n",
            "Iteration 61, loss = 0.55275448\n",
            "Iteration 62, loss = 0.55246826\n",
            "Iteration 63, loss = 0.55210971\n",
            "Iteration 64, loss = 0.55175282\n",
            "Iteration 65, loss = 0.55137494\n",
            "Iteration 66, loss = 0.55113308\n",
            "Iteration 67, loss = 0.55081752\n",
            "Iteration 68, loss = 0.55067280\n",
            "Iteration 69, loss = 0.55037239\n",
            "Iteration 70, loss = 0.55015762\n",
            "Iteration 71, loss = 0.54984322\n",
            "Iteration 72, loss = 0.54962258\n",
            "Iteration 73, loss = 0.54941433\n",
            "Iteration 74, loss = 0.54910659\n",
            "Iteration 75, loss = 0.54882617\n",
            "Iteration 76, loss = 0.54858009\n",
            "Iteration 77, loss = 0.54842549\n",
            "Iteration 78, loss = 0.54826356\n",
            "Iteration 79, loss = 0.54817975\n",
            "Iteration 80, loss = 0.54779764\n",
            "Iteration 81, loss = 0.54763140\n",
            "Iteration 82, loss = 0.54736580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 83, loss = 0.54705013\n",
            "Iteration 84, loss = 0.54689619\n",
            "Iteration 85, loss = 0.54652246\n",
            "Iteration 86, loss = 0.54618831\n",
            "Iteration 87, loss = 0.54586111\n",
            "Iteration 88, loss = 0.54563663\n",
            "Iteration 89, loss = 0.54532937\n",
            "Iteration 90, loss = 0.54516871\n",
            "Iteration 91, loss = 0.54484111\n",
            "Iteration 92, loss = 0.54457027\n",
            "Iteration 93, loss = 0.54439667\n",
            "Iteration 94, loss = 0.54405331\n",
            "Iteration 95, loss = 0.54382871\n",
            "Iteration 96, loss = 0.54356907\n",
            "Iteration 97, loss = 0.54331510\n",
            "Iteration 98, loss = 0.54317759\n",
            "Iteration 99, loss = 0.54304768\n",
            "Iteration 100, loss = 0.54279147\n",
            "Iteration 1, loss = 0.93166195\n",
            "Iteration 2, loss = 0.91234492\n",
            "Iteration 3, loss = 0.88372313\n",
            "Iteration 4, loss = 0.84961251\n",
            "Iteration 5, loss = 0.81641210\n",
            "Iteration 6, loss = 0.78527885\n",
            "Iteration 7, loss = 0.75872117\n",
            "Iteration 8, loss = 0.73530918\n",
            "Iteration 9, loss = 0.71535645\n",
            "Iteration 10, loss = 0.69760505\n",
            "Iteration 11, loss = 0.68188671\n",
            "Iteration 12, loss = 0.66752288\n",
            "Iteration 13, loss = 0.65465085\n",
            "Iteration 14, loss = 0.64368593\n",
            "Iteration 15, loss = 0.63335085\n",
            "Iteration 16, loss = 0.62497229\n",
            "Iteration 17, loss = 0.61851546\n",
            "Iteration 18, loss = 0.61250769\n",
            "Iteration 19, loss = 0.60725642\n",
            "Iteration 20, loss = 0.60264352\n",
            "Iteration 21, loss = 0.59848665\n",
            "Iteration 22, loss = 0.59465851\n",
            "Iteration 23, loss = 0.59134538\n",
            "Iteration 24, loss = 0.58812798\n",
            "Iteration 25, loss = 0.58555756\n",
            "Iteration 26, loss = 0.58347267\n",
            "Iteration 27, loss = 0.58105626\n",
            "Iteration 28, loss = 0.57898547\n",
            "Iteration 29, loss = 0.57707857\n",
            "Iteration 30, loss = 0.57510602\n",
            "Iteration 31, loss = 0.57303366\n",
            "Iteration 32, loss = 0.57121707\n",
            "Iteration 33, loss = 0.56956037\n",
            "Iteration 34, loss = 0.56781174\n",
            "Iteration 35, loss = 0.56635167\n",
            "Iteration 36, loss = 0.56496659\n",
            "Iteration 37, loss = 0.56388825\n",
            "Iteration 38, loss = 0.56265033\n",
            "Iteration 39, loss = 0.56150979\n",
            "Iteration 40, loss = 0.56060243\n",
            "Iteration 41, loss = 0.55974129\n",
            "Iteration 42, loss = 0.55864499\n",
            "Iteration 43, loss = 0.55766563\n",
            "Iteration 44, loss = 0.55679573\n",
            "Iteration 45, loss = 0.55585704\n",
            "Iteration 46, loss = 0.55484301\n",
            "Iteration 47, loss = 0.55397414\n",
            "Iteration 48, loss = 0.55323994\n",
            "Iteration 49, loss = 0.55246703\n",
            "Iteration 50, loss = 0.55180371\n",
            "Iteration 51, loss = 0.55112381\n",
            "Iteration 52, loss = 0.55041014\n",
            "Iteration 53, loss = 0.54992329\n",
            "Iteration 54, loss = 0.54928756\n",
            "Iteration 55, loss = 0.54895870\n",
            "Iteration 56, loss = 0.54836925\n",
            "Iteration 57, loss = 0.54789274\n",
            "Iteration 58, loss = 0.54743070\n",
            "Iteration 59, loss = 0.54683477\n",
            "Iteration 60, loss = 0.54645133\n",
            "Iteration 61, loss = 0.54598155\n",
            "Iteration 62, loss = 0.54552238\n",
            "Iteration 63, loss = 0.54515072\n",
            "Iteration 64, loss = 0.54483821\n",
            "Iteration 65, loss = 0.54450111\n",
            "Iteration 66, loss = 0.54421242\n",
            "Iteration 67, loss = 0.54376193\n",
            "Iteration 68, loss = 0.54363452\n",
            "Iteration 69, loss = 0.54321728\n",
            "Iteration 70, loss = 0.54288609\n",
            "Iteration 71, loss = 0.54252895\n",
            "Iteration 72, loss = 0.54228397\n",
            "Iteration 73, loss = 0.54204462\n",
            "Iteration 74, loss = 0.54173786\n",
            "Iteration 75, loss = 0.54144168\n",
            "Iteration 76, loss = 0.54119500\n",
            "Iteration 77, loss = 0.54091047\n",
            "Iteration 78, loss = 0.54069623\n",
            "Iteration 79, loss = 0.54067683\n",
            "Iteration 80, loss = 0.54013276\n",
            "Iteration 81, loss = 0.53989053\n",
            "Iteration 82, loss = 0.53959924\n",
            "Iteration 83, loss = 0.53936832\n",
            "Iteration 84, loss = 0.53909921\n",
            "Iteration 85, loss = 0.53901869\n",
            "Iteration 86, loss = 0.53886630\n",
            "Iteration 87, loss = 0.53863528\n",
            "Iteration 88, loss = 0.53844614\n",
            "Iteration 89, loss = 0.53823882\n",
            "Iteration 90, loss = 0.53804482\n",
            "Iteration 91, loss = 0.53777906\n",
            "Iteration 92, loss = 0.53750544\n",
            "Iteration 93, loss = 0.53723586\n",
            "Iteration 94, loss = 0.53706081\n",
            "Iteration 95, loss = 0.53680307\n",
            "Iteration 96, loss = 0.53657594\n",
            "Iteration 97, loss = 0.53648131\n",
            "Iteration 98, loss = 0.53633934\n",
            "Iteration 99, loss = 0.53616061\n",
            "Iteration 100, loss = 0.53588690\n",
            "Iteration 1, loss = 0.93004949\n",
            "Iteration 2, loss = 0.90996806\n",
            "Iteration 3, loss = 0.88055072\n",
            "Iteration 4, loss = 0.84659018\n",
            "Iteration 5, loss = 0.81298067\n",
            "Iteration 6, loss = 0.78206991\n",
            "Iteration 7, loss = 0.75624473\n",
            "Iteration 8, loss = 0.73305263\n",
            "Iteration 9, loss = 0.71302062\n",
            "Iteration 10, loss = 0.69627033\n",
            "Iteration 11, loss = 0.68068281\n",
            "Iteration 12, loss = 0.66784432\n",
            "Iteration 13, loss = 0.65616483\n",
            "Iteration 14, loss = 0.64623851\n",
            "Iteration 15, loss = 0.63710321\n",
            "Iteration 16, loss = 0.62947245\n",
            "Iteration 17, loss = 0.62332210\n",
            "Iteration 18, loss = 0.61772357\n",
            "Iteration 19, loss = 0.61296514\n",
            "Iteration 20, loss = 0.60893825\n",
            "Iteration 21, loss = 0.60537285\n",
            "Iteration 22, loss = 0.60197233\n",
            "Iteration 23, loss = 0.59888836\n",
            "Iteration 24, loss = 0.59592724\n",
            "Iteration 25, loss = 0.59330154\n",
            "Iteration 26, loss = 0.59119156\n",
            "Iteration 27, loss = 0.58870339\n",
            "Iteration 28, loss = 0.58650396\n",
            "Iteration 29, loss = 0.58465794\n",
            "Iteration 30, loss = 0.58280816\n",
            "Iteration 31, loss = 0.58096325\n",
            "Iteration 32, loss = 0.57914326\n",
            "Iteration 33, loss = 0.57759947\n",
            "Iteration 34, loss = 0.57596723\n",
            "Iteration 35, loss = 0.57461254\n",
            "Iteration 36, loss = 0.57309404\n",
            "Iteration 37, loss = 0.57195132\n",
            "Iteration 38, loss = 0.57082853\n",
            "Iteration 39, loss = 0.56968276\n",
            "Iteration 40, loss = 0.56876378\n",
            "Iteration 41, loss = 0.56795050\n",
            "Iteration 42, loss = 0.56670730\n",
            "Iteration 43, loss = 0.56589047\n",
            "Iteration 44, loss = 0.56489212\n",
            "Iteration 45, loss = 0.56408490\n",
            "Iteration 46, loss = 0.56314279\n",
            "Iteration 47, loss = 0.56221700\n",
            "Iteration 48, loss = 0.56123991\n",
            "Iteration 49, loss = 0.56021024\n",
            "Iteration 50, loss = 0.55950861\n",
            "Iteration 51, loss = 0.55881625\n",
            "Iteration 52, loss = 0.55804567\n",
            "Iteration 53, loss = 0.55743272\n",
            "Iteration 54, loss = 0.55675730\n",
            "Iteration 55, loss = 0.55651589\n",
            "Iteration 56, loss = 0.55608563\n",
            "Iteration 57, loss = 0.55564424\n",
            "Iteration 58, loss = 0.55526565\n",
            "Iteration 59, loss = 0.55479143\n",
            "Iteration 60, loss = 0.55430114\n",
            "Iteration 61, loss = 0.55392714\n",
            "Iteration 62, loss = 0.55358673\n",
            "Iteration 63, loss = 0.55323272\n",
            "Iteration 64, loss = 0.55316209\n",
            "Iteration 65, loss = 0.55273953\n",
            "Iteration 66, loss = 0.55263798\n",
            "Iteration 67, loss = 0.55236876\n",
            "Iteration 68, loss = 0.55214964\n",
            "Iteration 69, loss = 0.55181669\n",
            "Iteration 70, loss = 0.55156614\n",
            "Iteration 71, loss = 0.55140596\n",
            "Iteration 72, loss = 0.55129497\n",
            "Iteration 73, loss = 0.55112913\n",
            "Iteration 74, loss = 0.55095720\n",
            "Iteration 75, loss = 0.55084412\n",
            "Iteration 76, loss = 0.55071812\n",
            "Iteration 77, loss = 0.55048781\n",
            "Iteration 78, loss = 0.55036839\n",
            "Iteration 79, loss = 0.55043769\n",
            "Iteration 80, loss = 0.55008919\n",
            "Iteration 81, loss = 0.54996789\n",
            "Iteration 82, loss = 0.54984336\n",
            "Iteration 83, loss = 0.54956273\n",
            "Iteration 84, loss = 0.54928818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 85, loss = 0.54921401\n",
            "Iteration 86, loss = 0.54901825\n",
            "Iteration 87, loss = 0.54890758\n",
            "Iteration 88, loss = 0.54882581\n",
            "Iteration 89, loss = 0.54877306\n",
            "Iteration 90, loss = 0.54856873\n",
            "Iteration 91, loss = 0.54851207\n",
            "Iteration 92, loss = 0.54835563\n",
            "Iteration 93, loss = 0.54816557\n",
            "Iteration 94, loss = 0.54804833\n",
            "Iteration 95, loss = 0.54792993\n",
            "Iteration 96, loss = 0.54784983\n",
            "Iteration 97, loss = 0.54774266\n",
            "Iteration 98, loss = 0.54772164\n",
            "Iteration 99, loss = 0.54760421\n",
            "Iteration 100, loss = 0.54742928\n",
            "Iteration 1, loss = 0.93650373\n",
            "Iteration 2, loss = 0.91614372\n",
            "Iteration 3, loss = 0.88631046\n",
            "Iteration 4, loss = 0.85327913\n",
            "Iteration 5, loss = 0.82138194\n",
            "Iteration 6, loss = 0.79229617\n",
            "Iteration 7, loss = 0.76753456\n",
            "Iteration 8, loss = 0.74573124\n",
            "Iteration 9, loss = 0.72648054\n",
            "Iteration 10, loss = 0.71056910\n",
            "Iteration 11, loss = 0.69588855\n",
            "Iteration 12, loss = 0.68427106\n",
            "Iteration 13, loss = 0.67305324\n",
            "Iteration 14, loss = 0.66346976\n",
            "Iteration 15, loss = 0.65484224\n",
            "Iteration 16, loss = 0.64765771\n",
            "Iteration 17, loss = 0.64160092\n",
            "Iteration 18, loss = 0.63596695\n",
            "Iteration 19, loss = 0.63145300\n",
            "Iteration 20, loss = 0.62738962\n",
            "Iteration 21, loss = 0.62388968\n",
            "Iteration 22, loss = 0.62051025\n",
            "Iteration 23, loss = 0.61719150\n",
            "Iteration 24, loss = 0.61415457\n",
            "Iteration 25, loss = 0.61154190\n",
            "Iteration 26, loss = 0.60921490\n",
            "Iteration 27, loss = 0.60680333\n",
            "Iteration 28, loss = 0.60463037\n",
            "Iteration 29, loss = 0.60261868\n",
            "Iteration 30, loss = 0.60053790\n",
            "Iteration 31, loss = 0.59861214\n",
            "Iteration 32, loss = 0.59687878\n",
            "Iteration 33, loss = 0.59521436\n",
            "Iteration 34, loss = 0.59373168\n",
            "Iteration 35, loss = 0.59231031\n",
            "Iteration 36, loss = 0.59089091\n",
            "Iteration 37, loss = 0.58975222\n",
            "Iteration 38, loss = 0.58844761\n",
            "Iteration 39, loss = 0.58743580\n",
            "Iteration 40, loss = 0.58644674\n",
            "Iteration 41, loss = 0.58550410\n",
            "Iteration 42, loss = 0.58440164\n",
            "Iteration 43, loss = 0.58328570\n",
            "Iteration 44, loss = 0.58228471\n",
            "Iteration 45, loss = 0.58128176\n",
            "Iteration 46, loss = 0.58040523\n",
            "Iteration 47, loss = 0.57952866\n",
            "Iteration 48, loss = 0.57856752\n",
            "Iteration 49, loss = 0.57759396\n",
            "Iteration 50, loss = 0.57673645\n",
            "Iteration 51, loss = 0.57577758\n",
            "Iteration 52, loss = 0.57477461\n",
            "Iteration 53, loss = 0.57391312\n",
            "Iteration 54, loss = 0.57295768\n",
            "Iteration 55, loss = 0.57235619\n",
            "Iteration 56, loss = 0.57150601\n",
            "Iteration 57, loss = 0.57068862\n",
            "Iteration 58, loss = 0.56995885\n",
            "Iteration 59, loss = 0.56926414\n",
            "Iteration 60, loss = 0.56848196\n",
            "Iteration 61, loss = 0.56771898\n",
            "Iteration 62, loss = 0.56713732\n",
            "Iteration 63, loss = 0.56642712\n",
            "Iteration 64, loss = 0.56596670\n",
            "Iteration 65, loss = 0.56543547\n",
            "Iteration 66, loss = 0.56509810\n",
            "Iteration 67, loss = 0.56455514\n",
            "Iteration 68, loss = 0.56433789\n",
            "Iteration 69, loss = 0.56375333\n",
            "Iteration 70, loss = 0.56327808\n",
            "Iteration 71, loss = 0.56292281\n",
            "Iteration 72, loss = 0.56257013\n",
            "Iteration 73, loss = 0.56215551\n",
            "Iteration 74, loss = 0.56183300\n",
            "Iteration 75, loss = 0.56170533\n",
            "Iteration 76, loss = 0.56121154\n",
            "Iteration 77, loss = 0.56080663\n",
            "Iteration 78, loss = 0.56057794\n",
            "Iteration 79, loss = 0.56030054\n",
            "Iteration 80, loss = 0.56000938\n",
            "Iteration 81, loss = 0.55969876\n",
            "Iteration 82, loss = 0.55937165\n",
            "Iteration 83, loss = 0.55913175\n",
            "Iteration 84, loss = 0.55876924\n",
            "Iteration 85, loss = 0.55859709\n",
            "Iteration 86, loss = 0.55830636\n",
            "Iteration 87, loss = 0.55803344\n",
            "Iteration 88, loss = 0.55769157\n",
            "Iteration 89, loss = 0.55763339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 90, loss = 0.55740669\n",
            "Iteration 91, loss = 0.55726678\n",
            "Iteration 92, loss = 0.55707046\n",
            "Iteration 93, loss = 0.55675904\n",
            "Iteration 94, loss = 0.55659666\n",
            "Iteration 95, loss = 0.55647031\n",
            "Iteration 96, loss = 0.55623035\n",
            "Iteration 97, loss = 0.55605166\n",
            "Iteration 98, loss = 0.55588317\n",
            "Iteration 99, loss = 0.55568320\n",
            "Iteration 100, loss = 0.55547851\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 100 and for layer number 4 : 0.7025\n",
            "Iteration 1, loss = 0.88336594\n",
            "Iteration 2, loss = 0.85371585\n",
            "Iteration 3, loss = 0.81114998\n",
            "Iteration 4, loss = 0.76878568\n",
            "Iteration 5, loss = 0.72891304\n",
            "Iteration 6, loss = 0.69812822\n",
            "Iteration 7, loss = 0.67784648\n",
            "Iteration 8, loss = 0.65873975\n",
            "Iteration 9, loss = 0.64585588\n",
            "Iteration 10, loss = 0.63646662\n",
            "Iteration 11, loss = 0.62908658\n",
            "Iteration 12, loss = 0.62375837\n",
            "Iteration 13, loss = 0.61915746\n",
            "Iteration 14, loss = 0.61496245\n",
            "Iteration 15, loss = 0.61163021\n",
            "Iteration 16, loss = 0.60865001\n",
            "Iteration 17, loss = 0.60613154\n",
            "Iteration 18, loss = 0.60378173\n",
            "Iteration 19, loss = 0.60170407\n",
            "Iteration 20, loss = 0.59968740\n",
            "Iteration 21, loss = 0.59787396\n",
            "Iteration 22, loss = 0.59617398\n",
            "Iteration 23, loss = 0.59435058\n",
            "Iteration 24, loss = 0.59263873\n",
            "Iteration 25, loss = 0.59096900\n",
            "Iteration 26, loss = 0.58936046\n",
            "Iteration 27, loss = 0.58778572\n",
            "Iteration 28, loss = 0.58636900\n",
            "Iteration 29, loss = 0.58490469\n",
            "Iteration 30, loss = 0.58355581\n",
            "Iteration 31, loss = 0.58232012\n",
            "Iteration 32, loss = 0.58092064\n",
            "Iteration 33, loss = 0.57974507\n",
            "Iteration 34, loss = 0.57855058\n",
            "Iteration 35, loss = 0.57753596\n",
            "Iteration 36, loss = 0.57642234\n",
            "Iteration 37, loss = 0.57550100\n",
            "Iteration 38, loss = 0.57458003\n",
            "Iteration 39, loss = 0.57363445\n",
            "Iteration 40, loss = 0.57283919\n",
            "Iteration 41, loss = 0.57182274\n",
            "Iteration 42, loss = 0.57091657\n",
            "Iteration 43, loss = 0.57011771\n",
            "Iteration 44, loss = 0.56937148\n",
            "Iteration 45, loss = 0.56858382\n",
            "Iteration 46, loss = 0.56782347\n",
            "Iteration 47, loss = 0.56719403\n",
            "Iteration 48, loss = 0.56660501\n",
            "Iteration 49, loss = 0.56591930\n",
            "Iteration 50, loss = 0.56508853\n",
            "Iteration 51, loss = 0.56449271\n",
            "Iteration 52, loss = 0.56368499\n",
            "Iteration 53, loss = 0.56297676\n",
            "Iteration 54, loss = 0.56236424\n",
            "Iteration 55, loss = 0.56186295\n",
            "Iteration 56, loss = 0.56108268\n",
            "Iteration 57, loss = 0.56055555\n",
            "Iteration 58, loss = 0.55999247\n",
            "Iteration 59, loss = 0.55958193\n",
            "Iteration 60, loss = 0.55901068\n",
            "Iteration 61, loss = 0.55863943\n",
            "Iteration 62, loss = 0.55821083\n",
            "Iteration 63, loss = 0.55788734\n",
            "Iteration 64, loss = 0.55735403\n",
            "Iteration 65, loss = 0.55691118\n",
            "Iteration 66, loss = 0.55652812\n",
            "Iteration 67, loss = 0.55606649\n",
            "Iteration 68, loss = 0.55577844\n",
            "Iteration 69, loss = 0.55533429\n",
            "Iteration 70, loss = 0.55504655\n",
            "Iteration 71, loss = 0.55474930\n",
            "Iteration 72, loss = 0.55424172\n",
            "Iteration 73, loss = 0.55390727\n",
            "Iteration 74, loss = 0.55356165\n",
            "Iteration 75, loss = 0.55313879\n",
            "Iteration 76, loss = 0.55269980\n",
            "Iteration 77, loss = 0.55238251\n",
            "Iteration 78, loss = 0.55204490\n",
            "Iteration 79, loss = 0.55161358\n",
            "Iteration 80, loss = 0.55135349\n",
            "Iteration 81, loss = 0.55094036\n",
            "Iteration 82, loss = 0.55070037\n",
            "Iteration 83, loss = 0.55039199\n",
            "Iteration 84, loss = 0.54990062\n",
            "Iteration 85, loss = 0.54951022\n",
            "Iteration 86, loss = 0.54923000\n",
            "Iteration 87, loss = 0.54888291\n",
            "Iteration 88, loss = 0.54852080\n",
            "Iteration 89, loss = 0.54819029\n",
            "Iteration 90, loss = 0.54788580\n",
            "Iteration 91, loss = 0.54753336\n",
            "Iteration 92, loss = 0.54723242\n",
            "Iteration 93, loss = 0.54696185\n",
            "Iteration 94, loss = 0.54663047\n",
            "Iteration 95, loss = 0.54642423\n",
            "Iteration 96, loss = 0.54612106\n",
            "Iteration 97, loss = 0.54588052\n",
            "Iteration 98, loss = 0.54566276\n",
            "Iteration 99, loss = 0.54547018\n",
            "Iteration 100, loss = 0.54522125\n",
            "Iteration 1, loss = 0.88215379\n",
            "Iteration 2, loss = 0.85222359\n",
            "Iteration 3, loss = 0.81083995\n",
            "Iteration 4, loss = 0.76741428\n",
            "Iteration 5, loss = 0.72754261\n",
            "Iteration 6, loss = 0.69573379\n",
            "Iteration 7, loss = 0.67445780\n",
            "Iteration 8, loss = 0.65527871\n",
            "Iteration 9, loss = 0.64178806\n",
            "Iteration 10, loss = 0.63239091\n",
            "Iteration 11, loss = 0.62427738\n",
            "Iteration 12, loss = 0.61862901\n",
            "Iteration 13, loss = 0.61370662\n",
            "Iteration 14, loss = 0.60941738\n",
            "Iteration 15, loss = 0.60603175\n",
            "Iteration 16, loss = 0.60286680\n",
            "Iteration 17, loss = 0.60003633\n",
            "Iteration 18, loss = 0.59747356\n",
            "Iteration 19, loss = 0.59516755\n",
            "Iteration 20, loss = 0.59302637\n",
            "Iteration 21, loss = 0.59102069\n",
            "Iteration 22, loss = 0.58906081\n",
            "Iteration 23, loss = 0.58713216\n",
            "Iteration 24, loss = 0.58522686\n",
            "Iteration 25, loss = 0.58343551\n",
            "Iteration 26, loss = 0.58140821\n",
            "Iteration 27, loss = 0.57962191\n",
            "Iteration 28, loss = 0.57773768\n",
            "Iteration 29, loss = 0.57599283\n",
            "Iteration 30, loss = 0.57440911\n",
            "Iteration 31, loss = 0.57292679\n",
            "Iteration 32, loss = 0.57134210\n",
            "Iteration 33, loss = 0.57005743\n",
            "Iteration 34, loss = 0.56866518\n",
            "Iteration 35, loss = 0.56746551\n",
            "Iteration 36, loss = 0.56631215\n",
            "Iteration 37, loss = 0.56506011\n",
            "Iteration 38, loss = 0.56388150\n",
            "Iteration 39, loss = 0.56266819\n",
            "Iteration 40, loss = 0.56150018\n",
            "Iteration 41, loss = 0.56027643\n",
            "Iteration 42, loss = 0.55923365\n",
            "Iteration 43, loss = 0.55839916\n",
            "Iteration 44, loss = 0.55764037\n",
            "Iteration 45, loss = 0.55671588\n",
            "Iteration 46, loss = 0.55603532\n",
            "Iteration 47, loss = 0.55539730\n",
            "Iteration 48, loss = 0.55489305\n",
            "Iteration 49, loss = 0.55430102\n",
            "Iteration 50, loss = 0.55366618\n",
            "Iteration 51, loss = 0.55308823\n",
            "Iteration 52, loss = 0.55248338\n",
            "Iteration 53, loss = 0.55180195\n",
            "Iteration 54, loss = 0.55138032\n",
            "Iteration 55, loss = 0.55064507\n",
            "Iteration 56, loss = 0.55004513\n",
            "Iteration 57, loss = 0.54956125\n",
            "Iteration 58, loss = 0.54905056\n",
            "Iteration 59, loss = 0.54869579\n",
            "Iteration 60, loss = 0.54806567\n",
            "Iteration 61, loss = 0.54766645\n",
            "Iteration 62, loss = 0.54719837\n",
            "Iteration 63, loss = 0.54672621\n",
            "Iteration 64, loss = 0.54622130\n",
            "Iteration 65, loss = 0.54567083\n",
            "Iteration 66, loss = 0.54529271\n",
            "Iteration 67, loss = 0.54475088\n",
            "Iteration 68, loss = 0.54431846\n",
            "Iteration 69, loss = 0.54386110\n",
            "Iteration 70, loss = 0.54360986\n",
            "Iteration 71, loss = 0.54328716\n",
            "Iteration 72, loss = 0.54281576\n",
            "Iteration 73, loss = 0.54245138\n",
            "Iteration 74, loss = 0.54203901\n",
            "Iteration 75, loss = 0.54159343\n",
            "Iteration 76, loss = 0.54118515\n",
            "Iteration 77, loss = 0.54086813\n",
            "Iteration 78, loss = 0.54042066\n",
            "Iteration 79, loss = 0.53996870\n",
            "Iteration 80, loss = 0.53966351\n",
            "Iteration 81, loss = 0.53922575\n",
            "Iteration 82, loss = 0.53883012\n",
            "Iteration 83, loss = 0.53845198\n",
            "Iteration 84, loss = 0.53801889\n",
            "Iteration 85, loss = 0.53775136\n",
            "Iteration 86, loss = 0.53734075\n",
            "Iteration 87, loss = 0.53694553\n",
            "Iteration 88, loss = 0.53650020\n",
            "Iteration 89, loss = 0.53610993\n",
            "Iteration 90, loss = 0.53573081\n",
            "Iteration 91, loss = 0.53537613\n",
            "Iteration 92, loss = 0.53504234\n",
            "Iteration 93, loss = 0.53466072\n",
            "Iteration 94, loss = 0.53441641\n",
            "Iteration 95, loss = 0.53430791\n",
            "Iteration 96, loss = 0.53402009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 97, loss = 0.53367372\n",
            "Iteration 98, loss = 0.53342543\n",
            "Iteration 99, loss = 0.53312309\n",
            "Iteration 100, loss = 0.53285196\n",
            "Iteration 1, loss = 0.88505012\n",
            "Iteration 2, loss = 0.85225471\n",
            "Iteration 3, loss = 0.80692705\n",
            "Iteration 4, loss = 0.76187066\n",
            "Iteration 5, loss = 0.72125962\n",
            "Iteration 6, loss = 0.69001636\n",
            "Iteration 7, loss = 0.66839879\n",
            "Iteration 8, loss = 0.65016370\n",
            "Iteration 9, loss = 0.63775462\n",
            "Iteration 10, loss = 0.62892995\n",
            "Iteration 11, loss = 0.62138454\n",
            "Iteration 12, loss = 0.61613777\n",
            "Iteration 13, loss = 0.61152308\n",
            "Iteration 14, loss = 0.60716429\n",
            "Iteration 15, loss = 0.60391754\n",
            "Iteration 16, loss = 0.60054268\n",
            "Iteration 17, loss = 0.59793934\n",
            "Iteration 18, loss = 0.59555825\n",
            "Iteration 19, loss = 0.59346251\n",
            "Iteration 20, loss = 0.59155139\n",
            "Iteration 21, loss = 0.58951223\n",
            "Iteration 22, loss = 0.58759183\n",
            "Iteration 23, loss = 0.58570953\n",
            "Iteration 24, loss = 0.58384280\n",
            "Iteration 25, loss = 0.58210152\n",
            "Iteration 26, loss = 0.58016830\n",
            "Iteration 27, loss = 0.57837726\n",
            "Iteration 28, loss = 0.57656482\n",
            "Iteration 29, loss = 0.57466052\n",
            "Iteration 30, loss = 0.57280573\n",
            "Iteration 31, loss = 0.57095358\n",
            "Iteration 32, loss = 0.56918098\n",
            "Iteration 33, loss = 0.56766041\n",
            "Iteration 34, loss = 0.56606900\n",
            "Iteration 35, loss = 0.56460579\n",
            "Iteration 36, loss = 0.56336991\n",
            "Iteration 37, loss = 0.56205355\n",
            "Iteration 38, loss = 0.56082783\n",
            "Iteration 39, loss = 0.55947813\n",
            "Iteration 40, loss = 0.55812098\n",
            "Iteration 41, loss = 0.55681742\n",
            "Iteration 42, loss = 0.55563897\n",
            "Iteration 43, loss = 0.55440444\n",
            "Iteration 44, loss = 0.55338679\n",
            "Iteration 45, loss = 0.55235047\n",
            "Iteration 46, loss = 0.55162732\n",
            "Iteration 47, loss = 0.55084266\n",
            "Iteration 48, loss = 0.55021080\n",
            "Iteration 49, loss = 0.54960170\n",
            "Iteration 50, loss = 0.54880901\n",
            "Iteration 51, loss = 0.54801736\n",
            "Iteration 52, loss = 0.54727104\n",
            "Iteration 53, loss = 0.54650042\n",
            "Iteration 54, loss = 0.54590923\n",
            "Iteration 55, loss = 0.54515730\n",
            "Iteration 56, loss = 0.54447942\n",
            "Iteration 57, loss = 0.54393684\n",
            "Iteration 58, loss = 0.54339839\n",
            "Iteration 59, loss = 0.54306135\n",
            "Iteration 60, loss = 0.54242856\n",
            "Iteration 61, loss = 0.54192597\n",
            "Iteration 62, loss = 0.54135690\n",
            "Iteration 63, loss = 0.54087972\n",
            "Iteration 64, loss = 0.54047878\n",
            "Iteration 65, loss = 0.53981990\n",
            "Iteration 66, loss = 0.53953724\n",
            "Iteration 67, loss = 0.53893992\n",
            "Iteration 68, loss = 0.53851477\n",
            "Iteration 69, loss = 0.53807490\n",
            "Iteration 70, loss = 0.53759873\n",
            "Iteration 71, loss = 0.53721866\n",
            "Iteration 72, loss = 0.53669204\n",
            "Iteration 73, loss = 0.53617939\n",
            "Iteration 74, loss = 0.53568681\n",
            "Iteration 75, loss = 0.53513738\n",
            "Iteration 76, loss = 0.53456490\n",
            "Iteration 77, loss = 0.53422560\n",
            "Iteration 78, loss = 0.53376142\n",
            "Iteration 79, loss = 0.53334794\n",
            "Iteration 80, loss = 0.53301828\n",
            "Iteration 81, loss = 0.53262479\n",
            "Iteration 82, loss = 0.53209680\n",
            "Iteration 83, loss = 0.53182712\n",
            "Iteration 84, loss = 0.53126292\n",
            "Iteration 85, loss = 0.53094452\n",
            "Iteration 86, loss = 0.53046918\n",
            "Iteration 87, loss = 0.53009524\n",
            "Iteration 88, loss = 0.52966948\n",
            "Iteration 89, loss = 0.52938844\n",
            "Iteration 90, loss = 0.52908594\n",
            "Iteration 91, loss = 0.52882262\n",
            "Iteration 92, loss = 0.52844590\n",
            "Iteration 93, loss = 0.52818897\n",
            "Iteration 94, loss = 0.52793869\n",
            "Iteration 95, loss = 0.52779602\n",
            "Iteration 96, loss = 0.52731289\n",
            "Iteration 97, loss = 0.52709549\n",
            "Iteration 98, loss = 0.52691321\n",
            "Iteration 99, loss = 0.52654699\n",
            "Iteration 100, loss = 0.52629573\n",
            "Iteration 1, loss = 0.87868239\n",
            "Iteration 2, loss = 0.84534527\n",
            "Iteration 3, loss = 0.80264634\n",
            "Iteration 4, loss = 0.75856716\n",
            "Iteration 5, loss = 0.72047810\n",
            "Iteration 6, loss = 0.69020470\n",
            "Iteration 7, loss = 0.66899651\n",
            "Iteration 8, loss = 0.65325101\n",
            "Iteration 9, loss = 0.64084348\n",
            "Iteration 10, loss = 0.63256251\n",
            "Iteration 11, loss = 0.62593344\n",
            "Iteration 12, loss = 0.62085925\n",
            "Iteration 13, loss = 0.61668379\n",
            "Iteration 14, loss = 0.61298618\n",
            "Iteration 15, loss = 0.61027972\n",
            "Iteration 16, loss = 0.60738093\n",
            "Iteration 17, loss = 0.60519015\n",
            "Iteration 18, loss = 0.60314661\n",
            "Iteration 19, loss = 0.60138211\n",
            "Iteration 20, loss = 0.59990388\n",
            "Iteration 21, loss = 0.59816570\n",
            "Iteration 22, loss = 0.59667437\n",
            "Iteration 23, loss = 0.59523266\n",
            "Iteration 24, loss = 0.59383865\n",
            "Iteration 25, loss = 0.59244065\n",
            "Iteration 26, loss = 0.59109045\n",
            "Iteration 27, loss = 0.58969415\n",
            "Iteration 28, loss = 0.58839863\n",
            "Iteration 29, loss = 0.58706673\n",
            "Iteration 30, loss = 0.58587255\n",
            "Iteration 31, loss = 0.58455172\n",
            "Iteration 32, loss = 0.58331959\n",
            "Iteration 33, loss = 0.58212022\n",
            "Iteration 34, loss = 0.58094719\n",
            "Iteration 35, loss = 0.57976950\n",
            "Iteration 36, loss = 0.57873844\n",
            "Iteration 37, loss = 0.57759582\n",
            "Iteration 38, loss = 0.57646096\n",
            "Iteration 39, loss = 0.57532754\n",
            "Iteration 40, loss = 0.57432229\n",
            "Iteration 41, loss = 0.57318304\n",
            "Iteration 42, loss = 0.57217843\n",
            "Iteration 43, loss = 0.57098995\n",
            "Iteration 44, loss = 0.56998664\n",
            "Iteration 45, loss = 0.56894697\n",
            "Iteration 46, loss = 0.56822469\n",
            "Iteration 47, loss = 0.56721476\n",
            "Iteration 48, loss = 0.56646854\n",
            "Iteration 49, loss = 0.56570712\n",
            "Iteration 50, loss = 0.56480305\n",
            "Iteration 51, loss = 0.56388092\n",
            "Iteration 52, loss = 0.56300559\n",
            "Iteration 53, loss = 0.56215389\n",
            "Iteration 54, loss = 0.56146918\n",
            "Iteration 55, loss = 0.56066012\n",
            "Iteration 56, loss = 0.56010518\n",
            "Iteration 57, loss = 0.55936017\n",
            "Iteration 58, loss = 0.55867960\n",
            "Iteration 59, loss = 0.55806008\n",
            "Iteration 60, loss = 0.55745688\n",
            "Iteration 61, loss = 0.55683435\n",
            "Iteration 62, loss = 0.55617745\n",
            "Iteration 63, loss = 0.55567840\n",
            "Iteration 64, loss = 0.55522448\n",
            "Iteration 65, loss = 0.55451282\n",
            "Iteration 66, loss = 0.55408871\n",
            "Iteration 67, loss = 0.55340767\n",
            "Iteration 68, loss = 0.55308825\n",
            "Iteration 69, loss = 0.55248680\n",
            "Iteration 70, loss = 0.55197835\n",
            "Iteration 71, loss = 0.55151704\n",
            "Iteration 72, loss = 0.55094640\n",
            "Iteration 73, loss = 0.55043214\n",
            "Iteration 74, loss = 0.54999353\n",
            "Iteration 75, loss = 0.54958755\n",
            "Iteration 76, loss = 0.54921556\n",
            "Iteration 77, loss = 0.54881768\n",
            "Iteration 78, loss = 0.54841853\n",
            "Iteration 79, loss = 0.54806308\n",
            "Iteration 80, loss = 0.54755768\n",
            "Iteration 81, loss = 0.54715637\n",
            "Iteration 82, loss = 0.54664993\n",
            "Iteration 83, loss = 0.54633178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 84, loss = 0.54586308\n",
            "Iteration 85, loss = 0.54531629\n",
            "Iteration 86, loss = 0.54479784\n",
            "Iteration 87, loss = 0.54431345\n",
            "Iteration 88, loss = 0.54395825\n",
            "Iteration 89, loss = 0.54347640\n",
            "Iteration 90, loss = 0.54309495\n",
            "Iteration 91, loss = 0.54277762\n",
            "Iteration 92, loss = 0.54228906\n",
            "Iteration 93, loss = 0.54182477\n",
            "Iteration 94, loss = 0.54136635\n",
            "Iteration 95, loss = 0.54103392\n",
            "Iteration 96, loss = 0.54055500\n",
            "Iteration 97, loss = 0.54020524\n",
            "Iteration 98, loss = 0.53990612\n",
            "Iteration 99, loss = 0.53946956\n",
            "Iteration 100, loss = 0.53922342\n",
            "Iteration 1, loss = 0.88420876\n",
            "Iteration 2, loss = 0.85029778\n",
            "Iteration 3, loss = 0.80666346\n",
            "Iteration 4, loss = 0.76251740\n",
            "Iteration 5, loss = 0.72239930\n",
            "Iteration 6, loss = 0.69102003\n",
            "Iteration 7, loss = 0.66914065\n",
            "Iteration 8, loss = 0.65366285\n",
            "Iteration 9, loss = 0.64129681\n",
            "Iteration 10, loss = 0.63364634\n",
            "Iteration 11, loss = 0.62684979\n",
            "Iteration 12, loss = 0.62195144\n",
            "Iteration 13, loss = 0.61783002\n",
            "Iteration 14, loss = 0.61387722\n",
            "Iteration 15, loss = 0.61079756\n",
            "Iteration 16, loss = 0.60786781\n",
            "Iteration 17, loss = 0.60543311\n",
            "Iteration 18, loss = 0.60300274\n",
            "Iteration 19, loss = 0.60112791\n",
            "Iteration 20, loss = 0.59925533\n",
            "Iteration 21, loss = 0.59731542\n",
            "Iteration 22, loss = 0.59563197\n",
            "Iteration 23, loss = 0.59403794\n",
            "Iteration 24, loss = 0.59252695\n",
            "Iteration 25, loss = 0.59087718\n",
            "Iteration 26, loss = 0.58932431\n",
            "Iteration 27, loss = 0.58783735\n",
            "Iteration 28, loss = 0.58652909\n",
            "Iteration 29, loss = 0.58526168\n",
            "Iteration 30, loss = 0.58384581\n",
            "Iteration 31, loss = 0.58249022\n",
            "Iteration 32, loss = 0.58107302\n",
            "Iteration 33, loss = 0.57968523\n",
            "Iteration 34, loss = 0.57826974\n",
            "Iteration 35, loss = 0.57685409\n",
            "Iteration 36, loss = 0.57570653\n",
            "Iteration 37, loss = 0.57454359\n",
            "Iteration 38, loss = 0.57337072\n",
            "Iteration 39, loss = 0.57236750\n",
            "Iteration 40, loss = 0.57151315\n",
            "Iteration 41, loss = 0.57056749\n",
            "Iteration 42, loss = 0.56961456\n",
            "Iteration 43, loss = 0.56866093\n",
            "Iteration 44, loss = 0.56779404\n",
            "Iteration 45, loss = 0.56702039\n",
            "Iteration 46, loss = 0.56636969\n",
            "Iteration 47, loss = 0.56567263\n",
            "Iteration 48, loss = 0.56506709\n",
            "Iteration 49, loss = 0.56456720\n",
            "Iteration 50, loss = 0.56395569\n",
            "Iteration 51, loss = 0.56323524\n",
            "Iteration 52, loss = 0.56254141\n",
            "Iteration 53, loss = 0.56184053\n",
            "Iteration 54, loss = 0.56130169\n",
            "Iteration 55, loss = 0.56068746\n",
            "Iteration 56, loss = 0.56027446\n",
            "Iteration 57, loss = 0.55964821\n",
            "Iteration 58, loss = 0.55911947\n",
            "Iteration 59, loss = 0.55863265\n",
            "Iteration 60, loss = 0.55817650\n",
            "Iteration 61, loss = 0.55778245\n",
            "Iteration 62, loss = 0.55727190\n",
            "Iteration 63, loss = 0.55689895\n",
            "Iteration 64, loss = 0.55664518\n",
            "Iteration 65, loss = 0.55617918\n",
            "Iteration 66, loss = 0.55581549\n",
            "Iteration 67, loss = 0.55536771\n",
            "Iteration 68, loss = 0.55518132\n",
            "Iteration 69, loss = 0.55482123\n",
            "Iteration 70, loss = 0.55467827\n",
            "Iteration 71, loss = 0.55436780\n",
            "Iteration 72, loss = 0.55404453\n",
            "Iteration 73, loss = 0.55373464\n",
            "Iteration 74, loss = 0.55344119\n",
            "Iteration 75, loss = 0.55320977\n",
            "Iteration 76, loss = 0.55299683\n",
            "Iteration 77, loss = 0.55278411\n",
            "Iteration 78, loss = 0.55251193\n",
            "Iteration 79, loss = 0.55228212\n",
            "Iteration 80, loss = 0.55204654\n",
            "Iteration 81, loss = 0.55182542\n",
            "Iteration 82, loss = 0.55150055\n",
            "Iteration 83, loss = 0.55129268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 84, loss = 0.55103820\n",
            "Iteration 85, loss = 0.55075821\n",
            "Iteration 86, loss = 0.55045633\n",
            "Iteration 87, loss = 0.55023268\n",
            "Iteration 88, loss = 0.55010605\n",
            "Iteration 89, loss = 0.54982742\n",
            "Iteration 90, loss = 0.54971483\n",
            "Iteration 91, loss = 0.54972757\n",
            "Iteration 92, loss = 0.54939568\n",
            "Iteration 93, loss = 0.54898982\n",
            "Iteration 94, loss = 0.54881902\n",
            "Iteration 95, loss = 0.54858255\n",
            "Iteration 96, loss = 0.54831362\n",
            "Iteration 97, loss = 0.54811557\n",
            "Iteration 98, loss = 0.54799809\n",
            "Iteration 99, loss = 0.54778299\n",
            "Iteration 100, loss = 0.54765785\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 100 and for layer number 5 : 0.7025\n",
            "Iteration 1, loss = 1.36577264\n",
            "Iteration 2, loss = 1.21631564\n",
            "Iteration 3, loss = 1.02778964\n",
            "Iteration 4, loss = 0.86161348\n",
            "Iteration 5, loss = 0.75517608\n",
            "Iteration 6, loss = 0.69677016\n",
            "Iteration 7, loss = 0.67054166\n",
            "Iteration 8, loss = 0.65696349\n",
            "Iteration 9, loss = 0.64924227\n",
            "Iteration 10, loss = 0.64200040\n",
            "Iteration 11, loss = 0.63502000\n",
            "Iteration 12, loss = 0.62804496\n",
            "Iteration 13, loss = 0.62121423\n",
            "Iteration 14, loss = 0.61544081\n",
            "Iteration 15, loss = 0.60966913\n",
            "Iteration 16, loss = 0.60458321\n",
            "Iteration 17, loss = 0.60053459\n",
            "Iteration 18, loss = 0.59669563\n",
            "Iteration 19, loss = 0.59365093\n",
            "Iteration 20, loss = 0.59065223\n",
            "Iteration 21, loss = 0.58804416\n",
            "Iteration 22, loss = 0.58582895\n",
            "Iteration 23, loss = 0.58349854\n",
            "Iteration 24, loss = 0.58165816\n",
            "Iteration 25, loss = 0.57966754\n",
            "Iteration 26, loss = 0.57764872\n",
            "Iteration 27, loss = 0.57592592\n",
            "Iteration 28, loss = 0.57428512\n",
            "Iteration 29, loss = 0.57254934\n",
            "Iteration 30, loss = 0.57118603\n",
            "Iteration 31, loss = 0.56960122\n",
            "Iteration 32, loss = 0.56838182\n",
            "Iteration 33, loss = 0.56685777\n",
            "Iteration 34, loss = 0.56551142\n",
            "Iteration 35, loss = 0.56440679\n",
            "Iteration 36, loss = 0.56326099\n",
            "Iteration 37, loss = 0.56204087\n",
            "Iteration 38, loss = 0.56109031\n",
            "Iteration 39, loss = 0.55986212\n",
            "Iteration 40, loss = 0.55891318\n",
            "Iteration 41, loss = 0.55804948\n",
            "Iteration 42, loss = 0.55708778\n",
            "Iteration 43, loss = 0.55612363\n",
            "Iteration 44, loss = 0.55520370\n",
            "Iteration 45, loss = 0.55433500\n",
            "Iteration 46, loss = 0.55324730\n",
            "Iteration 47, loss = 0.55239553\n",
            "Iteration 48, loss = 0.55173779\n",
            "Iteration 49, loss = 0.55101393\n",
            "Iteration 50, loss = 0.55038937\n",
            "Iteration 51, loss = 0.54971687\n",
            "Iteration 52, loss = 0.54916416\n",
            "Iteration 53, loss = 0.54852380\n",
            "Iteration 54, loss = 0.54794764\n",
            "Iteration 55, loss = 0.54748227\n",
            "Iteration 56, loss = 0.54684467\n",
            "Iteration 57, loss = 0.54634707\n",
            "Iteration 58, loss = 0.54580420\n",
            "Iteration 59, loss = 0.54519260\n",
            "Iteration 60, loss = 0.54468122\n",
            "Iteration 61, loss = 0.54419114\n",
            "Iteration 62, loss = 0.54359535\n",
            "Iteration 63, loss = 0.54298356\n",
            "Iteration 64, loss = 0.54246837\n",
            "Iteration 65, loss = 0.54181965\n",
            "Iteration 66, loss = 0.54124425\n",
            "Iteration 67, loss = 0.54066048\n",
            "Iteration 68, loss = 0.54002205\n",
            "Iteration 69, loss = 0.53948822\n",
            "Iteration 70, loss = 0.53892974\n",
            "Iteration 71, loss = 0.53834934\n",
            "Iteration 72, loss = 0.53786761\n",
            "Iteration 73, loss = 0.53739308\n",
            "Iteration 74, loss = 0.53716736\n",
            "Iteration 75, loss = 0.53641757\n",
            "Iteration 76, loss = 0.53606271\n",
            "Iteration 77, loss = 0.53557915\n",
            "Iteration 78, loss = 0.53511829\n",
            "Iteration 79, loss = 0.53464439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 80, loss = 0.53433061\n",
            "Iteration 81, loss = 0.53404424\n",
            "Iteration 82, loss = 0.53369755\n",
            "Iteration 83, loss = 0.53324430\n",
            "Iteration 84, loss = 0.53294977\n",
            "Iteration 85, loss = 0.53250768\n",
            "Iteration 86, loss = 0.53216655\n",
            "Iteration 87, loss = 0.53175103\n",
            "Iteration 88, loss = 0.53142669\n",
            "Iteration 89, loss = 0.53126093\n",
            "Iteration 90, loss = 0.53081181\n",
            "Iteration 91, loss = 0.53051382\n",
            "Iteration 92, loss = 0.53014209\n",
            "Iteration 93, loss = 0.52983351\n",
            "Iteration 94, loss = 0.52955779\n",
            "Iteration 95, loss = 0.52924251\n",
            "Iteration 96, loss = 0.52894063\n",
            "Iteration 97, loss = 0.52859632\n",
            "Iteration 98, loss = 0.52836618\n",
            "Iteration 99, loss = 0.52816789\n",
            "Iteration 100, loss = 0.52777562\n",
            "Iteration 1, loss = 1.35514429\n",
            "Iteration 2, loss = 1.20785553\n",
            "Iteration 3, loss = 1.02088892\n",
            "Iteration 4, loss = 0.85738136\n",
            "Iteration 5, loss = 0.75279440\n",
            "Iteration 6, loss = 0.69511781\n",
            "Iteration 7, loss = 0.66830518\n",
            "Iteration 8, loss = 0.65594188\n",
            "Iteration 9, loss = 0.64756070\n",
            "Iteration 10, loss = 0.63958029\n",
            "Iteration 11, loss = 0.63226591\n",
            "Iteration 12, loss = 0.62537086\n",
            "Iteration 13, loss = 0.61859528\n",
            "Iteration 14, loss = 0.61262202\n",
            "Iteration 15, loss = 0.60683866\n",
            "Iteration 16, loss = 0.60192057\n",
            "Iteration 17, loss = 0.59769403\n",
            "Iteration 18, loss = 0.59407468\n",
            "Iteration 19, loss = 0.59087969\n",
            "Iteration 20, loss = 0.58787421\n",
            "Iteration 21, loss = 0.58490220\n",
            "Iteration 22, loss = 0.58260316\n",
            "Iteration 23, loss = 0.57994422\n",
            "Iteration 24, loss = 0.57770562\n",
            "Iteration 25, loss = 0.57568745\n",
            "Iteration 26, loss = 0.57360864\n",
            "Iteration 27, loss = 0.57185682\n",
            "Iteration 28, loss = 0.57009674\n",
            "Iteration 29, loss = 0.56836102\n",
            "Iteration 30, loss = 0.56690076\n",
            "Iteration 31, loss = 0.56525335\n",
            "Iteration 32, loss = 0.56398245\n",
            "Iteration 33, loss = 0.56232638\n",
            "Iteration 34, loss = 0.56101582\n",
            "Iteration 35, loss = 0.55982524\n",
            "Iteration 36, loss = 0.55856738\n",
            "Iteration 37, loss = 0.55735555\n",
            "Iteration 38, loss = 0.55627713\n",
            "Iteration 39, loss = 0.55510543\n",
            "Iteration 40, loss = 0.55407973\n",
            "Iteration 41, loss = 0.55330870\n",
            "Iteration 42, loss = 0.55233209\n",
            "Iteration 43, loss = 0.55132070\n",
            "Iteration 44, loss = 0.55036474\n",
            "Iteration 45, loss = 0.54944197\n",
            "Iteration 46, loss = 0.54837056\n",
            "Iteration 47, loss = 0.54738358\n",
            "Iteration 48, loss = 0.54652592\n",
            "Iteration 49, loss = 0.54571200\n",
            "Iteration 50, loss = 0.54485943\n",
            "Iteration 51, loss = 0.54400365\n",
            "Iteration 52, loss = 0.54341597\n",
            "Iteration 53, loss = 0.54259256\n",
            "Iteration 54, loss = 0.54195325\n",
            "Iteration 55, loss = 0.54135470\n",
            "Iteration 56, loss = 0.54070979\n",
            "Iteration 57, loss = 0.54016796\n",
            "Iteration 58, loss = 0.53969689\n",
            "Iteration 59, loss = 0.53917235\n",
            "Iteration 60, loss = 0.53869064\n",
            "Iteration 61, loss = 0.53838360\n",
            "Iteration 62, loss = 0.53768739\n",
            "Iteration 63, loss = 0.53703995\n",
            "Iteration 64, loss = 0.53649471\n",
            "Iteration 65, loss = 0.53589237\n",
            "Iteration 66, loss = 0.53544138\n",
            "Iteration 67, loss = 0.53490436\n",
            "Iteration 68, loss = 0.53439025\n",
            "Iteration 69, loss = 0.53396900\n",
            "Iteration 70, loss = 0.53351046\n",
            "Iteration 71, loss = 0.53296450\n",
            "Iteration 72, loss = 0.53245616\n",
            "Iteration 73, loss = 0.53198883\n",
            "Iteration 74, loss = 0.53171321\n",
            "Iteration 75, loss = 0.53101324\n",
            "Iteration 76, loss = 0.53067590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 77, loss = 0.53026639\n",
            "Iteration 78, loss = 0.52987296\n",
            "Iteration 79, loss = 0.52957369\n",
            "Iteration 80, loss = 0.52923171\n",
            "Iteration 81, loss = 0.52877918\n",
            "Iteration 82, loss = 0.52845922\n",
            "Iteration 83, loss = 0.52806888\n",
            "Iteration 84, loss = 0.52770694\n",
            "Iteration 85, loss = 0.52724198\n",
            "Iteration 86, loss = 0.52678667\n",
            "Iteration 87, loss = 0.52638671\n",
            "Iteration 88, loss = 0.52591062\n",
            "Iteration 89, loss = 0.52570284\n",
            "Iteration 90, loss = 0.52524291\n",
            "Iteration 91, loss = 0.52484179\n",
            "Iteration 92, loss = 0.52438492\n",
            "Iteration 93, loss = 0.52405214\n",
            "Iteration 94, loss = 0.52381102\n",
            "Iteration 95, loss = 0.52339081\n",
            "Iteration 96, loss = 0.52297938\n",
            "Iteration 97, loss = 0.52261205\n",
            "Iteration 98, loss = 0.52217822\n",
            "Iteration 99, loss = 0.52179697\n",
            "Iteration 100, loss = 0.52129746\n",
            "Iteration 1, loss = 1.35569250\n",
            "Iteration 2, loss = 1.20977855\n",
            "Iteration 3, loss = 1.02421468\n",
            "Iteration 4, loss = 0.86114766\n",
            "Iteration 5, loss = 0.75352175\n",
            "Iteration 6, loss = 0.69589482\n",
            "Iteration 7, loss = 0.66854088\n",
            "Iteration 8, loss = 0.65502259\n",
            "Iteration 9, loss = 0.64663631\n",
            "Iteration 10, loss = 0.63885391\n",
            "Iteration 11, loss = 0.63126598\n",
            "Iteration 12, loss = 0.62391326\n",
            "Iteration 13, loss = 0.61710692\n",
            "Iteration 14, loss = 0.61051424\n",
            "Iteration 15, loss = 0.60440264\n",
            "Iteration 16, loss = 0.59904249\n",
            "Iteration 17, loss = 0.59421694\n",
            "Iteration 18, loss = 0.58983593\n",
            "Iteration 19, loss = 0.58606930\n",
            "Iteration 20, loss = 0.58266201\n",
            "Iteration 21, loss = 0.57939851\n",
            "Iteration 22, loss = 0.57651751\n",
            "Iteration 23, loss = 0.57331381\n",
            "Iteration 24, loss = 0.57061743\n",
            "Iteration 25, loss = 0.56816832\n",
            "Iteration 26, loss = 0.56558906\n",
            "Iteration 27, loss = 0.56354880\n",
            "Iteration 28, loss = 0.56127377\n",
            "Iteration 29, loss = 0.55916195\n",
            "Iteration 30, loss = 0.55726156\n",
            "Iteration 31, loss = 0.55534880\n",
            "Iteration 32, loss = 0.55363341\n",
            "Iteration 33, loss = 0.55176312\n",
            "Iteration 34, loss = 0.55028963\n",
            "Iteration 35, loss = 0.54884212\n",
            "Iteration 36, loss = 0.54733604\n",
            "Iteration 37, loss = 0.54583440\n",
            "Iteration 38, loss = 0.54439355\n",
            "Iteration 39, loss = 0.54300394\n",
            "Iteration 40, loss = 0.54174970\n",
            "Iteration 41, loss = 0.54070347\n",
            "Iteration 42, loss = 0.53965873\n",
            "Iteration 43, loss = 0.53854609\n",
            "Iteration 44, loss = 0.53749954\n",
            "Iteration 45, loss = 0.53666066\n",
            "Iteration 46, loss = 0.53548315\n",
            "Iteration 47, loss = 0.53456246\n",
            "Iteration 48, loss = 0.53368647\n",
            "Iteration 49, loss = 0.53288963\n",
            "Iteration 50, loss = 0.53210325\n",
            "Iteration 51, loss = 0.53127099\n",
            "Iteration 52, loss = 0.53064426\n",
            "Iteration 53, loss = 0.52966743\n",
            "Iteration 54, loss = 0.52903463\n",
            "Iteration 55, loss = 0.52829084\n",
            "Iteration 56, loss = 0.52752085\n",
            "Iteration 57, loss = 0.52692793\n",
            "Iteration 58, loss = 0.52620528\n",
            "Iteration 59, loss = 0.52564618\n",
            "Iteration 60, loss = 0.52501302\n",
            "Iteration 61, loss = 0.52461704\n",
            "Iteration 62, loss = 0.52397652\n",
            "Iteration 63, loss = 0.52328006\n",
            "Iteration 64, loss = 0.52274315\n",
            "Iteration 65, loss = 0.52209173\n",
            "Iteration 66, loss = 0.52161141\n",
            "Iteration 67, loss = 0.52102971\n",
            "Iteration 68, loss = 0.52051457\n",
            "Iteration 69, loss = 0.51994342\n",
            "Iteration 70, loss = 0.51949139\n",
            "Iteration 71, loss = 0.51894718\n",
            "Iteration 72, loss = 0.51834461\n",
            "Iteration 73, loss = 0.51783013\n",
            "Iteration 74, loss = 0.51731223\n",
            "Iteration 75, loss = 0.51688740\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 76, loss = 0.51646968\n",
            "Iteration 77, loss = 0.51623903\n",
            "Iteration 78, loss = 0.51580940\n",
            "Iteration 79, loss = 0.51535095\n",
            "Iteration 80, loss = 0.51513440\n",
            "Iteration 81, loss = 0.51460954\n",
            "Iteration 82, loss = 0.51437374\n",
            "Iteration 83, loss = 0.51399374\n",
            "Iteration 84, loss = 0.51366061\n",
            "Iteration 85, loss = 0.51329859\n",
            "Iteration 86, loss = 0.51284274\n",
            "Iteration 87, loss = 0.51240931\n",
            "Iteration 88, loss = 0.51200998\n",
            "Iteration 89, loss = 0.51163393\n",
            "Iteration 90, loss = 0.51123178\n",
            "Iteration 91, loss = 0.51078401\n",
            "Iteration 92, loss = 0.51037211\n",
            "Iteration 93, loss = 0.51000532\n",
            "Iteration 94, loss = 0.50948955\n",
            "Iteration 95, loss = 0.50923813\n",
            "Iteration 96, loss = 0.50879173\n",
            "Iteration 97, loss = 0.50827459\n",
            "Iteration 98, loss = 0.50788042\n",
            "Iteration 99, loss = 0.50765154\n",
            "Iteration 100, loss = 0.50721205\n",
            "Iteration 1, loss = 1.36723929\n",
            "Iteration 2, loss = 1.21905982\n",
            "Iteration 3, loss = 1.03107889\n",
            "Iteration 4, loss = 0.86195746\n",
            "Iteration 5, loss = 0.75246019\n",
            "Iteration 6, loss = 0.69773006\n",
            "Iteration 7, loss = 0.67069081\n",
            "Iteration 8, loss = 0.65769343\n",
            "Iteration 9, loss = 0.64888341\n",
            "Iteration 10, loss = 0.64099600\n",
            "Iteration 11, loss = 0.63323861\n",
            "Iteration 12, loss = 0.62561845\n",
            "Iteration 13, loss = 0.61893797\n",
            "Iteration 14, loss = 0.61269725\n",
            "Iteration 15, loss = 0.60721156\n",
            "Iteration 16, loss = 0.60205105\n",
            "Iteration 17, loss = 0.59786319\n",
            "Iteration 18, loss = 0.59406231\n",
            "Iteration 19, loss = 0.59071031\n",
            "Iteration 20, loss = 0.58803417\n",
            "Iteration 21, loss = 0.58526381\n",
            "Iteration 22, loss = 0.58253915\n",
            "Iteration 23, loss = 0.57958451\n",
            "Iteration 24, loss = 0.57694386\n",
            "Iteration 25, loss = 0.57480505\n",
            "Iteration 26, loss = 0.57235466\n",
            "Iteration 27, loss = 0.57040575\n",
            "Iteration 28, loss = 0.56840383\n",
            "Iteration 29, loss = 0.56663716\n",
            "Iteration 30, loss = 0.56492198\n",
            "Iteration 31, loss = 0.56329133\n",
            "Iteration 32, loss = 0.56191467\n",
            "Iteration 33, loss = 0.56045268\n",
            "Iteration 34, loss = 0.55912756\n",
            "Iteration 35, loss = 0.55794052\n",
            "Iteration 36, loss = 0.55654732\n",
            "Iteration 37, loss = 0.55516411\n",
            "Iteration 38, loss = 0.55392465\n",
            "Iteration 39, loss = 0.55276870\n",
            "Iteration 40, loss = 0.55161758\n",
            "Iteration 41, loss = 0.55055639\n",
            "Iteration 42, loss = 0.54954353\n",
            "Iteration 43, loss = 0.54854303\n",
            "Iteration 44, loss = 0.54750768\n",
            "Iteration 45, loss = 0.54666345\n",
            "Iteration 46, loss = 0.54566846\n",
            "Iteration 47, loss = 0.54496082\n",
            "Iteration 48, loss = 0.54419184\n",
            "Iteration 49, loss = 0.54348414\n",
            "Iteration 50, loss = 0.54278759\n",
            "Iteration 51, loss = 0.54212916\n",
            "Iteration 52, loss = 0.54153784\n",
            "Iteration 53, loss = 0.54082423\n",
            "Iteration 54, loss = 0.54022182\n",
            "Iteration 55, loss = 0.53954595\n",
            "Iteration 56, loss = 0.53892056\n",
            "Iteration 57, loss = 0.53836775\n",
            "Iteration 58, loss = 0.53769234\n",
            "Iteration 59, loss = 0.53724987\n",
            "Iteration 60, loss = 0.53663885\n",
            "Iteration 61, loss = 0.53612754\n",
            "Iteration 62, loss = 0.53547468\n",
            "Iteration 63, loss = 0.53504210\n",
            "Iteration 64, loss = 0.53427768\n",
            "Iteration 65, loss = 0.53366504\n",
            "Iteration 66, loss = 0.53303150\n",
            "Iteration 67, loss = 0.53238321\n",
            "Iteration 68, loss = 0.53194125\n",
            "Iteration 69, loss = 0.53133059\n",
            "Iteration 70, loss = 0.53074222\n",
            "Iteration 71, loss = 0.53014142\n",
            "Iteration 72, loss = 0.52957523\n",
            "Iteration 73, loss = 0.52908565\n",
            "Iteration 74, loss = 0.52854630\n",
            "Iteration 75, loss = 0.52804022\n",
            "Iteration 76, loss = 0.52757641\n",
            "Iteration 77, loss = 0.52717378\n",
            "Iteration 78, loss = 0.52668493\n",
            "Iteration 79, loss = 0.52635280\n",
            "Iteration 80, loss = 0.52604054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 81, loss = 0.52543617\n",
            "Iteration 82, loss = 0.52503550\n",
            "Iteration 83, loss = 0.52449942\n",
            "Iteration 84, loss = 0.52396905\n",
            "Iteration 85, loss = 0.52358350\n",
            "Iteration 86, loss = 0.52302113\n",
            "Iteration 87, loss = 0.52252843\n",
            "Iteration 88, loss = 0.52205340\n",
            "Iteration 89, loss = 0.52165514\n",
            "Iteration 90, loss = 0.52115976\n",
            "Iteration 91, loss = 0.52074615\n",
            "Iteration 92, loss = 0.52025791\n",
            "Iteration 93, loss = 0.51985299\n",
            "Iteration 94, loss = 0.51946563\n",
            "Iteration 95, loss = 0.51903519\n",
            "Iteration 96, loss = 0.51853386\n",
            "Iteration 97, loss = 0.51813098\n",
            "Iteration 98, loss = 0.51771788\n",
            "Iteration 99, loss = 0.51733601\n",
            "Iteration 100, loss = 0.51689669\n",
            "Iteration 1, loss = 1.34182386\n",
            "Iteration 2, loss = 1.20486792\n",
            "Iteration 3, loss = 1.03052691\n",
            "Iteration 4, loss = 0.87573110\n",
            "Iteration 5, loss = 0.76767102\n",
            "Iteration 6, loss = 0.71005122\n",
            "Iteration 7, loss = 0.68025523\n",
            "Iteration 8, loss = 0.66452169\n",
            "Iteration 9, loss = 0.65582803\n",
            "Iteration 10, loss = 0.64798558\n",
            "Iteration 11, loss = 0.64058304\n",
            "Iteration 12, loss = 0.63342237\n",
            "Iteration 13, loss = 0.62701482\n",
            "Iteration 14, loss = 0.62068021\n",
            "Iteration 15, loss = 0.61547319\n",
            "Iteration 16, loss = 0.61076412\n",
            "Iteration 17, loss = 0.60674878\n",
            "Iteration 18, loss = 0.60287881\n",
            "Iteration 19, loss = 0.59941555\n",
            "Iteration 20, loss = 0.59658795\n",
            "Iteration 21, loss = 0.59379273\n",
            "Iteration 22, loss = 0.59137819\n",
            "Iteration 23, loss = 0.58868711\n",
            "Iteration 24, loss = 0.58623099\n",
            "Iteration 25, loss = 0.58407023\n",
            "Iteration 26, loss = 0.58184564\n",
            "Iteration 27, loss = 0.57995834\n",
            "Iteration 28, loss = 0.57811124\n",
            "Iteration 29, loss = 0.57634992\n",
            "Iteration 30, loss = 0.57458462\n",
            "Iteration 31, loss = 0.57298054\n",
            "Iteration 32, loss = 0.57144934\n",
            "Iteration 33, loss = 0.56999777\n",
            "Iteration 34, loss = 0.56847783\n",
            "Iteration 35, loss = 0.56719457\n",
            "Iteration 36, loss = 0.56571401\n",
            "Iteration 37, loss = 0.56429184\n",
            "Iteration 38, loss = 0.56282301\n",
            "Iteration 39, loss = 0.56135988\n",
            "Iteration 40, loss = 0.55987407\n",
            "Iteration 41, loss = 0.55831365\n",
            "Iteration 42, loss = 0.55708077\n",
            "Iteration 43, loss = 0.55575808\n",
            "Iteration 44, loss = 0.55445939\n",
            "Iteration 45, loss = 0.55327371\n",
            "Iteration 46, loss = 0.55209696\n",
            "Iteration 47, loss = 0.55130947\n",
            "Iteration 48, loss = 0.55024949\n",
            "Iteration 49, loss = 0.54932537\n",
            "Iteration 50, loss = 0.54833315\n",
            "Iteration 51, loss = 0.54736602\n",
            "Iteration 52, loss = 0.54661875\n",
            "Iteration 53, loss = 0.54587476\n",
            "Iteration 54, loss = 0.54513703\n",
            "Iteration 55, loss = 0.54429737\n",
            "Iteration 56, loss = 0.54357680\n",
            "Iteration 57, loss = 0.54285780\n",
            "Iteration 58, loss = 0.54213330\n",
            "Iteration 59, loss = 0.54160968\n",
            "Iteration 60, loss = 0.54088100\n",
            "Iteration 61, loss = 0.54034655\n",
            "Iteration 62, loss = 0.53973508\n",
            "Iteration 63, loss = 0.53941835\n",
            "Iteration 64, loss = 0.53872499\n",
            "Iteration 65, loss = 0.53810126\n",
            "Iteration 66, loss = 0.53764695\n",
            "Iteration 67, loss = 0.53703482\n",
            "Iteration 68, loss = 0.53660654\n",
            "Iteration 69, loss = 0.53600849\n",
            "Iteration 70, loss = 0.53553453\n",
            "Iteration 71, loss = 0.53495367\n",
            "Iteration 72, loss = 0.53445071\n",
            "Iteration 73, loss = 0.53399473\n",
            "Iteration 74, loss = 0.53354493\n",
            "Iteration 75, loss = 0.53323114\n",
            "Iteration 76, loss = 0.53281414\n",
            "Iteration 77, loss = 0.53234541\n",
            "Iteration 78, loss = 0.53195831\n",
            "Iteration 79, loss = 0.53161136\n",
            "Iteration 80, loss = 0.53126047\n",
            "Iteration 81, loss = 0.53085038\n",
            "Iteration 82, loss = 0.53049602\n",
            "Iteration 83, loss = 0.53010471\n",
            "Iteration 84, loss = 0.52972940\n",
            "Iteration 85, loss = 0.52942890\n",
            "Iteration 86, loss = 0.52903268\n",
            "Iteration 87, loss = 0.52860258\n",
            "Iteration 88, loss = 0.52823206\n",
            "Iteration 89, loss = 0.52794637\n",
            "Iteration 90, loss = 0.52760289\n",
            "Iteration 91, loss = 0.52728967\n",
            "Iteration 92, loss = 0.52693172\n",
            "Iteration 93, loss = 0.52664655\n",
            "Iteration 94, loss = 0.52628103\n",
            "Iteration 95, loss = 0.52585124\n",
            "Iteration 96, loss = 0.52557953\n",
            "Iteration 97, loss = 0.52530700\n",
            "Iteration 98, loss = 0.52495444\n",
            "Iteration 99, loss = 0.52463324\n",
            "Iteration 100, loss = 0.52435696\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 100 and for layer number 6 : 0.73\n",
            "Iteration 1, loss = 0.61686348\n",
            "Iteration 2, loss = 0.61615212\n",
            "Iteration 3, loss = 0.61509889\n",
            "Iteration 4, loss = 0.61393355\n",
            "Iteration 5, loss = 0.61267100\n",
            "Iteration 6, loss = 0.61156262\n",
            "Iteration 7, loss = 0.61039793\n",
            "Iteration 8, loss = 0.60920464\n",
            "Iteration 9, loss = 0.60819776\n",
            "Iteration 10, loss = 0.60727368\n",
            "Iteration 11, loss = 0.60643854\n",
            "Iteration 12, loss = 0.60574605\n",
            "Iteration 13, loss = 0.60503921\n",
            "Iteration 14, loss = 0.60439293\n",
            "Iteration 15, loss = 0.60372900\n",
            "Iteration 16, loss = 0.60312998\n",
            "Iteration 17, loss = 0.60257188\n",
            "Iteration 18, loss = 0.60207421\n",
            "Iteration 19, loss = 0.60156745\n",
            "Iteration 20, loss = 0.60103417\n",
            "Iteration 21, loss = 0.60046005\n",
            "Iteration 22, loss = 0.60001837\n",
            "Iteration 23, loss = 0.59949961\n",
            "Iteration 24, loss = 0.59907076\n",
            "Iteration 25, loss = 0.59862261\n",
            "Iteration 26, loss = 0.59821262\n",
            "Iteration 27, loss = 0.59768043\n",
            "Iteration 28, loss = 0.59727575\n",
            "Iteration 29, loss = 0.59686577\n",
            "Iteration 30, loss = 0.59634456\n",
            "Iteration 31, loss = 0.59592913\n",
            "Iteration 32, loss = 0.59551543\n",
            "Iteration 33, loss = 0.59511982\n",
            "Iteration 34, loss = 0.59488057\n",
            "Iteration 35, loss = 0.59437990\n",
            "Iteration 36, loss = 0.59393766\n",
            "Iteration 37, loss = 0.59343460\n",
            "Iteration 38, loss = 0.59299992\n",
            "Iteration 39, loss = 0.59251347\n",
            "Iteration 40, loss = 0.59210013\n",
            "Iteration 41, loss = 0.59158753\n",
            "Iteration 42, loss = 0.59116433\n",
            "Iteration 43, loss = 0.59067977\n",
            "Iteration 44, loss = 0.59030424\n",
            "Iteration 45, loss = 0.58985025\n",
            "Iteration 46, loss = 0.58934126\n",
            "Iteration 47, loss = 0.58893644\n",
            "Iteration 48, loss = 0.58838509\n",
            "Iteration 49, loss = 0.58789781\n",
            "Iteration 50, loss = 0.58745034\n",
            "Iteration 51, loss = 0.58705463\n",
            "Iteration 52, loss = 0.58659520\n",
            "Iteration 53, loss = 0.58621788\n",
            "Iteration 54, loss = 0.58582979\n",
            "Iteration 55, loss = 0.58542994\n",
            "Iteration 56, loss = 0.58504137\n",
            "Iteration 57, loss = 0.58463168\n",
            "Iteration 58, loss = 0.58429345\n",
            "Iteration 59, loss = 0.58388301\n",
            "Iteration 60, loss = 0.58354039\n",
            "Iteration 61, loss = 0.58310095\n",
            "Iteration 62, loss = 0.58279069\n",
            "Iteration 63, loss = 0.58237879\n",
            "Iteration 64, loss = 0.58204179\n",
            "Iteration 65, loss = 0.58167540\n",
            "Iteration 66, loss = 0.58131958\n",
            "Iteration 67, loss = 0.58107527\n",
            "Iteration 68, loss = 0.58075012\n",
            "Iteration 69, loss = 0.58043709\n",
            "Iteration 70, loss = 0.58009698\n",
            "Iteration 71, loss = 0.57979144\n",
            "Iteration 72, loss = 0.57952207\n",
            "Iteration 73, loss = 0.57918802\n",
            "Iteration 74, loss = 0.57892494\n",
            "Iteration 75, loss = 0.57862554\n",
            "Iteration 76, loss = 0.57835511\n",
            "Iteration 77, loss = 0.57808078\n",
            "Iteration 78, loss = 0.57780786\n",
            "Iteration 79, loss = 0.57753624\n",
            "Iteration 80, loss = 0.57722696\n",
            "Iteration 81, loss = 0.57695431\n",
            "Iteration 82, loss = 0.57660842\n",
            "Iteration 83, loss = 0.57639990\n",
            "Iteration 84, loss = 0.57623494\n",
            "Iteration 85, loss = 0.57594787\n",
            "Iteration 86, loss = 0.57557150\n",
            "Iteration 87, loss = 0.57527755\n",
            "Iteration 88, loss = 0.57490136\n",
            "Iteration 89, loss = 0.57463870\n",
            "Iteration 90, loss = 0.57420350\n",
            "Iteration 91, loss = 0.57382071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 92, loss = 0.57349973\n",
            "Iteration 93, loss = 0.57316240\n",
            "Iteration 94, loss = 0.57277331\n",
            "Iteration 95, loss = 0.57224303\n",
            "Iteration 96, loss = 0.57170654\n",
            "Iteration 97, loss = 0.57127444\n",
            "Iteration 98, loss = 0.57075803\n",
            "Iteration 99, loss = 0.57034183\n",
            "Iteration 100, loss = 0.56991355\n",
            "Iteration 101, loss = 0.56954816\n",
            "Iteration 102, loss = 0.56908193\n",
            "Iteration 103, loss = 0.56854709\n",
            "Iteration 104, loss = 0.56805126\n",
            "Iteration 105, loss = 0.56750147\n",
            "Iteration 106, loss = 0.56701195\n",
            "Iteration 107, loss = 0.56647560\n",
            "Iteration 108, loss = 0.56597761\n",
            "Iteration 109, loss = 0.56545971\n",
            "Iteration 110, loss = 0.56495461\n",
            "Iteration 111, loss = 0.56441826\n",
            "Iteration 112, loss = 0.56379794\n",
            "Iteration 113, loss = 0.56314473\n",
            "Iteration 114, loss = 0.56254899\n",
            "Iteration 115, loss = 0.56209955\n",
            "Iteration 116, loss = 0.56160359\n",
            "Iteration 117, loss = 0.56105735\n",
            "Iteration 118, loss = 0.56070239\n",
            "Iteration 119, loss = 0.56032899\n",
            "Iteration 120, loss = 0.55995455\n",
            "Iteration 121, loss = 0.55955228\n",
            "Iteration 122, loss = 0.55918703\n",
            "Iteration 123, loss = 0.55884484\n",
            "Iteration 124, loss = 0.55838076\n",
            "Iteration 125, loss = 0.55808569\n",
            "Iteration 126, loss = 0.55751445\n",
            "Iteration 127, loss = 0.55718773\n",
            "Iteration 128, loss = 0.55680289\n",
            "Iteration 129, loss = 0.55641844\n",
            "Iteration 130, loss = 0.55609968\n",
            "Iteration 131, loss = 0.55576557\n",
            "Iteration 132, loss = 0.55544813\n",
            "Iteration 133, loss = 0.55514959\n",
            "Iteration 134, loss = 0.55485585\n",
            "Iteration 135, loss = 0.55465213\n",
            "Iteration 136, loss = 0.55439104\n",
            "Iteration 137, loss = 0.55423854\n",
            "Iteration 138, loss = 0.55397484\n",
            "Iteration 139, loss = 0.55382484\n",
            "Iteration 140, loss = 0.55362496\n",
            "Iteration 141, loss = 0.55350449\n",
            "Iteration 142, loss = 0.55330935\n",
            "Iteration 143, loss = 0.55312696\n",
            "Iteration 144, loss = 0.55287228\n",
            "Iteration 145, loss = 0.55263719\n",
            "Iteration 146, loss = 0.55255738\n",
            "Iteration 147, loss = 0.55232271\n",
            "Iteration 148, loss = 0.55213450\n",
            "Iteration 149, loss = 0.55196895\n",
            "Iteration 150, loss = 0.55176033\n",
            "Iteration 1, loss = 0.61762838\n",
            "Iteration 2, loss = 0.61685222\n",
            "Iteration 3, loss = 0.61571214\n",
            "Iteration 4, loss = 0.61441758\n",
            "Iteration 5, loss = 0.61318349\n",
            "Iteration 6, loss = 0.61195746\n",
            "Iteration 7, loss = 0.61086455\n",
            "Iteration 8, loss = 0.60959350\n",
            "Iteration 9, loss = 0.60852999\n",
            "Iteration 10, loss = 0.60759281\n",
            "Iteration 11, loss = 0.60667038\n",
            "Iteration 12, loss = 0.60588256\n",
            "Iteration 13, loss = 0.60500642\n",
            "Iteration 14, loss = 0.60429505\n",
            "Iteration 15, loss = 0.60355287\n",
            "Iteration 16, loss = 0.60296533\n",
            "Iteration 17, loss = 0.60238875\n",
            "Iteration 18, loss = 0.60183941\n",
            "Iteration 19, loss = 0.60124485\n",
            "Iteration 20, loss = 0.60063271\n",
            "Iteration 21, loss = 0.60004317\n",
            "Iteration 22, loss = 0.59951396\n",
            "Iteration 23, loss = 0.59887438\n",
            "Iteration 24, loss = 0.59834187\n",
            "Iteration 25, loss = 0.59783710\n",
            "Iteration 26, loss = 0.59731217\n",
            "Iteration 27, loss = 0.59680236\n",
            "Iteration 28, loss = 0.59634036\n",
            "Iteration 29, loss = 0.59585225\n",
            "Iteration 30, loss = 0.59540926\n",
            "Iteration 31, loss = 0.59494784\n",
            "Iteration 32, loss = 0.59442520\n",
            "Iteration 33, loss = 0.59397329\n",
            "Iteration 34, loss = 0.59352410\n",
            "Iteration 35, loss = 0.59310715\n",
            "Iteration 36, loss = 0.59259080\n",
            "Iteration 37, loss = 0.59198806\n",
            "Iteration 38, loss = 0.59141069\n",
            "Iteration 39, loss = 0.59091519\n",
            "Iteration 40, loss = 0.59042012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 41, loss = 0.58996507\n",
            "Iteration 42, loss = 0.58947078\n",
            "Iteration 43, loss = 0.58903801\n",
            "Iteration 44, loss = 0.58869976\n",
            "Iteration 45, loss = 0.58832512\n",
            "Iteration 46, loss = 0.58801003\n",
            "Iteration 47, loss = 0.58772939\n",
            "Iteration 48, loss = 0.58749791\n",
            "Iteration 49, loss = 0.58718487\n",
            "Iteration 50, loss = 0.58696384\n",
            "Iteration 51, loss = 0.58679751\n",
            "Iteration 52, loss = 0.58653756\n",
            "Iteration 53, loss = 0.58627720\n",
            "Iteration 54, loss = 0.58609678\n",
            "Iteration 55, loss = 0.58587111\n",
            "Iteration 56, loss = 0.58563655\n",
            "Iteration 57, loss = 0.58540147\n",
            "Iteration 58, loss = 0.58515985\n",
            "Iteration 59, loss = 0.58495083\n",
            "Iteration 60, loss = 0.58467493\n",
            "Iteration 61, loss = 0.58436402\n",
            "Iteration 62, loss = 0.58419131\n",
            "Iteration 63, loss = 0.58386634\n",
            "Iteration 64, loss = 0.58367166\n",
            "Iteration 65, loss = 0.58340516\n",
            "Iteration 66, loss = 0.58311369\n",
            "Iteration 67, loss = 0.58294822\n",
            "Iteration 68, loss = 0.58272389\n",
            "Iteration 69, loss = 0.58248177\n",
            "Iteration 70, loss = 0.58222376\n",
            "Iteration 71, loss = 0.58199437\n",
            "Iteration 72, loss = 0.58183608\n",
            "Iteration 73, loss = 0.58158739\n",
            "Iteration 74, loss = 0.58128332\n",
            "Iteration 75, loss = 0.58109528\n",
            "Iteration 76, loss = 0.58079797\n",
            "Iteration 77, loss = 0.58055032\n",
            "Iteration 78, loss = 0.58038713\n",
            "Iteration 79, loss = 0.58011096\n",
            "Iteration 80, loss = 0.57987169\n",
            "Iteration 81, loss = 0.57960002\n",
            "Iteration 82, loss = 0.57928697\n",
            "Iteration 83, loss = 0.57902631\n",
            "Iteration 84, loss = 0.57873116\n",
            "Iteration 85, loss = 0.57846126\n",
            "Iteration 86, loss = 0.57816501\n",
            "Iteration 87, loss = 0.57791457\n",
            "Iteration 88, loss = 0.57758301\n",
            "Iteration 89, loss = 0.57729328\n",
            "Iteration 90, loss = 0.57700737\n",
            "Iteration 91, loss = 0.57667723\n",
            "Iteration 92, loss = 0.57637588\n",
            "Iteration 93, loss = 0.57608463\n",
            "Iteration 94, loss = 0.57589851\n",
            "Iteration 95, loss = 0.57547867\n",
            "Iteration 96, loss = 0.57515434\n",
            "Iteration 97, loss = 0.57484887\n",
            "Iteration 98, loss = 0.57450644\n",
            "Iteration 99, loss = 0.57423971\n",
            "Iteration 100, loss = 0.57394845\n",
            "Iteration 101, loss = 0.57367106\n",
            "Iteration 102, loss = 0.57345373\n",
            "Iteration 103, loss = 0.57315003\n",
            "Iteration 104, loss = 0.57286482\n",
            "Iteration 105, loss = 0.57257109\n",
            "Iteration 106, loss = 0.57236005\n",
            "Iteration 107, loss = 0.57210592\n",
            "Iteration 108, loss = 0.57182320\n",
            "Iteration 109, loss = 0.57157851\n",
            "Iteration 110, loss = 0.57130851\n",
            "Iteration 111, loss = 0.57099353\n",
            "Iteration 112, loss = 0.57063653\n",
            "Iteration 113, loss = 0.57026642\n",
            "Iteration 114, loss = 0.56991901\n",
            "Iteration 115, loss = 0.56953357\n",
            "Iteration 116, loss = 0.56930145\n",
            "Iteration 117, loss = 0.56896447\n",
            "Iteration 118, loss = 0.56864356\n",
            "Iteration 119, loss = 0.56836572\n",
            "Iteration 120, loss = 0.56804066\n",
            "Iteration 121, loss = 0.56773562\n",
            "Iteration 122, loss = 0.56755539\n",
            "Iteration 123, loss = 0.56736130\n",
            "Iteration 124, loss = 0.56711146\n",
            "Iteration 125, loss = 0.56683908\n",
            "Iteration 126, loss = 0.56647484\n",
            "Iteration 127, loss = 0.56619872\n",
            "Iteration 128, loss = 0.56586599\n",
            "Iteration 129, loss = 0.56559488\n",
            "Iteration 130, loss = 0.56533314\n",
            "Iteration 131, loss = 0.56508778\n",
            "Iteration 132, loss = 0.56473660\n",
            "Iteration 133, loss = 0.56450473\n",
            "Iteration 134, loss = 0.56427530\n",
            "Iteration 135, loss = 0.56395923\n",
            "Iteration 136, loss = 0.56372237\n",
            "Iteration 137, loss = 0.56337833\n",
            "Iteration 138, loss = 0.56311224\n",
            "Iteration 139, loss = 0.56286408\n",
            "Iteration 140, loss = 0.56260020\n",
            "Iteration 141, loss = 0.56237318\n",
            "Iteration 142, loss = 0.56214346\n",
            "Iteration 143, loss = 0.56184605\n",
            "Iteration 144, loss = 0.56160014\n",
            "Iteration 145, loss = 0.56122180\n",
            "Iteration 146, loss = 0.56102411\n",
            "Iteration 147, loss = 0.56075902\n",
            "Iteration 148, loss = 0.56051512\n",
            "Iteration 149, loss = 0.56014889\n",
            "Iteration 150, loss = 0.55996943\n",
            "Iteration 1, loss = 0.61756281\n",
            "Iteration 2, loss = 0.61678746\n",
            "Iteration 3, loss = 0.61551433\n",
            "Iteration 4, loss = 0.61406466\n",
            "Iteration 5, loss = 0.61275832\n",
            "Iteration 6, loss = 0.61139014\n",
            "Iteration 7, loss = 0.61018275\n",
            "Iteration 8, loss = 0.60882083\n",
            "Iteration 9, loss = 0.60770150\n",
            "Iteration 10, loss = 0.60664596\n",
            "Iteration 11, loss = 0.60580536\n",
            "Iteration 12, loss = 0.60503210\n",
            "Iteration 13, loss = 0.60414541\n",
            "Iteration 14, loss = 0.60345742\n",
            "Iteration 15, loss = 0.60273866\n",
            "Iteration 16, loss = 0.60214876\n",
            "Iteration 17, loss = 0.60155645\n",
            "Iteration 18, loss = 0.60098285\n",
            "Iteration 19, loss = 0.60041376\n",
            "Iteration 20, loss = 0.59981786\n",
            "Iteration 21, loss = 0.59929265\n",
            "Iteration 22, loss = 0.59865761\n",
            "Iteration 23, loss = 0.59789106\n",
            "Iteration 24, loss = 0.59730522\n",
            "Iteration 25, loss = 0.59667276\n",
            "Iteration 26, loss = 0.59602085\n",
            "Iteration 27, loss = 0.59538918\n",
            "Iteration 28, loss = 0.59476517\n",
            "Iteration 29, loss = 0.59420415\n",
            "Iteration 30, loss = 0.59360792\n",
            "Iteration 31, loss = 0.59298919\n",
            "Iteration 32, loss = 0.59233785\n",
            "Iteration 33, loss = 0.59173787\n",
            "Iteration 34, loss = 0.59105348\n",
            "Iteration 35, loss = 0.59039394\n",
            "Iteration 36, loss = 0.58983637\n",
            "Iteration 37, loss = 0.58907064\n",
            "Iteration 38, loss = 0.58833953\n",
            "Iteration 39, loss = 0.58772116\n",
            "Iteration 40, loss = 0.58701155\n",
            "Iteration 41, loss = 0.58632754\n",
            "Iteration 42, loss = 0.58559078\n",
            "Iteration 43, loss = 0.58496621\n",
            "Iteration 44, loss = 0.58428972\n",
            "Iteration 45, loss = 0.58358511\n",
            "Iteration 46, loss = 0.58293101\n",
            "Iteration 47, loss = 0.58227798\n",
            "Iteration 48, loss = 0.58170222\n",
            "Iteration 49, loss = 0.58101512\n",
            "Iteration 50, loss = 0.58043029\n",
            "Iteration 51, loss = 0.57991960\n",
            "Iteration 52, loss = 0.57933529\n",
            "Iteration 53, loss = 0.57870244\n",
            "Iteration 54, loss = 0.57829782\n",
            "Iteration 55, loss = 0.57767445\n",
            "Iteration 56, loss = 0.57715199\n",
            "Iteration 57, loss = 0.57663478\n",
            "Iteration 58, loss = 0.57604727\n",
            "Iteration 59, loss = 0.57548153\n",
            "Iteration 60, loss = 0.57475842\n",
            "Iteration 61, loss = 0.57416184\n",
            "Iteration 62, loss = 0.57353961\n",
            "Iteration 63, loss = 0.57287798\n",
            "Iteration 64, loss = 0.57223335\n",
            "Iteration 65, loss = 0.57144876\n",
            "Iteration 66, loss = 0.57074095\n",
            "Iteration 67, loss = 0.57018441\n",
            "Iteration 68, loss = 0.56959535\n",
            "Iteration 69, loss = 0.56896543\n",
            "Iteration 70, loss = 0.56840160\n",
            "Iteration 71, loss = 0.56782256\n",
            "Iteration 72, loss = 0.56726268\n",
            "Iteration 73, loss = 0.56657180\n",
            "Iteration 74, loss = 0.56591685\n",
            "Iteration 75, loss = 0.56546255\n",
            "Iteration 76, loss = 0.56487449\n",
            "Iteration 77, loss = 0.56432787\n",
            "Iteration 78, loss = 0.56379671\n",
            "Iteration 79, loss = 0.56323004\n",
            "Iteration 80, loss = 0.56263426\n",
            "Iteration 81, loss = 0.56212573\n",
            "Iteration 82, loss = 0.56151629\n",
            "Iteration 83, loss = 0.56097539\n",
            "Iteration 84, loss = 0.56051914\n",
            "Iteration 85, loss = 0.56006355\n",
            "Iteration 86, loss = 0.55956176\n",
            "Iteration 87, loss = 0.55907687\n",
            "Iteration 88, loss = 0.55861192\n",
            "Iteration 89, loss = 0.55803563\n",
            "Iteration 90, loss = 0.55760848\n",
            "Iteration 91, loss = 0.55701483\n",
            "Iteration 92, loss = 0.55655069\n",
            "Iteration 93, loss = 0.55609322\n",
            "Iteration 94, loss = 0.55567123\n",
            "Iteration 95, loss = 0.55502534\n",
            "Iteration 96, loss = 0.55460817\n",
            "Iteration 97, loss = 0.55402209\n",
            "Iteration 98, loss = 0.55355900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 99, loss = 0.55318405\n",
            "Iteration 100, loss = 0.55277358\n",
            "Iteration 101, loss = 0.55236759\n",
            "Iteration 102, loss = 0.55205879\n",
            "Iteration 103, loss = 0.55161370\n",
            "Iteration 104, loss = 0.55123778\n",
            "Iteration 105, loss = 0.55086666\n",
            "Iteration 106, loss = 0.55047447\n",
            "Iteration 107, loss = 0.55012781\n",
            "Iteration 108, loss = 0.54966725\n",
            "Iteration 109, loss = 0.54927395\n",
            "Iteration 110, loss = 0.54888330\n",
            "Iteration 111, loss = 0.54853697\n",
            "Iteration 112, loss = 0.54803161\n",
            "Iteration 113, loss = 0.54759915\n",
            "Iteration 114, loss = 0.54717141\n",
            "Iteration 115, loss = 0.54685299\n",
            "Iteration 116, loss = 0.54647567\n",
            "Iteration 117, loss = 0.54607307\n",
            "Iteration 118, loss = 0.54574090\n",
            "Iteration 119, loss = 0.54544404\n",
            "Iteration 120, loss = 0.54504897\n",
            "Iteration 121, loss = 0.54472741\n",
            "Iteration 122, loss = 0.54438772\n",
            "Iteration 123, loss = 0.54404215\n",
            "Iteration 124, loss = 0.54369341\n",
            "Iteration 125, loss = 0.54343200\n",
            "Iteration 126, loss = 0.54302948\n",
            "Iteration 127, loss = 0.54279117\n",
            "Iteration 128, loss = 0.54249796\n",
            "Iteration 129, loss = 0.54233272\n",
            "Iteration 130, loss = 0.54201110\n",
            "Iteration 131, loss = 0.54157225\n",
            "Iteration 132, loss = 0.54113379\n",
            "Iteration 133, loss = 0.54074148\n",
            "Iteration 134, loss = 0.54038149\n",
            "Iteration 135, loss = 0.53995048\n",
            "Iteration 136, loss = 0.53962117\n",
            "Iteration 137, loss = 0.53916907\n",
            "Iteration 138, loss = 0.53877409\n",
            "Iteration 139, loss = 0.53841380\n",
            "Iteration 140, loss = 0.53806495\n",
            "Iteration 141, loss = 0.53771208\n",
            "Iteration 142, loss = 0.53742659\n",
            "Iteration 143, loss = 0.53737448\n",
            "Iteration 144, loss = 0.53686941\n",
            "Iteration 145, loss = 0.53651881\n",
            "Iteration 146, loss = 0.53625293\n",
            "Iteration 147, loss = 0.53595237\n",
            "Iteration 148, loss = 0.53573440\n",
            "Iteration 149, loss = 0.53537873\n",
            "Iteration 150, loss = 0.53518124\n",
            "Iteration 1, loss = 0.62079721\n",
            "Iteration 2, loss = 0.62005324\n",
            "Iteration 3, loss = 0.61900678\n",
            "Iteration 4, loss = 0.61766780\n",
            "Iteration 5, loss = 0.61643559\n",
            "Iteration 6, loss = 0.61518085\n",
            "Iteration 7, loss = 0.61403740\n",
            "Iteration 8, loss = 0.61288976\n",
            "Iteration 9, loss = 0.61190780\n",
            "Iteration 10, loss = 0.61098604\n",
            "Iteration 11, loss = 0.61018985\n",
            "Iteration 12, loss = 0.60952147\n",
            "Iteration 13, loss = 0.60886466\n",
            "Iteration 14, loss = 0.60818565\n",
            "Iteration 15, loss = 0.60758049\n",
            "Iteration 16, loss = 0.60707209\n",
            "Iteration 17, loss = 0.60648497\n",
            "Iteration 18, loss = 0.60597777\n",
            "Iteration 19, loss = 0.60548418\n",
            "Iteration 20, loss = 0.60501951\n",
            "Iteration 21, loss = 0.60460897\n",
            "Iteration 22, loss = 0.60410824\n",
            "Iteration 23, loss = 0.60353209\n",
            "Iteration 24, loss = 0.60308578\n",
            "Iteration 25, loss = 0.60261600\n",
            "Iteration 26, loss = 0.60213958\n",
            "Iteration 27, loss = 0.60173715\n",
            "Iteration 28, loss = 0.60121898\n",
            "Iteration 29, loss = 0.60074744\n",
            "Iteration 30, loss = 0.60026547\n",
            "Iteration 31, loss = 0.59976581\n",
            "Iteration 32, loss = 0.59927205\n",
            "Iteration 33, loss = 0.59878003\n",
            "Iteration 34, loss = 0.59832639\n",
            "Iteration 35, loss = 0.59786446\n",
            "Iteration 36, loss = 0.59746968\n",
            "Iteration 37, loss = 0.59693135\n",
            "Iteration 38, loss = 0.59639720\n",
            "Iteration 39, loss = 0.59583464\n",
            "Iteration 40, loss = 0.59533087\n",
            "Iteration 41, loss = 0.59475384\n",
            "Iteration 42, loss = 0.59425957\n",
            "Iteration 43, loss = 0.59373095\n",
            "Iteration 44, loss = 0.59323666\n",
            "Iteration 45, loss = 0.59264518\n",
            "Iteration 46, loss = 0.59213675\n",
            "Iteration 47, loss = 0.59166031\n",
            "Iteration 48, loss = 0.59110663\n",
            "Iteration 49, loss = 0.59053353\n",
            "Iteration 50, loss = 0.58998709\n",
            "Iteration 51, loss = 0.58950475\n",
            "Iteration 52, loss = 0.58904938\n",
            "Iteration 53, loss = 0.58852797\n",
            "Iteration 54, loss = 0.58812035\n",
            "Iteration 55, loss = 0.58762289\n",
            "Iteration 56, loss = 0.58712541\n",
            "Iteration 57, loss = 0.58666104\n",
            "Iteration 58, loss = 0.58615629\n",
            "Iteration 59, loss = 0.58561809\n",
            "Iteration 60, loss = 0.58512577"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 61, loss = 0.58466198\n",
            "Iteration 62, loss = 0.58411778\n",
            "Iteration 63, loss = 0.58359615\n",
            "Iteration 64, loss = 0.58300139\n",
            "Iteration 65, loss = 0.58244389\n",
            "Iteration 66, loss = 0.58190597\n",
            "Iteration 67, loss = 0.58141029\n",
            "Iteration 68, loss = 0.58085057\n",
            "Iteration 69, loss = 0.58035273\n",
            "Iteration 70, loss = 0.57970416\n",
            "Iteration 71, loss = 0.57915712\n",
            "Iteration 72, loss = 0.57860550\n",
            "Iteration 73, loss = 0.57797929\n",
            "Iteration 74, loss = 0.57738233\n",
            "Iteration 75, loss = 0.57704390\n",
            "Iteration 76, loss = 0.57639729\n",
            "Iteration 77, loss = 0.57581682\n",
            "Iteration 78, loss = 0.57543752\n",
            "Iteration 79, loss = 0.57485995\n",
            "Iteration 80, loss = 0.57445793\n",
            "Iteration 81, loss = 0.57398971\n",
            "Iteration 82, loss = 0.57361636\n",
            "Iteration 83, loss = 0.57318059\n",
            "Iteration 84, loss = 0.57282423\n",
            "Iteration 85, loss = 0.57252176\n",
            "Iteration 86, loss = 0.57218478\n",
            "Iteration 87, loss = 0.57185258\n",
            "Iteration 88, loss = 0.57147761\n",
            "Iteration 89, loss = 0.57112446\n",
            "Iteration 90, loss = 0.57071716\n",
            "Iteration 91, loss = 0.57028615\n",
            "Iteration 92, loss = 0.56993985\n",
            "Iteration 93, loss = 0.56954471\n",
            "Iteration 94, loss = 0.56924308\n",
            "Iteration 95, loss = 0.56872118\n",
            "Iteration 96, loss = 0.56844243\n",
            "Iteration 97, loss = 0.56803433\n",
            "Iteration 98, loss = 0.56776972\n",
            "Iteration 99, loss = 0.56749363\n",
            "Iteration 100, loss = 0.56718606\n",
            "Iteration 101, loss = 0.56689778\n",
            "Iteration 102, loss = 0.56664384\n",
            "Iteration 103, loss = 0.56635293\n",
            "Iteration 104, loss = 0.56623406\n",
            "Iteration 105, loss = 0.56601177\n",
            "Iteration 106, loss = 0.56568366\n",
            "Iteration 107, loss = 0.56551641\n",
            "Iteration 108, loss = 0.56510360\n",
            "Iteration 109, loss = 0.56480059\n",
            "Iteration 110, loss = 0.56454979\n",
            "Iteration 111, loss = 0.56429531\n",
            "Iteration 112, loss = 0.56395747\n",
            "Iteration 113, loss = 0.56367286\n",
            "Iteration 114, loss = 0.56345796\n",
            "Iteration 115, loss = 0.56326450\n",
            "Iteration 116, loss = 0.56287617\n",
            "Iteration 117, loss = 0.56259053\n",
            "Iteration 118, loss = 0.56238330\n",
            "Iteration 119, loss = 0.56215496\n",
            "Iteration 120, loss = 0.56186900\n",
            "Iteration 121, loss = 0.56167566\n",
            "Iteration 122, loss = 0.56132266\n",
            "Iteration 123, loss = 0.56106314\n",
            "Iteration 124, loss = 0.56088367\n",
            "Iteration 125, loss = 0.56067779\n",
            "Iteration 126, loss = 0.56034009\n",
            "Iteration 127, loss = 0.56018241\n",
            "Iteration 128, loss = 0.55983784\n",
            "Iteration 129, loss = 0.55972969\n",
            "Iteration 130, loss = 0.55940300\n",
            "Iteration 131, loss = 0.55919037\n",
            "Iteration 132, loss = 0.55889555\n",
            "Iteration 133, loss = 0.55867239\n",
            "Iteration 134, loss = 0.55855201\n",
            "Iteration 135, loss = 0.55830447\n",
            "Iteration 136, loss = 0.55815356\n",
            "Iteration 137, loss = 0.55796883\n",
            "Iteration 138, loss = 0.55783932\n",
            "Iteration 139, loss = 0.55769593\n",
            "Iteration 140, loss = 0.55754655\n",
            "Iteration 141, loss = 0.55735505\n",
            "Iteration 142, loss = 0.55723094\n",
            "Iteration 143, loss = 0.55714053\n",
            "Iteration 144, loss = 0.55691044\n",
            "Iteration 145, loss = 0.55677425\n",
            "Iteration 146, loss = 0.55657889\n",
            "Iteration 147, loss = 0.55637529\n",
            "Iteration 148, loss = 0.55623800\n",
            "Iteration 149, loss = 0.55604883\n",
            "Iteration 150, loss = 0.55589847\n",
            "Iteration 1, loss = 0.62010582\n",
            "Iteration 2, loss = 0.61930514\n",
            "Iteration 3, loss = 0.61826319\n",
            "Iteration 4, loss = 0.61682911\n",
            "Iteration 5, loss = 0.61543360\n",
            "Iteration 6, loss = 0.61413706\n",
            "Iteration 7, loss = 0.61295141\n",
            "Iteration 8, loss = 0.61172441\n",
            "Iteration 9, loss = 0.61081242\n",
            "Iteration 10, loss = 0.60991460\n",
            "Iteration 11, loss = 0.60917637\n",
            "Iteration 12, loss = 0.60858965\n",
            "Iteration 13, loss = 0.60792078\n",
            "Iteration 14, loss = 0.60730029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 15, loss = 0.60678248\n",
            "Iteration 16, loss = 0.60632478\n",
            "Iteration 17, loss = 0.60582791\n",
            "Iteration 18, loss = 0.60531306\n",
            "Iteration 19, loss = 0.60480910\n",
            "Iteration 20, loss = 0.60441098\n",
            "Iteration 21, loss = 0.60409300\n",
            "Iteration 22, loss = 0.60359792\n",
            "Iteration 23, loss = 0.60325513\n",
            "Iteration 24, loss = 0.60288068\n",
            "Iteration 25, loss = 0.60251982\n",
            "Iteration 26, loss = 0.60216641\n",
            "Iteration 27, loss = 0.60186024\n",
            "Iteration 28, loss = 0.60148385\n",
            "Iteration 29, loss = 0.60117667\n",
            "Iteration 30, loss = 0.60087078\n",
            "Iteration 31, loss = 0.60049222\n",
            "Iteration 32, loss = 0.60013302\n",
            "Iteration 33, loss = 0.59970023\n",
            "Iteration 34, loss = 0.59933655\n",
            "Iteration 35, loss = 0.59892240\n",
            "Iteration 36, loss = 0.59855545\n",
            "Iteration 37, loss = 0.59817110\n",
            "Iteration 38, loss = 0.59773733\n",
            "Iteration 39, loss = 0.59731738\n",
            "Iteration 40, loss = 0.59701270\n",
            "Iteration 41, loss = 0.59659154\n",
            "Iteration 42, loss = 0.59622730\n",
            "Iteration 43, loss = 0.59577996\n",
            "Iteration 44, loss = 0.59559981\n",
            "Iteration 45, loss = 0.59513820\n",
            "Iteration 46, loss = 0.59480242\n",
            "Iteration 47, loss = 0.59452497\n",
            "Iteration 48, loss = 0.59421464\n",
            "Iteration 49, loss = 0.59389969\n",
            "Iteration 50, loss = 0.59360376\n",
            "Iteration 51, loss = 0.59338566\n",
            "Iteration 52, loss = 0.59307514\n",
            "Iteration 53, loss = 0.59278326\n",
            "Iteration 54, loss = 0.59252864\n",
            "Iteration 55, loss = 0.59218267\n",
            "Iteration 56, loss = 0.59179838\n",
            "Iteration 57, loss = 0.59144292\n",
            "Iteration 58, loss = 0.59107432\n",
            "Iteration 59, loss = 0.59072772\n",
            "Iteration 60, loss = 0.59033379\n",
            "Iteration 61, loss = 0.58997606\n",
            "Iteration 62, loss = 0.58954634\n",
            "Iteration 63, loss = 0.58924779\n",
            "Iteration 64, loss = 0.58890475\n",
            "Iteration 65, loss = 0.58864831\n",
            "Iteration 66, loss = 0.58836352\n",
            "Iteration 67, loss = 0.58815782\n",
            "Iteration 68, loss = 0.58783305\n",
            "Iteration 69, loss = 0.58756811\n",
            "Iteration 70, loss = 0.58721380\n",
            "Iteration 71, loss = 0.58694958\n",
            "Iteration 72, loss = 0.58664618\n",
            "Iteration 73, loss = 0.58633497\n",
            "Iteration 74, loss = 0.58601464\n",
            "Iteration 75, loss = 0.58576382\n",
            "Iteration 76, loss = 0.58549933\n",
            "Iteration 77, loss = 0.58529067\n",
            "Iteration 78, loss = 0.58500034\n",
            "Iteration 79, loss = 0.58465738\n",
            "Iteration 80, loss = 0.58453297\n",
            "Iteration 81, loss = 0.58413051\n",
            "Iteration 82, loss = 0.58400797\n",
            "Iteration 83, loss = 0.58362854\n",
            "Iteration 84, loss = 0.58342692\n",
            "Iteration 85, loss = 0.58325088\n",
            "Iteration 86, loss = 0.58308466\n",
            "Iteration 87, loss = 0.58297834\n",
            "Iteration 88, loss = 0.58282060\n",
            "Iteration 89, loss = 0.58260746\n",
            "Iteration 90, loss = 0.58236208\n",
            "Iteration 91, loss = 0.58209636\n",
            "Iteration 92, loss = 0.58198327\n",
            "Iteration 93, loss = 0.58176341\n",
            "Iteration 94, loss = 0.58164160\n",
            "Iteration 95, loss = 0.58140872\n",
            "Iteration 96, loss = 0.58129105\n",
            "Iteration 97, loss = 0.58108386\n",
            "Iteration 98, loss = 0.58097283\n",
            "Iteration 99, loss = 0.58070632\n",
            "Iteration 100, loss = 0.58053302\n",
            "Iteration 101, loss = 0.58039990\n",
            "Iteration 102, loss = 0.58017624\n",
            "Iteration 103, loss = 0.57999113\n",
            "Iteration 104, loss = 0.57984540\n",
            "Iteration 105, loss = 0.57970107\n",
            "Iteration 106, loss = 0.57950922\n",
            "Iteration 107, loss = 0.57948545\n",
            "Iteration 108, loss = 0.57919349\n",
            "Iteration 109, loss = 0.57905324\n",
            "Iteration 110, loss = 0.57889101\n",
            "Iteration 111, loss = 0.57872914\n",
            "Iteration 112, loss = 0.57862720\n",
            "Iteration 113, loss = 0.57851013\n",
            "Iteration 114, loss = 0.57835774\n",
            "Iteration 115, loss = 0.57827363\n",
            "Iteration 116, loss = 0.57808403\n",
            "Iteration 117, loss = 0.57797619\n",
            "Iteration 118, loss = 0.57786429\n",
            "Iteration 119, loss = 0.57780876\n",
            "Iteration 120, loss = 0.57769954\n",
            "Iteration 121, loss = 0.57759897\n",
            "Iteration 122, loss = 0.57748308\n",
            "Iteration 123, loss = 0.57731200\n",
            "Iteration 124, loss = 0.57724820\n",
            "Iteration 125, loss = 0.57711188\n",
            "Iteration 126, loss = 0.57700233\n",
            "Iteration 127, loss = 0.57690475\n",
            "Iteration 128, loss = 0.57675941\n",
            "Iteration 129, loss = 0.57675736\n",
            "Iteration 130, loss = 0.57667384\n",
            "Iteration 131, loss = 0.57661817\n",
            "Iteration 132, loss = 0.57641709\n",
            "Iteration 133, loss = 0.57628851\n",
            "Iteration 134, loss = 0.57625054\n",
            "Iteration 135, loss = 0.57607321\n",
            "Iteration 136, loss = 0.57602258\n",
            "Iteration 137, loss = 0.57590272\n",
            "Iteration 138, loss = 0.57586608\n",
            "Iteration 139, loss = 0.57581485\n",
            "Iteration 140, loss = 0.57567184\n",
            "Iteration 141, loss = 0.57558977\n",
            "Iteration 142, loss = 0.57546602\n",
            "Iteration 143, loss = 0.57543567\n",
            "Iteration 144, loss = 0.57535966\n",
            "Iteration 145, loss = 0.57534248\n",
            "Iteration 146, loss = 0.57522330\n",
            "Iteration 147, loss = 0.57510579\n",
            "Iteration 148, loss = 0.57502251\n",
            "Iteration 149, loss = 0.57493491\n",
            "Iteration 150, loss = 0.57488632\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 150 and for layer number 2 : 0.6975\n",
            "Iteration 1, loss = 0.84424359\n",
            "Iteration 2, loss = 0.81824608\n",
            "Iteration 3, loss = 0.78301818\n",
            "Iteration 4, loss = 0.74369898\n",
            "Iteration 5, loss = 0.70772940\n",
            "Iteration 6, loss = 0.67905372\n",
            "Iteration 7, loss = 0.65459215\n",
            "Iteration 8, loss = 0.63750703\n",
            "Iteration 9, loss = 0.62609078\n",
            "Iteration 10, loss = 0.61732840\n",
            "Iteration 11, loss = 0.61055362\n",
            "Iteration 12, loss = 0.60475642\n",
            "Iteration 13, loss = 0.60059365\n",
            "Iteration 14, loss = 0.59676833\n",
            "Iteration 15, loss = 0.59386373\n",
            "Iteration 16, loss = 0.59105742\n",
            "Iteration 17, loss = 0.58857593\n",
            "Iteration 18, loss = 0.58599616\n",
            "Iteration 19, loss = 0.58413312\n",
            "Iteration 20, loss = 0.58213654\n",
            "Iteration 21, loss = 0.58047540\n",
            "Iteration 22, loss = 0.57899956\n",
            "Iteration 23, loss = 0.57767412\n",
            "Iteration 24, loss = 0.57662470\n",
            "Iteration 25, loss = 0.57549965\n",
            "Iteration 26, loss = 0.57448619\n",
            "Iteration 27, loss = 0.57350458\n",
            "Iteration 28, loss = 0.57272338\n",
            "Iteration 29, loss = 0.57195199\n",
            "Iteration 30, loss = 0.57117292\n",
            "Iteration 31, loss = 0.57052970\n",
            "Iteration 32, loss = 0.56989653\n",
            "Iteration 33, loss = 0.56924336\n",
            "Iteration 34, loss = 0.56855370\n",
            "Iteration 35, loss = 0.56793890\n",
            "Iteration 36, loss = 0.56726158\n",
            "Iteration 37, loss = 0.56681235\n",
            "Iteration 38, loss = 0.56619017\n",
            "Iteration 39, loss = 0.56555914\n",
            "Iteration 40, loss = 0.56496310\n",
            "Iteration 41, loss = 0.56418147\n",
            "Iteration 42, loss = 0.56369303\n",
            "Iteration 43, loss = 0.56314826\n",
            "Iteration 44, loss = 0.56264112\n",
            "Iteration 45, loss = 0.56218156\n",
            "Iteration 46, loss = 0.56170246\n",
            "Iteration 47, loss = 0.56127065\n",
            "Iteration 48, loss = 0.56095972\n",
            "Iteration 49, loss = 0.56051664\n",
            "Iteration 50, loss = 0.56016687\n",
            "Iteration 51, loss = 0.55986462\n",
            "Iteration 52, loss = 0.55963340\n",
            "Iteration 53, loss = 0.55921424\n",
            "Iteration 54, loss = 0.55880487\n",
            "Iteration 55, loss = 0.55855142\n",
            "Iteration 56, loss = 0.55808873\n",
            "Iteration 57, loss = 0.55768277\n",
            "Iteration 58, loss = 0.55738430\n",
            "Iteration 59, loss = 0.55704771\n",
            "Iteration 60, loss = 0.55673806\n",
            "Iteration 61, loss = 0.55628107\n",
            "Iteration 62, loss = 0.55598312\n",
            "Iteration 63, loss = 0.55560140\n",
            "Iteration 64, loss = 0.55527998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 65, loss = 0.55510876\n",
            "Iteration 66, loss = 0.55472229\n",
            "Iteration 67, loss = 0.55441299\n",
            "Iteration 68, loss = 0.55417873\n",
            "Iteration 69, loss = 0.55397098\n",
            "Iteration 70, loss = 0.55377076\n",
            "Iteration 71, loss = 0.55358584\n",
            "Iteration 72, loss = 0.55322471\n",
            "Iteration 73, loss = 0.55307920\n",
            "Iteration 74, loss = 0.55268158\n",
            "Iteration 75, loss = 0.55243838\n",
            "Iteration 76, loss = 0.55227362\n",
            "Iteration 77, loss = 0.55205947\n",
            "Iteration 78, loss = 0.55176691\n",
            "Iteration 79, loss = 0.55148431\n",
            "Iteration 80, loss = 0.55117819\n",
            "Iteration 81, loss = 0.55094568\n",
            "Iteration 82, loss = 0.55068506\n",
            "Iteration 83, loss = 0.55041758\n",
            "Iteration 84, loss = 0.55009782\n",
            "Iteration 85, loss = 0.54980800\n",
            "Iteration 86, loss = 0.54956132\n",
            "Iteration 87, loss = 0.54938696\n",
            "Iteration 88, loss = 0.54914225\n",
            "Iteration 89, loss = 0.54883945\n",
            "Iteration 90, loss = 0.54857565\n",
            "Iteration 91, loss = 0.54837773\n",
            "Iteration 92, loss = 0.54813146\n",
            "Iteration 93, loss = 0.54776457\n",
            "Iteration 94, loss = 0.54734763\n",
            "Iteration 95, loss = 0.54709515\n",
            "Iteration 96, loss = 0.54679073\n",
            "Iteration 97, loss = 0.54658268\n",
            "Iteration 98, loss = 0.54627656\n",
            "Iteration 99, loss = 0.54604382\n",
            "Iteration 100, loss = 0.54579330\n",
            "Iteration 101, loss = 0.54559553\n",
            "Iteration 102, loss = 0.54534505\n",
            "Iteration 103, loss = 0.54529176\n",
            "Iteration 104, loss = 0.54483518\n",
            "Iteration 105, loss = 0.54470627\n",
            "Iteration 106, loss = 0.54448019\n",
            "Iteration 107, loss = 0.54428649\n",
            "Iteration 108, loss = 0.54412449\n",
            "Iteration 109, loss = 0.54402894\n",
            "Iteration 110, loss = 0.54384063\n",
            "Iteration 111, loss = 0.54366888\n",
            "Iteration 112, loss = 0.54333863\n",
            "Iteration 113, loss = 0.54319544\n",
            "Iteration 114, loss = 0.54289790\n",
            "Iteration 115, loss = 0.54277961\n",
            "Iteration 116, loss = 0.54261873\n",
            "Iteration 117, loss = 0.54243487\n",
            "Iteration 118, loss = 0.54229018\n",
            "Iteration 119, loss = 0.54212852\n",
            "Iteration 120, loss = 0.54200008\n",
            "Iteration 121, loss = 0.54194031\n",
            "Iteration 122, loss = 0.54168361\n",
            "Iteration 123, loss = 0.54151179\n",
            "Iteration 124, loss = 0.54123880\n",
            "Iteration 125, loss = 0.54108897\n",
            "Iteration 126, loss = 0.54103856\n",
            "Iteration 127, loss = 0.54088066\n",
            "Iteration 128, loss = 0.54072815\n",
            "Iteration 129, loss = 0.54062706\n",
            "Iteration 130, loss = 0.54044568\n",
            "Iteration 131, loss = 0.54035252\n",
            "Iteration 132, loss = 0.54026052\n",
            "Iteration 133, loss = 0.54013545\n",
            "Iteration 134, loss = 0.54012645\n",
            "Iteration 135, loss = 0.54000457\n",
            "Iteration 136, loss = 0.53986669\n",
            "Iteration 137, loss = 0.53978250\n",
            "Iteration 138, loss = 0.53960434\n",
            "Iteration 139, loss = 0.53949676\n",
            "Iteration 140, loss = 0.53943493\n",
            "Iteration 141, loss = 0.53923818\n",
            "Iteration 142, loss = 0.53912301\n",
            "Iteration 143, loss = 0.53894386\n",
            "Iteration 144, loss = 0.53878587\n",
            "Iteration 145, loss = 0.53861539\n",
            "Iteration 146, loss = 0.53862490\n",
            "Iteration 147, loss = 0.53832874\n",
            "Iteration 148, loss = 0.53820565\n",
            "Iteration 149, loss = 0.53811370\n",
            "Iteration 150, loss = 0.53803219\n",
            "Iteration 1, loss = 0.84514500\n",
            "Iteration 2, loss = 0.81909328\n",
            "Iteration 3, loss = 0.78652477\n",
            "Iteration 4, loss = 0.74811660\n",
            "Iteration 5, loss = 0.71383507\n",
            "Iteration 6, loss = 0.68552273\n",
            "Iteration 7, loss = 0.66168581\n",
            "Iteration 8, loss = 0.64338982\n",
            "Iteration 9, loss = 0.63160645\n",
            "Iteration 10, loss = 0.62136865\n",
            "Iteration 11, loss = 0.61380923\n",
            "Iteration 12, loss = 0.60791210\n",
            "Iteration 13, loss = 0.60355558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 14, loss = 0.59965695\n",
            "Iteration 15, loss = 0.59624464\n",
            "Iteration 16, loss = 0.59328627\n",
            "Iteration 17, loss = 0.59072275\n",
            "Iteration 18, loss = 0.58830866\n",
            "Iteration 19, loss = 0.58624201\n",
            "Iteration 20, loss = 0.58419678\n",
            "Iteration 21, loss = 0.58224472\n",
            "Iteration 22, loss = 0.58048793\n",
            "Iteration 23, loss = 0.57892935\n",
            "Iteration 24, loss = 0.57765823\n",
            "Iteration 25, loss = 0.57634431\n",
            "Iteration 26, loss = 0.57526519\n",
            "Iteration 27, loss = 0.57409528\n",
            "Iteration 28, loss = 0.57316406\n",
            "Iteration 29, loss = 0.57222788\n",
            "Iteration 30, loss = 0.57132622\n",
            "Iteration 31, loss = 0.57066822\n",
            "Iteration 32, loss = 0.56993909\n",
            "Iteration 33, loss = 0.56926119\n",
            "Iteration 34, loss = 0.56861361\n",
            "Iteration 35, loss = 0.56790836\n",
            "Iteration 36, loss = 0.56732057\n",
            "Iteration 37, loss = 0.56682809\n",
            "Iteration 38, loss = 0.56615271\n",
            "Iteration 39, loss = 0.56557409\n",
            "Iteration 40, loss = 0.56501093\n",
            "Iteration 41, loss = 0.56427012\n",
            "Iteration 42, loss = 0.56383162\n",
            "Iteration 43, loss = 0.56324564\n",
            "Iteration 44, loss = 0.56257550\n",
            "Iteration 45, loss = 0.56205907\n",
            "Iteration 46, loss = 0.56161548\n",
            "Iteration 47, loss = 0.56097687\n",
            "Iteration 48, loss = 0.56050221\n",
            "Iteration 49, loss = 0.55997645\n",
            "Iteration 50, loss = 0.55950729\n",
            "Iteration 51, loss = 0.55905100\n",
            "Iteration 52, loss = 0.55864909\n",
            "Iteration 53, loss = 0.55822691\n",
            "Iteration 54, loss = 0.55777537\n",
            "Iteration 55, loss = 0.55742865\n",
            "Iteration 56, loss = 0.55691692\n",
            "Iteration 57, loss = 0.55641760\n",
            "Iteration 58, loss = 0.55598568\n",
            "Iteration 59, loss = 0.55547413\n",
            "Iteration 60, loss = 0.55496972\n",
            "Iteration 61, loss = 0.55443302\n",
            "Iteration 62, loss = 0.55393228\n",
            "Iteration 63, loss = 0.55347585\n",
            "Iteration 64, loss = 0.55302202\n",
            "Iteration 65, loss = 0.55257241\n",
            "Iteration 66, loss = 0.55210691\n",
            "Iteration 67, loss = 0.55157379\n",
            "Iteration 68, loss = 0.55117839\n",
            "Iteration 69, loss = 0.55073343\n",
            "Iteration 70, loss = 0.55030951\n",
            "Iteration 71, loss = 0.54994572\n",
            "Iteration 72, loss = 0.54945093\n",
            "Iteration 73, loss = 0.54910117\n",
            "Iteration 74, loss = 0.54861316\n",
            "Iteration 75, loss = 0.54822427\n",
            "Iteration 76, loss = 0.54796648\n",
            "Iteration 77, loss = 0.54761925\n",
            "Iteration 78, loss = 0.54726992\n",
            "Iteration 79, loss = 0.54702011\n",
            "Iteration 80, loss = 0.54682356\n",
            "Iteration 81, loss = 0.54654362\n",
            "Iteration 82, loss = 0.54640230\n",
            "Iteration 83, loss = 0.54610336\n",
            "Iteration 84, loss = 0.54585375\n",
            "Iteration 85, loss = 0.54554325\n",
            "Iteration 86, loss = 0.54522963\n",
            "Iteration 87, loss = 0.54487368\n",
            "Iteration 88, loss = 0.54453649\n",
            "Iteration 89, loss = 0.54424273\n",
            "Iteration 90, loss = 0.54393287\n",
            "Iteration 91, loss = 0.54370091\n",
            "Iteration 92, loss = 0.54354063\n",
            "Iteration 93, loss = 0.54315551\n",
            "Iteration 94, loss = 0.54284090\n",
            "Iteration 95, loss = 0.54260775\n",
            "Iteration 96, loss = 0.54226539\n",
            "Iteration 97, loss = 0.54218460\n",
            "Iteration 98, loss = 0.54188639\n",
            "Iteration 99, loss = 0.54171213\n",
            "Iteration 100, loss = 0.54137443\n",
            "Iteration 101, loss = 0.54115176\n",
            "Iteration 102, loss = 0.54098741\n",
            "Iteration 103, loss = 0.54078534\n",
            "Iteration 104, loss = 0.54053260\n",
            "Iteration 105, loss = 0.54039428\n",
            "Iteration 106, loss = 0.54021413\n",
            "Iteration 107, loss = 0.54002222\n",
            "Iteration 108, loss = 0.53982397\n",
            "Iteration 109, loss = 0.53977123\n",
            "Iteration 110, loss = 0.53963840\n",
            "Iteration 111, loss = 0.53947559\n",
            "Iteration 112, loss = 0.53924744\n",
            "Iteration 113, loss = 0.53900389\n",
            "Iteration 114, loss = 0.53876616\n",
            "Iteration 115, loss = 0.53869541\n",
            "Iteration 116, loss = 0.53860712\n",
            "Iteration 117, loss = 0.53842544\n",
            "Iteration 118, loss = 0.53824884\n",
            "Iteration 119, loss = 0.53811920\n",
            "Iteration 120, loss = 0.53789437\n",
            "Iteration 121, loss = 0.53774857\n",
            "Iteration 122, loss = 0.53752471\n",
            "Iteration 123, loss = 0.53742103\n",
            "Iteration 124, loss = 0.53716134\n",
            "Iteration 125, loss = 0.53690587\n",
            "Iteration 126, loss = 0.53674444\n",
            "Iteration 127, loss = 0.53646586\n",
            "Iteration 128, loss = 0.53635349\n",
            "Iteration 129, loss = 0.53616121\n",
            "Iteration 130, loss = 0.53596747\n",
            "Iteration 131, loss = 0.53575548\n",
            "Iteration 132, loss = 0.53558699\n",
            "Iteration 133, loss = 0.53546551\n",
            "Iteration 134, loss = 0.53532027\n",
            "Iteration 135, loss = 0.53521455\n",
            "Iteration 136, loss = 0.53504836\n",
            "Iteration 137, loss = 0.53493607\n",
            "Iteration 138, loss = 0.53485314\n",
            "Iteration 139, loss = 0.53471083\n",
            "Iteration 140, loss = 0.53461208\n",
            "Iteration 141, loss = 0.53447452\n",
            "Iteration 142, loss = 0.53424195\n",
            "Iteration 143, loss = 0.53399683\n",
            "Iteration 144, loss = 0.53371405\n",
            "Iteration 145, loss = 0.53362581\n",
            "Iteration 146, loss = 0.53337330\n",
            "Iteration 147, loss = 0.53311798\n",
            "Iteration 148, loss = 0.53298677\n",
            "Iteration 149, loss = 0.53284512\n",
            "Iteration 150, loss = 0.53266775\n",
            "Iteration 1, loss = 0.86511432\n",
            "Iteration 2, loss = 0.83659966\n",
            "Iteration 3, loss = 0.80055969\n",
            "Iteration 4, loss = 0.75853561\n",
            "Iteration 5, loss = 0.72086132\n",
            "Iteration 6, loss = 0.68969566\n",
            "Iteration 7, loss = 0.66421861\n",
            "Iteration 8, loss = 0.64434004\n",
            "Iteration 9, loss = 0.63160029\n",
            "Iteration 10, loss = 0.62011071\n",
            "Iteration 11, loss = 0.61155999\n",
            "Iteration 12, loss = 0.60497986\n",
            "Iteration 13, loss = 0.59971010\n",
            "Iteration 14, loss = 0.59516710\n",
            "Iteration 15, loss = 0.59089636\n",
            "Iteration 16, loss = 0.58736900\n",
            "Iteration 17, loss = 0.58433935\n",
            "Iteration 18, loss = 0.58137018\n",
            "Iteration 19, loss = 0.57880625\n",
            "Iteration 20, loss = 0.57652473\n",
            "Iteration 21, loss = 0.57419227\n",
            "Iteration 22, loss = 0.57221919\n",
            "Iteration 23, loss = 0.57043440\n",
            "Iteration 24, loss = 0.56876358\n",
            "Iteration 25, loss = 0.56716867\n",
            "Iteration 26, loss = 0.56584442\n",
            "Iteration 27, loss = 0.56433635\n",
            "Iteration 28, loss = 0.56311521\n",
            "Iteration 29, loss = 0.56189909\n",
            "Iteration 30, loss = 0.56081779\n",
            "Iteration 31, loss = 0.55979345\n",
            "Iteration 32, loss = 0.55889623\n",
            "Iteration 33, loss = 0.55803597\n",
            "Iteration 34, loss = 0.55720608\n",
            "Iteration 35, loss = 0.55645261\n",
            "Iteration 36, loss = 0.55572053\n",
            "Iteration 37, loss = 0.55504676\n",
            "Iteration 38, loss = 0.55442651\n",
            "Iteration 39, loss = 0.55373834\n",
            "Iteration 40, loss = 0.55307272\n",
            "Iteration 41, loss = 0.55223474\n",
            "Iteration 42, loss = 0.55169607\n",
            "Iteration 43, loss = 0.55108669\n",
            "Iteration 44, loss = 0.55038771\n",
            "Iteration 45, loss = 0.54992049\n",
            "Iteration 46, loss = 0.54944630\n",
            "Iteration 47, loss = 0.54894736\n",
            "Iteration 48, loss = 0.54847299\n",
            "Iteration 49, loss = 0.54798903\n",
            "Iteration 50, loss = 0.54744079\n",
            "Iteration 51, loss = 0.54699818\n",
            "Iteration 52, loss = 0.54644728\n",
            "Iteration 53, loss = 0.54582215\n",
            "Iteration 54, loss = 0.54523601\n",
            "Iteration 55, loss = 0.54486145\n",
            "Iteration 56, loss = 0.54441470\n",
            "Iteration 57, loss = 0.54396780\n",
            "Iteration 58, loss = 0.54354432\n",
            "Iteration 59, loss = 0.54317617\n",
            "Iteration 60, loss = 0.54271414\n",
            "Iteration 61, loss = 0.54228241\n",
            "Iteration 62, loss = 0.54187862\n",
            "Iteration 63, loss = 0.54145488\n",
            "Iteration 64, loss = 0.54105818\n",
            "Iteration 65, loss = 0.54070902\n",
            "Iteration 66, loss = 0.54033059\n",
            "Iteration 67, loss = 0.53986242\n",
            "Iteration 68, loss = 0.53947009\n",
            "Iteration 69, loss = 0.53907710\n",
            "Iteration 70, loss = 0.53872273\n",
            "Iteration 71, loss = 0.53852677\n",
            "Iteration 72, loss = 0.53825027\n",
            "Iteration 73, loss = 0.53802444\n",
            "Iteration 74, loss = 0.53759156\n",
            "Iteration 75, loss = 0.53719528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 76, loss = 0.53695689\n",
            "Iteration 77, loss = 0.53658347\n",
            "Iteration 78, loss = 0.53635432\n",
            "Iteration 79, loss = 0.53605595\n",
            "Iteration 80, loss = 0.53582474\n",
            "Iteration 81, loss = 0.53554773\n",
            "Iteration 82, loss = 0.53535155\n",
            "Iteration 83, loss = 0.53515822\n",
            "Iteration 84, loss = 0.53487830\n",
            "Iteration 85, loss = 0.53471838\n",
            "Iteration 86, loss = 0.53452936\n",
            "Iteration 87, loss = 0.53428679\n",
            "Iteration 88, loss = 0.53411485\n",
            "Iteration 89, loss = 0.53393016\n",
            "Iteration 90, loss = 0.53375943\n",
            "Iteration 91, loss = 0.53365426\n",
            "Iteration 92, loss = 0.53351596\n",
            "Iteration 93, loss = 0.53327774\n",
            "Iteration 94, loss = 0.53314514\n",
            "Iteration 95, loss = 0.53281641\n",
            "Iteration 96, loss = 0.53262599\n",
            "Iteration 97, loss = 0.53239384\n",
            "Iteration 98, loss = 0.53232582\n",
            "Iteration 99, loss = 0.53211350\n",
            "Iteration 100, loss = 0.53190029\n",
            "Iteration 101, loss = 0.53173212\n",
            "Iteration 102, loss = 0.53161053\n",
            "Iteration 103, loss = 0.53145592\n",
            "Iteration 104, loss = 0.53128751\n",
            "Iteration 105, loss = 0.53120559\n",
            "Iteration 106, loss = 0.53110726\n",
            "Iteration 107, loss = 0.53099419\n",
            "Iteration 108, loss = 0.53085608\n",
            "Iteration 109, loss = 0.53079898\n",
            "Iteration 110, loss = 0.53075564\n",
            "Iteration 111, loss = 0.53057516\n",
            "Iteration 112, loss = 0.53049676\n",
            "Iteration 113, loss = 0.53038255\n",
            "Iteration 114, loss = 0.53021973\n",
            "Iteration 115, loss = 0.53023859\n",
            "Iteration 116, loss = 0.53017259\n",
            "Iteration 117, loss = 0.53007937\n",
            "Iteration 118, loss = 0.52994985\n",
            "Iteration 119, loss = 0.52982128\n",
            "Iteration 120, loss = 0.52974964\n",
            "Iteration 121, loss = 0.52960121\n",
            "Iteration 122, loss = 0.52940096\n",
            "Iteration 123, loss = 0.52934112\n",
            "Iteration 124, loss = 0.52931373\n",
            "Iteration 125, loss = 0.52915815\n",
            "Iteration 126, loss = 0.52909309\n",
            "Iteration 127, loss = 0.52895152\n",
            "Iteration 128, loss = 0.52882711\n",
            "Iteration 129, loss = 0.52876681\n",
            "Iteration 130, loss = 0.52866889\n",
            "Iteration 131, loss = 0.52865310\n",
            "Iteration 132, loss = 0.52858254\n",
            "Iteration 133, loss = 0.52862777\n",
            "Iteration 134, loss = 0.52852957\n",
            "Iteration 135, loss = 0.52859374\n",
            "Iteration 136, loss = 0.52849947\n",
            "Iteration 137, loss = 0.52844982\n",
            "Iteration 138, loss = 0.52852134\n",
            "Iteration 139, loss = 0.52836411\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.84459665\n",
            "Iteration 2, loss = 0.81870228\n",
            "Iteration 3, loss = 0.78595389\n",
            "Iteration 4, loss = 0.74703750\n",
            "Iteration 5, loss = 0.71258339\n",
            "Iteration 6, loss = 0.68531183\n",
            "Iteration 7, loss = 0.66229352\n",
            "Iteration 8, loss = 0.64435029\n",
            "Iteration 9, loss = 0.63245378\n",
            "Iteration 10, loss = 0.62203620\n",
            "Iteration 11, loss = 0.61422600\n",
            "Iteration 12, loss = 0.60819304\n",
            "Iteration 13, loss = 0.60394945\n",
            "Iteration 14, loss = 0.60001920\n",
            "Iteration 15, loss = 0.59682229\n",
            "Iteration 16, loss = 0.59409794\n",
            "Iteration 17, loss = 0.59187271\n",
            "Iteration 18, loss = 0.58971012\n",
            "Iteration 19, loss = 0.58782321\n",
            "Iteration 20, loss = 0.58622969\n",
            "Iteration 21, loss = 0.58456990\n",
            "Iteration 22, loss = 0.58308345\n",
            "Iteration 23, loss = 0.58175424\n",
            "Iteration 24, loss = 0.58053072\n",
            "Iteration 25, loss = 0.57943520\n",
            "Iteration 26, loss = 0.57847461\n",
            "Iteration 27, loss = 0.57744098\n",
            "Iteration 28, loss = 0.57649424\n",
            "Iteration 29, loss = 0.57561182\n",
            "Iteration 30, loss = 0.57472458\n",
            "Iteration 31, loss = 0.57386337\n",
            "Iteration 32, loss = 0.57320966\n",
            "Iteration 33, loss = 0.57256386\n",
            "Iteration 34, loss = 0.57190437\n",
            "Iteration 35, loss = 0.57129547\n",
            "Iteration 36, loss = 0.57075666\n",
            "Iteration 37, loss = 0.57013798\n",
            "Iteration 38, loss = 0.56964642\n",
            "Iteration 39, loss = 0.56913319\n",
            "Iteration 40, loss = 0.56867031\n",
            "Iteration 41, loss = 0.56805439\n",
            "Iteration 42, loss = 0.56757060\n",
            "Iteration 43, loss = 0.56705874\n",
            "Iteration 44, loss = 0.56652038\n",
            "Iteration 45, loss = 0.56613455\n",
            "Iteration 46, loss = 0.56564884\n",
            "Iteration 47, loss = 0.56532256\n",
            "Iteration 48, loss = 0.56486247\n",
            "Iteration 49, loss = 0.56442581\n",
            "Iteration 50, loss = 0.56400200\n",
            "Iteration 51, loss = 0.56363125\n",
            "Iteration 52, loss = 0.56319439\n",
            "Iteration 53, loss = 0.56270183\n",
            "Iteration 54, loss = 0.56225417\n",
            "Iteration 55, loss = 0.56195227\n",
            "Iteration 56, loss = 0.56154741\n",
            "Iteration 57, loss = 0.56111386\n",
            "Iteration 58, loss = 0.56077495\n",
            "Iteration 59, loss = 0.56040345\n",
            "Iteration 60, loss = 0.56005605\n",
            "Iteration 61, loss = 0.55961365\n",
            "Iteration 62, loss = 0.55924774\n",
            "Iteration 63, loss = 0.55888500\n",
            "Iteration 64, loss = 0.55855013\n",
            "Iteration 65, loss = 0.55820503\n",
            "Iteration 66, loss = 0.55783153\n",
            "Iteration 67, loss = 0.55740062\n",
            "Iteration 68, loss = 0.55713550\n",
            "Iteration 69, loss = 0.55676995\n",
            "Iteration 70, loss = 0.55640435\n",
            "Iteration 71, loss = 0.55623051\n",
            "Iteration 72, loss = 0.55586898\n",
            "Iteration 73, loss = 0.55551588\n",
            "Iteration 74, loss = 0.55517806\n",
            "Iteration 75, loss = 0.55474794\n",
            "Iteration 76, loss = 0.55447635\n",
            "Iteration 77, loss = 0.55405120\n",
            "Iteration 78, loss = 0.55367833\n",
            "Iteration 79, loss = 0.55337910\n",
            "Iteration 80, loss = 0.55296347\n",
            "Iteration 81, loss = 0.55258527\n",
            "Iteration 82, loss = 0.55233845\n",
            "Iteration 83, loss = 0.55204456\n",
            "Iteration 84, loss = 0.55171987\n",
            "Iteration 85, loss = 0.55152066\n",
            "Iteration 86, loss = 0.55125547\n",
            "Iteration 87, loss = 0.55102926\n",
            "Iteration 88, loss = 0.55078327\n",
            "Iteration 89, loss = 0.55049191\n",
            "Iteration 90, loss = 0.55022934\n",
            "Iteration 91, loss = 0.55002248\n",
            "Iteration 92, loss = 0.54970664\n",
            "Iteration 93, loss = 0.54939765\n",
            "Iteration 94, loss = 0.54915329\n",
            "Iteration 95, loss = 0.54878256\n",
            "Iteration 96, loss = 0.54848819\n",
            "Iteration 97, loss = 0.54816994\n",
            "Iteration 98, loss = 0.54797776\n",
            "Iteration 99, loss = 0.54764892\n",
            "Iteration 100, loss = 0.54741962\n",
            "Iteration 101, loss = 0.54716725\n",
            "Iteration 102, loss = 0.54694231\n",
            "Iteration 103, loss = 0.54675451\n",
            "Iteration 104, loss = 0.54660119\n",
            "Iteration 105, loss = 0.54638037\n",
            "Iteration 106, loss = 0.54624164\n",
            "Iteration 107, loss = 0.54596770\n",
            "Iteration 108, loss = 0.54578311\n",
            "Iteration 109, loss = 0.54553795\n",
            "Iteration 110, loss = 0.54538834\n",
            "Iteration 111, loss = 0.54519370\n",
            "Iteration 112, loss = 0.54493799\n",
            "Iteration 113, loss = 0.54472679\n",
            "Iteration 114, loss = 0.54447468\n",
            "Iteration 115, loss = 0.54438885\n",
            "Iteration 116, loss = 0.54411089\n",
            "Iteration 117, loss = 0.54395248\n",
            "Iteration 118, loss = 0.54370300\n",
            "Iteration 119, loss = 0.54354589\n",
            "Iteration 120, loss = 0.54332472\n",
            "Iteration 121, loss = 0.54318818\n",
            "Iteration 122, loss = 0.54290722\n",
            "Iteration 123, loss = 0.54273256\n",
            "Iteration 124, loss = 0.54249093\n",
            "Iteration 125, loss = 0.54230916\n",
            "Iteration 126, loss = 0.54212763\n",
            "Iteration 127, loss = 0.54181847\n",
            "Iteration 128, loss = 0.54165533\n",
            "Iteration 129, loss = 0.54141417\n",
            "Iteration 130, loss = 0.54112551\n",
            "Iteration 131, loss = 0.54102938\n",
            "Iteration 132, loss = 0.54077312\n",
            "Iteration 133, loss = 0.54061478\n",
            "Iteration 134, loss = 0.54037494\n",
            "Iteration 135, loss = 0.54008000\n",
            "Iteration 136, loss = 0.53980382\n",
            "Iteration 137, loss = 0.53957254\n",
            "Iteration 138, loss = 0.53941135\n",
            "Iteration 139, loss = 0.53915533\n",
            "Iteration 140, loss = 0.53879635\n",
            "Iteration 141, loss = 0.53861513\n",
            "Iteration 142, loss = 0.53826862\n",
            "Iteration 143, loss = 0.53797140\n",
            "Iteration 144, loss = 0.53767791\n",
            "Iteration 145, loss = 0.53758785\n",
            "Iteration 146, loss = 0.53735928\n",
            "Iteration 147, loss = 0.53707884\n",
            "Iteration 148, loss = 0.53685146\n",
            "Iteration 149, loss = 0.53667185\n",
            "Iteration 150, loss = 0.53646668\n",
            "Iteration 1, loss = 0.84006331\n",
            "Iteration 2, loss = 0.81376413\n",
            "Iteration 3, loss = 0.77923652\n",
            "Iteration 4, loss = 0.74192691\n",
            "Iteration 5, loss = 0.71081488\n",
            "Iteration 6, loss = 0.68542628\n",
            "Iteration 7, loss = 0.66332206\n",
            "Iteration 8, loss = 0.64635824\n",
            "Iteration 9, loss = 0.63430855\n",
            "Iteration 10, loss = 0.62333389\n",
            "Iteration 11, loss = 0.61592732\n",
            "Iteration 12, loss = 0.61059120\n",
            "Iteration 13, loss = 0.60572314\n",
            "Iteration 14, loss = 0.60162257\n",
            "Iteration 15, loss = 0.59818823\n",
            "Iteration 16, loss = 0.59510392\n",
            "Iteration 17, loss = 0.59243361\n",
            "Iteration 18, loss = 0.58999220\n",
            "Iteration 19, loss = 0.58775910\n",
            "Iteration 20, loss = 0.58570466\n",
            "Iteration 21, loss = 0.58382153\n",
            "Iteration 22, loss = 0.58221027\n",
            "Iteration 23, loss = 0.58074782\n",
            "Iteration 24, loss = 0.57942583\n",
            "Iteration 25, loss = 0.57825592\n",
            "Iteration 26, loss = 0.57718551\n",
            "Iteration 27, loss = 0.57609237\n",
            "Iteration 28, loss = 0.57502039\n",
            "Iteration 29, loss = 0.57416546\n",
            "Iteration 30, loss = 0.57336706\n",
            "Iteration 31, loss = 0.57254110\n",
            "Iteration 32, loss = 0.57178136\n",
            "Iteration 33, loss = 0.57121735\n",
            "Iteration 34, loss = 0.57065672\n",
            "Iteration 35, loss = 0.57011026\n",
            "Iteration 36, loss = 0.56962549\n",
            "Iteration 37, loss = 0.56905940\n",
            "Iteration 38, loss = 0.56870017\n",
            "Iteration 39, loss = 0.56841004\n",
            "Iteration 40, loss = 0.56813621\n",
            "Iteration 41, loss = 0.56767469\n",
            "Iteration 42, loss = 0.56735452\n",
            "Iteration 43, loss = 0.56692246\n",
            "Iteration 44, loss = 0.56655300\n",
            "Iteration 45, loss = 0.56632262\n",
            "Iteration 46, loss = 0.56592314\n",
            "Iteration 47, loss = 0.56566446\n",
            "Iteration 48, loss = 0.56536787\n",
            "Iteration 49, loss = 0.56507362\n",
            "Iteration 50, loss = 0.56477701\n",
            "Iteration 51, loss = 0.56458465\n",
            "Iteration 52, loss = 0.56430101\n",
            "Iteration 53, loss = 0.56397691\n",
            "Iteration 54, loss = 0.56383676\n",
            "Iteration 55, loss = 0.56355069\n",
            "Iteration 56, loss = 0.56333377\n",
            "Iteration 57, loss = 0.56300657\n",
            "Iteration 58, loss = 0.56280934\n",
            "Iteration 59, loss = 0.56259266\n",
            "Iteration 60, loss = 0.56235040\n",
            "Iteration 61, loss = 0.56206429\n",
            "Iteration 62, loss = 0.56184369\n",
            "Iteration 63, loss = 0.56162229\n",
            "Iteration 64, loss = 0.56138660\n",
            "Iteration 65, loss = 0.56115882\n",
            "Iteration 66, loss = 0.56092963\n",
            "Iteration 67, loss = 0.56062588\n",
            "Iteration 68, loss = 0.56041223\n",
            "Iteration 69, loss = 0.56023079\n",
            "Iteration 70, loss = 0.55999148\n",
            "Iteration 71, loss = 0.55977984\n",
            "Iteration 72, loss = 0.55951684\n",
            "Iteration 73, loss = 0.55937821\n",
            "Iteration 74, loss = 0.55921986\n",
            "Iteration 75, loss = 0.55887053\n",
            "Iteration 76, loss = 0.55866763\n",
            "Iteration 77, loss = 0.55835573\n",
            "Iteration 78, loss = 0.55806711\n",
            "Iteration 79, loss = 0.55774697\n",
            "Iteration 80, loss = 0.55738107\n",
            "Iteration 81, loss = 0.55707906\n",
            "Iteration 82, loss = 0.55689875\n",
            "Iteration 83, loss = 0.55667676\n",
            "Iteration 84, loss = 0.55639916\n",
            "Iteration 85, loss = 0.55622890\n",
            "Iteration 86, loss = 0.55600852\n",
            "Iteration 87, loss = 0.55583013\n",
            "Iteration 88, loss = 0.55565230\n",
            "Iteration 89, loss = 0.55543481\n",
            "Iteration 90, loss = 0.55528561\n",
            "Iteration 91, loss = 0.55506069\n",
            "Iteration 92, loss = 0.55477530\n",
            "Iteration 93, loss = 0.55451576\n",
            "Iteration 94, loss = 0.55430681\n",
            "Iteration 95, loss = 0.55400776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 96, loss = 0.55369259\n",
            "Iteration 97, loss = 0.55338130\n",
            "Iteration 98, loss = 0.55315037\n",
            "Iteration 99, loss = 0.55291589\n",
            "Iteration 100, loss = 0.55264640\n",
            "Iteration 101, loss = 0.55247134\n",
            "Iteration 102, loss = 0.55233386\n",
            "Iteration 103, loss = 0.55216016\n",
            "Iteration 104, loss = 0.55193526\n",
            "Iteration 105, loss = 0.55171716\n",
            "Iteration 106, loss = 0.55161165\n",
            "Iteration 107, loss = 0.55133595\n",
            "Iteration 108, loss = 0.55128412\n",
            "Iteration 109, loss = 0.55106182\n",
            "Iteration 110, loss = 0.55094599\n",
            "Iteration 111, loss = 0.55088210\n",
            "Iteration 112, loss = 0.55070015\n",
            "Iteration 113, loss = 0.55049384\n",
            "Iteration 114, loss = 0.55041983\n",
            "Iteration 115, loss = 0.55029610\n",
            "Iteration 116, loss = 0.55008228\n",
            "Iteration 117, loss = 0.55000776\n",
            "Iteration 118, loss = 0.54975533\n",
            "Iteration 119, loss = 0.54956440\n",
            "Iteration 120, loss = 0.54945570\n",
            "Iteration 121, loss = 0.54927225\n",
            "Iteration 122, loss = 0.54911288\n",
            "Iteration 123, loss = 0.54895665\n",
            "Iteration 124, loss = 0.54875025\n",
            "Iteration 125, loss = 0.54857615\n",
            "Iteration 126, loss = 0.54849266\n",
            "Iteration 127, loss = 0.54822139\n",
            "Iteration 128, loss = 0.54812265\n",
            "Iteration 129, loss = 0.54801617\n",
            "Iteration 130, loss = 0.54771625\n",
            "Iteration 131, loss = 0.54773805\n",
            "Iteration 132, loss = 0.54743297\n",
            "Iteration 133, loss = 0.54737811\n",
            "Iteration 134, loss = 0.54717746\n",
            "Iteration 135, loss = 0.54696117\n",
            "Iteration 136, loss = 0.54676712\n",
            "Iteration 137, loss = 0.54661184\n",
            "Iteration 138, loss = 0.54652299\n",
            "Iteration 139, loss = 0.54633499\n",
            "Iteration 140, loss = 0.54617776\n",
            "Iteration 141, loss = 0.54619609\n",
            "Iteration 142, loss = 0.54596633\n",
            "Iteration 143, loss = 0.54583598\n",
            "Iteration 144, loss = 0.54563675\n",
            "Iteration 145, loss = 0.54564187\n",
            "Iteration 146, loss = 0.54546484\n",
            "Iteration 147, loss = 0.54532720\n",
            "Iteration 148, loss = 0.54518740\n",
            "Iteration 149, loss = 0.54503367\n",
            "Iteration 150, loss = 0.54491398\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 150 and for layer number 3 : 0.70625\n",
            "Iteration 1, loss = 0.92540177\n",
            "Iteration 2, loss = 0.90606988\n",
            "Iteration 3, loss = 0.87636521\n",
            "Iteration 4, loss = 0.84453499\n",
            "Iteration 5, loss = 0.81069894\n",
            "Iteration 6, loss = 0.78049825\n",
            "Iteration 7, loss = 0.75458834\n",
            "Iteration 8, loss = 0.73221444\n",
            "Iteration 9, loss = 0.71299243\n",
            "Iteration 10, loss = 0.69629293\n",
            "Iteration 11, loss = 0.68190019\n",
            "Iteration 12, loss = 0.66823391\n",
            "Iteration 13, loss = 0.65677101\n",
            "Iteration 14, loss = 0.64656579\n",
            "Iteration 15, loss = 0.63780899\n",
            "Iteration 16, loss = 0.63058347\n",
            "Iteration 17, loss = 0.62420683\n",
            "Iteration 18, loss = 0.61911339\n",
            "Iteration 19, loss = 0.61402184\n",
            "Iteration 20, loss = 0.60966985\n",
            "Iteration 21, loss = 0.60588786\n",
            "Iteration 22, loss = 0.60253512\n",
            "Iteration 23, loss = 0.59952335\n",
            "Iteration 24, loss = 0.59636324\n",
            "Iteration 25, loss = 0.59415604\n",
            "Iteration 26, loss = 0.59185568\n",
            "Iteration 27, loss = 0.58962051\n",
            "Iteration 28, loss = 0.58753568\n",
            "Iteration 29, loss = 0.58565227\n",
            "Iteration 30, loss = 0.58359418\n",
            "Iteration 31, loss = 0.58153387\n",
            "Iteration 32, loss = 0.57977999\n",
            "Iteration 33, loss = 0.57793385\n",
            "Iteration 34, loss = 0.57632165\n",
            "Iteration 35, loss = 0.57508868\n",
            "Iteration 36, loss = 0.57386065\n",
            "Iteration 37, loss = 0.57284075\n",
            "Iteration 38, loss = 0.57151683\n",
            "Iteration 39, loss = 0.57035367\n",
            "Iteration 40, loss = 0.56936141\n",
            "Iteration 41, loss = 0.56829218\n",
            "Iteration 42, loss = 0.56718683\n",
            "Iteration 43, loss = 0.56628285\n",
            "Iteration 44, loss = 0.56540172\n",
            "Iteration 45, loss = 0.56450610\n",
            "Iteration 46, loss = 0.56359359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 47, loss = 0.56274113\n",
            "Iteration 48, loss = 0.56205347\n",
            "Iteration 49, loss = 0.56141325\n",
            "Iteration 50, loss = 0.56076824\n",
            "Iteration 51, loss = 0.56010553\n",
            "Iteration 52, loss = 0.55955061\n",
            "Iteration 53, loss = 0.55909395\n",
            "Iteration 54, loss = 0.55851126\n",
            "Iteration 55, loss = 0.55829418\n",
            "Iteration 56, loss = 0.55769846\n",
            "Iteration 57, loss = 0.55725754\n",
            "Iteration 58, loss = 0.55675838\n",
            "Iteration 59, loss = 0.55638045\n",
            "Iteration 60, loss = 0.55586590\n",
            "Iteration 61, loss = 0.55559102\n",
            "Iteration 62, loss = 0.55526240\n",
            "Iteration 63, loss = 0.55501197\n",
            "Iteration 64, loss = 0.55472593\n",
            "Iteration 65, loss = 0.55441042\n",
            "Iteration 66, loss = 0.55417765\n",
            "Iteration 67, loss = 0.55391604\n",
            "Iteration 68, loss = 0.55367443\n",
            "Iteration 69, loss = 0.55341227\n",
            "Iteration 70, loss = 0.55306736\n",
            "Iteration 71, loss = 0.55291328\n",
            "Iteration 72, loss = 0.55263420\n",
            "Iteration 73, loss = 0.55247432\n",
            "Iteration 74, loss = 0.55217557\n",
            "Iteration 75, loss = 0.55197460\n",
            "Iteration 76, loss = 0.55176851\n",
            "Iteration 77, loss = 0.55160822\n",
            "Iteration 78, loss = 0.55124273\n",
            "Iteration 79, loss = 0.55130751\n",
            "Iteration 80, loss = 0.55101272\n",
            "Iteration 81, loss = 0.55080159\n",
            "Iteration 82, loss = 0.55055801\n",
            "Iteration 83, loss = 0.55028475\n",
            "Iteration 84, loss = 0.55008004\n",
            "Iteration 85, loss = 0.54978130\n",
            "Iteration 86, loss = 0.54946212\n",
            "Iteration 87, loss = 0.54921940\n",
            "Iteration 88, loss = 0.54905529\n",
            "Iteration 89, loss = 0.54883849\n",
            "Iteration 90, loss = 0.54866519\n",
            "Iteration 91, loss = 0.54847662\n",
            "Iteration 92, loss = 0.54822737\n",
            "Iteration 93, loss = 0.54809133\n",
            "Iteration 94, loss = 0.54783013\n",
            "Iteration 95, loss = 0.54764196\n",
            "Iteration 96, loss = 0.54742739\n",
            "Iteration 97, loss = 0.54731608\n",
            "Iteration 98, loss = 0.54716179\n",
            "Iteration 99, loss = 0.54712924\n",
            "Iteration 100, loss = 0.54680985\n",
            "Iteration 101, loss = 0.54673239\n",
            "Iteration 102, loss = 0.54647657\n",
            "Iteration 103, loss = 0.54624866\n",
            "Iteration 104, loss = 0.54612778\n",
            "Iteration 105, loss = 0.54598568\n",
            "Iteration 106, loss = 0.54591547\n",
            "Iteration 107, loss = 0.54585241\n",
            "Iteration 108, loss = 0.54591879\n",
            "Iteration 109, loss = 0.54577268\n",
            "Iteration 110, loss = 0.54575403\n",
            "Iteration 111, loss = 0.54565059\n",
            "Iteration 112, loss = 0.54569602\n",
            "Iteration 113, loss = 0.54556721\n",
            "Iteration 114, loss = 0.54536502\n",
            "Iteration 115, loss = 0.54530154\n",
            "Iteration 116, loss = 0.54523272\n",
            "Iteration 117, loss = 0.54506782\n",
            "Iteration 118, loss = 0.54500117\n",
            "Iteration 119, loss = 0.54493549\n",
            "Iteration 120, loss = 0.54482254\n",
            "Iteration 121, loss = 0.54475022\n",
            "Iteration 122, loss = 0.54464520\n",
            "Iteration 123, loss = 0.54462680\n",
            "Iteration 124, loss = 0.54460052\n",
            "Iteration 125, loss = 0.54447940\n",
            "Iteration 126, loss = 0.54456956\n",
            "Iteration 127, loss = 0.54425805\n",
            "Iteration 128, loss = 0.54413216\n",
            "Iteration 129, loss = 0.54384069\n",
            "Iteration 130, loss = 0.54371370\n",
            "Iteration 131, loss = 0.54352732\n",
            "Iteration 132, loss = 0.54335967\n",
            "Iteration 133, loss = 0.54329342\n",
            "Iteration 134, loss = 0.54322469\n",
            "Iteration 135, loss = 0.54321215\n",
            "Iteration 136, loss = 0.54299755\n",
            "Iteration 137, loss = 0.54285669\n",
            "Iteration 138, loss = 0.54275060\n",
            "Iteration 139, loss = 0.54247230\n",
            "Iteration 140, loss = 0.54230632\n",
            "Iteration 141, loss = 0.54220545\n",
            "Iteration 142, loss = 0.54214524\n",
            "Iteration 143, loss = 0.54205900\n",
            "Iteration 144, loss = 0.54184695\n",
            "Iteration 145, loss = 0.54182552\n",
            "Iteration 146, loss = 0.54157341\n",
            "Iteration 147, loss = 0.54145732\n",
            "Iteration 148, loss = 0.54126089\n",
            "Iteration 149, loss = 0.54101512\n",
            "Iteration 150, loss = 0.54097605\n",
            "Iteration 1, loss = 0.92236300\n",
            "Iteration 2, loss = 0.90372668\n",
            "Iteration 3, loss = 0.87500833\n",
            "Iteration 4, loss = 0.84285035\n",
            "Iteration 5, loss = 0.80913380\n",
            "Iteration 6, loss = 0.77972252\n",
            "Iteration 7, loss = 0.75423862\n",
            "Iteration 8, loss = 0.73244226\n",
            "Iteration 9, loss = 0.71337957\n",
            "Iteration 10, loss = 0.69694930\n",
            "Iteration 11, loss = 0.68235517\n",
            "Iteration 12, loss = 0.66874001\n",
            "Iteration 13, loss = 0.65701897\n",
            "Iteration 14, loss = 0.64666486\n",
            "Iteration 15, loss = 0.63680422\n",
            "Iteration 16, loss = 0.62903974\n",
            "Iteration 17, loss = 0.62282581\n",
            "Iteration 18, loss = 0.61753655\n",
            "Iteration 19, loss = 0.61255961\n",
            "Iteration 20, loss = 0.60822250\n",
            "Iteration 21, loss = 0.60455223\n",
            "Iteration 22, loss = 0.60093557\n",
            "Iteration 23, loss = 0.59766462\n",
            "Iteration 24, loss = 0.59443533\n",
            "Iteration 25, loss = 0.59186429\n",
            "Iteration 26, loss = 0.58954458\n",
            "Iteration 27, loss = 0.58722957\n",
            "Iteration 28, loss = 0.58504645\n",
            "Iteration 29, loss = 0.58302508\n",
            "Iteration 30, loss = 0.58092002\n",
            "Iteration 31, loss = 0.57890737\n",
            "Iteration 32, loss = 0.57713204\n",
            "Iteration 33, loss = 0.57553550\n",
            "Iteration 34, loss = 0.57399110\n",
            "Iteration 35, loss = 0.57258257\n",
            "Iteration 36, loss = 0.57132345\n",
            "Iteration 37, loss = 0.57036989\n",
            "Iteration 38, loss = 0.56919804\n",
            "Iteration 39, loss = 0.56804072\n",
            "Iteration 40, loss = 0.56720233\n",
            "Iteration 41, loss = 0.56616219\n",
            "Iteration 42, loss = 0.56523123\n",
            "Iteration 43, loss = 0.56418669\n",
            "Iteration 44, loss = 0.56340104\n",
            "Iteration 45, loss = 0.56229242\n",
            "Iteration 46, loss = 0.56136004\n",
            "Iteration 47, loss = 0.56050574\n",
            "Iteration 48, loss = 0.55975555\n",
            "Iteration 49, loss = 0.55893402\n",
            "Iteration 50, loss = 0.55825810\n",
            "Iteration 51, loss = 0.55750539\n",
            "Iteration 52, loss = 0.55689701\n",
            "Iteration 53, loss = 0.55630534\n",
            "Iteration 54, loss = 0.55554960\n",
            "Iteration 55, loss = 0.55530925\n",
            "Iteration 56, loss = 0.55458557\n",
            "Iteration 57, loss = 0.55417055\n",
            "Iteration 58, loss = 0.55375697\n",
            "Iteration 59, loss = 0.55343024\n",
            "Iteration 60, loss = 0.55303828\n",
            "Iteration 61, loss = 0.55275448\n",
            "Iteration 62, loss = 0.55246826\n",
            "Iteration 63, loss = 0.55210971\n",
            "Iteration 64, loss = 0.55175282\n",
            "Iteration 65, loss = 0.55137494\n",
            "Iteration 66, loss = 0.55113308\n",
            "Iteration 67, loss = 0.55081752\n",
            "Iteration 68, loss = 0.55067280\n",
            "Iteration 69, loss = 0.55037239\n",
            "Iteration 70, loss = 0.55015762\n",
            "Iteration 71, loss = 0.54984322\n",
            "Iteration 72, loss = 0.54962258\n",
            "Iteration 73, loss = 0.54941433\n",
            "Iteration 74, loss = 0.54910659\n",
            "Iteration 75, loss = 0.54882617\n",
            "Iteration 76, loss = 0.54858009\n",
            "Iteration 77, loss = 0.54842549\n",
            "Iteration 78, loss = 0.54826356\n",
            "Iteration 79, loss = 0.54817975\n",
            "Iteration 80, loss = 0.54779764\n",
            "Iteration 81, loss = 0.54763140\n",
            "Iteration 82, loss = 0.54736580\n",
            "Iteration 83, loss = 0.54705013\n",
            "Iteration 84, loss = 0.54689619\n",
            "Iteration 85, loss = 0.54652246\n",
            "Iteration 86, loss = 0.54618831\n",
            "Iteration 87, loss = 0.54586111\n",
            "Iteration 88, loss = 0.54563663\n",
            "Iteration 89, loss = 0.54532937\n",
            "Iteration 90, loss = 0.54516871\n",
            "Iteration 91, loss = 0.54484111\n",
            "Iteration 92, loss = 0.54457027\n",
            "Iteration 93, loss = 0.54439667\n",
            "Iteration 94, loss = 0.54405331\n",
            "Iteration 95, loss = 0.54382871\n",
            "Iteration 96, loss = 0.54356907\n",
            "Iteration 97, loss = 0.54331510\n",
            "Iteration 98, loss = 0.54317759\n",
            "Iteration 99, loss = 0.54304768\n",
            "Iteration 100, loss = 0.54279147\n",
            "Iteration 101, loss = 0.54261840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 102, loss = 0.54245119\n",
            "Iteration 103, loss = 0.54230165\n",
            "Iteration 104, loss = 0.54201321\n",
            "Iteration 105, loss = 0.54196650\n",
            "Iteration 106, loss = 0.54187452\n",
            "Iteration 107, loss = 0.54182689\n",
            "Iteration 108, loss = 0.54186328\n",
            "Iteration 109, loss = 0.54161841\n",
            "Iteration 110, loss = 0.54159098\n",
            "Iteration 111, loss = 0.54140119\n",
            "Iteration 112, loss = 0.54132090\n",
            "Iteration 113, loss = 0.54133298\n",
            "Iteration 114, loss = 0.54113774\n",
            "Iteration 115, loss = 0.54102169\n",
            "Iteration 116, loss = 0.54094796\n",
            "Iteration 117, loss = 0.54082351\n",
            "Iteration 118, loss = 0.54077253\n",
            "Iteration 119, loss = 0.54069632\n",
            "Iteration 120, loss = 0.54060634\n",
            "Iteration 121, loss = 0.54040797\n",
            "Iteration 122, loss = 0.54024138\n",
            "Iteration 123, loss = 0.54028824\n",
            "Iteration 124, loss = 0.54029877\n",
            "Iteration 125, loss = 0.54022868\n",
            "Iteration 126, loss = 0.54001368\n",
            "Iteration 127, loss = 0.53985196\n",
            "Iteration 128, loss = 0.53976040\n",
            "Iteration 129, loss = 0.53951986\n",
            "Iteration 130, loss = 0.53942221\n",
            "Iteration 131, loss = 0.53929777\n",
            "Iteration 132, loss = 0.53916102\n",
            "Iteration 133, loss = 0.53903189\n",
            "Iteration 134, loss = 0.53910032\n",
            "Iteration 135, loss = 0.53900365\n",
            "Iteration 136, loss = 0.53908288\n",
            "Iteration 137, loss = 0.53888749\n",
            "Iteration 138, loss = 0.53882052\n",
            "Iteration 139, loss = 0.53874478\n",
            "Iteration 140, loss = 0.53866072\n",
            "Iteration 141, loss = 0.53861333\n",
            "Iteration 142, loss = 0.53871510\n",
            "Iteration 143, loss = 0.53874044\n",
            "Iteration 144, loss = 0.53861559\n",
            "Iteration 145, loss = 0.53866957\n",
            "Iteration 146, loss = 0.53851373\n",
            "Iteration 147, loss = 0.53844718\n",
            "Iteration 148, loss = 0.53822626\n",
            "Iteration 149, loss = 0.53814144\n",
            "Iteration 150, loss = 0.53820563\n",
            "Iteration 1, loss = 0.93166195\n",
            "Iteration 2, loss = 0.91234492\n",
            "Iteration 3, loss = 0.88372313\n",
            "Iteration 4, loss = 0.84961251\n",
            "Iteration 5, loss = 0.81641210\n",
            "Iteration 6, loss = 0.78527885\n",
            "Iteration 7, loss = 0.75872117\n",
            "Iteration 8, loss = 0.73530918\n",
            "Iteration 9, loss = 0.71535645\n",
            "Iteration 10, loss = 0.69760505\n",
            "Iteration 11, loss = 0.68188671\n",
            "Iteration 12, loss = 0.66752288\n",
            "Iteration 13, loss = 0.65465085\n",
            "Iteration 14, loss = 0.64368593\n",
            "Iteration 15, loss = 0.63335085\n",
            "Iteration 16, loss = 0.62497229\n",
            "Iteration 17, loss = 0.61851546\n",
            "Iteration 18, loss = 0.61250769\n",
            "Iteration 19, loss = 0.60725642\n",
            "Iteration 20, loss = 0.60264352\n",
            "Iteration 21, loss = 0.59848665\n",
            "Iteration 22, loss = 0.59465851\n",
            "Iteration 23, loss = 0.59134538\n",
            "Iteration 24, loss = 0.58812798\n",
            "Iteration 25, loss = 0.58555756\n",
            "Iteration 26, loss = 0.58347267\n",
            "Iteration 27, loss = 0.58105626\n",
            "Iteration 28, loss = 0.57898547\n",
            "Iteration 29, loss = 0.57707857\n",
            "Iteration 30, loss = 0.57510602\n",
            "Iteration 31, loss = 0.57303366\n",
            "Iteration 32, loss = 0.57121707\n",
            "Iteration 33, loss = 0.56956037\n",
            "Iteration 34, loss = 0.56781174\n",
            "Iteration 35, loss = 0.56635167\n",
            "Iteration 36, loss = 0.56496659\n",
            "Iteration 37, loss = 0.56388825\n",
            "Iteration 38, loss = 0.56265033\n",
            "Iteration 39, loss = 0.56150979\n",
            "Iteration 40, loss = 0.56060243\n",
            "Iteration 41, loss = 0.55974129\n",
            "Iteration 42, loss = 0.55864499\n",
            "Iteration 43, loss = 0.55766563\n",
            "Iteration 44, loss = 0.55679573\n",
            "Iteration 45, loss = 0.55585704\n",
            "Iteration 46, loss = 0.55484301\n",
            "Iteration 47, loss = 0.55397414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 48, loss = 0.55323994\n",
            "Iteration 49, loss = 0.55246703\n",
            "Iteration 50, loss = 0.55180371\n",
            "Iteration 51, loss = 0.55112381\n",
            "Iteration 52, loss = 0.55041014\n",
            "Iteration 53, loss = 0.54992329\n",
            "Iteration 54, loss = 0.54928756\n",
            "Iteration 55, loss = 0.54895870\n",
            "Iteration 56, loss = 0.54836925\n",
            "Iteration 57, loss = 0.54789274\n",
            "Iteration 58, loss = 0.54743070\n",
            "Iteration 59, loss = 0.54683477\n",
            "Iteration 60, loss = 0.54645133\n",
            "Iteration 61, loss = 0.54598155\n",
            "Iteration 62, loss = 0.54552238\n",
            "Iteration 63, loss = 0.54515072\n",
            "Iteration 64, loss = 0.54483821\n",
            "Iteration 65, loss = 0.54450111\n",
            "Iteration 66, loss = 0.54421242\n",
            "Iteration 67, loss = 0.54376193\n",
            "Iteration 68, loss = 0.54363452\n",
            "Iteration 69, loss = 0.54321728\n",
            "Iteration 70, loss = 0.54288609\n",
            "Iteration 71, loss = 0.54252895\n",
            "Iteration 72, loss = 0.54228397\n",
            "Iteration 73, loss = 0.54204462\n",
            "Iteration 74, loss = 0.54173786\n",
            "Iteration 75, loss = 0.54144168\n",
            "Iteration 76, loss = 0.54119500\n",
            "Iteration 77, loss = 0.54091047\n",
            "Iteration 78, loss = 0.54069623\n",
            "Iteration 79, loss = 0.54067683\n",
            "Iteration 80, loss = 0.54013276\n",
            "Iteration 81, loss = 0.53989053\n",
            "Iteration 82, loss = 0.53959924\n",
            "Iteration 83, loss = 0.53936832\n",
            "Iteration 84, loss = 0.53909921\n",
            "Iteration 85, loss = 0.53901869\n",
            "Iteration 86, loss = 0.53886630\n",
            "Iteration 87, loss = 0.53863528\n",
            "Iteration 88, loss = 0.53844614\n",
            "Iteration 89, loss = 0.53823882\n",
            "Iteration 90, loss = 0.53804482\n",
            "Iteration 91, loss = 0.53777906\n",
            "Iteration 92, loss = 0.53750544\n",
            "Iteration 93, loss = 0.53723586\n",
            "Iteration 94, loss = 0.53706081\n",
            "Iteration 95, loss = 0.53680307\n",
            "Iteration 96, loss = 0.53657594\n",
            "Iteration 97, loss = 0.53648131\n",
            "Iteration 98, loss = 0.53633934\n",
            "Iteration 99, loss = 0.53616061\n",
            "Iteration 100, loss = 0.53588690\n",
            "Iteration 101, loss = 0.53572652\n",
            "Iteration 102, loss = 0.53554122\n",
            "Iteration 103, loss = 0.53541845\n",
            "Iteration 104, loss = 0.53524751\n",
            "Iteration 105, loss = 0.53511494\n",
            "Iteration 106, loss = 0.53496457\n",
            "Iteration 107, loss = 0.53490532\n",
            "Iteration 108, loss = 0.53475815\n",
            "Iteration 109, loss = 0.53455697\n",
            "Iteration 110, loss = 0.53447970\n",
            "Iteration 111, loss = 0.53441300\n",
            "Iteration 112, loss = 0.53425956\n",
            "Iteration 113, loss = 0.53414518\n",
            "Iteration 114, loss = 0.53402485\n",
            "Iteration 115, loss = 0.53396639\n",
            "Iteration 116, loss = 0.53392574\n",
            "Iteration 117, loss = 0.53373909\n",
            "Iteration 118, loss = 0.53377461\n",
            "Iteration 119, loss = 0.53363294\n",
            "Iteration 120, loss = 0.53362123\n",
            "Iteration 121, loss = 0.53359790\n",
            "Iteration 122, loss = 0.53336937\n",
            "Iteration 123, loss = 0.53322864\n",
            "Iteration 124, loss = 0.53314603\n",
            "Iteration 125, loss = 0.53310922\n",
            "Iteration 126, loss = 0.53300059\n",
            "Iteration 127, loss = 0.53300315\n",
            "Iteration 128, loss = 0.53294813\n",
            "Iteration 129, loss = 0.53288857\n",
            "Iteration 130, loss = 0.53290528\n",
            "Iteration 131, loss = 0.53289451\n",
            "Iteration 132, loss = 0.53291395\n",
            "Iteration 133, loss = 0.53283436\n",
            "Iteration 134, loss = 0.53261960\n",
            "Iteration 135, loss = 0.53265552\n",
            "Iteration 136, loss = 0.53270260\n",
            "Iteration 137, loss = 0.53254937\n",
            "Iteration 138, loss = 0.53253935\n",
            "Iteration 139, loss = 0.53249355\n",
            "Iteration 140, loss = 0.53237285\n",
            "Iteration 141, loss = 0.53230035\n",
            "Iteration 142, loss = 0.53216267\n",
            "Iteration 143, loss = 0.53203890\n",
            "Iteration 144, loss = 0.53195989\n",
            "Iteration 145, loss = 0.53192892\n",
            "Iteration 146, loss = 0.53178198\n",
            "Iteration 147, loss = 0.53173814\n",
            "Iteration 148, loss = 0.53150166\n",
            "Iteration 149, loss = 0.53143147\n",
            "Iteration 150, loss = 0.53135531\n",
            "Iteration 1, loss = 0.93004949\n",
            "Iteration 2, loss = 0.90996806\n",
            "Iteration 3, loss = 0.88055072\n",
            "Iteration 4, loss = 0.84659018\n",
            "Iteration 5, loss = 0.81298067\n",
            "Iteration 6, loss = 0.78206991\n",
            "Iteration 7, loss = 0.75624473\n",
            "Iteration 8, loss = 0.73305263\n",
            "Iteration 9, loss = 0.71302062\n",
            "Iteration 10, loss = 0.69627033\n",
            "Iteration 11, loss = 0.68068281\n",
            "Iteration 12, loss = 0.66784432\n",
            "Iteration 13, loss = 0.65616483\n",
            "Iteration 14, loss = 0.64623851\n",
            "Iteration 15, loss = 0.63710321\n",
            "Iteration 16, loss = 0.62947245\n",
            "Iteration 17, loss = 0.62332210\n",
            "Iteration 18, loss = 0.61772357\n",
            "Iteration 19, loss = 0.61296514\n",
            "Iteration 20, loss = 0.60893825\n",
            "Iteration 21, loss = 0.60537285\n",
            "Iteration 22, loss = 0.60197233\n",
            "Iteration 23, loss = 0.59888836\n",
            "Iteration 24, loss = 0.59592724\n",
            "Iteration 25, loss = 0.59330154\n",
            "Iteration 26, loss = 0.59119156\n",
            "Iteration 27, loss = 0.58870339\n",
            "Iteration 28, loss = 0.58650396\n",
            "Iteration 29, loss = 0.58465794\n",
            "Iteration 30, loss = 0.58280816\n",
            "Iteration 31, loss = 0.58096325\n",
            "Iteration 32, loss = 0.57914326\n",
            "Iteration 33, loss = 0.57759947\n",
            "Iteration 34, loss = 0.57596723\n",
            "Iteration 35, loss = 0.57461254\n",
            "Iteration 36, loss = 0.57309404\n",
            "Iteration 37, loss = 0.57195132\n",
            "Iteration 38, loss = 0.57082853\n",
            "Iteration 39, loss = 0.56968276\n",
            "Iteration 40, loss = 0.56876378\n",
            "Iteration 41, loss = 0.56795050\n",
            "Iteration 42, loss = 0.56670730\n",
            "Iteration 43, loss = 0.56589047\n",
            "Iteration 44, loss = 0.56489212\n",
            "Iteration 45, loss = 0.56408490\n",
            "Iteration 46, loss = 0.56314279\n",
            "Iteration 47, loss = 0.56221700\n",
            "Iteration 48, loss = 0.56123991\n",
            "Iteration 49, loss = 0.56021024\n",
            "Iteration 50, loss = 0.55950861\n",
            "Iteration 51, loss = 0.55881625\n",
            "Iteration 52, loss = 0.55804567\n",
            "Iteration 53, loss = 0.55743272\n",
            "Iteration 54, loss = 0.55675730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 55, loss = 0.55651589\n",
            "Iteration 56, loss = 0.55608563\n",
            "Iteration 57, loss = 0.55564424\n",
            "Iteration 58, loss = 0.55526565\n",
            "Iteration 59, loss = 0.55479143\n",
            "Iteration 60, loss = 0.55430114\n",
            "Iteration 61, loss = 0.55392714\n",
            "Iteration 62, loss = 0.55358673\n",
            "Iteration 63, loss = 0.55323272\n",
            "Iteration 64, loss = 0.55316209\n",
            "Iteration 65, loss = 0.55273953\n",
            "Iteration 66, loss = 0.55263798\n",
            "Iteration 67, loss = 0.55236876\n",
            "Iteration 68, loss = 0.55214964\n",
            "Iteration 69, loss = 0.55181669\n",
            "Iteration 70, loss = 0.55156614\n",
            "Iteration 71, loss = 0.55140596\n",
            "Iteration 72, loss = 0.55129497\n",
            "Iteration 73, loss = 0.55112913\n",
            "Iteration 74, loss = 0.55095720\n",
            "Iteration 75, loss = 0.55084412\n",
            "Iteration 76, loss = 0.55071812\n",
            "Iteration 77, loss = 0.55048781\n",
            "Iteration 78, loss = 0.55036839\n",
            "Iteration 79, loss = 0.55043769\n",
            "Iteration 80, loss = 0.55008919\n",
            "Iteration 81, loss = 0.54996789\n",
            "Iteration 82, loss = 0.54984336\n",
            "Iteration 83, loss = 0.54956273\n",
            "Iteration 84, loss = 0.54928818\n",
            "Iteration 85, loss = 0.54921401\n",
            "Iteration 86, loss = 0.54901825\n",
            "Iteration 87, loss = 0.54890758\n",
            "Iteration 88, loss = 0.54882581\n",
            "Iteration 89, loss = 0.54877306\n",
            "Iteration 90, loss = 0.54856873\n",
            "Iteration 91, loss = 0.54851207\n",
            "Iteration 92, loss = 0.54835563\n",
            "Iteration 93, loss = 0.54816557\n",
            "Iteration 94, loss = 0.54804833\n",
            "Iteration 95, loss = 0.54792993\n",
            "Iteration 96, loss = 0.54784983\n",
            "Iteration 97, loss = 0.54774266\n",
            "Iteration 98, loss = 0.54772164\n",
            "Iteration 99, loss = 0.54760421\n",
            "Iteration 100, loss = 0.54742928\n",
            "Iteration 101, loss = 0.54728446\n",
            "Iteration 102, loss = 0.54712408\n",
            "Iteration 103, loss = 0.54705767\n",
            "Iteration 104, loss = 0.54683412\n",
            "Iteration 105, loss = 0.54670329\n",
            "Iteration 106, loss = 0.54657429\n",
            "Iteration 107, loss = 0.54647062\n",
            "Iteration 108, loss = 0.54652613\n",
            "Iteration 109, loss = 0.54649605\n",
            "Iteration 110, loss = 0.54649798\n",
            "Iteration 111, loss = 0.54647283\n",
            "Iteration 112, loss = 0.54633499\n",
            "Iteration 113, loss = 0.54633940\n",
            "Iteration 114, loss = 0.54621142\n",
            "Iteration 115, loss = 0.54618250\n",
            "Iteration 116, loss = 0.54615024\n",
            "Iteration 117, loss = 0.54607173\n",
            "Iteration 118, loss = 0.54600383\n",
            "Iteration 119, loss = 0.54598453\n",
            "Iteration 120, loss = 0.54603966\n",
            "Iteration 121, loss = 0.54605032\n",
            "Iteration 122, loss = 0.54588161\n",
            "Iteration 123, loss = 0.54578006\n",
            "Iteration 124, loss = 0.54578094\n",
            "Iteration 125, loss = 0.54571232\n",
            "Iteration 126, loss = 0.54566232\n",
            "Iteration 127, loss = 0.54577442\n",
            "Iteration 128, loss = 0.54571461\n",
            "Iteration 129, loss = 0.54566955\n",
            "Iteration 130, loss = 0.54569243\n",
            "Iteration 131, loss = 0.54551785\n",
            "Iteration 132, loss = 0.54545224\n",
            "Iteration 133, loss = 0.54541232\n",
            "Iteration 134, loss = 0.54536686\n",
            "Iteration 135, loss = 0.54543813\n",
            "Iteration 136, loss = 0.54543235\n",
            "Iteration 137, loss = 0.54534983\n",
            "Iteration 138, loss = 0.54535801\n",
            "Iteration 139, loss = 0.54530951\n",
            "Iteration 140, loss = 0.54522699\n",
            "Iteration 141, loss = 0.54519488\n",
            "Iteration 142, loss = 0.54516024\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.93650373\n",
            "Iteration 2, loss = 0.91614372\n",
            "Iteration 3, loss = 0.88631046\n",
            "Iteration 4, loss = 0.85327913\n",
            "Iteration 5, loss = 0.82138194\n",
            "Iteration 6, loss = 0.79229617\n",
            "Iteration 7, loss = 0.76753456\n",
            "Iteration 8, loss = 0.74573124\n",
            "Iteration 9, loss = 0.72648054\n",
            "Iteration 10, loss = 0.71056910\n",
            "Iteration 11, loss = 0.69588855\n",
            "Iteration 12, loss = 0.68427106\n",
            "Iteration 13, loss = 0.67305324\n",
            "Iteration 14, loss = 0.66346976\n",
            "Iteration 15, loss = 0.65484224\n",
            "Iteration 16, loss = 0.64765771\n",
            "Iteration 17, loss = 0.64160092\n",
            "Iteration 18, loss = 0.63596695\n",
            "Iteration 19, loss = 0.63145300\n",
            "Iteration 20, loss = 0.62738962\n",
            "Iteration 21, loss = 0.62388968\n",
            "Iteration 22, loss = 0.62051025\n",
            "Iteration 23, loss = 0.61719150\n",
            "Iteration 24, loss = 0.61415457\n",
            "Iteration 25, loss = 0.61154190\n",
            "Iteration 26, loss = 0.60921490\n",
            "Iteration 27, loss = 0.60680333\n",
            "Iteration 28, loss = 0.60463037\n",
            "Iteration 29, loss = 0.60261868\n",
            "Iteration 30, loss = 0.60053790\n",
            "Iteration 31, loss = 0.59861214\n",
            "Iteration 32, loss = 0.59687878\n",
            "Iteration 33, loss = 0.59521436\n",
            "Iteration 34, loss = 0.59373168\n",
            "Iteration 35, loss = 0.59231031\n",
            "Iteration 36, loss = 0.59089091\n",
            "Iteration 37, loss = 0.58975222\n",
            "Iteration 38, loss = 0.58844761\n",
            "Iteration 39, loss = 0.58743580\n",
            "Iteration 40, loss = 0.58644674\n",
            "Iteration 41, loss = 0.58550410\n",
            "Iteration 42, loss = 0.58440164\n",
            "Iteration 43, loss = 0.58328570\n",
            "Iteration 44, loss = 0.58228471\n",
            "Iteration 45, loss = 0.58128176\n",
            "Iteration 46, loss = 0.58040523\n",
            "Iteration 47, loss = 0.57952866\n",
            "Iteration 48, loss = 0.57856752\n",
            "Iteration 49, loss = 0.57759396\n",
            "Iteration 50, loss = 0.57673645\n",
            "Iteration 51, loss = 0.57577758\n",
            "Iteration 52, loss = 0.57477461\n",
            "Iteration 53, loss = 0.57391312\n",
            "Iteration 54, loss = 0.57295768\n",
            "Iteration 55, loss = 0.57235619\n",
            "Iteration 56, loss = 0.57150601\n",
            "Iteration 57, loss = 0.57068862\n",
            "Iteration 58, loss = 0.56995885\n",
            "Iteration 59, loss = 0.56926414\n",
            "Iteration 60, loss = 0.56848196\n",
            "Iteration 61, loss = 0.56771898\n",
            "Iteration 62, loss = 0.56713732\n",
            "Iteration 63, loss = 0.56642712\n",
            "Iteration 64, loss = 0.56596670\n",
            "Iteration 65, loss = 0.56543547\n",
            "Iteration 66, loss = 0.56509810\n",
            "Iteration 67, loss = 0.56455514\n",
            "Iteration 68, loss = 0.56433789\n",
            "Iteration 69, loss = 0.56375333\n",
            "Iteration 70, loss = 0.56327808\n",
            "Iteration 71, loss = 0.56292281\n",
            "Iteration 72, loss = 0.56257013\n",
            "Iteration 73, loss = 0.56215551\n",
            "Iteration 74, loss = 0.56183300\n",
            "Iteration 75, loss = 0.56170533\n",
            "Iteration 76, loss = 0.56121154\n",
            "Iteration 77, loss = 0.56080663\n",
            "Iteration 78, loss = 0.56057794\n",
            "Iteration 79, loss = 0.56030054\n",
            "Iteration 80, loss = 0.56000938\n",
            "Iteration 81, loss = 0.55969876\n",
            "Iteration 82, loss = 0.55937165\n",
            "Iteration 83, loss = 0.55913175\n",
            "Iteration 84, loss = 0.55876924\n",
            "Iteration 85, loss = 0.55859709\n",
            "Iteration 86, loss = 0.55830636\n",
            "Iteration 87, loss = 0.55803344\n",
            "Iteration 88, loss = 0.55769157\n",
            "Iteration 89, loss = 0.55763339\n",
            "Iteration 90, loss = 0.55740669\n",
            "Iteration 91, loss = 0.55726678\n",
            "Iteration 92, loss = 0.55707046\n",
            "Iteration 93, loss = 0.55675904\n",
            "Iteration 94, loss = 0.55659666\n",
            "Iteration 95, loss = 0.55647031\n",
            "Iteration 96, loss = 0.55623035\n",
            "Iteration 97, loss = 0.55605166\n",
            "Iteration 98, loss = 0.55588317\n",
            "Iteration 99, loss = 0.55568320\n",
            "Iteration 100, loss = 0.55547851\n",
            "Iteration 101, loss = 0.55526354\n",
            "Iteration 102, loss = 0.55509953\n",
            "Iteration 103, loss = 0.55498294\n",
            "Iteration 104, loss = 0.55484102\n",
            "Iteration 105, loss = 0.55457174\n",
            "Iteration 106, loss = 0.55451573\n",
            "Iteration 107, loss = 0.55438375\n",
            "Iteration 108, loss = 0.55428149\n",
            "Iteration 109, loss = 0.55413290\n",
            "Iteration 110, loss = 0.55398899\n",
            "Iteration 111, loss = 0.55399261\n",
            "Iteration 112, loss = 0.55390406\n",
            "Iteration 113, loss = 0.55378590\n",
            "Iteration 114, loss = 0.55384290\n",
            "Iteration 115, loss = 0.55370452\n",
            "Iteration 116, loss = 0.55361862\n",
            "Iteration 117, loss = 0.55354360\n",
            "Iteration 118, loss = 0.55337221\n",
            "Iteration 119, loss = 0.55329853\n",
            "Iteration 120, loss = 0.55319982\n",
            "Iteration 121, loss = 0.55327211\n",
            "Iteration 122, loss = 0.55295736\n",
            "Iteration 123, loss = 0.55293288\n",
            "Iteration 124, loss = 0.55279517\n",
            "Iteration 125, loss = 0.55270083\n",
            "Iteration 126, loss = 0.55261500\n",
            "Iteration 127, loss = 0.55259929\n",
            "Iteration 128, loss = 0.55243765\n",
            "Iteration 129, loss = 0.55236737\n",
            "Iteration 130, loss = 0.55227829\n",
            "Iteration 131, loss = 0.55212700\n",
            "Iteration 132, loss = 0.55216285\n",
            "Iteration 133, loss = 0.55201594\n",
            "Iteration 134, loss = 0.55203048\n",
            "Iteration 135, loss = 0.55192038\n",
            "Iteration 136, loss = 0.55194002\n",
            "Iteration 137, loss = 0.55188866\n",
            "Iteration 138, loss = 0.55194771\n",
            "Iteration 139, loss = 0.55176543\n",
            "Iteration 140, loss = 0.55170046\n",
            "Iteration 141, loss = 0.55163516\n",
            "Iteration 142, loss = 0.55161189\n",
            "Iteration 143, loss = 0.55169968\n",
            "Iteration 144, loss = 0.55153053\n",
            "Iteration 145, loss = 0.55147029\n",
            "Iteration 146, loss = 0.55144082\n",
            "Iteration 147, loss = 0.55136457\n",
            "Iteration 148, loss = 0.55126246\n",
            "Iteration 149, loss = 0.55128184\n",
            "Iteration 150, loss = 0.55115483\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 150 and for layer number 4 : 0.7074999999999999\n",
            "Iteration 1, loss = 0.88336594\n",
            "Iteration 2, loss = 0.85371585\n",
            "Iteration 3, loss = 0.81114998\n",
            "Iteration 4, loss = 0.76878568\n",
            "Iteration 5, loss = 0.72891304\n",
            "Iteration 6, loss = 0.69812822\n",
            "Iteration 7, loss = 0.67784648\n",
            "Iteration 8, loss = 0.65873975\n",
            "Iteration 9, loss = 0.64585588\n",
            "Iteration 10, loss = 0.63646662\n",
            "Iteration 11, loss = 0.62908658\n",
            "Iteration 12, loss = 0.62375837\n",
            "Iteration 13, loss = 0.61915746\n",
            "Iteration 14, loss = 0.61496245\n",
            "Iteration 15, loss = 0.61163021\n",
            "Iteration 16, loss = 0.60865001\n",
            "Iteration 17, loss = 0.60613154\n",
            "Iteration 18, loss = 0.60378173\n",
            "Iteration 19, loss = 0.60170407\n",
            "Iteration 20, loss = 0.59968740\n",
            "Iteration 21, loss = 0.59787396\n",
            "Iteration 22, loss = 0.59617398\n",
            "Iteration 23, loss = 0.59435058\n",
            "Iteration 24, loss = 0.59263873\n",
            "Iteration 25, loss = 0.59096900\n",
            "Iteration 26, loss = 0.58936046\n",
            "Iteration 27, loss = 0.58778572\n",
            "Iteration 28, loss = 0.58636900\n",
            "Iteration 29, loss = 0.58490469\n",
            "Iteration 30, loss = 0.58355581\n",
            "Iteration 31, loss = 0.58232012\n",
            "Iteration 32, loss = 0.58092064\n",
            "Iteration 33, loss = 0.57974507\n",
            "Iteration 34, loss = 0.57855058\n",
            "Iteration 35, loss = 0.57753596\n",
            "Iteration 36, loss = 0.57642234\n",
            "Iteration 37, loss = 0.57550100\n",
            "Iteration 38, loss = 0.57458003\n",
            "Iteration 39, loss = 0.57363445\n",
            "Iteration 40, loss = 0.57283919\n",
            "Iteration 41, loss = 0.57182274\n",
            "Iteration 42, loss = 0.57091657\n",
            "Iteration 43, loss = 0.57011771\n",
            "Iteration 44, loss = 0.56937148\n",
            "Iteration 45, loss = 0.56858382\n",
            "Iteration 46, loss = 0.56782347\n",
            "Iteration 47, loss = 0.56719403\n",
            "Iteration 48, loss = 0.56660501\n",
            "Iteration 49, loss = 0.56591930\n",
            "Iteration 50, loss = 0.56508853\n",
            "Iteration 51, loss = 0.56449271\n",
            "Iteration 52, loss = 0.56368499\n",
            "Iteration 53, loss = 0.56297676\n",
            "Iteration 54, loss = 0.56236424\n",
            "Iteration 55, loss = 0.56186295\n",
            "Iteration 56, loss = 0.56108268\n",
            "Iteration 57, loss = 0.56055555\n",
            "Iteration 58, loss = 0.55999247\n",
            "Iteration 59, loss = 0.55958193\n",
            "Iteration 60, loss = 0.55901068\n",
            "Iteration 61, loss = 0.55863943\n",
            "Iteration 62, loss = 0.55821083\n",
            "Iteration 63, loss = 0.55788734\n",
            "Iteration 64, loss = 0.55735403\n",
            "Iteration 65, loss = 0.55691118\n",
            "Iteration 66, loss = 0.55652812\n",
            "Iteration 67, loss = 0.55606649\n",
            "Iteration 68, loss = 0.55577844\n",
            "Iteration 69, loss = 0.55533429\n",
            "Iteration 70, loss = 0.55504655\n",
            "Iteration 71, loss = 0.55474930"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 72, loss = 0.55424172\n",
            "Iteration 73, loss = 0.55390727\n",
            "Iteration 74, loss = 0.55356165\n",
            "Iteration 75, loss = 0.55313879\n",
            "Iteration 76, loss = 0.55269980\n",
            "Iteration 77, loss = 0.55238251\n",
            "Iteration 78, loss = 0.55204490\n",
            "Iteration 79, loss = 0.55161358\n",
            "Iteration 80, loss = 0.55135349\n",
            "Iteration 81, loss = 0.55094036\n",
            "Iteration 82, loss = 0.55070037\n",
            "Iteration 83, loss = 0.55039199\n",
            "Iteration 84, loss = 0.54990062\n",
            "Iteration 85, loss = 0.54951022\n",
            "Iteration 86, loss = 0.54923000\n",
            "Iteration 87, loss = 0.54888291\n",
            "Iteration 88, loss = 0.54852080\n",
            "Iteration 89, loss = 0.54819029\n",
            "Iteration 90, loss = 0.54788580\n",
            "Iteration 91, loss = 0.54753336\n",
            "Iteration 92, loss = 0.54723242\n",
            "Iteration 93, loss = 0.54696185\n",
            "Iteration 94, loss = 0.54663047\n",
            "Iteration 95, loss = 0.54642423\n",
            "Iteration 96, loss = 0.54612106\n",
            "Iteration 97, loss = 0.54588052\n",
            "Iteration 98, loss = 0.54566276\n",
            "Iteration 99, loss = 0.54547018\n",
            "Iteration 100, loss = 0.54522125\n",
            "Iteration 101, loss = 0.54497855\n",
            "Iteration 102, loss = 0.54484424\n",
            "Iteration 103, loss = 0.54454975\n",
            "Iteration 104, loss = 0.54432694\n",
            "Iteration 105, loss = 0.54405871\n",
            "Iteration 106, loss = 0.54400529\n",
            "Iteration 107, loss = 0.54365645\n",
            "Iteration 108, loss = 0.54342444\n",
            "Iteration 109, loss = 0.54319360\n",
            "Iteration 110, loss = 0.54296398\n",
            "Iteration 111, loss = 0.54259789\n",
            "Iteration 112, loss = 0.54246263\n",
            "Iteration 113, loss = 0.54222654\n",
            "Iteration 114, loss = 0.54204101\n",
            "Iteration 115, loss = 0.54189419\n",
            "Iteration 116, loss = 0.54166685\n",
            "Iteration 117, loss = 0.54157145\n",
            "Iteration 118, loss = 0.54127755\n",
            "Iteration 119, loss = 0.54110824\n",
            "Iteration 120, loss = 0.54091335\n",
            "Iteration 121, loss = 0.54080406\n",
            "Iteration 122, loss = 0.54057483\n",
            "Iteration 123, loss = 0.54047833\n",
            "Iteration 124, loss = 0.54036178\n",
            "Iteration 125, loss = 0.54025044\n",
            "Iteration 126, loss = 0.54020928\n",
            "Iteration 127, loss = 0.53992647\n",
            "Iteration 128, loss = 0.53984166\n",
            "Iteration 129, loss = 0.53959034\n",
            "Iteration 130, loss = 0.53946540\n",
            "Iteration 131, loss = 0.53925245\n",
            "Iteration 132, loss = 0.53923818\n",
            "Iteration 133, loss = 0.53914175\n",
            "Iteration 134, loss = 0.53886620\n",
            "Iteration 135, loss = 0.53879700\n",
            "Iteration 136, loss = 0.53871831\n",
            "Iteration 137, loss = 0.53870791\n",
            "Iteration 138, loss = 0.53862767\n",
            "Iteration 139, loss = 0.53858188\n",
            "Iteration 140, loss = 0.53852717\n",
            "Iteration 141, loss = 0.53850989\n",
            "Iteration 142, loss = 0.53834784\n",
            "Iteration 143, loss = 0.53825307\n",
            "Iteration 144, loss = 0.53806887\n",
            "Iteration 145, loss = 0.53794010\n",
            "Iteration 146, loss = 0.53774559\n",
            "Iteration 147, loss = 0.53758361\n",
            "Iteration 148, loss = 0.53737071\n",
            "Iteration 149, loss = 0.53726470\n",
            "Iteration 150, loss = 0.53734451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.88215379\n",
            "Iteration 2, loss = 0.85222359\n",
            "Iteration 3, loss = 0.81083995\n",
            "Iteration 4, loss = 0.76741428\n",
            "Iteration 5, loss = 0.72754261\n",
            "Iteration 6, loss = 0.69573379\n",
            "Iteration 7, loss = 0.67445780\n",
            "Iteration 8, loss = 0.65527871\n",
            "Iteration 9, loss = 0.64178806\n",
            "Iteration 10, loss = 0.63239091\n",
            "Iteration 11, loss = 0.62427738\n",
            "Iteration 12, loss = 0.61862901\n",
            "Iteration 13, loss = 0.61370662\n",
            "Iteration 14, loss = 0.60941738\n",
            "Iteration 15, loss = 0.60603175\n",
            "Iteration 16, loss = 0.60286680\n",
            "Iteration 17, loss = 0.60003633\n",
            "Iteration 18, loss = 0.59747356\n",
            "Iteration 19, loss = 0.59516755\n",
            "Iteration 20, loss = 0.59302637\n",
            "Iteration 21, loss = 0.59102069\n",
            "Iteration 22, loss = 0.58906081\n",
            "Iteration 23, loss = 0.58713216\n",
            "Iteration 24, loss = 0.58522686\n",
            "Iteration 25, loss = 0.58343551\n",
            "Iteration 26, loss = 0.58140821\n",
            "Iteration 27, loss = 0.57962191\n",
            "Iteration 28, loss = 0.57773768\n",
            "Iteration 29, loss = 0.57599283\n",
            "Iteration 30, loss = 0.57440911\n",
            "Iteration 31, loss = 0.57292679\n",
            "Iteration 32, loss = 0.57134210\n",
            "Iteration 33, loss = 0.57005743\n",
            "Iteration 34, loss = 0.56866518\n",
            "Iteration 35, loss = 0.56746551\n",
            "Iteration 36, loss = 0.56631215\n",
            "Iteration 37, loss = 0.56506011\n",
            "Iteration 38, loss = 0.56388150\n",
            "Iteration 39, loss = 0.56266819\n",
            "Iteration 40, loss = 0.56150018\n",
            "Iteration 41, loss = 0.56027643\n",
            "Iteration 42, loss = 0.55923365\n",
            "Iteration 43, loss = 0.55839916\n",
            "Iteration 44, loss = 0.55764037\n",
            "Iteration 45, loss = 0.55671588\n",
            "Iteration 46, loss = 0.55603532\n",
            "Iteration 47, loss = 0.55539730\n",
            "Iteration 48, loss = 0.55489305\n",
            "Iteration 49, loss = 0.55430102\n",
            "Iteration 50, loss = 0.55366618\n",
            "Iteration 51, loss = 0.55308823\n",
            "Iteration 52, loss = 0.55248338\n",
            "Iteration 53, loss = 0.55180195\n",
            "Iteration 54, loss = 0.55138032\n",
            "Iteration 55, loss = 0.55064507\n",
            "Iteration 56, loss = 0.55004513\n",
            "Iteration 57, loss = 0.54956125\n",
            "Iteration 58, loss = 0.54905056\n",
            "Iteration 59, loss = 0.54869579\n",
            "Iteration 60, loss = 0.54806567\n",
            "Iteration 61, loss = 0.54766645\n",
            "Iteration 62, loss = 0.54719837\n",
            "Iteration 63, loss = 0.54672621\n",
            "Iteration 64, loss = 0.54622130\n",
            "Iteration 65, loss = 0.54567083\n",
            "Iteration 66, loss = 0.54529271\n",
            "Iteration 67, loss = 0.54475088\n",
            "Iteration 68, loss = 0.54431846\n",
            "Iteration 69, loss = 0.54386110\n",
            "Iteration 70, loss = 0.54360986\n",
            "Iteration 71, loss = 0.54328716\n",
            "Iteration 72, loss = 0.54281576\n",
            "Iteration 73, loss = 0.54245138\n",
            "Iteration 74, loss = 0.54203901\n",
            "Iteration 75, loss = 0.54159343\n",
            "Iteration 76, loss = 0.54118515\n",
            "Iteration 77, loss = 0.54086813\n",
            "Iteration 78, loss = 0.54042066\n",
            "Iteration 79, loss = 0.53996870\n",
            "Iteration 80, loss = 0.53966351\n",
            "Iteration 81, loss = 0.53922575\n",
            "Iteration 82, loss = 0.53883012\n",
            "Iteration 83, loss = 0.53845198\n",
            "Iteration 84, loss = 0.53801889\n",
            "Iteration 85, loss = 0.53775136\n",
            "Iteration 86, loss = 0.53734075\n",
            "Iteration 87, loss = 0.53694553\n",
            "Iteration 88, loss = 0.53650020\n",
            "Iteration 89, loss = 0.53610993\n",
            "Iteration 90, loss = 0.53573081\n",
            "Iteration 91, loss = 0.53537613\n",
            "Iteration 92, loss = 0.53504234\n",
            "Iteration 93, loss = 0.53466072\n",
            "Iteration 94, loss = 0.53441641\n",
            "Iteration 95, loss = 0.53430791\n",
            "Iteration 96, loss = 0.53402009\n",
            "Iteration 97, loss = 0.53367372\n",
            "Iteration 98, loss = 0.53342543\n",
            "Iteration 99, loss = 0.53312309\n",
            "Iteration 100, loss = 0.53285196\n",
            "Iteration 101, loss = 0.53257083\n",
            "Iteration 102, loss = 0.53226454\n",
            "Iteration 103, loss = 0.53199796\n",
            "Iteration 104, loss = 0.53182985\n",
            "Iteration 105, loss = 0.53158153\n",
            "Iteration 106, loss = 0.53145871\n",
            "Iteration 107, loss = 0.53126859\n",
            "Iteration 108, loss = 0.53101534\n",
            "Iteration 109, loss = 0.53072079\n",
            "Iteration 110, loss = 0.53037952\n",
            "Iteration 111, loss = 0.53005599\n",
            "Iteration 112, loss = 0.52990832\n",
            "Iteration 113, loss = 0.52973400\n",
            "Iteration 114, loss = 0.52956077\n",
            "Iteration 115, loss = 0.52935312\n",
            "Iteration 116, loss = 0.52936955\n",
            "Iteration 117, loss = 0.52935374\n",
            "Iteration 118, loss = 0.52917633\n",
            "Iteration 119, loss = 0.52910919\n",
            "Iteration 120, loss = 0.52891145\n",
            "Iteration 121, loss = 0.52874484\n",
            "Iteration 122, loss = 0.52855772\n",
            "Iteration 123, loss = 0.52843582\n",
            "Iteration 124, loss = 0.52818377\n",
            "Iteration 125, loss = 0.52808388\n",
            "Iteration 126, loss = 0.52787556\n",
            "Iteration 127, loss = 0.52750644\n",
            "Iteration 128, loss = 0.52727445\n",
            "Iteration 129, loss = 0.52683233\n",
            "Iteration 130, loss = 0.52675250\n",
            "Iteration 131, loss = 0.52643472\n",
            "Iteration 132, loss = 0.52630503\n",
            "Iteration 133, loss = 0.52617970\n",
            "Iteration 134, loss = 0.52593767\n",
            "Iteration 135, loss = 0.52584011\n",
            "Iteration 136, loss = 0.52570856\n",
            "Iteration 137, loss = 0.52566357\n",
            "Iteration 138, loss = 0.52552836\n",
            "Iteration 139, loss = 0.52532887\n",
            "Iteration 140, loss = 0.52528387\n",
            "Iteration 141, loss = 0.52526329\n",
            "Iteration 142, loss = 0.52515066\n",
            "Iteration 143, loss = 0.52518009\n",
            "Iteration 144, loss = 0.52495126\n",
            "Iteration 145, loss = 0.52484589\n",
            "Iteration 146, loss = 0.52458742\n",
            "Iteration 147, loss = 0.52429887\n",
            "Iteration 148, loss = 0.52416796\n",
            "Iteration 149, loss = 0.52398389\n",
            "Iteration 150, loss = 0.52379132\n",
            "Iteration 1, loss = 0.88505012\n",
            "Iteration 2, loss = 0.85225471\n",
            "Iteration 3, loss = 0.80692705\n",
            "Iteration 4, loss = 0.76187066\n",
            "Iteration 5, loss = 0.72125962\n",
            "Iteration 6, loss = 0.69001636\n",
            "Iteration 7, loss = 0.66839879\n",
            "Iteration 8, loss = 0.65016370\n",
            "Iteration 9, loss = 0.63775462"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 10, loss = 0.62892995\n",
            "Iteration 11, loss = 0.62138454\n",
            "Iteration 12, loss = 0.61613777\n",
            "Iteration 13, loss = 0.61152308\n",
            "Iteration 14, loss = 0.60716429\n",
            "Iteration 15, loss = 0.60391754\n",
            "Iteration 16, loss = 0.60054268\n",
            "Iteration 17, loss = 0.59793934\n",
            "Iteration 18, loss = 0.59555825\n",
            "Iteration 19, loss = 0.59346251\n",
            "Iteration 20, loss = 0.59155139\n",
            "Iteration 21, loss = 0.58951223\n",
            "Iteration 22, loss = 0.58759183\n",
            "Iteration 23, loss = 0.58570953\n",
            "Iteration 24, loss = 0.58384280\n",
            "Iteration 25, loss = 0.58210152\n",
            "Iteration 26, loss = 0.58016830\n",
            "Iteration 27, loss = 0.57837726\n",
            "Iteration 28, loss = 0.57656482\n",
            "Iteration 29, loss = 0.57466052\n",
            "Iteration 30, loss = 0.57280573\n",
            "Iteration 31, loss = 0.57095358\n",
            "Iteration 32, loss = 0.56918098\n",
            "Iteration 33, loss = 0.56766041\n",
            "Iteration 34, loss = 0.56606900\n",
            "Iteration 35, loss = 0.56460579\n",
            "Iteration 36, loss = 0.56336991\n",
            "Iteration 37, loss = 0.56205355\n",
            "Iteration 38, loss = 0.56082783\n",
            "Iteration 39, loss = 0.55947813\n",
            "Iteration 40, loss = 0.55812098\n",
            "Iteration 41, loss = 0.55681742\n",
            "Iteration 42, loss = 0.55563897\n",
            "Iteration 43, loss = 0.55440444\n",
            "Iteration 44, loss = 0.55338679\n",
            "Iteration 45, loss = 0.55235047\n",
            "Iteration 46, loss = 0.55162732\n",
            "Iteration 47, loss = 0.55084266\n",
            "Iteration 48, loss = 0.55021080\n",
            "Iteration 49, loss = 0.54960170\n",
            "Iteration 50, loss = 0.54880901\n",
            "Iteration 51, loss = 0.54801736\n",
            "Iteration 52, loss = 0.54727104\n",
            "Iteration 53, loss = 0.54650042\n",
            "Iteration 54, loss = 0.54590923\n",
            "Iteration 55, loss = 0.54515730\n",
            "Iteration 56, loss = 0.54447942\n",
            "Iteration 57, loss = 0.54393684\n",
            "Iteration 58, loss = 0.54339839\n",
            "Iteration 59, loss = 0.54306135\n",
            "Iteration 60, loss = 0.54242856\n",
            "Iteration 61, loss = 0.54192597\n",
            "Iteration 62, loss = 0.54135690\n",
            "Iteration 63, loss = 0.54087972\n",
            "Iteration 64, loss = 0.54047878\n",
            "Iteration 65, loss = 0.53981990\n",
            "Iteration 66, loss = 0.53953724\n",
            "Iteration 67, loss = 0.53893992\n",
            "Iteration 68, loss = 0.53851477\n",
            "Iteration 69, loss = 0.53807490\n",
            "Iteration 70, loss = 0.53759873\n",
            "Iteration 71, loss = 0.53721866\n",
            "Iteration 72, loss = 0.53669204\n",
            "Iteration 73, loss = 0.53617939\n",
            "Iteration 74, loss = 0.53568681\n",
            "Iteration 75, loss = 0.53513738\n",
            "Iteration 76, loss = 0.53456490\n",
            "Iteration 77, loss = 0.53422560\n",
            "Iteration 78, loss = 0.53376142\n",
            "Iteration 79, loss = 0.53334794\n",
            "Iteration 80, loss = 0.53301828\n",
            "Iteration 81, loss = 0.53262479\n",
            "Iteration 82, loss = 0.53209680\n",
            "Iteration 83, loss = 0.53182712\n",
            "Iteration 84, loss = 0.53126292\n",
            "Iteration 85, loss = 0.53094452\n",
            "Iteration 86, loss = 0.53046918\n",
            "Iteration 87, loss = 0.53009524\n",
            "Iteration 88, loss = 0.52966948\n",
            "Iteration 89, loss = 0.52938844\n",
            "Iteration 90, loss = 0.52908594\n",
            "Iteration 91, loss = 0.52882262\n",
            "Iteration 92, loss = 0.52844590\n",
            "Iteration 93, loss = 0.52818897\n",
            "Iteration 94, loss = 0.52793869\n",
            "Iteration 95, loss = 0.52779602\n",
            "Iteration 96, loss = 0.52731289\n",
            "Iteration 97, loss = 0.52709549\n",
            "Iteration 98, loss = 0.52691321\n",
            "Iteration 99, loss = 0.52654699\n",
            "Iteration 100, loss = 0.52629573\n",
            "Iteration 101, loss = 0.52601449\n",
            "Iteration 102, loss = 0.52594261\n",
            "Iteration 103, loss = 0.52562273\n",
            "Iteration 104, loss = 0.52541581\n",
            "Iteration 105, loss = 0.52523782\n",
            "Iteration 106, loss = 0.52512014\n",
            "Iteration 107, loss = 0.52499645\n",
            "Iteration 108, loss = 0.52478098\n",
            "Iteration 109, loss = 0.52445755\n",
            "Iteration 110, loss = 0.52428922\n",
            "Iteration 111, loss = 0.52412815\n",
            "Iteration 112, loss = 0.52393271\n",
            "Iteration 113, loss = 0.52375826\n",
            "Iteration 114, loss = 0.52350129\n",
            "Iteration 115, loss = 0.52330822\n",
            "Iteration 116, loss = 0.52333016\n",
            "Iteration 117, loss = 0.52320371\n",
            "Iteration 118, loss = 0.52315890\n",
            "Iteration 119, loss = 0.52315360\n",
            "Iteration 120, loss = 0.52334969\n",
            "Iteration 121, loss = 0.52307553\n",
            "Iteration 122, loss = 0.52281812\n",
            "Iteration 123, loss = 0.52249253\n",
            "Iteration 124, loss = 0.52219434\n",
            "Iteration 125, loss = 0.52201704\n",
            "Iteration 126, loss = 0.52175775\n",
            "Iteration 127, loss = 0.52149214\n",
            "Iteration 128, loss = 0.52131010\n",
            "Iteration 129, loss = 0.52120647\n",
            "Iteration 130, loss = 0.52107551\n",
            "Iteration 131, loss = 0.52088518\n",
            "Iteration 132, loss = 0.52069858\n",
            "Iteration 133, loss = 0.52066429\n",
            "Iteration 134, loss = 0.52056682\n",
            "Iteration 135, loss = 0.52044684\n",
            "Iteration 136, loss = 0.52035116\n",
            "Iteration 137, loss = 0.52039273\n",
            "Iteration 138, loss = 0.52031628\n",
            "Iteration 139, loss = 0.52016205\n",
            "Iteration 140, loss = 0.52018246\n",
            "Iteration 141, loss = 0.52019436\n",
            "Iteration 142, loss = 0.52010632\n",
            "Iteration 143, loss = 0.52026974\n",
            "Iteration 144, loss = 0.52010303\n",
            "Iteration 145, loss = 0.51996273\n",
            "Iteration 146, loss = 0.51972732\n",
            "Iteration 147, loss = 0.51938107\n",
            "Iteration 148, loss = 0.51915831\n",
            "Iteration 149, loss = 0.51880936\n",
            "Iteration 150, loss = 0.51883335\n",
            "Iteration 1, loss = 0.87868239\n",
            "Iteration 2, loss = 0.84534527\n",
            "Iteration 3, loss = 0.80264634\n",
            "Iteration 4, loss = 0.75856716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5, loss = 0.72047810\n",
            "Iteration 6, loss = 0.69020470\n",
            "Iteration 7, loss = 0.66899651\n",
            "Iteration 8, loss = 0.65325101\n",
            "Iteration 9, loss = 0.64084348\n",
            "Iteration 10, loss = 0.63256251\n",
            "Iteration 11, loss = 0.62593344\n",
            "Iteration 12, loss = 0.62085925\n",
            "Iteration 13, loss = 0.61668379\n",
            "Iteration 14, loss = 0.61298618\n",
            "Iteration 15, loss = 0.61027972\n",
            "Iteration 16, loss = 0.60738093\n",
            "Iteration 17, loss = 0.60519015\n",
            "Iteration 18, loss = 0.60314661\n",
            "Iteration 19, loss = 0.60138211\n",
            "Iteration 20, loss = 0.59990388\n",
            "Iteration 21, loss = 0.59816570\n",
            "Iteration 22, loss = 0.59667437\n",
            "Iteration 23, loss = 0.59523266\n",
            "Iteration 24, loss = 0.59383865\n",
            "Iteration 25, loss = 0.59244065\n",
            "Iteration 26, loss = 0.59109045\n",
            "Iteration 27, loss = 0.58969415\n",
            "Iteration 28, loss = 0.58839863\n",
            "Iteration 29, loss = 0.58706673\n",
            "Iteration 30, loss = 0.58587255\n",
            "Iteration 31, loss = 0.58455172\n",
            "Iteration 32, loss = 0.58331959\n",
            "Iteration 33, loss = 0.58212022\n",
            "Iteration 34, loss = 0.58094719\n",
            "Iteration 35, loss = 0.57976950\n",
            "Iteration 36, loss = 0.57873844\n",
            "Iteration 37, loss = 0.57759582\n",
            "Iteration 38, loss = 0.57646096\n",
            "Iteration 39, loss = 0.57532754\n",
            "Iteration 40, loss = 0.57432229\n",
            "Iteration 41, loss = 0.57318304\n",
            "Iteration 42, loss = 0.57217843\n",
            "Iteration 43, loss = 0.57098995\n",
            "Iteration 44, loss = 0.56998664\n",
            "Iteration 45, loss = 0.56894697\n",
            "Iteration 46, loss = 0.56822469\n",
            "Iteration 47, loss = 0.56721476\n",
            "Iteration 48, loss = 0.56646854\n",
            "Iteration 49, loss = 0.56570712\n",
            "Iteration 50, loss = 0.56480305\n",
            "Iteration 51, loss = 0.56388092\n",
            "Iteration 52, loss = 0.56300559\n",
            "Iteration 53, loss = 0.56215389\n",
            "Iteration 54, loss = 0.56146918\n",
            "Iteration 55, loss = 0.56066012\n",
            "Iteration 56, loss = 0.56010518\n",
            "Iteration 57, loss = 0.55936017\n",
            "Iteration 58, loss = 0.55867960\n",
            "Iteration 59, loss = 0.55806008\n",
            "Iteration 60, loss = 0.55745688\n",
            "Iteration 61, loss = 0.55683435\n",
            "Iteration 62, loss = 0.55617745\n",
            "Iteration 63, loss = 0.55567840\n",
            "Iteration 64, loss = 0.55522448\n",
            "Iteration 65, loss = 0.55451282\n",
            "Iteration 66, loss = 0.55408871\n",
            "Iteration 67, loss = 0.55340767\n",
            "Iteration 68, loss = 0.55308825\n",
            "Iteration 69, loss = 0.55248680\n",
            "Iteration 70, loss = 0.55197835\n",
            "Iteration 71, loss = 0.55151704\n",
            "Iteration 72, loss = 0.55094640\n",
            "Iteration 73, loss = 0.55043214\n",
            "Iteration 74, loss = 0.54999353\n",
            "Iteration 75, loss = 0.54958755\n",
            "Iteration 76, loss = 0.54921556\n",
            "Iteration 77, loss = 0.54881768\n",
            "Iteration 78, loss = 0.54841853\n",
            "Iteration 79, loss = 0.54806308\n",
            "Iteration 80, loss = 0.54755768\n",
            "Iteration 81, loss = 0.54715637\n",
            "Iteration 82, loss = 0.54664993\n",
            "Iteration 83, loss = 0.54633178\n",
            "Iteration 84, loss = 0.54586308\n",
            "Iteration 85, loss = 0.54531629\n",
            "Iteration 86, loss = 0.54479784\n",
            "Iteration 87, loss = 0.54431345\n",
            "Iteration 88, loss = 0.54395825\n",
            "Iteration 89, loss = 0.54347640\n",
            "Iteration 90, loss = 0.54309495\n",
            "Iteration 91, loss = 0.54277762\n",
            "Iteration 92, loss = 0.54228906\n",
            "Iteration 93, loss = 0.54182477\n",
            "Iteration 94, loss = 0.54136635\n",
            "Iteration 95, loss = 0.54103392\n",
            "Iteration 96, loss = 0.54055500\n",
            "Iteration 97, loss = 0.54020524\n",
            "Iteration 98, loss = 0.53990612\n",
            "Iteration 99, loss = 0.53946956\n",
            "Iteration 100, loss = 0.53922342\n",
            "Iteration 101, loss = 0.53886199\n",
            "Iteration 102, loss = 0.53863403\n",
            "Iteration 103, loss = 0.53835363\n",
            "Iteration 104, loss = 0.53807245\n",
            "Iteration 105, loss = 0.53787585\n",
            "Iteration 106, loss = 0.53772048\n",
            "Iteration 107, loss = 0.53739586\n",
            "Iteration 108, loss = 0.53703830\n",
            "Iteration 109, loss = 0.53674683\n",
            "Iteration 110, loss = 0.53654103\n",
            "Iteration 111, loss = 0.53635130\n",
            "Iteration 112, loss = 0.53605854\n",
            "Iteration 113, loss = 0.53586204\n",
            "Iteration 114, loss = 0.53548591\n",
            "Iteration 115, loss = 0.53527808\n",
            "Iteration 116, loss = 0.53506597\n",
            "Iteration 117, loss = 0.53496093\n",
            "Iteration 118, loss = 0.53466493\n",
            "Iteration 119, loss = 0.53449262\n",
            "Iteration 120, loss = 0.53439704\n",
            "Iteration 121, loss = 0.53412809\n",
            "Iteration 122, loss = 0.53385286\n",
            "Iteration 123, loss = 0.53358210\n",
            "Iteration 124, loss = 0.53332720\n",
            "Iteration 125, loss = 0.53317083\n",
            "Iteration 126, loss = 0.53286274\n",
            "Iteration 127, loss = 0.53277989\n",
            "Iteration 128, loss = 0.53248450\n",
            "Iteration 129, loss = 0.53231624\n",
            "Iteration 130, loss = 0.53223747\n",
            "Iteration 131, loss = 0.53207475\n",
            "Iteration 132, loss = 0.53183436\n",
            "Iteration 133, loss = 0.53166183\n",
            "Iteration 134, loss = 0.53146902\n",
            "Iteration 135, loss = 0.53131621\n",
            "Iteration 136, loss = 0.53113035\n",
            "Iteration 137, loss = 0.53099015\n",
            "Iteration 138, loss = 0.53085964\n",
            "Iteration 139, loss = 0.53063896\n",
            "Iteration 140, loss = 0.53049519\n",
            "Iteration 141, loss = 0.53028788\n",
            "Iteration 142, loss = 0.53009605\n",
            "Iteration 143, loss = 0.53001063\n",
            "Iteration 144, loss = 0.52968939\n",
            "Iteration 145, loss = 0.52955078\n",
            "Iteration 146, loss = 0.52932441\n",
            "Iteration 147, loss = 0.52903402\n",
            "Iteration 148, loss = 0.52892468\n",
            "Iteration 149, loss = 0.52868529\n",
            "Iteration 150, loss = 0.52853744\n",
            "Iteration 1, loss = 0.88420876\n",
            "Iteration 2, loss = 0.85029778\n",
            "Iteration 3, loss = 0.80666346\n",
            "Iteration 4, loss = 0.76251740\n",
            "Iteration 5, loss = 0.72239930\n",
            "Iteration 6, loss = 0.69102003\n",
            "Iteration 7, loss = 0.66914065\n",
            "Iteration 8, loss = 0.65366285"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 9, loss = 0.64129681\n",
            "Iteration 10, loss = 0.63364634\n",
            "Iteration 11, loss = 0.62684979\n",
            "Iteration 12, loss = 0.62195144\n",
            "Iteration 13, loss = 0.61783002\n",
            "Iteration 14, loss = 0.61387722\n",
            "Iteration 15, loss = 0.61079756\n",
            "Iteration 16, loss = 0.60786781\n",
            "Iteration 17, loss = 0.60543311\n",
            "Iteration 18, loss = 0.60300274\n",
            "Iteration 19, loss = 0.60112791\n",
            "Iteration 20, loss = 0.59925533\n",
            "Iteration 21, loss = 0.59731542\n",
            "Iteration 22, loss = 0.59563197\n",
            "Iteration 23, loss = 0.59403794\n",
            "Iteration 24, loss = 0.59252695\n",
            "Iteration 25, loss = 0.59087718\n",
            "Iteration 26, loss = 0.58932431\n",
            "Iteration 27, loss = 0.58783735\n",
            "Iteration 28, loss = 0.58652909\n",
            "Iteration 29, loss = 0.58526168\n",
            "Iteration 30, loss = 0.58384581\n",
            "Iteration 31, loss = 0.58249022\n",
            "Iteration 32, loss = 0.58107302\n",
            "Iteration 33, loss = 0.57968523\n",
            "Iteration 34, loss = 0.57826974\n",
            "Iteration 35, loss = 0.57685409\n",
            "Iteration 36, loss = 0.57570653\n",
            "Iteration 37, loss = 0.57454359\n",
            "Iteration 38, loss = 0.57337072\n",
            "Iteration 39, loss = 0.57236750\n",
            "Iteration 40, loss = 0.57151315\n",
            "Iteration 41, loss = 0.57056749\n",
            "Iteration 42, loss = 0.56961456\n",
            "Iteration 43, loss = 0.56866093\n",
            "Iteration 44, loss = 0.56779404\n",
            "Iteration 45, loss = 0.56702039\n",
            "Iteration 46, loss = 0.56636969\n",
            "Iteration 47, loss = 0.56567263\n",
            "Iteration 48, loss = 0.56506709\n",
            "Iteration 49, loss = 0.56456720\n",
            "Iteration 50, loss = 0.56395569\n",
            "Iteration 51, loss = 0.56323524\n",
            "Iteration 52, loss = 0.56254141\n",
            "Iteration 53, loss = 0.56184053\n",
            "Iteration 54, loss = 0.56130169\n",
            "Iteration 55, loss = 0.56068746\n",
            "Iteration 56, loss = 0.56027446\n",
            "Iteration 57, loss = 0.55964821\n",
            "Iteration 58, loss = 0.55911947\n",
            "Iteration 59, loss = 0.55863265\n",
            "Iteration 60, loss = 0.55817650\n",
            "Iteration 61, loss = 0.55778245\n",
            "Iteration 62, loss = 0.55727190\n",
            "Iteration 63, loss = 0.55689895\n",
            "Iteration 64, loss = 0.55664518\n",
            "Iteration 65, loss = 0.55617918\n",
            "Iteration 66, loss = 0.55581549\n",
            "Iteration 67, loss = 0.55536771\n",
            "Iteration 68, loss = 0.55518132\n",
            "Iteration 69, loss = 0.55482123\n",
            "Iteration 70, loss = 0.55467827\n",
            "Iteration 71, loss = 0.55436780\n",
            "Iteration 72, loss = 0.55404453\n",
            "Iteration 73, loss = 0.55373464\n",
            "Iteration 74, loss = 0.55344119\n",
            "Iteration 75, loss = 0.55320977\n",
            "Iteration 76, loss = 0.55299683\n",
            "Iteration 77, loss = 0.55278411\n",
            "Iteration 78, loss = 0.55251193\n",
            "Iteration 79, loss = 0.55228212\n",
            "Iteration 80, loss = 0.55204654\n",
            "Iteration 81, loss = 0.55182542\n",
            "Iteration 82, loss = 0.55150055\n",
            "Iteration 83, loss = 0.55129268\n",
            "Iteration 84, loss = 0.55103820\n",
            "Iteration 85, loss = 0.55075821\n",
            "Iteration 86, loss = 0.55045633\n",
            "Iteration 87, loss = 0.55023268\n",
            "Iteration 88, loss = 0.55010605\n",
            "Iteration 89, loss = 0.54982742\n",
            "Iteration 90, loss = 0.54971483\n",
            "Iteration 91, loss = 0.54972757\n",
            "Iteration 92, loss = 0.54939568\n",
            "Iteration 93, loss = 0.54898982\n",
            "Iteration 94, loss = 0.54881902\n",
            "Iteration 95, loss = 0.54858255\n",
            "Iteration 96, loss = 0.54831362\n",
            "Iteration 97, loss = 0.54811557\n",
            "Iteration 98, loss = 0.54799809\n",
            "Iteration 99, loss = 0.54778299\n",
            "Iteration 100, loss = 0.54765785\n",
            "Iteration 101, loss = 0.54740884\n",
            "Iteration 102, loss = 0.54725663\n",
            "Iteration 103, loss = 0.54706772\n",
            "Iteration 104, loss = 0.54692220\n",
            "Iteration 105, loss = 0.54679752\n",
            "Iteration 106, loss = 0.54662427\n",
            "Iteration 107, loss = 0.54639225\n",
            "Iteration 108, loss = 0.54621099\n",
            "Iteration 109, loss = 0.54610797\n",
            "Iteration 110, loss = 0.54593550\n",
            "Iteration 111, loss = 0.54586041\n",
            "Iteration 112, loss = 0.54564492\n",
            "Iteration 113, loss = 0.54554842\n",
            "Iteration 114, loss = 0.54546008\n",
            "Iteration 115, loss = 0.54524899\n",
            "Iteration 116, loss = 0.54515192\n",
            "Iteration 117, loss = 0.54505288\n",
            "Iteration 118, loss = 0.54479587\n",
            "Iteration 119, loss = 0.54463250\n",
            "Iteration 120, loss = 0.54452734\n",
            "Iteration 121, loss = 0.54425207\n",
            "Iteration 122, loss = 0.54408486\n",
            "Iteration 123, loss = 0.54394875\n",
            "Iteration 124, loss = 0.54371186\n",
            "Iteration 125, loss = 0.54363433\n",
            "Iteration 126, loss = 0.54331513\n",
            "Iteration 127, loss = 0.54320696\n",
            "Iteration 128, loss = 0.54298310\n",
            "Iteration 129, loss = 0.54284629\n",
            "Iteration 130, loss = 0.54268682\n",
            "Iteration 131, loss = 0.54248868\n",
            "Iteration 132, loss = 0.54230650\n",
            "Iteration 133, loss = 0.54214827\n",
            "Iteration 134, loss = 0.54197261\n",
            "Iteration 135, loss = 0.54181534\n",
            "Iteration 136, loss = 0.54167607\n",
            "Iteration 137, loss = 0.54167254\n",
            "Iteration 138, loss = 0.54154016\n",
            "Iteration 139, loss = 0.54135835\n",
            "Iteration 140, loss = 0.54119593\n",
            "Iteration 141, loss = 0.54097055\n",
            "Iteration 142, loss = 0.54079159\n",
            "Iteration 143, loss = 0.54062195\n",
            "Iteration 144, loss = 0.54035561\n",
            "Iteration 145, loss = 0.54022586\n",
            "Iteration 146, loss = 0.54003420\n",
            "Iteration 147, loss = 0.53985661\n",
            "Iteration 148, loss = 0.53970262\n",
            "Iteration 149, loss = 0.53956974\n",
            "Iteration 150, loss = 0.53951964\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 150 and for layer number 5 : 0.70625\n",
            "Iteration 1, loss = 1.36577264\n",
            "Iteration 2, loss = 1.21631564\n",
            "Iteration 3, loss = 1.02778964\n",
            "Iteration 4, loss = 0.86161348\n",
            "Iteration 5, loss = 0.75517608\n",
            "Iteration 6, loss = 0.69677016\n",
            "Iteration 7, loss = 0.67054166\n",
            "Iteration 8, loss = 0.65696349\n",
            "Iteration 9, loss = 0.64924227"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 10, loss = 0.64200040\n",
            "Iteration 11, loss = 0.63502000\n",
            "Iteration 12, loss = 0.62804496\n",
            "Iteration 13, loss = 0.62121423\n",
            "Iteration 14, loss = 0.61544081\n",
            "Iteration 15, loss = 0.60966913\n",
            "Iteration 16, loss = 0.60458321\n",
            "Iteration 17, loss = 0.60053459\n",
            "Iteration 18, loss = 0.59669563\n",
            "Iteration 19, loss = 0.59365093\n",
            "Iteration 20, loss = 0.59065223\n",
            "Iteration 21, loss = 0.58804416\n",
            "Iteration 22, loss = 0.58582895\n",
            "Iteration 23, loss = 0.58349854\n",
            "Iteration 24, loss = 0.58165816\n",
            "Iteration 25, loss = 0.57966754\n",
            "Iteration 26, loss = 0.57764872\n",
            "Iteration 27, loss = 0.57592592\n",
            "Iteration 28, loss = 0.57428512\n",
            "Iteration 29, loss = 0.57254934\n",
            "Iteration 30, loss = 0.57118603\n",
            "Iteration 31, loss = 0.56960122\n",
            "Iteration 32, loss = 0.56838182\n",
            "Iteration 33, loss = 0.56685777\n",
            "Iteration 34, loss = 0.56551142\n",
            "Iteration 35, loss = 0.56440679\n",
            "Iteration 36, loss = 0.56326099\n",
            "Iteration 37, loss = 0.56204087\n",
            "Iteration 38, loss = 0.56109031\n",
            "Iteration 39, loss = 0.55986212\n",
            "Iteration 40, loss = 0.55891318\n",
            "Iteration 41, loss = 0.55804948\n",
            "Iteration 42, loss = 0.55708778\n",
            "Iteration 43, loss = 0.55612363\n",
            "Iteration 44, loss = 0.55520370\n",
            "Iteration 45, loss = 0.55433500\n",
            "Iteration 46, loss = 0.55324730\n",
            "Iteration 47, loss = 0.55239553\n",
            "Iteration 48, loss = 0.55173779\n",
            "Iteration 49, loss = 0.55101393\n",
            "Iteration 50, loss = 0.55038937\n",
            "Iteration 51, loss = 0.54971687\n",
            "Iteration 52, loss = 0.54916416\n",
            "Iteration 53, loss = 0.54852380\n",
            "Iteration 54, loss = 0.54794764\n",
            "Iteration 55, loss = 0.54748227\n",
            "Iteration 56, loss = 0.54684467\n",
            "Iteration 57, loss = 0.54634707\n",
            "Iteration 58, loss = 0.54580420\n",
            "Iteration 59, loss = 0.54519260\n",
            "Iteration 60, loss = 0.54468122\n",
            "Iteration 61, loss = 0.54419114\n",
            "Iteration 62, loss = 0.54359535\n",
            "Iteration 63, loss = 0.54298356\n",
            "Iteration 64, loss = 0.54246837\n",
            "Iteration 65, loss = 0.54181965\n",
            "Iteration 66, loss = 0.54124425\n",
            "Iteration 67, loss = 0.54066048\n",
            "Iteration 68, loss = 0.54002205\n",
            "Iteration 69, loss = 0.53948822\n",
            "Iteration 70, loss = 0.53892974\n",
            "Iteration 71, loss = 0.53834934\n",
            "Iteration 72, loss = 0.53786761\n",
            "Iteration 73, loss = 0.53739308\n",
            "Iteration 74, loss = 0.53716736\n",
            "Iteration 75, loss = 0.53641757\n",
            "Iteration 76, loss = 0.53606271\n",
            "Iteration 77, loss = 0.53557915\n",
            "Iteration 78, loss = 0.53511829\n",
            "Iteration 79, loss = 0.53464439\n",
            "Iteration 80, loss = 0.53433061\n",
            "Iteration 81, loss = 0.53404424\n",
            "Iteration 82, loss = 0.53369755\n",
            "Iteration 83, loss = 0.53324430\n",
            "Iteration 84, loss = 0.53294977\n",
            "Iteration 85, loss = 0.53250768\n",
            "Iteration 86, loss = 0.53216655\n",
            "Iteration 87, loss = 0.53175103\n",
            "Iteration 88, loss = 0.53142669\n",
            "Iteration 89, loss = 0.53126093\n",
            "Iteration 90, loss = 0.53081181\n",
            "Iteration 91, loss = 0.53051382\n",
            "Iteration 92, loss = 0.53014209\n",
            "Iteration 93, loss = 0.52983351\n",
            "Iteration 94, loss = 0.52955779\n",
            "Iteration 95, loss = 0.52924251\n",
            "Iteration 96, loss = 0.52894063\n",
            "Iteration 97, loss = 0.52859632\n",
            "Iteration 98, loss = 0.52836618\n",
            "Iteration 99, loss = 0.52816789\n",
            "Iteration 100, loss = 0.52777562\n",
            "Iteration 101, loss = 0.52736082\n",
            "Iteration 102, loss = 0.52689572\n",
            "Iteration 103, loss = 0.52655346\n",
            "Iteration 104, loss = 0.52632362\n",
            "Iteration 105, loss = 0.52602093\n",
            "Iteration 106, loss = 0.52565738\n",
            "Iteration 107, loss = 0.52529612\n",
            "Iteration 108, loss = 0.52513463\n",
            "Iteration 109, loss = 0.52454681\n",
            "Iteration 110, loss = 0.52419489\n",
            "Iteration 111, loss = 0.52393741\n",
            "Iteration 112, loss = 0.52364714\n",
            "Iteration 113, loss = 0.52336835\n",
            "Iteration 114, loss = 0.52292347\n",
            "Iteration 115, loss = 0.52261625\n",
            "Iteration 116, loss = 0.52216289\n",
            "Iteration 117, loss = 0.52191115\n",
            "Iteration 118, loss = 0.52167411\n",
            "Iteration 119, loss = 0.52129701\n",
            "Iteration 120, loss = 0.52086030\n",
            "Iteration 121, loss = 0.52065410\n",
            "Iteration 122, loss = 0.52036663\n",
            "Iteration 123, loss = 0.52007206\n",
            "Iteration 124, loss = 0.51971518\n",
            "Iteration 125, loss = 0.51938290\n",
            "Iteration 126, loss = 0.51906406\n",
            "Iteration 127, loss = 0.51878832\n",
            "Iteration 128, loss = 0.51843197\n",
            "Iteration 129, loss = 0.51804059\n",
            "Iteration 130, loss = 0.51767480\n",
            "Iteration 131, loss = 0.51750205\n",
            "Iteration 132, loss = 0.51704063\n",
            "Iteration 133, loss = 0.51670742\n",
            "Iteration 134, loss = 0.51652978\n",
            "Iteration 135, loss = 0.51621894\n",
            "Iteration 136, loss = 0.51597440\n",
            "Iteration 137, loss = 0.51563672\n",
            "Iteration 138, loss = 0.51525275\n",
            "Iteration 139, loss = 0.51503894\n",
            "Iteration 140, loss = 0.51466413\n",
            "Iteration 141, loss = 0.51433617\n",
            "Iteration 142, loss = 0.51414569\n",
            "Iteration 143, loss = 0.51380601\n",
            "Iteration 144, loss = 0.51343096\n",
            "Iteration 145, loss = 0.51312277\n",
            "Iteration 146, loss = 0.51284898\n",
            "Iteration 147, loss = 0.51248492\n",
            "Iteration 148, loss = 0.51225234\n",
            "Iteration 149, loss = 0.51185209\n",
            "Iteration 150, loss = 0.51170754\n",
            "Iteration 1, loss = 1.35514429\n",
            "Iteration 2, loss = 1.20785553\n",
            "Iteration 3, loss = 1.02088892\n",
            "Iteration 4, loss = 0.85738136\n",
            "Iteration 5, loss = 0.75279440\n",
            "Iteration 6, loss = 0.69511781\n",
            "Iteration 7, loss = 0.66830518\n",
            "Iteration 8, loss = 0.65594188\n",
            "Iteration 9, loss = 0.64756070\n",
            "Iteration 10, loss = 0.63958029"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 11, loss = 0.63226591\n",
            "Iteration 12, loss = 0.62537086\n",
            "Iteration 13, loss = 0.61859528\n",
            "Iteration 14, loss = 0.61262202\n",
            "Iteration 15, loss = 0.60683866\n",
            "Iteration 16, loss = 0.60192057\n",
            "Iteration 17, loss = 0.59769403\n",
            "Iteration 18, loss = 0.59407468\n",
            "Iteration 19, loss = 0.59087969\n",
            "Iteration 20, loss = 0.58787421\n",
            "Iteration 21, loss = 0.58490220\n",
            "Iteration 22, loss = 0.58260316\n",
            "Iteration 23, loss = 0.57994422\n",
            "Iteration 24, loss = 0.57770562\n",
            "Iteration 25, loss = 0.57568745\n",
            "Iteration 26, loss = 0.57360864\n",
            "Iteration 27, loss = 0.57185682\n",
            "Iteration 28, loss = 0.57009674\n",
            "Iteration 29, loss = 0.56836102\n",
            "Iteration 30, loss = 0.56690076\n",
            "Iteration 31, loss = 0.56525335\n",
            "Iteration 32, loss = 0.56398245\n",
            "Iteration 33, loss = 0.56232638\n",
            "Iteration 34, loss = 0.56101582\n",
            "Iteration 35, loss = 0.55982524\n",
            "Iteration 36, loss = 0.55856738\n",
            "Iteration 37, loss = 0.55735555\n",
            "Iteration 38, loss = 0.55627713\n",
            "Iteration 39, loss = 0.55510543\n",
            "Iteration 40, loss = 0.55407973\n",
            "Iteration 41, loss = 0.55330870\n",
            "Iteration 42, loss = 0.55233209\n",
            "Iteration 43, loss = 0.55132070\n",
            "Iteration 44, loss = 0.55036474\n",
            "Iteration 45, loss = 0.54944197\n",
            "Iteration 46, loss = 0.54837056\n",
            "Iteration 47, loss = 0.54738358\n",
            "Iteration 48, loss = 0.54652592\n",
            "Iteration 49, loss = 0.54571200\n",
            "Iteration 50, loss = 0.54485943\n",
            "Iteration 51, loss = 0.54400365\n",
            "Iteration 52, loss = 0.54341597\n",
            "Iteration 53, loss = 0.54259256\n",
            "Iteration 54, loss = 0.54195325\n",
            "Iteration 55, loss = 0.54135470\n",
            "Iteration 56, loss = 0.54070979\n",
            "Iteration 57, loss = 0.54016796\n",
            "Iteration 58, loss = 0.53969689\n",
            "Iteration 59, loss = 0.53917235\n",
            "Iteration 60, loss = 0.53869064\n",
            "Iteration 61, loss = 0.53838360\n",
            "Iteration 62, loss = 0.53768739\n",
            "Iteration 63, loss = 0.53703995\n",
            "Iteration 64, loss = 0.53649471\n",
            "Iteration 65, loss = 0.53589237\n",
            "Iteration 66, loss = 0.53544138\n",
            "Iteration 67, loss = 0.53490436\n",
            "Iteration 68, loss = 0.53439025\n",
            "Iteration 69, loss = 0.53396900\n",
            "Iteration 70, loss = 0.53351046\n",
            "Iteration 71, loss = 0.53296450\n",
            "Iteration 72, loss = 0.53245616\n",
            "Iteration 73, loss = 0.53198883\n",
            "Iteration 74, loss = 0.53171321\n",
            "Iteration 75, loss = 0.53101324\n",
            "Iteration 76, loss = 0.53067590\n",
            "Iteration 77, loss = 0.53026639\n",
            "Iteration 78, loss = 0.52987296\n",
            "Iteration 79, loss = 0.52957369\n",
            "Iteration 80, loss = 0.52923171\n",
            "Iteration 81, loss = 0.52877918\n",
            "Iteration 82, loss = 0.52845922\n",
            "Iteration 83, loss = 0.52806888\n",
            "Iteration 84, loss = 0.52770694\n",
            "Iteration 85, loss = 0.52724198\n",
            "Iteration 86, loss = 0.52678667\n",
            "Iteration 87, loss = 0.52638671\n",
            "Iteration 88, loss = 0.52591062\n",
            "Iteration 89, loss = 0.52570284\n",
            "Iteration 90, loss = 0.52524291\n",
            "Iteration 91, loss = 0.52484179\n",
            "Iteration 92, loss = 0.52438492\n",
            "Iteration 93, loss = 0.52405214\n",
            "Iteration 94, loss = 0.52381102\n",
            "Iteration 95, loss = 0.52339081\n",
            "Iteration 96, loss = 0.52297938\n",
            "Iteration 97, loss = 0.52261205\n",
            "Iteration 98, loss = 0.52217822\n",
            "Iteration 99, loss = 0.52179697\n",
            "Iteration 100, loss = 0.52129746\n",
            "Iteration 101, loss = 0.52101927\n",
            "Iteration 102, loss = 0.52061005\n",
            "Iteration 103, loss = 0.52034454\n",
            "Iteration 104, loss = 0.52015692\n",
            "Iteration 105, loss = 0.51979616\n",
            "Iteration 106, loss = 0.51940073\n",
            "Iteration 107, loss = 0.51912936\n",
            "Iteration 108, loss = 0.51893490\n",
            "Iteration 109, loss = 0.51845004\n",
            "Iteration 110, loss = 0.51820597\n",
            "Iteration 111, loss = 0.51794268\n",
            "Iteration 112, loss = 0.51767564\n",
            "Iteration 113, loss = 0.51748003\n",
            "Iteration 114, loss = 0.51712466\n",
            "Iteration 115, loss = 0.51692308\n",
            "Iteration 116, loss = 0.51660287\n",
            "Iteration 117, loss = 0.51631498\n",
            "Iteration 118, loss = 0.51610752\n",
            "Iteration 119, loss = 0.51573181\n",
            "Iteration 120, loss = 0.51538704\n",
            "Iteration 121, loss = 0.51495356\n",
            "Iteration 122, loss = 0.51486355\n",
            "Iteration 123, loss = 0.51441849\n",
            "Iteration 124, loss = 0.51415901\n",
            "Iteration 125, loss = 0.51390740\n",
            "Iteration 126, loss = 0.51359845\n",
            "Iteration 127, loss = 0.51340911\n",
            "Iteration 128, loss = 0.51297929\n",
            "Iteration 129, loss = 0.51269676\n",
            "Iteration 130, loss = 0.51232668\n",
            "Iteration 131, loss = 0.51219657\n",
            "Iteration 132, loss = 0.51169586\n",
            "Iteration 133, loss = 0.51136207\n",
            "Iteration 134, loss = 0.51109099\n",
            "Iteration 135, loss = 0.51087480\n",
            "Iteration 136, loss = 0.51056979\n",
            "Iteration 137, loss = 0.51032456\n",
            "Iteration 138, loss = 0.51005568\n",
            "Iteration 139, loss = 0.50985310\n",
            "Iteration 140, loss = 0.50955879\n",
            "Iteration 141, loss = 0.50929265\n",
            "Iteration 142, loss = 0.50926849\n",
            "Iteration 143, loss = 0.50876538\n",
            "Iteration 144, loss = 0.50839873\n",
            "Iteration 145, loss = 0.50811519\n",
            "Iteration 146, loss = 0.50771953\n",
            "Iteration 147, loss = 0.50735215\n",
            "Iteration 148, loss = 0.50708792\n",
            "Iteration 149, loss = 0.50679498\n",
            "Iteration 150, loss = 0.50649485\n",
            "Iteration 1, loss = 1.35569250\n",
            "Iteration 2, loss = 1.20977855"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 3, loss = 1.02421468\n",
            "Iteration 4, loss = 0.86114766\n",
            "Iteration 5, loss = 0.75352175\n",
            "Iteration 6, loss = 0.69589482\n",
            "Iteration 7, loss = 0.66854088\n",
            "Iteration 8, loss = 0.65502259\n",
            "Iteration 9, loss = 0.64663631\n",
            "Iteration 10, loss = 0.63885391\n",
            "Iteration 11, loss = 0.63126598\n",
            "Iteration 12, loss = 0.62391326\n",
            "Iteration 13, loss = 0.61710692\n",
            "Iteration 14, loss = 0.61051424\n",
            "Iteration 15, loss = 0.60440264\n",
            "Iteration 16, loss = 0.59904249\n",
            "Iteration 17, loss = 0.59421694\n",
            "Iteration 18, loss = 0.58983593\n",
            "Iteration 19, loss = 0.58606930\n",
            "Iteration 20, loss = 0.58266201\n",
            "Iteration 21, loss = 0.57939851\n",
            "Iteration 22, loss = 0.57651751\n",
            "Iteration 23, loss = 0.57331381\n",
            "Iteration 24, loss = 0.57061743\n",
            "Iteration 25, loss = 0.56816832\n",
            "Iteration 26, loss = 0.56558906\n",
            "Iteration 27, loss = 0.56354880\n",
            "Iteration 28, loss = 0.56127377\n",
            "Iteration 29, loss = 0.55916195\n",
            "Iteration 30, loss = 0.55726156\n",
            "Iteration 31, loss = 0.55534880\n",
            "Iteration 32, loss = 0.55363341\n",
            "Iteration 33, loss = 0.55176312\n",
            "Iteration 34, loss = 0.55028963\n",
            "Iteration 35, loss = 0.54884212\n",
            "Iteration 36, loss = 0.54733604\n",
            "Iteration 37, loss = 0.54583440\n",
            "Iteration 38, loss = 0.54439355\n",
            "Iteration 39, loss = 0.54300394\n",
            "Iteration 40, loss = 0.54174970\n",
            "Iteration 41, loss = 0.54070347\n",
            "Iteration 42, loss = 0.53965873\n",
            "Iteration 43, loss = 0.53854609\n",
            "Iteration 44, loss = 0.53749954\n",
            "Iteration 45, loss = 0.53666066\n",
            "Iteration 46, loss = 0.53548315\n",
            "Iteration 47, loss = 0.53456246\n",
            "Iteration 48, loss = 0.53368647\n",
            "Iteration 49, loss = 0.53288963\n",
            "Iteration 50, loss = 0.53210325\n",
            "Iteration 51, loss = 0.53127099\n",
            "Iteration 52, loss = 0.53064426\n",
            "Iteration 53, loss = 0.52966743\n",
            "Iteration 54, loss = 0.52903463\n",
            "Iteration 55, loss = 0.52829084\n",
            "Iteration 56, loss = 0.52752085\n",
            "Iteration 57, loss = 0.52692793\n",
            "Iteration 58, loss = 0.52620528\n",
            "Iteration 59, loss = 0.52564618\n",
            "Iteration 60, loss = 0.52501302\n",
            "Iteration 61, loss = 0.52461704\n",
            "Iteration 62, loss = 0.52397652\n",
            "Iteration 63, loss = 0.52328006\n",
            "Iteration 64, loss = 0.52274315\n",
            "Iteration 65, loss = 0.52209173\n",
            "Iteration 66, loss = 0.52161141\n",
            "Iteration 67, loss = 0.52102971\n",
            "Iteration 68, loss = 0.52051457\n",
            "Iteration 69, loss = 0.51994342\n",
            "Iteration 70, loss = 0.51949139\n",
            "Iteration 71, loss = 0.51894718\n",
            "Iteration 72, loss = 0.51834461\n",
            "Iteration 73, loss = 0.51783013\n",
            "Iteration 74, loss = 0.51731223\n",
            "Iteration 75, loss = 0.51688740\n",
            "Iteration 76, loss = 0.51646968\n",
            "Iteration 77, loss = 0.51623903\n",
            "Iteration 78, loss = 0.51580940\n",
            "Iteration 79, loss = 0.51535095\n",
            "Iteration 80, loss = 0.51513440\n",
            "Iteration 81, loss = 0.51460954\n",
            "Iteration 82, loss = 0.51437374\n",
            "Iteration 83, loss = 0.51399374\n",
            "Iteration 84, loss = 0.51366061\n",
            "Iteration 85, loss = 0.51329859\n",
            "Iteration 86, loss = 0.51284274\n",
            "Iteration 87, loss = 0.51240931\n",
            "Iteration 88, loss = 0.51200998\n",
            "Iteration 89, loss = 0.51163393\n",
            "Iteration 90, loss = 0.51123178\n",
            "Iteration 91, loss = 0.51078401\n",
            "Iteration 92, loss = 0.51037211\n",
            "Iteration 93, loss = 0.51000532\n",
            "Iteration 94, loss = 0.50948955\n",
            "Iteration 95, loss = 0.50923813\n",
            "Iteration 96, loss = 0.50879173\n",
            "Iteration 97, loss = 0.50827459\n",
            "Iteration 98, loss = 0.50788042\n",
            "Iteration 99, loss = 0.50765154\n",
            "Iteration 100, loss = 0.50721205\n",
            "Iteration 101, loss = 0.50691786\n",
            "Iteration 102, loss = 0.50651989\n",
            "Iteration 103, loss = 0.50615824\n",
            "Iteration 104, loss = 0.50586519\n",
            "Iteration 105, loss = 0.50556665\n",
            "Iteration 106, loss = 0.50509417\n",
            "Iteration 107, loss = 0.50485705\n",
            "Iteration 108, loss = 0.50463847\n",
            "Iteration 109, loss = 0.50429215\n",
            "Iteration 110, loss = 0.50390829\n",
            "Iteration 111, loss = 0.50366847\n",
            "Iteration 112, loss = 0.50339423\n",
            "Iteration 113, loss = 0.50308668\n",
            "Iteration 114, loss = 0.50277058\n",
            "Iteration 115, loss = 0.50258803\n",
            "Iteration 116, loss = 0.50232659\n",
            "Iteration 117, loss = 0.50196972\n",
            "Iteration 118, loss = 0.50175203\n",
            "Iteration 119, loss = 0.50146362\n",
            "Iteration 120, loss = 0.50133419\n",
            "Iteration 121, loss = 0.50107893\n",
            "Iteration 122, loss = 0.50094707\n",
            "Iteration 123, loss = 0.50063183\n",
            "Iteration 124, loss = 0.50039005\n",
            "Iteration 125, loss = 0.50015109\n",
            "Iteration 126, loss = 0.50000054\n",
            "Iteration 127, loss = 0.49987877\n",
            "Iteration 128, loss = 0.49961840\n",
            "Iteration 129, loss = 0.49932302\n",
            "Iteration 130, loss = 0.49909672\n",
            "Iteration 131, loss = 0.49906614\n",
            "Iteration 132, loss = 0.49867396\n",
            "Iteration 133, loss = 0.49839738\n",
            "Iteration 134, loss = 0.49828698\n",
            "Iteration 135, loss = 0.49812284\n",
            "Iteration 136, loss = 0.49781359\n",
            "Iteration 137, loss = 0.49766731\n",
            "Iteration 138, loss = 0.49742966\n",
            "Iteration 139, loss = 0.49729896\n",
            "Iteration 140, loss = 0.49693897\n",
            "Iteration 141, loss = 0.49679213\n",
            "Iteration 142, loss = 0.49665055\n",
            "Iteration 143, loss = 0.49641770\n",
            "Iteration 144, loss = 0.49617866\n",
            "Iteration 145, loss = 0.49592519\n",
            "Iteration 146, loss = 0.49574596\n",
            "Iteration 147, loss = 0.49556817\n",
            "Iteration 148, loss = 0.49543211\n",
            "Iteration 149, loss = 0.49532935\n",
            "Iteration 150, loss = 0.49496914\n",
            "Iteration 1, loss = 1.36723929\n",
            "Iteration 2, loss = 1.21905982\n",
            "Iteration 3, loss = 1.03107889\n",
            "Iteration 4, loss = 0.86195746\n",
            "Iteration 5, loss = 0.75246019\n",
            "Iteration 6, loss = 0.69773006\n",
            "Iteration 7, loss = 0.67069081\n",
            "Iteration 8, loss = 0.65769343\n",
            "Iteration 9, loss = 0.64888341\n",
            "Iteration 10, loss = 0.64099600\n",
            "Iteration 11, loss = 0.63323861\n",
            "Iteration 12, loss = 0.62561845\n",
            "Iteration 13, loss = 0.61893797\n",
            "Iteration 14, loss = 0.61269725\n",
            "Iteration 15, loss = 0.60721156\n",
            "Iteration 16, loss = 0.60205105\n",
            "Iteration 17, loss = 0.59786319\n",
            "Iteration 18, loss = 0.59406231\n",
            "Iteration 19, loss = 0.59071031\n",
            "Iteration 20, loss = 0.58803417\n",
            "Iteration 21, loss = 0.58526381\n",
            "Iteration 22, loss = 0.58253915\n",
            "Iteration 23, loss = 0.57958451\n",
            "Iteration 24, loss = 0.57694386\n",
            "Iteration 25, loss = 0.57480505\n",
            "Iteration 26, loss = 0.57235466\n",
            "Iteration 27, loss = 0.57040575\n",
            "Iteration 28, loss = 0.56840383\n",
            "Iteration 29, loss = 0.56663716\n",
            "Iteration 30, loss = 0.56492198\n",
            "Iteration 31, loss = 0.56329133\n",
            "Iteration 32, loss = 0.56191467\n",
            "Iteration 33, loss = 0.56045268\n",
            "Iteration 34, loss = 0.55912756\n",
            "Iteration 35, loss = 0.55794052\n",
            "Iteration 36, loss = 0.55654732\n",
            "Iteration 37, loss = 0.55516411\n",
            "Iteration 38, loss = 0.55392465\n",
            "Iteration 39, loss = 0.55276870\n",
            "Iteration 40, loss = 0.55161758\n",
            "Iteration 41, loss = 0.55055639\n",
            "Iteration 42, loss = 0.54954353\n",
            "Iteration 43, loss = 0.54854303\n",
            "Iteration 44, loss = 0.54750768\n",
            "Iteration 45, loss = 0.54666345\n",
            "Iteration 46, loss = 0.54566846\n",
            "Iteration 47, loss = 0.54496082\n",
            "Iteration 48, loss = 0.54419184\n",
            "Iteration 49, loss = 0.54348414\n",
            "Iteration 50, loss = 0.54278759\n",
            "Iteration 51, loss = 0.54212916\n",
            "Iteration 52, loss = 0.54153784\n",
            "Iteration 53, loss = 0.54082423\n",
            "Iteration 54, loss = 0.54022182\n",
            "Iteration 55, loss = 0.53954595\n",
            "Iteration 56, loss = 0.53892056\n",
            "Iteration 57, loss = 0.53836775\n",
            "Iteration 58, loss = 0.53769234\n",
            "Iteration 59, loss = 0.53724987\n",
            "Iteration 60, loss = 0.53663885\n",
            "Iteration 61, loss = 0.53612754\n",
            "Iteration 62, loss = 0.53547468\n",
            "Iteration 63, loss = 0.53504210\n",
            "Iteration 64, loss = 0.53427768\n",
            "Iteration 65, loss = 0.53366504\n",
            "Iteration 66, loss = 0.53303150\n",
            "Iteration 67, loss = 0.53238321\n",
            "Iteration 68, loss = 0.53194125\n",
            "Iteration 69, loss = 0.53133059\n",
            "Iteration 70, loss = 0.53074222\n",
            "Iteration 71, loss = 0.53014142\n",
            "Iteration 72, loss = 0.52957523"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 73, loss = 0.52908565\n",
            "Iteration 74, loss = 0.52854630\n",
            "Iteration 75, loss = 0.52804022\n",
            "Iteration 76, loss = 0.52757641\n",
            "Iteration 77, loss = 0.52717378\n",
            "Iteration 78, loss = 0.52668493\n",
            "Iteration 79, loss = 0.52635280\n",
            "Iteration 80, loss = 0.52604054\n",
            "Iteration 81, loss = 0.52543617\n",
            "Iteration 82, loss = 0.52503550\n",
            "Iteration 83, loss = 0.52449942\n",
            "Iteration 84, loss = 0.52396905\n",
            "Iteration 85, loss = 0.52358350\n",
            "Iteration 86, loss = 0.52302113\n",
            "Iteration 87, loss = 0.52252843\n",
            "Iteration 88, loss = 0.52205340\n",
            "Iteration 89, loss = 0.52165514\n",
            "Iteration 90, loss = 0.52115976\n",
            "Iteration 91, loss = 0.52074615\n",
            "Iteration 92, loss = 0.52025791\n",
            "Iteration 93, loss = 0.51985299\n",
            "Iteration 94, loss = 0.51946563\n",
            "Iteration 95, loss = 0.51903519\n",
            "Iteration 96, loss = 0.51853386\n",
            "Iteration 97, loss = 0.51813098\n",
            "Iteration 98, loss = 0.51771788\n",
            "Iteration 99, loss = 0.51733601\n",
            "Iteration 100, loss = 0.51689669\n",
            "Iteration 101, loss = 0.51656435\n",
            "Iteration 102, loss = 0.51608763\n",
            "Iteration 103, loss = 0.51582360\n",
            "Iteration 104, loss = 0.51553747\n",
            "Iteration 105, loss = 0.51525748\n",
            "Iteration 106, loss = 0.51499401\n",
            "Iteration 107, loss = 0.51478558\n",
            "Iteration 108, loss = 0.51448378\n",
            "Iteration 109, loss = 0.51413283\n",
            "Iteration 110, loss = 0.51371076\n",
            "Iteration 111, loss = 0.51337152\n",
            "Iteration 112, loss = 0.51291200\n",
            "Iteration 113, loss = 0.51249863\n",
            "Iteration 114, loss = 0.51205963\n",
            "Iteration 115, loss = 0.51173540\n",
            "Iteration 116, loss = 0.51146686\n",
            "Iteration 117, loss = 0.51119443\n",
            "Iteration 118, loss = 0.51078437\n",
            "Iteration 119, loss = 0.51062081\n",
            "Iteration 120, loss = 0.51031826\n",
            "Iteration 121, loss = 0.50998651\n",
            "Iteration 122, loss = 0.50978558\n",
            "Iteration 123, loss = 0.50941361\n",
            "Iteration 124, loss = 0.50913029\n",
            "Iteration 125, loss = 0.50883698\n",
            "Iteration 126, loss = 0.50869105\n",
            "Iteration 127, loss = 0.50858617\n",
            "Iteration 128, loss = 0.50816938\n",
            "Iteration 129, loss = 0.50793804\n",
            "Iteration 130, loss = 0.50764031\n",
            "Iteration 131, loss = 0.50742087\n",
            "Iteration 132, loss = 0.50712230\n",
            "Iteration 133, loss = 0.50692221\n",
            "Iteration 134, loss = 0.50678377\n",
            "Iteration 135, loss = 0.50639626\n",
            "Iteration 136, loss = 0.50603770\n",
            "Iteration 137, loss = 0.50586967\n",
            "Iteration 138, loss = 0.50564069\n",
            "Iteration 139, loss = 0.50535606\n",
            "Iteration 140, loss = 0.50516058\n",
            "Iteration 141, loss = 0.50497520\n",
            "Iteration 142, loss = 0.50468096\n",
            "Iteration 143, loss = 0.50442426\n",
            "Iteration 144, loss = 0.50413265\n",
            "Iteration 145, loss = 0.50386107\n",
            "Iteration 146, loss = 0.50353061\n",
            "Iteration 147, loss = 0.50334634\n",
            "Iteration 148, loss = 0.50311783\n",
            "Iteration 149, loss = 0.50288664\n",
            "Iteration 150, loss = 0.50236505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.34182386\n",
            "Iteration 2, loss = 1.20486792\n",
            "Iteration 3, loss = 1.03052691\n",
            "Iteration 4, loss = 0.87573110\n",
            "Iteration 5, loss = 0.76767102\n",
            "Iteration 6, loss = 0.71005122\n",
            "Iteration 7, loss = 0.68025523\n",
            "Iteration 8, loss = 0.66452169\n",
            "Iteration 9, loss = 0.65582803\n",
            "Iteration 10, loss = 0.64798558\n",
            "Iteration 11, loss = 0.64058304\n",
            "Iteration 12, loss = 0.63342237\n",
            "Iteration 13, loss = 0.62701482\n",
            "Iteration 14, loss = 0.62068021\n",
            "Iteration 15, loss = 0.61547319\n",
            "Iteration 16, loss = 0.61076412\n",
            "Iteration 17, loss = 0.60674878\n",
            "Iteration 18, loss = 0.60287881\n",
            "Iteration 19, loss = 0.59941555\n",
            "Iteration 20, loss = 0.59658795\n",
            "Iteration 21, loss = 0.59379273\n",
            "Iteration 22, loss = 0.59137819\n",
            "Iteration 23, loss = 0.58868711\n",
            "Iteration 24, loss = 0.58623099\n",
            "Iteration 25, loss = 0.58407023\n",
            "Iteration 26, loss = 0.58184564\n",
            "Iteration 27, loss = 0.57995834\n",
            "Iteration 28, loss = 0.57811124\n",
            "Iteration 29, loss = 0.57634992\n",
            "Iteration 30, loss = 0.57458462\n",
            "Iteration 31, loss = 0.57298054\n",
            "Iteration 32, loss = 0.57144934\n",
            "Iteration 33, loss = 0.56999777\n",
            "Iteration 34, loss = 0.56847783\n",
            "Iteration 35, loss = 0.56719457\n",
            "Iteration 36, loss = 0.56571401\n",
            "Iteration 37, loss = 0.56429184\n",
            "Iteration 38, loss = 0.56282301\n",
            "Iteration 39, loss = 0.56135988\n",
            "Iteration 40, loss = 0.55987407\n",
            "Iteration 41, loss = 0.55831365\n",
            "Iteration 42, loss = 0.55708077\n",
            "Iteration 43, loss = 0.55575808\n",
            "Iteration 44, loss = 0.55445939\n",
            "Iteration 45, loss = 0.55327371\n",
            "Iteration 46, loss = 0.55209696\n",
            "Iteration 47, loss = 0.55130947\n",
            "Iteration 48, loss = 0.55024949\n",
            "Iteration 49, loss = 0.54932537\n",
            "Iteration 50, loss = 0.54833315\n",
            "Iteration 51, loss = 0.54736602\n",
            "Iteration 52, loss = 0.54661875\n",
            "Iteration 53, loss = 0.54587476\n",
            "Iteration 54, loss = 0.54513703\n",
            "Iteration 55, loss = 0.54429737\n",
            "Iteration 56, loss = 0.54357680\n",
            "Iteration 57, loss = 0.54285780\n",
            "Iteration 58, loss = 0.54213330\n",
            "Iteration 59, loss = 0.54160968\n",
            "Iteration 60, loss = 0.54088100\n",
            "Iteration 61, loss = 0.54034655\n",
            "Iteration 62, loss = 0.53973508\n",
            "Iteration 63, loss = 0.53941835\n",
            "Iteration 64, loss = 0.53872499\n",
            "Iteration 65, loss = 0.53810126\n",
            "Iteration 66, loss = 0.53764695\n",
            "Iteration 67, loss = 0.53703482\n",
            "Iteration 68, loss = 0.53660654\n",
            "Iteration 69, loss = 0.53600849\n",
            "Iteration 70, loss = 0.53553453\n",
            "Iteration 71, loss = 0.53495367\n",
            "Iteration 72, loss = 0.53445071\n",
            "Iteration 73, loss = 0.53399473\n",
            "Iteration 74, loss = 0.53354493\n",
            "Iteration 75, loss = 0.53323114\n",
            "Iteration 76, loss = 0.53281414\n",
            "Iteration 77, loss = 0.53234541\n",
            "Iteration 78, loss = 0.53195831\n",
            "Iteration 79, loss = 0.53161136\n",
            "Iteration 80, loss = 0.53126047\n",
            "Iteration 81, loss = 0.53085038\n",
            "Iteration 82, loss = 0.53049602\n",
            "Iteration 83, loss = 0.53010471\n",
            "Iteration 84, loss = 0.52972940\n",
            "Iteration 85, loss = 0.52942890\n",
            "Iteration 86, loss = 0.52903268\n",
            "Iteration 87, loss = 0.52860258\n",
            "Iteration 88, loss = 0.52823206\n",
            "Iteration 89, loss = 0.52794637\n",
            "Iteration 90, loss = 0.52760289\n",
            "Iteration 91, loss = 0.52728967\n",
            "Iteration 92, loss = 0.52693172\n",
            "Iteration 93, loss = 0.52664655\n",
            "Iteration 94, loss = 0.52628103\n",
            "Iteration 95, loss = 0.52585124\n",
            "Iteration 96, loss = 0.52557953\n",
            "Iteration 97, loss = 0.52530700\n",
            "Iteration 98, loss = 0.52495444\n",
            "Iteration 99, loss = 0.52463324\n",
            "Iteration 100, loss = 0.52435696\n",
            "Iteration 101, loss = 0.52406326\n",
            "Iteration 102, loss = 0.52373880\n",
            "Iteration 103, loss = 0.52357136\n",
            "Iteration 104, loss = 0.52330951\n",
            "Iteration 105, loss = 0.52302206\n",
            "Iteration 106, loss = 0.52279563\n",
            "Iteration 107, loss = 0.52267320\n",
            "Iteration 108, loss = 0.52232405\n",
            "Iteration 109, loss = 0.52213232\n",
            "Iteration 110, loss = 0.52180247\n",
            "Iteration 111, loss = 0.52145225\n",
            "Iteration 112, loss = 0.52126538\n",
            "Iteration 113, loss = 0.52094972\n",
            "Iteration 114, loss = 0.52063710\n",
            "Iteration 115, loss = 0.52041963\n",
            "Iteration 116, loss = 0.52016501\n",
            "Iteration 117, loss = 0.51994950\n",
            "Iteration 118, loss = 0.51968411\n",
            "Iteration 119, loss = 0.51963826\n",
            "Iteration 120, loss = 0.51927120\n",
            "Iteration 121, loss = 0.51907988\n",
            "Iteration 122, loss = 0.51882328\n",
            "Iteration 123, loss = 0.51856471\n",
            "Iteration 124, loss = 0.51839948\n",
            "Iteration 125, loss = 0.51809568\n",
            "Iteration 126, loss = 0.51792599\n",
            "Iteration 127, loss = 0.51780522\n",
            "Iteration 128, loss = 0.51755738\n",
            "Iteration 129, loss = 0.51728052\n",
            "Iteration 130, loss = 0.51698920\n",
            "Iteration 131, loss = 0.51668455\n",
            "Iteration 132, loss = 0.51661627\n",
            "Iteration 133, loss = 0.51626315\n",
            "Iteration 134, loss = 0.51614263\n",
            "Iteration 135, loss = 0.51589745\n",
            "Iteration 136, loss = 0.51566872\n",
            "Iteration 137, loss = 0.51559520\n",
            "Iteration 138, loss = 0.51540490\n",
            "Iteration 139, loss = 0.51520248\n",
            "Iteration 140, loss = 0.51494708\n",
            "Iteration 141, loss = 0.51480726\n",
            "Iteration 142, loss = 0.51462756\n",
            "Iteration 143, loss = 0.51451956\n",
            "Iteration 144, loss = 0.51423886\n",
            "Iteration 145, loss = 0.51421063\n",
            "Iteration 146, loss = 0.51394809\n",
            "Iteration 147, loss = 0.51387299\n",
            "Iteration 148, loss = 0.51367021\n",
            "Iteration 149, loss = 0.51357374\n",
            "Iteration 150, loss = 0.51334572\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 150 and for layer number 6 : 0.7212500000000001\n",
            "Iteration 1, loss = 0.61686348\n",
            "Iteration 2, loss = 0.61615212\n",
            "Iteration 3, loss = 0.61509889\n",
            "Iteration 4, loss = 0.61393355\n",
            "Iteration 5, loss = 0.61267100\n",
            "Iteration 6, loss = 0.61156262\n",
            "Iteration 7, loss = 0.61039793\n",
            "Iteration 8, loss = 0.60920464\n",
            "Iteration 9, loss = 0.60819776\n",
            "Iteration 10, loss = 0.60727368\n",
            "Iteration 11, loss = 0.60643854\n",
            "Iteration 12, loss = 0.60574605\n",
            "Iteration 13, loss = 0.60503921\n",
            "Iteration 14, loss = 0.60439293\n",
            "Iteration 15, loss = 0.60372900\n",
            "Iteration 16, loss = 0.60312998\n",
            "Iteration 17, loss = 0.60257188\n",
            "Iteration 18, loss = 0.60207421\n",
            "Iteration 19, loss = 0.60156745\n",
            "Iteration 20, loss = 0.60103417\n",
            "Iteration 21, loss = 0.60046005\n",
            "Iteration 22, loss = 0.60001837\n",
            "Iteration 23, loss = 0.59949961\n",
            "Iteration 24, loss = 0.59907076\n",
            "Iteration 25, loss = 0.59862261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 26, loss = 0.59821262\n",
            "Iteration 27, loss = 0.59768043\n",
            "Iteration 28, loss = 0.59727575\n",
            "Iteration 29, loss = 0.59686577\n",
            "Iteration 30, loss = 0.59634456\n",
            "Iteration 31, loss = 0.59592913\n",
            "Iteration 32, loss = 0.59551543\n",
            "Iteration 33, loss = 0.59511982\n",
            "Iteration 34, loss = 0.59488057\n",
            "Iteration 35, loss = 0.59437990\n",
            "Iteration 36, loss = 0.59393766\n",
            "Iteration 37, loss = 0.59343460\n",
            "Iteration 38, loss = 0.59299992\n",
            "Iteration 39, loss = 0.59251347\n",
            "Iteration 40, loss = 0.59210013\n",
            "Iteration 41, loss = 0.59158753\n",
            "Iteration 42, loss = 0.59116433\n",
            "Iteration 43, loss = 0.59067977\n",
            "Iteration 44, loss = 0.59030424\n",
            "Iteration 45, loss = 0.58985025\n",
            "Iteration 46, loss = 0.58934126\n",
            "Iteration 47, loss = 0.58893644\n",
            "Iteration 48, loss = 0.58838509\n",
            "Iteration 49, loss = 0.58789781\n",
            "Iteration 50, loss = 0.58745034\n",
            "Iteration 51, loss = 0.58705463\n",
            "Iteration 52, loss = 0.58659520\n",
            "Iteration 53, loss = 0.58621788\n",
            "Iteration 54, loss = 0.58582979\n",
            "Iteration 55, loss = 0.58542994\n",
            "Iteration 56, loss = 0.58504137\n",
            "Iteration 57, loss = 0.58463168\n",
            "Iteration 58, loss = 0.58429345\n",
            "Iteration 59, loss = 0.58388301\n",
            "Iteration 60, loss = 0.58354039\n",
            "Iteration 61, loss = 0.58310095\n",
            "Iteration 62, loss = 0.58279069\n",
            "Iteration 63, loss = 0.58237879\n",
            "Iteration 64, loss = 0.58204179\n",
            "Iteration 65, loss = 0.58167540\n",
            "Iteration 66, loss = 0.58131958\n",
            "Iteration 67, loss = 0.58107527\n",
            "Iteration 68, loss = 0.58075012\n",
            "Iteration 69, loss = 0.58043709\n",
            "Iteration 70, loss = 0.58009698\n",
            "Iteration 71, loss = 0.57979144\n",
            "Iteration 72, loss = 0.57952207\n",
            "Iteration 73, loss = 0.57918802\n",
            "Iteration 74, loss = 0.57892494\n",
            "Iteration 75, loss = 0.57862554\n",
            "Iteration 76, loss = 0.57835511\n",
            "Iteration 77, loss = 0.57808078\n",
            "Iteration 78, loss = 0.57780786\n",
            "Iteration 79, loss = 0.57753624\n",
            "Iteration 80, loss = 0.57722696\n",
            "Iteration 81, loss = 0.57695431\n",
            "Iteration 82, loss = 0.57660842\n",
            "Iteration 83, loss = 0.57639990\n",
            "Iteration 84, loss = 0.57623494\n",
            "Iteration 85, loss = 0.57594787\n",
            "Iteration 86, loss = 0.57557150\n",
            "Iteration 87, loss = 0.57527755\n",
            "Iteration 88, loss = 0.57490136\n",
            "Iteration 89, loss = 0.57463870\n",
            "Iteration 90, loss = 0.57420350\n",
            "Iteration 91, loss = 0.57382071\n",
            "Iteration 92, loss = 0.57349973\n",
            "Iteration 93, loss = 0.57316240\n",
            "Iteration 94, loss = 0.57277331\n",
            "Iteration 95, loss = 0.57224303\n",
            "Iteration 96, loss = 0.57170654\n",
            "Iteration 97, loss = 0.57127444\n",
            "Iteration 98, loss = 0.57075803\n",
            "Iteration 99, loss = 0.57034183\n",
            "Iteration 100, loss = 0.56991355\n",
            "Iteration 101, loss = 0.56954816\n",
            "Iteration 102, loss = 0.56908193\n",
            "Iteration 103, loss = 0.56854709\n",
            "Iteration 104, loss = 0.56805126\n",
            "Iteration 105, loss = 0.56750147\n",
            "Iteration 106, loss = 0.56701195\n",
            "Iteration 107, loss = 0.56647560\n",
            "Iteration 108, loss = 0.56597761\n",
            "Iteration 109, loss = 0.56545971\n",
            "Iteration 110, loss = 0.56495461\n",
            "Iteration 111, loss = 0.56441826\n",
            "Iteration 112, loss = 0.56379794\n",
            "Iteration 113, loss = 0.56314473\n",
            "Iteration 114, loss = 0.56254899\n",
            "Iteration 115, loss = 0.56209955\n",
            "Iteration 116, loss = 0.56160359\n",
            "Iteration 117, loss = 0.56105735\n",
            "Iteration 118, loss = 0.56070239\n",
            "Iteration 119, loss = 0.56032899\n",
            "Iteration 120, loss = 0.55995455\n",
            "Iteration 121, loss = 0.55955228\n",
            "Iteration 122, loss = 0.55918703\n",
            "Iteration 123, loss = 0.55884484\n",
            "Iteration 124, loss = 0.55838076\n",
            "Iteration 125, loss = 0.55808569\n",
            "Iteration 126, loss = 0.55751445\n",
            "Iteration 127, loss = 0.55718773\n",
            "Iteration 128, loss = 0.55680289\n",
            "Iteration 129, loss = 0.55641844\n",
            "Iteration 130, loss = 0.55609968\n",
            "Iteration 131, loss = 0.55576557\n",
            "Iteration 132, loss = 0.55544813\n",
            "Iteration 133, loss = 0.55514959\n",
            "Iteration 134, loss = 0.55485585\n",
            "Iteration 135, loss = 0.55465213\n",
            "Iteration 136, loss = 0.55439104\n",
            "Iteration 137, loss = 0.55423854\n",
            "Iteration 138, loss = 0.55397484\n",
            "Iteration 139, loss = 0.55382484\n",
            "Iteration 140, loss = 0.55362496\n",
            "Iteration 141, loss = 0.55350449\n",
            "Iteration 142, loss = 0.55330935\n",
            "Iteration 143, loss = 0.55312696\n",
            "Iteration 144, loss = 0.55287228\n",
            "Iteration 145, loss = 0.55263719\n",
            "Iteration 146, loss = 0.55255738\n",
            "Iteration 147, loss = 0.55232271\n",
            "Iteration 148, loss = 0.55213450\n",
            "Iteration 149, loss = 0.55196895\n",
            "Iteration 150, loss = 0.55176033\n",
            "Iteration 151, loss = 0.55161366\n",
            "Iteration 152, loss = 0.55140052\n",
            "Iteration 153, loss = 0.55143007\n",
            "Iteration 154, loss = 0.55119174\n",
            "Iteration 155, loss = 0.55115582\n",
            "Iteration 156, loss = 0.55100698\n",
            "Iteration 157, loss = 0.55081218\n",
            "Iteration 158, loss = 0.55060617\n",
            "Iteration 159, loss = 0.55034760\n",
            "Iteration 160, loss = 0.55028817\n",
            "Iteration 161, loss = 0.54998891\n",
            "Iteration 162, loss = 0.54978916\n",
            "Iteration 163, loss = 0.54968667\n",
            "Iteration 164, loss = 0.54957281\n",
            "Iteration 165, loss = 0.54949291\n",
            "Iteration 166, loss = 0.54940404\n",
            "Iteration 167, loss = 0.54932224\n",
            "Iteration 168, loss = 0.54918298\n",
            "Iteration 169, loss = 0.54903869\n",
            "Iteration 170, loss = 0.54893535\n",
            "Iteration 171, loss = 0.54886424\n",
            "Iteration 172, loss = 0.54867618\n",
            "Iteration 173, loss = 0.54861745\n",
            "Iteration 174, loss = 0.54838514\n",
            "Iteration 175, loss = 0.54830627\n",
            "Iteration 176, loss = 0.54819492\n",
            "Iteration 177, loss = 0.54813816\n",
            "Iteration 178, loss = 0.54797866\n",
            "Iteration 179, loss = 0.54781127\n",
            "Iteration 180, loss = 0.54768839\n",
            "Iteration 181, loss = 0.54748322\n",
            "Iteration 182, loss = 0.54751076\n",
            "Iteration 183, loss = 0.54725576\n",
            "Iteration 184, loss = 0.54712612\n",
            "Iteration 185, loss = 0.54701994\n",
            "Iteration 186, loss = 0.54699073\n",
            "Iteration 187, loss = 0.54690768\n",
            "Iteration 188, loss = 0.54685276\n",
            "Iteration 189, loss = 0.54672685\n",
            "Iteration 190, loss = 0.54660527\n",
            "Iteration 191, loss = 0.54656267\n",
            "Iteration 192, loss = 0.54645821\n",
            "Iteration 193, loss = 0.54626282\n",
            "Iteration 194, loss = 0.54609695\n",
            "Iteration 195, loss = 0.54589554\n",
            "Iteration 196, loss = 0.54579439\n",
            "Iteration 197, loss = 0.54571042\n",
            "Iteration 198, loss = 0.54559076\n",
            "Iteration 199, loss = 0.54544129\n",
            "Iteration 200, loss = 0.54534156\n",
            "Iteration 1, loss = 0.61762838\n",
            "Iteration 2, loss = 0.61685222\n",
            "Iteration 3, loss = 0.61571214\n",
            "Iteration 4, loss = 0.61441758\n",
            "Iteration 5, loss = 0.61318349\n",
            "Iteration 6, loss = 0.61195746\n",
            "Iteration 7, loss = 0.61086455\n",
            "Iteration 8, loss = 0.60959350\n",
            "Iteration 9, loss = 0.60852999\n",
            "Iteration 10, loss = 0.60759281\n",
            "Iteration 11, loss = 0.60667038\n",
            "Iteration 12, loss = 0.60588256\n",
            "Iteration 13, loss = 0.60500642\n",
            "Iteration 14, loss = 0.60429505\n",
            "Iteration 15, loss = 0.60355287\n",
            "Iteration 16, loss = 0.60296533\n",
            "Iteration 17, loss = 0.60238875\n",
            "Iteration 18, loss = 0.60183941\n",
            "Iteration 19, loss = 0.60124485\n",
            "Iteration 20, loss = 0.60063271\n",
            "Iteration 21, loss = 0.60004317\n",
            "Iteration 22, loss = 0.59951396\n",
            "Iteration 23, loss = 0.59887438\n",
            "Iteration 24, loss = 0.59834187\n",
            "Iteration 25, loss = 0.59783710\n",
            "Iteration 26, loss = 0.59731217\n",
            "Iteration 27, loss = 0.59680236\n",
            "Iteration 28, loss = 0.59634036\n",
            "Iteration 29, loss = 0.59585225\n",
            "Iteration 30, loss = 0.59540926\n",
            "Iteration 31, loss = 0.59494784\n",
            "Iteration 32, loss = 0.59442520\n",
            "Iteration 33, loss = 0.59397329\n",
            "Iteration 34, loss = 0.59352410\n",
            "Iteration 35, loss = 0.59310715\n",
            "Iteration 36, loss = 0.59259080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 37, loss = 0.59198806\n",
            "Iteration 38, loss = 0.59141069\n",
            "Iteration 39, loss = 0.59091519\n",
            "Iteration 40, loss = 0.59042012\n",
            "Iteration 41, loss = 0.58996507\n",
            "Iteration 42, loss = 0.58947078\n",
            "Iteration 43, loss = 0.58903801\n",
            "Iteration 44, loss = 0.58869976\n",
            "Iteration 45, loss = 0.58832512\n",
            "Iteration 46, loss = 0.58801003\n",
            "Iteration 47, loss = 0.58772939\n",
            "Iteration 48, loss = 0.58749791\n",
            "Iteration 49, loss = 0.58718487\n",
            "Iteration 50, loss = 0.58696384\n",
            "Iteration 51, loss = 0.58679751\n",
            "Iteration 52, loss = 0.58653756\n",
            "Iteration 53, loss = 0.58627720\n",
            "Iteration 54, loss = 0.58609678\n",
            "Iteration 55, loss = 0.58587111\n",
            "Iteration 56, loss = 0.58563655\n",
            "Iteration 57, loss = 0.58540147\n",
            "Iteration 58, loss = 0.58515985\n",
            "Iteration 59, loss = 0.58495083\n",
            "Iteration 60, loss = 0.58467493\n",
            "Iteration 61, loss = 0.58436402\n",
            "Iteration 62, loss = 0.58419131\n",
            "Iteration 63, loss = 0.58386634\n",
            "Iteration 64, loss = 0.58367166\n",
            "Iteration 65, loss = 0.58340516\n",
            "Iteration 66, loss = 0.58311369\n",
            "Iteration 67, loss = 0.58294822\n",
            "Iteration 68, loss = 0.58272389\n",
            "Iteration 69, loss = 0.58248177\n",
            "Iteration 70, loss = 0.58222376\n",
            "Iteration 71, loss = 0.58199437\n",
            "Iteration 72, loss = 0.58183608\n",
            "Iteration 73, loss = 0.58158739\n",
            "Iteration 74, loss = 0.58128332\n",
            "Iteration 75, loss = 0.58109528\n",
            "Iteration 76, loss = 0.58079797\n",
            "Iteration 77, loss = 0.58055032\n",
            "Iteration 78, loss = 0.58038713\n",
            "Iteration 79, loss = 0.58011096\n",
            "Iteration 80, loss = 0.57987169\n",
            "Iteration 81, loss = 0.57960002\n",
            "Iteration 82, loss = 0.57928697\n",
            "Iteration 83, loss = 0.57902631\n",
            "Iteration 84, loss = 0.57873116\n",
            "Iteration 85, loss = 0.57846126\n",
            "Iteration 86, loss = 0.57816501\n",
            "Iteration 87, loss = 0.57791457\n",
            "Iteration 88, loss = 0.57758301\n",
            "Iteration 89, loss = 0.57729328\n",
            "Iteration 90, loss = 0.57700737\n",
            "Iteration 91, loss = 0.57667723\n",
            "Iteration 92, loss = 0.57637588\n",
            "Iteration 93, loss = 0.57608463\n",
            "Iteration 94, loss = 0.57589851\n",
            "Iteration 95, loss = 0.57547867\n",
            "Iteration 96, loss = 0.57515434\n",
            "Iteration 97, loss = 0.57484887\n",
            "Iteration 98, loss = 0.57450644\n",
            "Iteration 99, loss = 0.57423971\n",
            "Iteration 100, loss = 0.57394845\n",
            "Iteration 101, loss = 0.57367106\n",
            "Iteration 102, loss = 0.57345373\n",
            "Iteration 103, loss = 0.57315003\n",
            "Iteration 104, loss = 0.57286482\n",
            "Iteration 105, loss = 0.57257109\n",
            "Iteration 106, loss = 0.57236005\n",
            "Iteration 107, loss = 0.57210592\n",
            "Iteration 108, loss = 0.57182320\n",
            "Iteration 109, loss = 0.57157851\n",
            "Iteration 110, loss = 0.57130851\n",
            "Iteration 111, loss = 0.57099353\n",
            "Iteration 112, loss = 0.57063653\n",
            "Iteration 113, loss = 0.57026642\n",
            "Iteration 114, loss = 0.56991901\n",
            "Iteration 115, loss = 0.56953357\n",
            "Iteration 116, loss = 0.56930145\n",
            "Iteration 117, loss = 0.56896447\n",
            "Iteration 118, loss = 0.56864356\n",
            "Iteration 119, loss = 0.56836572\n",
            "Iteration 120, loss = 0.56804066\n",
            "Iteration 121, loss = 0.56773562\n",
            "Iteration 122, loss = 0.56755539\n",
            "Iteration 123, loss = 0.56736130\n",
            "Iteration 124, loss = 0.56711146\n",
            "Iteration 125, loss = 0.56683908\n",
            "Iteration 126, loss = 0.56647484\n",
            "Iteration 127, loss = 0.56619872\n",
            "Iteration 128, loss = 0.56586599\n",
            "Iteration 129, loss = 0.56559488\n",
            "Iteration 130, loss = 0.56533314\n",
            "Iteration 131, loss = 0.56508778\n",
            "Iteration 132, loss = 0.56473660\n",
            "Iteration 133, loss = 0.56450473\n",
            "Iteration 134, loss = 0.56427530\n",
            "Iteration 135, loss = 0.56395923\n",
            "Iteration 136, loss = 0.56372237\n",
            "Iteration 137, loss = 0.56337833\n",
            "Iteration 138, loss = 0.56311224\n",
            "Iteration 139, loss = 0.56286408\n",
            "Iteration 140, loss = 0.56260020\n",
            "Iteration 141, loss = 0.56237318\n",
            "Iteration 142, loss = 0.56214346\n",
            "Iteration 143, loss = 0.56184605\n",
            "Iteration 144, loss = 0.56160014\n",
            "Iteration 145, loss = 0.56122180\n",
            "Iteration 146, loss = 0.56102411\n",
            "Iteration 147, loss = 0.56075902\n",
            "Iteration 148, loss = 0.56051512\n",
            "Iteration 149, loss = 0.56014889\n",
            "Iteration 150, loss = 0.55996943\n",
            "Iteration 151, loss = 0.55974368\n",
            "Iteration 152, loss = 0.55959805\n",
            "Iteration 153, loss = 0.55969770\n",
            "Iteration 154, loss = 0.55946636\n",
            "Iteration 155, loss = 0.55930271\n",
            "Iteration 156, loss = 0.55903201\n",
            "Iteration 157, loss = 0.55884274\n",
            "Iteration 158, loss = 0.55846432\n",
            "Iteration 159, loss = 0.55821890\n",
            "Iteration 160, loss = 0.55801632\n",
            "Iteration 161, loss = 0.55771887\n",
            "Iteration 162, loss = 0.55752476\n",
            "Iteration 163, loss = 0.55729095\n",
            "Iteration 164, loss = 0.55711046\n",
            "Iteration 165, loss = 0.55699770\n",
            "Iteration 166, loss = 0.55679932\n",
            "Iteration 167, loss = 0.55662522\n",
            "Iteration 168, loss = 0.55637875\n",
            "Iteration 169, loss = 0.55615027\n",
            "Iteration 170, loss = 0.55596352\n",
            "Iteration 171, loss = 0.55576055\n",
            "Iteration 172, loss = 0.55555154\n",
            "Iteration 173, loss = 0.55535588\n",
            "Iteration 174, loss = 0.55513544\n",
            "Iteration 175, loss = 0.55498198\n",
            "Iteration 176, loss = 0.55483895\n",
            "Iteration 177, loss = 0.55465485\n",
            "Iteration 178, loss = 0.55443179\n",
            "Iteration 179, loss = 0.55429233\n",
            "Iteration 180, loss = 0.55408917\n",
            "Iteration 181, loss = 0.55384782\n",
            "Iteration 182, loss = 0.55370002\n",
            "Iteration 183, loss = 0.55346441\n",
            "Iteration 184, loss = 0.55332935\n",
            "Iteration 185, loss = 0.55310535\n",
            "Iteration 186, loss = 0.55298333\n",
            "Iteration 187, loss = 0.55277597\n",
            "Iteration 188, loss = 0.55259738\n",
            "Iteration 189, loss = 0.55240019\n",
            "Iteration 190, loss = 0.55254449\n",
            "Iteration 191, loss = 0.55251468\n",
            "Iteration 192, loss = 0.55239200\n",
            "Iteration 193, loss = 0.55209030\n",
            "Iteration 194, loss = 0.55187264\n",
            "Iteration 195, loss = 0.55160011\n",
            "Iteration 196, loss = 0.55134965\n",
            "Iteration 197, loss = 0.55123327\n",
            "Iteration 198, loss = 0.55098521\n",
            "Iteration 199, loss = 0.55082595\n",
            "Iteration 200, loss = 0.55064578\n",
            "Iteration 1, loss = 0.61756281\n",
            "Iteration 2, loss = 0.61678746\n",
            "Iteration 3, loss = 0.61551433\n",
            "Iteration 4, loss = 0.61406466\n",
            "Iteration 5, loss = 0.61275832\n",
            "Iteration 6, loss = 0.61139014\n",
            "Iteration 7, loss = 0.61018275\n",
            "Iteration 8, loss = 0.60882083\n",
            "Iteration 9, loss = 0.60770150\n",
            "Iteration 10, loss = 0.60664596\n",
            "Iteration 11, loss = 0.60580536\n",
            "Iteration 12, loss = 0.60503210\n",
            "Iteration 13, loss = 0.60414541\n",
            "Iteration 14, loss = 0.60345742\n",
            "Iteration 15, loss = 0.60273866\n",
            "Iteration 16, loss = 0.60214876\n",
            "Iteration 17, loss = 0.60155645\n",
            "Iteration 18, loss = 0.60098285\n",
            "Iteration 19, loss = 0.60041376\n",
            "Iteration 20, loss = 0.59981786\n",
            "Iteration 21, loss = 0.59929265\n",
            "Iteration 22, loss = 0.59865761\n",
            "Iteration 23, loss = 0.59789106\n",
            "Iteration 24, loss = 0.59730522\n",
            "Iteration 25, loss = 0.59667276\n",
            "Iteration 26, loss = 0.59602085\n",
            "Iteration 27, loss = 0.59538918\n",
            "Iteration 28, loss = 0.59476517\n",
            "Iteration 29, loss = 0.59420415\n",
            "Iteration 30, loss = 0.59360792\n",
            "Iteration 31, loss = 0.59298919\n",
            "Iteration 32, loss = 0.59233785\n",
            "Iteration 33, loss = 0.59173787\n",
            "Iteration 34, loss = 0.59105348\n",
            "Iteration 35, loss = 0.59039394\n",
            "Iteration 36, loss = 0.58983637\n",
            "Iteration 37, loss = 0.58907064\n",
            "Iteration 38, loss = 0.58833953\n",
            "Iteration 39, loss = 0.58772116\n",
            "Iteration 40, loss = 0.58701155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 41, loss = 0.58632754\n",
            "Iteration 42, loss = 0.58559078\n",
            "Iteration 43, loss = 0.58496621\n",
            "Iteration 44, loss = 0.58428972\n",
            "Iteration 45, loss = 0.58358511\n",
            "Iteration 46, loss = 0.58293101\n",
            "Iteration 47, loss = 0.58227798\n",
            "Iteration 48, loss = 0.58170222\n",
            "Iteration 49, loss = 0.58101512\n",
            "Iteration 50, loss = 0.58043029\n",
            "Iteration 51, loss = 0.57991960\n",
            "Iteration 52, loss = 0.57933529\n",
            "Iteration 53, loss = 0.57870244\n",
            "Iteration 54, loss = 0.57829782\n",
            "Iteration 55, loss = 0.57767445\n",
            "Iteration 56, loss = 0.57715199\n",
            "Iteration 57, loss = 0.57663478\n",
            "Iteration 58, loss = 0.57604727\n",
            "Iteration 59, loss = 0.57548153\n",
            "Iteration 60, loss = 0.57475842\n",
            "Iteration 61, loss = 0.57416184\n",
            "Iteration 62, loss = 0.57353961\n",
            "Iteration 63, loss = 0.57287798\n",
            "Iteration 64, loss = 0.57223335\n",
            "Iteration 65, loss = 0.57144876\n",
            "Iteration 66, loss = 0.57074095\n",
            "Iteration 67, loss = 0.57018441\n",
            "Iteration 68, loss = 0.56959535\n",
            "Iteration 69, loss = 0.56896543\n",
            "Iteration 70, loss = 0.56840160\n",
            "Iteration 71, loss = 0.56782256\n",
            "Iteration 72, loss = 0.56726268\n",
            "Iteration 73, loss = 0.56657180\n",
            "Iteration 74, loss = 0.56591685\n",
            "Iteration 75, loss = 0.56546255\n",
            "Iteration 76, loss = 0.56487449\n",
            "Iteration 77, loss = 0.56432787\n",
            "Iteration 78, loss = 0.56379671\n",
            "Iteration 79, loss = 0.56323004\n",
            "Iteration 80, loss = 0.56263426\n",
            "Iteration 81, loss = 0.56212573\n",
            "Iteration 82, loss = 0.56151629\n",
            "Iteration 83, loss = 0.56097539\n",
            "Iteration 84, loss = 0.56051914\n",
            "Iteration 85, loss = 0.56006355\n",
            "Iteration 86, loss = 0.55956176\n",
            "Iteration 87, loss = 0.55907687\n",
            "Iteration 88, loss = 0.55861192\n",
            "Iteration 89, loss = 0.55803563\n",
            "Iteration 90, loss = 0.55760848\n",
            "Iteration 91, loss = 0.55701483\n",
            "Iteration 92, loss = 0.55655069\n",
            "Iteration 93, loss = 0.55609322\n",
            "Iteration 94, loss = 0.55567123\n",
            "Iteration 95, loss = 0.55502534\n",
            "Iteration 96, loss = 0.55460817\n",
            "Iteration 97, loss = 0.55402209\n",
            "Iteration 98, loss = 0.55355900\n",
            "Iteration 99, loss = 0.55318405\n",
            "Iteration 100, loss = 0.55277358\n",
            "Iteration 101, loss = 0.55236759\n",
            "Iteration 102, loss = 0.55205879\n",
            "Iteration 103, loss = 0.55161370\n",
            "Iteration 104, loss = 0.55123778\n",
            "Iteration 105, loss = 0.55086666\n",
            "Iteration 106, loss = 0.55047447\n",
            "Iteration 107, loss = 0.55012781\n",
            "Iteration 108, loss = 0.54966725\n",
            "Iteration 109, loss = 0.54927395\n",
            "Iteration 110, loss = 0.54888330\n",
            "Iteration 111, loss = 0.54853697\n",
            "Iteration 112, loss = 0.54803161\n",
            "Iteration 113, loss = 0.54759915\n",
            "Iteration 114, loss = 0.54717141\n",
            "Iteration 115, loss = 0.54685299\n",
            "Iteration 116, loss = 0.54647567\n",
            "Iteration 117, loss = 0.54607307\n",
            "Iteration 118, loss = 0.54574090\n",
            "Iteration 119, loss = 0.54544404\n",
            "Iteration 120, loss = 0.54504897\n",
            "Iteration 121, loss = 0.54472741\n",
            "Iteration 122, loss = 0.54438772\n",
            "Iteration 123, loss = 0.54404215\n",
            "Iteration 124, loss = 0.54369341\n",
            "Iteration 125, loss = 0.54343200\n",
            "Iteration 126, loss = 0.54302948\n",
            "Iteration 127, loss = 0.54279117\n",
            "Iteration 128, loss = 0.54249796\n",
            "Iteration 129, loss = 0.54233272\n",
            "Iteration 130, loss = 0.54201110\n",
            "Iteration 131, loss = 0.54157225\n",
            "Iteration 132, loss = 0.54113379\n",
            "Iteration 133, loss = 0.54074148\n",
            "Iteration 134, loss = 0.54038149\n",
            "Iteration 135, loss = 0.53995048\n",
            "Iteration 136, loss = 0.53962117\n",
            "Iteration 137, loss = 0.53916907\n",
            "Iteration 138, loss = 0.53877409\n",
            "Iteration 139, loss = 0.53841380\n",
            "Iteration 140, loss = 0.53806495\n",
            "Iteration 141, loss = 0.53771208\n",
            "Iteration 142, loss = 0.53742659\n",
            "Iteration 143, loss = 0.53737448\n",
            "Iteration 144, loss = 0.53686941\n",
            "Iteration 145, loss = 0.53651881\n",
            "Iteration 146, loss = 0.53625293\n",
            "Iteration 147, loss = 0.53595237\n",
            "Iteration 148, loss = 0.53573440\n",
            "Iteration 149, loss = 0.53537873\n",
            "Iteration 150, loss = 0.53518124\n",
            "Iteration 151, loss = 0.53496385\n",
            "Iteration 152, loss = 0.53475622\n",
            "Iteration 153, loss = 0.53472320\n",
            "Iteration 154, loss = 0.53453199\n",
            "Iteration 155, loss = 0.53435198\n",
            "Iteration 156, loss = 0.53418048\n",
            "Iteration 157, loss = 0.53399366\n",
            "Iteration 158, loss = 0.53376552\n",
            "Iteration 159, loss = 0.53339142\n",
            "Iteration 160, loss = 0.53325644\n",
            "Iteration 161, loss = 0.53286228\n",
            "Iteration 162, loss = 0.53275720\n",
            "Iteration 163, loss = 0.53253266\n",
            "Iteration 164, loss = 0.53243114\n",
            "Iteration 165, loss = 0.53231731\n",
            "Iteration 166, loss = 0.53206105\n",
            "Iteration 167, loss = 0.53198550\n",
            "Iteration 168, loss = 0.53170182\n",
            "Iteration 169, loss = 0.53153900\n",
            "Iteration 170, loss = 0.53138415\n",
            "Iteration 171, loss = 0.53125249\n",
            "Iteration 172, loss = 0.53109301\n",
            "Iteration 173, loss = 0.53093246\n",
            "Iteration 174, loss = 0.53079377\n",
            "Iteration 175, loss = 0.53062537\n",
            "Iteration 176, loss = 0.53049244\n",
            "Iteration 177, loss = 0.53036269\n",
            "Iteration 178, loss = 0.53025670\n",
            "Iteration 179, loss = 0.53006383\n",
            "Iteration 180, loss = 0.52990961\n",
            "Iteration 181, loss = 0.52975918\n",
            "Iteration 182, loss = 0.52968845\n",
            "Iteration 183, loss = 0.52949678\n",
            "Iteration 184, loss = 0.52942640\n",
            "Iteration 185, loss = 0.52931952\n",
            "Iteration 186, loss = 0.52921428\n",
            "Iteration 187, loss = 0.52909771\n",
            "Iteration 188, loss = 0.52892582\n",
            "Iteration 189, loss = 0.52886677\n",
            "Iteration 190, loss = 0.52882617\n",
            "Iteration 191, loss = 0.52890744\n",
            "Iteration 192, loss = 0.52882516\n",
            "Iteration 193, loss = 0.52871273\n",
            "Iteration 194, loss = 0.52859940\n",
            "Iteration 195, loss = 0.52848706\n",
            "Iteration 196, loss = 0.52831421\n",
            "Iteration 197, loss = 0.52822673\n",
            "Iteration 198, loss = 0.52806948\n",
            "Iteration 199, loss = 0.52793966\n",
            "Iteration 200, loss = 0.52777690\n",
            "Iteration 1, loss = 0.62079721\n",
            "Iteration 2, loss = 0.62005324\n",
            "Iteration 3, loss = 0.61900678\n",
            "Iteration 4, loss = 0.61766780\n",
            "Iteration 5, loss = 0.61643559\n",
            "Iteration 6, loss = 0.61518085\n",
            "Iteration 7, loss = 0.61403740\n",
            "Iteration 8, loss = 0.61288976\n",
            "Iteration 9, loss = 0.61190780\n",
            "Iteration 10, loss = 0.61098604\n",
            "Iteration 11, loss = 0.61018985\n",
            "Iteration 12, loss = 0.60952147\n",
            "Iteration 13, loss = 0.60886466\n",
            "Iteration 14, loss = 0.60818565\n",
            "Iteration 15, loss = 0.60758049\n",
            "Iteration 16, loss = 0.60707209\n",
            "Iteration 17, loss = 0.60648497\n",
            "Iteration 18, loss = 0.60597777\n",
            "Iteration 19, loss = 0.60548418\n",
            "Iteration 20, loss = 0.60501951\n",
            "Iteration 21, loss = 0.60460897\n",
            "Iteration 22, loss = 0.60410824\n",
            "Iteration 23, loss = 0.60353209\n",
            "Iteration 24, loss = 0.60308578\n",
            "Iteration 25, loss = 0.60261600\n",
            "Iteration 26, loss = 0.60213958\n",
            "Iteration 27, loss = 0.60173715\n",
            "Iteration 28, loss = 0.60121898\n",
            "Iteration 29, loss = 0.60074744\n",
            "Iteration 30, loss = 0.60026547\n",
            "Iteration 31, loss = 0.59976581\n",
            "Iteration 32, loss = 0.59927205\n",
            "Iteration 33, loss = 0.59878003\n",
            "Iteration 34, loss = 0.59832639\n",
            "Iteration 35, loss = 0.59786446\n",
            "Iteration 36, loss = 0.59746968\n",
            "Iteration 37, loss = 0.59693135\n",
            "Iteration 38, loss = 0.59639720\n",
            "Iteration 39, loss = 0.59583464\n",
            "Iteration 40, loss = 0.59533087\n",
            "Iteration 41, loss = 0.59475384\n",
            "Iteration 42, loss = 0.59425957\n",
            "Iteration 43, loss = 0.59373095\n",
            "Iteration 44, loss = 0.59323666\n",
            "Iteration 45, loss = 0.59264518\n",
            "Iteration 46, loss = 0.59213675\n",
            "Iteration 47, loss = 0.59166031\n",
            "Iteration 48, loss = 0.59110663\n",
            "Iteration 49, loss = 0.59053353\n",
            "Iteration 50, loss = 0.58998709\n",
            "Iteration 51, loss = 0.58950475\n",
            "Iteration 52, loss = 0.58904938\n",
            "Iteration 53, loss = 0.58852797\n",
            "Iteration 54, loss = 0.58812035\n",
            "Iteration 55, loss = 0.58762289\n",
            "Iteration 56, loss = 0.58712541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 57, loss = 0.58666104\n",
            "Iteration 58, loss = 0.58615629\n",
            "Iteration 59, loss = 0.58561809\n",
            "Iteration 60, loss = 0.58512577\n",
            "Iteration 61, loss = 0.58466198\n",
            "Iteration 62, loss = 0.58411778\n",
            "Iteration 63, loss = 0.58359615\n",
            "Iteration 64, loss = 0.58300139\n",
            "Iteration 65, loss = 0.58244389\n",
            "Iteration 66, loss = 0.58190597\n",
            "Iteration 67, loss = 0.58141029\n",
            "Iteration 68, loss = 0.58085057\n",
            "Iteration 69, loss = 0.58035273\n",
            "Iteration 70, loss = 0.57970416\n",
            "Iteration 71, loss = 0.57915712\n",
            "Iteration 72, loss = 0.57860550\n",
            "Iteration 73, loss = 0.57797929\n",
            "Iteration 74, loss = 0.57738233\n",
            "Iteration 75, loss = 0.57704390\n",
            "Iteration 76, loss = 0.57639729\n",
            "Iteration 77, loss = 0.57581682\n",
            "Iteration 78, loss = 0.57543752\n",
            "Iteration 79, loss = 0.57485995\n",
            "Iteration 80, loss = 0.57445793\n",
            "Iteration 81, loss = 0.57398971\n",
            "Iteration 82, loss = 0.57361636\n",
            "Iteration 83, loss = 0.57318059\n",
            "Iteration 84, loss = 0.57282423\n",
            "Iteration 85, loss = 0.57252176\n",
            "Iteration 86, loss = 0.57218478\n",
            "Iteration 87, loss = 0.57185258\n",
            "Iteration 88, loss = 0.57147761\n",
            "Iteration 89, loss = 0.57112446\n",
            "Iteration 90, loss = 0.57071716\n",
            "Iteration 91, loss = 0.57028615\n",
            "Iteration 92, loss = 0.56993985\n",
            "Iteration 93, loss = 0.56954471\n",
            "Iteration 94, loss = 0.56924308\n",
            "Iteration 95, loss = 0.56872118\n",
            "Iteration 96, loss = 0.56844243\n",
            "Iteration 97, loss = 0.56803433\n",
            "Iteration 98, loss = 0.56776972\n",
            "Iteration 99, loss = 0.56749363\n",
            "Iteration 100, loss = 0.56718606\n",
            "Iteration 101, loss = 0.56689778\n",
            "Iteration 102, loss = 0.56664384\n",
            "Iteration 103, loss = 0.56635293\n",
            "Iteration 104, loss = 0.56623406\n",
            "Iteration 105, loss = 0.56601177\n",
            "Iteration 106, loss = 0.56568366\n",
            "Iteration 107, loss = 0.56551641\n",
            "Iteration 108, loss = 0.56510360\n",
            "Iteration 109, loss = 0.56480059\n",
            "Iteration 110, loss = 0.56454979\n",
            "Iteration 111, loss = 0.56429531\n",
            "Iteration 112, loss = 0.56395747\n",
            "Iteration 113, loss = 0.56367286\n",
            "Iteration 114, loss = 0.56345796\n",
            "Iteration 115, loss = 0.56326450\n",
            "Iteration 116, loss = 0.56287617\n",
            "Iteration 117, loss = 0.56259053\n",
            "Iteration 118, loss = 0.56238330\n",
            "Iteration 119, loss = 0.56215496\n",
            "Iteration 120, loss = 0.56186900\n",
            "Iteration 121, loss = 0.56167566\n",
            "Iteration 122, loss = 0.56132266\n",
            "Iteration 123, loss = 0.56106314\n",
            "Iteration 124, loss = 0.56088367\n",
            "Iteration 125, loss = 0.56067779\n",
            "Iteration 126, loss = 0.56034009\n",
            "Iteration 127, loss = 0.56018241\n",
            "Iteration 128, loss = 0.55983784\n",
            "Iteration 129, loss = 0.55972969\n",
            "Iteration 130, loss = 0.55940300\n",
            "Iteration 131, loss = 0.55919037\n",
            "Iteration 132, loss = 0.55889555\n",
            "Iteration 133, loss = 0.55867239\n",
            "Iteration 134, loss = 0.55855201\n",
            "Iteration 135, loss = 0.55830447\n",
            "Iteration 136, loss = 0.55815356\n",
            "Iteration 137, loss = 0.55796883\n",
            "Iteration 138, loss = 0.55783932\n",
            "Iteration 139, loss = 0.55769593\n",
            "Iteration 140, loss = 0.55754655\n",
            "Iteration 141, loss = 0.55735505\n",
            "Iteration 142, loss = 0.55723094\n",
            "Iteration 143, loss = 0.55714053\n",
            "Iteration 144, loss = 0.55691044\n",
            "Iteration 145, loss = 0.55677425\n",
            "Iteration 146, loss = 0.55657889\n",
            "Iteration 147, loss = 0.55637529\n",
            "Iteration 148, loss = 0.55623800\n",
            "Iteration 149, loss = 0.55604883\n",
            "Iteration 150, loss = 0.55589847\n",
            "Iteration 151, loss = 0.55574381\n",
            "Iteration 152, loss = 0.55564436\n",
            "Iteration 153, loss = 0.55560322\n",
            "Iteration 154, loss = 0.55536007\n",
            "Iteration 155, loss = 0.55522516\n",
            "Iteration 156, loss = 0.55496692\n",
            "Iteration 157, loss = 0.55488775\n",
            "Iteration 158, loss = 0.55474647\n",
            "Iteration 159, loss = 0.55448185\n",
            "Iteration 160, loss = 0.55435752\n",
            "Iteration 161, loss = 0.55408465\n",
            "Iteration 162, loss = 0.55393144\n",
            "Iteration 163, loss = 0.55377149\n",
            "Iteration 164, loss = 0.55364084\n",
            "Iteration 165, loss = 0.55355824\n",
            "Iteration 166, loss = 0.55328245\n",
            "Iteration 167, loss = 0.55312906\n",
            "Iteration 168, loss = 0.55298317\n",
            "Iteration 169, loss = 0.55280599\n",
            "Iteration 170, loss = 0.55263238\n",
            "Iteration 171, loss = 0.55244581\n",
            "Iteration 172, loss = 0.55227972\n",
            "Iteration 173, loss = 0.55217219\n",
            "Iteration 174, loss = 0.55200676\n",
            "Iteration 175, loss = 0.55173998\n",
            "Iteration 176, loss = 0.55161848\n",
            "Iteration 177, loss = 0.55141735\n",
            "Iteration 178, loss = 0.55122916\n",
            "Iteration 179, loss = 0.55102234\n",
            "Iteration 180, loss = 0.55079965\n",
            "Iteration 181, loss = 0.55056503\n",
            "Iteration 182, loss = 0.55040293\n",
            "Iteration 183, loss = 0.55024137\n",
            "Iteration 184, loss = 0.55018047\n",
            "Iteration 185, loss = 0.55002542\n",
            "Iteration 186, loss = 0.54981871\n",
            "Iteration 187, loss = 0.54969793\n",
            "Iteration 188, loss = 0.54948811\n",
            "Iteration 189, loss = 0.54925702\n",
            "Iteration 190, loss = 0.54909133\n",
            "Iteration 191, loss = 0.54892199\n",
            "Iteration 192, loss = 0.54887892\n",
            "Iteration 193, loss = 0.54873508\n",
            "Iteration 194, loss = 0.54860612\n",
            "Iteration 195, loss = 0.54847942\n",
            "Iteration 196, loss = 0.54837547\n",
            "Iteration 197, loss = 0.54819570\n",
            "Iteration 198, loss = 0.54801947\n",
            "Iteration 199, loss = 0.54782665\n",
            "Iteration 200, loss = 0.54771213\n",
            "Iteration 1, loss = 0.62010582\n",
            "Iteration 2, loss = 0.61930514\n",
            "Iteration 3, loss = 0.61826319\n",
            "Iteration 4, loss = 0.61682911\n",
            "Iteration 5, loss = 0.61543360\n",
            "Iteration 6, loss = 0.61413706\n",
            "Iteration 7, loss = 0.61295141\n",
            "Iteration 8, loss = 0.61172441\n",
            "Iteration 9, loss = 0.61081242\n",
            "Iteration 10, loss = 0.60991460\n",
            "Iteration 11, loss = 0.60917637\n",
            "Iteration 12, loss = 0.60858965\n",
            "Iteration 13, loss = 0.60792078\n",
            "Iteration 14, loss = 0.60730029\n",
            "Iteration 15, loss = 0.60678248\n",
            "Iteration 16, loss = 0.60632478\n",
            "Iteration 17, loss = 0.60582791\n",
            "Iteration 18, loss = 0.60531306\n",
            "Iteration 19, loss = 0.60480910\n",
            "Iteration 20, loss = 0.60441098\n",
            "Iteration 21, loss = 0.60409300\n",
            "Iteration 22, loss = 0.60359792\n",
            "Iteration 23, loss = 0.60325513\n",
            "Iteration 24, loss = 0.60288068\n",
            "Iteration 25, loss = 0.60251982\n",
            "Iteration 26, loss = 0.60216641\n",
            "Iteration 27, loss = 0.60186024\n",
            "Iteration 28, loss = 0.60148385\n",
            "Iteration 29, loss = 0.60117667\n",
            "Iteration 30, loss = 0.60087078\n",
            "Iteration 31, loss = 0.60049222\n",
            "Iteration 32, loss = 0.60013302\n",
            "Iteration 33, loss = 0.59970023\n",
            "Iteration 34, loss = 0.59933655\n",
            "Iteration 35, loss = 0.59892240\n",
            "Iteration 36, loss = 0.59855545\n",
            "Iteration 37, loss = 0.59817110\n",
            "Iteration 38, loss = 0.59773733\n",
            "Iteration 39, loss = 0.59731738\n",
            "Iteration 40, loss = 0.59701270\n",
            "Iteration 41, loss = 0.59659154\n",
            "Iteration 42, loss = 0.59622730\n",
            "Iteration 43, loss = 0.59577996\n",
            "Iteration 44, loss = 0.59559981\n",
            "Iteration 45, loss = 0.59513820\n",
            "Iteration 46, loss = 0.59480242\n",
            "Iteration 47, loss = 0.59452497\n",
            "Iteration 48, loss = 0.59421464\n",
            "Iteration 49, loss = 0.59389969\n",
            "Iteration 50, loss = 0.59360376\n",
            "Iteration 51, loss = 0.59338566\n",
            "Iteration 52, loss = 0.59307514\n",
            "Iteration 53, loss = 0.59278326\n",
            "Iteration 54, loss = 0.59252864\n",
            "Iteration 55, loss = 0.59218267\n",
            "Iteration 56, loss = 0.59179838\n",
            "Iteration 57, loss = 0.59144292\n",
            "Iteration 58, loss = 0.59107432\n",
            "Iteration 59, loss = 0.59072772\n",
            "Iteration 60, loss = 0.59033379\n",
            "Iteration 61, loss = 0.58997606\n",
            "Iteration 62, loss = 0.58954634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 63, loss = 0.58924779\n",
            "Iteration 64, loss = 0.58890475\n",
            "Iteration 65, loss = 0.58864831\n",
            "Iteration 66, loss = 0.58836352\n",
            "Iteration 67, loss = 0.58815782\n",
            "Iteration 68, loss = 0.58783305\n",
            "Iteration 69, loss = 0.58756811\n",
            "Iteration 70, loss = 0.58721380\n",
            "Iteration 71, loss = 0.58694958\n",
            "Iteration 72, loss = 0.58664618\n",
            "Iteration 73, loss = 0.58633497\n",
            "Iteration 74, loss = 0.58601464\n",
            "Iteration 75, loss = 0.58576382\n",
            "Iteration 76, loss = 0.58549933\n",
            "Iteration 77, loss = 0.58529067\n",
            "Iteration 78, loss = 0.58500034\n",
            "Iteration 79, loss = 0.58465738\n",
            "Iteration 80, loss = 0.58453297\n",
            "Iteration 81, loss = 0.58413051\n",
            "Iteration 82, loss = 0.58400797\n",
            "Iteration 83, loss = 0.58362854\n",
            "Iteration 84, loss = 0.58342692\n",
            "Iteration 85, loss = 0.58325088\n",
            "Iteration 86, loss = 0.58308466\n",
            "Iteration 87, loss = 0.58297834\n",
            "Iteration 88, loss = 0.58282060\n",
            "Iteration 89, loss = 0.58260746\n",
            "Iteration 90, loss = 0.58236208\n",
            "Iteration 91, loss = 0.58209636\n",
            "Iteration 92, loss = 0.58198327\n",
            "Iteration 93, loss = 0.58176341\n",
            "Iteration 94, loss = 0.58164160\n",
            "Iteration 95, loss = 0.58140872\n",
            "Iteration 96, loss = 0.58129105\n",
            "Iteration 97, loss = 0.58108386\n",
            "Iteration 98, loss = 0.58097283\n",
            "Iteration 99, loss = 0.58070632\n",
            "Iteration 100, loss = 0.58053302\n",
            "Iteration 101, loss = 0.58039990\n",
            "Iteration 102, loss = 0.58017624\n",
            "Iteration 103, loss = 0.57999113\n",
            "Iteration 104, loss = 0.57984540\n",
            "Iteration 105, loss = 0.57970107\n",
            "Iteration 106, loss = 0.57950922\n",
            "Iteration 107, loss = 0.57948545\n",
            "Iteration 108, loss = 0.57919349\n",
            "Iteration 109, loss = 0.57905324\n",
            "Iteration 110, loss = 0.57889101\n",
            "Iteration 111, loss = 0.57872914\n",
            "Iteration 112, loss = 0.57862720\n",
            "Iteration 113, loss = 0.57851013\n",
            "Iteration 114, loss = 0.57835774\n",
            "Iteration 115, loss = 0.57827363\n",
            "Iteration 116, loss = 0.57808403\n",
            "Iteration 117, loss = 0.57797619\n",
            "Iteration 118, loss = 0.57786429\n",
            "Iteration 119, loss = 0.57780876\n",
            "Iteration 120, loss = 0.57769954\n",
            "Iteration 121, loss = 0.57759897\n",
            "Iteration 122, loss = 0.57748308\n",
            "Iteration 123, loss = 0.57731200\n",
            "Iteration 124, loss = 0.57724820\n",
            "Iteration 125, loss = 0.57711188\n",
            "Iteration 126, loss = 0.57700233\n",
            "Iteration 127, loss = 0.57690475\n",
            "Iteration 128, loss = 0.57675941\n",
            "Iteration 129, loss = 0.57675736\n",
            "Iteration 130, loss = 0.57667384\n",
            "Iteration 131, loss = 0.57661817\n",
            "Iteration 132, loss = 0.57641709\n",
            "Iteration 133, loss = 0.57628851\n",
            "Iteration 134, loss = 0.57625054\n",
            "Iteration 135, loss = 0.57607321\n",
            "Iteration 136, loss = 0.57602258\n",
            "Iteration 137, loss = 0.57590272\n",
            "Iteration 138, loss = 0.57586608\n",
            "Iteration 139, loss = 0.57581485\n",
            "Iteration 140, loss = 0.57567184\n",
            "Iteration 141, loss = 0.57558977\n",
            "Iteration 142, loss = 0.57546602\n",
            "Iteration 143, loss = 0.57543567\n",
            "Iteration 144, loss = 0.57535966\n",
            "Iteration 145, loss = 0.57534248\n",
            "Iteration 146, loss = 0.57522330\n",
            "Iteration 147, loss = 0.57510579\n",
            "Iteration 148, loss = 0.57502251\n",
            "Iteration 149, loss = 0.57493491\n",
            "Iteration 150, loss = 0.57488632\n",
            "Iteration 151, loss = 0.57483474\n",
            "Iteration 152, loss = 0.57481661\n",
            "Iteration 153, loss = 0.57477165\n",
            "Iteration 154, loss = 0.57468643\n",
            "Iteration 155, loss = 0.57459866\n",
            "Iteration 156, loss = 0.57443518\n",
            "Iteration 157, loss = 0.57446966\n",
            "Iteration 158, loss = 0.57436031\n",
            "Iteration 159, loss = 0.57432292\n",
            "Iteration 160, loss = 0.57429549\n",
            "Iteration 161, loss = 0.57417923\n",
            "Iteration 162, loss = 0.57408751\n",
            "Iteration 163, loss = 0.57398418\n",
            "Iteration 164, loss = 0.57403109\n",
            "Iteration 165, loss = 0.57398374\n",
            "Iteration 166, loss = 0.57381464\n",
            "Iteration 167, loss = 0.57374633\n",
            "Iteration 168, loss = 0.57370746\n",
            "Iteration 169, loss = 0.57363885\n",
            "Iteration 170, loss = 0.57350241\n",
            "Iteration 171, loss = 0.57338396\n",
            "Iteration 172, loss = 0.57331660\n",
            "Iteration 173, loss = 0.57328637\n",
            "Iteration 174, loss = 0.57322617\n",
            "Iteration 175, loss = 0.57311686\n",
            "Iteration 176, loss = 0.57308206\n",
            "Iteration 177, loss = 0.57297121\n",
            "Iteration 178, loss = 0.57289297\n",
            "Iteration 179, loss = 0.57281547\n",
            "Iteration 180, loss = 0.57273109\n",
            "Iteration 181, loss = 0.57260341\n",
            "Iteration 182, loss = 0.57259523\n",
            "Iteration 183, loss = 0.57252023\n",
            "Iteration 184, loss = 0.57246574\n",
            "Iteration 185, loss = 0.57240744\n",
            "Iteration 186, loss = 0.57231150\n",
            "Iteration 187, loss = 0.57229970\n",
            "Iteration 188, loss = 0.57216612\n",
            "Iteration 189, loss = 0.57206476\n",
            "Iteration 190, loss = 0.57199221\n",
            "Iteration 191, loss = 0.57194441\n",
            "Iteration 192, loss = 0.57183007\n",
            "Iteration 193, loss = 0.57177719\n",
            "Iteration 194, loss = 0.57174417\n",
            "Iteration 195, loss = 0.57168827\n",
            "Iteration 196, loss = 0.57156425\n",
            "Iteration 197, loss = 0.57145151\n",
            "Iteration 198, loss = 0.57137237\n",
            "Iteration 199, loss = 0.57128470\n",
            "Iteration 200, loss = 0.57123302\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 200 and for layer number 2 : 0.70625\n",
            "Iteration 1, loss = 0.84424359\n",
            "Iteration 2, loss = 0.81824608\n",
            "Iteration 3, loss = 0.78301818\n",
            "Iteration 4, loss = 0.74369898\n",
            "Iteration 5, loss = 0.70772940\n",
            "Iteration 6, loss = 0.67905372\n",
            "Iteration 7, loss = 0.65459215\n",
            "Iteration 8, loss = 0.63750703\n",
            "Iteration 9, loss = 0.62609078\n",
            "Iteration 10, loss = 0.61732840\n",
            "Iteration 11, loss = 0.61055362\n",
            "Iteration 12, loss = 0.60475642\n",
            "Iteration 13, loss = 0.60059365\n",
            "Iteration 14, loss = 0.59676833\n",
            "Iteration 15, loss = 0.59386373\n",
            "Iteration 16, loss = 0.59105742\n",
            "Iteration 17, loss = 0.58857593\n",
            "Iteration 18, loss = 0.58599616\n",
            "Iteration 19, loss = 0.58413312\n",
            "Iteration 20, loss = 0.58213654\n",
            "Iteration 21, loss = 0.58047540\n",
            "Iteration 22, loss = 0.57899956\n",
            "Iteration 23, loss = 0.57767412\n",
            "Iteration 24, loss = 0.57662470\n",
            "Iteration 25, loss = 0.57549965\n",
            "Iteration 26, loss = 0.57448619\n",
            "Iteration 27, loss = 0.57350458\n",
            "Iteration 28, loss = 0.57272338\n",
            "Iteration 29, loss = 0.57195199\n",
            "Iteration 30, loss = 0.57117292\n",
            "Iteration 31, loss = 0.57052970\n",
            "Iteration 32, loss = 0.56989653\n",
            "Iteration 33, loss = 0.56924336\n",
            "Iteration 34, loss = 0.56855370\n",
            "Iteration 35, loss = 0.56793890\n",
            "Iteration 36, loss = 0.56726158\n",
            "Iteration 37, loss = 0.56681235\n",
            "Iteration 38, loss = 0.56619017\n",
            "Iteration 39, loss = 0.56555914\n",
            "Iteration 40, loss = 0.56496310\n",
            "Iteration 41, loss = 0.56418147\n",
            "Iteration 42, loss = 0.56369303\n",
            "Iteration 43, loss = 0.56314826\n",
            "Iteration 44, loss = 0.56264112\n",
            "Iteration 45, loss = 0.56218156\n",
            "Iteration 46, loss = 0.56170246\n",
            "Iteration 47, loss = 0.56127065\n",
            "Iteration 48, loss = 0.56095972\n",
            "Iteration 49, loss = 0.56051664\n",
            "Iteration 50, loss = 0.56016687\n",
            "Iteration 51, loss = 0.55986462\n",
            "Iteration 52, loss = 0.55963340\n",
            "Iteration 53, loss = 0.55921424\n",
            "Iteration 54, loss = 0.55880487\n",
            "Iteration 55, loss = 0.55855142\n",
            "Iteration 56, loss = 0.55808873\n",
            "Iteration 57, loss = 0.55768277\n",
            "Iteration 58, loss = 0.55738430\n",
            "Iteration 59, loss = 0.55704771\n",
            "Iteration 60, loss = 0.55673806\n",
            "Iteration 61, loss = 0.55628107\n",
            "Iteration 62, loss = 0.55598312\n",
            "Iteration 63, loss = 0.55560140\n",
            "Iteration 64, loss = 0.55527998\n",
            "Iteration 65, loss = 0.55510876\n",
            "Iteration 66, loss = 0.55472229\n",
            "Iteration 67, loss = 0.55441299\n",
            "Iteration 68, loss = 0.55417873\n",
            "Iteration 69, loss = 0.55397098\n",
            "Iteration 70, loss = 0.55377076\n",
            "Iteration 71, loss = 0.55358584\n",
            "Iteration 72, loss = 0.55322471\n",
            "Iteration 73, loss = 0.55307920\n",
            "Iteration 74, loss = 0.55268158\n",
            "Iteration 75, loss = 0.55243838\n",
            "Iteration 76, loss = 0.55227362\n",
            "Iteration 77, loss = 0.55205947\n",
            "Iteration 78, loss = 0.55176691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 79, loss = 0.55148431\n",
            "Iteration 80, loss = 0.55117819\n",
            "Iteration 81, loss = 0.55094568\n",
            "Iteration 82, loss = 0.55068506\n",
            "Iteration 83, loss = 0.55041758\n",
            "Iteration 84, loss = 0.55009782\n",
            "Iteration 85, loss = 0.54980800\n",
            "Iteration 86, loss = 0.54956132\n",
            "Iteration 87, loss = 0.54938696\n",
            "Iteration 88, loss = 0.54914225\n",
            "Iteration 89, loss = 0.54883945\n",
            "Iteration 90, loss = 0.54857565\n",
            "Iteration 91, loss = 0.54837773\n",
            "Iteration 92, loss = 0.54813146\n",
            "Iteration 93, loss = 0.54776457\n",
            "Iteration 94, loss = 0.54734763\n",
            "Iteration 95, loss = 0.54709515\n",
            "Iteration 96, loss = 0.54679073\n",
            "Iteration 97, loss = 0.54658268\n",
            "Iteration 98, loss = 0.54627656\n",
            "Iteration 99, loss = 0.54604382\n",
            "Iteration 100, loss = 0.54579330\n",
            "Iteration 101, loss = 0.54559553\n",
            "Iteration 102, loss = 0.54534505\n",
            "Iteration 103, loss = 0.54529176\n",
            "Iteration 104, loss = 0.54483518\n",
            "Iteration 105, loss = 0.54470627\n",
            "Iteration 106, loss = 0.54448019\n",
            "Iteration 107, loss = 0.54428649\n",
            "Iteration 108, loss = 0.54412449\n",
            "Iteration 109, loss = 0.54402894\n",
            "Iteration 110, loss = 0.54384063\n",
            "Iteration 111, loss = 0.54366888\n",
            "Iteration 112, loss = 0.54333863\n",
            "Iteration 113, loss = 0.54319544\n",
            "Iteration 114, loss = 0.54289790\n",
            "Iteration 115, loss = 0.54277961\n",
            "Iteration 116, loss = 0.54261873\n",
            "Iteration 117, loss = 0.54243487\n",
            "Iteration 118, loss = 0.54229018\n",
            "Iteration 119, loss = 0.54212852\n",
            "Iteration 120, loss = 0.54200008\n",
            "Iteration 121, loss = 0.54194031\n",
            "Iteration 122, loss = 0.54168361\n",
            "Iteration 123, loss = 0.54151179\n",
            "Iteration 124, loss = 0.54123880\n",
            "Iteration 125, loss = 0.54108897\n",
            "Iteration 126, loss = 0.54103856\n",
            "Iteration 127, loss = 0.54088066\n",
            "Iteration 128, loss = 0.54072815\n",
            "Iteration 129, loss = 0.54062706\n",
            "Iteration 130, loss = 0.54044568\n",
            "Iteration 131, loss = 0.54035252\n",
            "Iteration 132, loss = 0.54026052\n",
            "Iteration 133, loss = 0.54013545\n",
            "Iteration 134, loss = 0.54012645\n",
            "Iteration 135, loss = 0.54000457\n",
            "Iteration 136, loss = 0.53986669\n",
            "Iteration 137, loss = 0.53978250\n",
            "Iteration 138, loss = 0.53960434\n",
            "Iteration 139, loss = 0.53949676\n",
            "Iteration 140, loss = 0.53943493\n",
            "Iteration 141, loss = 0.53923818\n",
            "Iteration 142, loss = 0.53912301\n",
            "Iteration 143, loss = 0.53894386\n",
            "Iteration 144, loss = 0.53878587\n",
            "Iteration 145, loss = 0.53861539\n",
            "Iteration 146, loss = 0.53862490\n",
            "Iteration 147, loss = 0.53832874\n",
            "Iteration 148, loss = 0.53820565\n",
            "Iteration 149, loss = 0.53811370\n",
            "Iteration 150, loss = 0.53803219\n",
            "Iteration 151, loss = 0.53794549\n",
            "Iteration 152, loss = 0.53784161\n",
            "Iteration 153, loss = 0.53767880\n",
            "Iteration 154, loss = 0.53757567\n",
            "Iteration 155, loss = 0.53749812\n",
            "Iteration 156, loss = 0.53738471\n",
            "Iteration 157, loss = 0.53737393\n",
            "Iteration 158, loss = 0.53719187\n",
            "Iteration 159, loss = 0.53715987\n",
            "Iteration 160, loss = 0.53705314\n",
            "Iteration 161, loss = 0.53689460\n",
            "Iteration 162, loss = 0.53685175\n",
            "Iteration 163, loss = 0.53661336\n",
            "Iteration 164, loss = 0.53647136\n",
            "Iteration 165, loss = 0.53637743\n",
            "Iteration 166, loss = 0.53615959\n",
            "Iteration 167, loss = 0.53609804\n",
            "Iteration 168, loss = 0.53587348\n",
            "Iteration 169, loss = 0.53579179\n",
            "Iteration 170, loss = 0.53565276\n",
            "Iteration 171, loss = 0.53550374\n",
            "Iteration 172, loss = 0.53537312\n",
            "Iteration 173, loss = 0.53522639\n",
            "Iteration 174, loss = 0.53507235\n",
            "Iteration 175, loss = 0.53501650\n",
            "Iteration 176, loss = 0.53496249\n",
            "Iteration 177, loss = 0.53482123\n",
            "Iteration 178, loss = 0.53462095\n",
            "Iteration 179, loss = 0.53446862\n",
            "Iteration 180, loss = 0.53428219\n",
            "Iteration 181, loss = 0.53417556\n",
            "Iteration 182, loss = 0.53403191\n",
            "Iteration 183, loss = 0.53391473\n",
            "Iteration 184, loss = 0.53384538\n",
            "Iteration 185, loss = 0.53366703\n",
            "Iteration 186, loss = 0.53361035\n",
            "Iteration 187, loss = 0.53352389\n",
            "Iteration 188, loss = 0.53347149\n",
            "Iteration 189, loss = 0.53338269\n",
            "Iteration 190, loss = 0.53327499\n",
            "Iteration 191, loss = 0.53312129\n",
            "Iteration 192, loss = 0.53316938\n",
            "Iteration 193, loss = 0.53293771\n",
            "Iteration 194, loss = 0.53289439\n",
            "Iteration 195, loss = 0.53279676\n",
            "Iteration 196, loss = 0.53272346\n",
            "Iteration 197, loss = 0.53276876\n",
            "Iteration 198, loss = 0.53273791\n",
            "Iteration 199, loss = 0.53269175\n",
            "Iteration 200, loss = 0.53250020\n",
            "Iteration 1, loss = 0.84514500\n",
            "Iteration 2, loss = 0.81909328\n",
            "Iteration 3, loss = 0.78652477\n",
            "Iteration 4, loss = 0.74811660\n",
            "Iteration 5, loss = 0.71383507\n",
            "Iteration 6, loss = 0.68552273\n",
            "Iteration 7, loss = 0.66168581\n",
            "Iteration 8, loss = 0.64338982\n",
            "Iteration 9, loss = 0.63160645\n",
            "Iteration 10, loss = 0.62136865\n",
            "Iteration 11, loss = 0.61380923\n",
            "Iteration 12, loss = 0.60791210\n",
            "Iteration 13, loss = 0.60355558\n",
            "Iteration 14, loss = 0.59965695\n",
            "Iteration 15, loss = 0.59624464\n",
            "Iteration 16, loss = 0.59328627\n",
            "Iteration 17, loss = 0.59072275\n",
            "Iteration 18, loss = 0.58830866\n",
            "Iteration 19, loss = 0.58624201\n",
            "Iteration 20, loss = 0.58419678\n",
            "Iteration 21, loss = 0.58224472\n",
            "Iteration 22, loss = 0.58048793\n",
            "Iteration 23, loss = 0.57892935\n",
            "Iteration 24, loss = 0.57765823\n",
            "Iteration 25, loss = 0.57634431\n",
            "Iteration 26, loss = 0.57526519\n",
            "Iteration 27, loss = 0.57409528\n",
            "Iteration 28, loss = 0.57316406\n",
            "Iteration 29, loss = 0.57222788\n",
            "Iteration 30, loss = 0.57132622\n",
            "Iteration 31, loss = 0.57066822\n",
            "Iteration 32, loss = 0.56993909\n",
            "Iteration 33, loss = 0.56926119\n",
            "Iteration 34, loss = 0.56861361\n",
            "Iteration 35, loss = 0.56790836\n",
            "Iteration 36, loss = 0.56732057\n",
            "Iteration 37, loss = 0.56682809\n",
            "Iteration 38, loss = 0.56615271\n",
            "Iteration 39, loss = 0.56557409\n",
            "Iteration 40, loss = 0.56501093\n",
            "Iteration 41, loss = 0.56427012\n",
            "Iteration 42, loss = 0.56383162\n",
            "Iteration 43, loss = 0.56324564\n",
            "Iteration 44, loss = 0.56257550\n",
            "Iteration 45, loss = 0.56205907\n",
            "Iteration 46, loss = 0.56161548\n",
            "Iteration 47, loss = 0.56097687\n",
            "Iteration 48, loss = 0.56050221\n",
            "Iteration 49, loss = 0.55997645\n",
            "Iteration 50, loss = 0.55950729\n",
            "Iteration 51, loss = 0.55905100\n",
            "Iteration 52, loss = 0.55864909\n",
            "Iteration 53, loss = 0.55822691\n",
            "Iteration 54, loss = 0.55777537\n",
            "Iteration 55, loss = 0.55742865\n",
            "Iteration 56, loss = 0.55691692\n",
            "Iteration 57, loss = 0.55641760\n",
            "Iteration 58, loss = 0.55598568\n",
            "Iteration 59, loss = 0.55547413\n",
            "Iteration 60, loss = 0.55496972\n",
            "Iteration 61, loss = 0.55443302\n",
            "Iteration 62, loss = 0.55393228\n",
            "Iteration 63, loss = 0.55347585\n",
            "Iteration 64, loss = 0.55302202\n",
            "Iteration 65, loss = 0.55257241\n",
            "Iteration 66, loss = 0.55210691\n",
            "Iteration 67, loss = 0.55157379\n",
            "Iteration 68, loss = 0.55117839\n",
            "Iteration 69, loss = 0.55073343\n",
            "Iteration 70, loss = 0.55030951\n",
            "Iteration 71, loss = 0.54994572\n",
            "Iteration 72, loss = 0.54945093\n",
            "Iteration 73, loss = 0.54910117\n",
            "Iteration 74, loss = 0.54861316\n",
            "Iteration 75, loss = 0.54822427\n",
            "Iteration 76, loss = 0.54796648\n",
            "Iteration 77, loss = 0.54761925\n",
            "Iteration 78, loss = 0.54726992\n",
            "Iteration 79, loss = 0.54702011\n",
            "Iteration 80, loss = 0.54682356\n",
            "Iteration 81, loss = 0.54654362\n",
            "Iteration 82, loss = 0.54640230\n",
            "Iteration 83, loss = 0.54610336\n",
            "Iteration 84, loss = 0.54585375\n",
            "Iteration 85, loss = 0.54554325\n",
            "Iteration 86, loss = 0.54522963\n",
            "Iteration 87, loss = 0.54487368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 88, loss = 0.54453649\n",
            "Iteration 89, loss = 0.54424273\n",
            "Iteration 90, loss = 0.54393287\n",
            "Iteration 91, loss = 0.54370091\n",
            "Iteration 92, loss = 0.54354063\n",
            "Iteration 93, loss = 0.54315551\n",
            "Iteration 94, loss = 0.54284090\n",
            "Iteration 95, loss = 0.54260775\n",
            "Iteration 96, loss = 0.54226539\n",
            "Iteration 97, loss = 0.54218460\n",
            "Iteration 98, loss = 0.54188639\n",
            "Iteration 99, loss = 0.54171213\n",
            "Iteration 100, loss = 0.54137443\n",
            "Iteration 101, loss = 0.54115176\n",
            "Iteration 102, loss = 0.54098741\n",
            "Iteration 103, loss = 0.54078534\n",
            "Iteration 104, loss = 0.54053260\n",
            "Iteration 105, loss = 0.54039428\n",
            "Iteration 106, loss = 0.54021413\n",
            "Iteration 107, loss = 0.54002222\n",
            "Iteration 108, loss = 0.53982397\n",
            "Iteration 109, loss = 0.53977123\n",
            "Iteration 110, loss = 0.53963840\n",
            "Iteration 111, loss = 0.53947559\n",
            "Iteration 112, loss = 0.53924744\n",
            "Iteration 113, loss = 0.53900389\n",
            "Iteration 114, loss = 0.53876616\n",
            "Iteration 115, loss = 0.53869541\n",
            "Iteration 116, loss = 0.53860712\n",
            "Iteration 117, loss = 0.53842544\n",
            "Iteration 118, loss = 0.53824884\n",
            "Iteration 119, loss = 0.53811920\n",
            "Iteration 120, loss = 0.53789437\n",
            "Iteration 121, loss = 0.53774857\n",
            "Iteration 122, loss = 0.53752471\n",
            "Iteration 123, loss = 0.53742103\n",
            "Iteration 124, loss = 0.53716134\n",
            "Iteration 125, loss = 0.53690587\n",
            "Iteration 126, loss = 0.53674444\n",
            "Iteration 127, loss = 0.53646586\n",
            "Iteration 128, loss = 0.53635349\n",
            "Iteration 129, loss = 0.53616121\n",
            "Iteration 130, loss = 0.53596747\n",
            "Iteration 131, loss = 0.53575548\n",
            "Iteration 132, loss = 0.53558699\n",
            "Iteration 133, loss = 0.53546551\n",
            "Iteration 134, loss = 0.53532027\n",
            "Iteration 135, loss = 0.53521455\n",
            "Iteration 136, loss = 0.53504836\n",
            "Iteration 137, loss = 0.53493607\n",
            "Iteration 138, loss = 0.53485314\n",
            "Iteration 139, loss = 0.53471083\n",
            "Iteration 140, loss = 0.53461208\n",
            "Iteration 141, loss = 0.53447452\n",
            "Iteration 142, loss = 0.53424195\n",
            "Iteration 143, loss = 0.53399683\n",
            "Iteration 144, loss = 0.53371405\n",
            "Iteration 145, loss = 0.53362581\n",
            "Iteration 146, loss = 0.53337330\n",
            "Iteration 147, loss = 0.53311798\n",
            "Iteration 148, loss = 0.53298677\n",
            "Iteration 149, loss = 0.53284512\n",
            "Iteration 150, loss = 0.53266775\n",
            "Iteration 151, loss = 0.53245484\n",
            "Iteration 152, loss = 0.53228468\n",
            "Iteration 153, loss = 0.53215305\n",
            "Iteration 154, loss = 0.53199622\n",
            "Iteration 155, loss = 0.53182095\n",
            "Iteration 156, loss = 0.53163534\n",
            "Iteration 157, loss = 0.53159536\n",
            "Iteration 158, loss = 0.53145652\n",
            "Iteration 159, loss = 0.53141267\n",
            "Iteration 160, loss = 0.53127749\n",
            "Iteration 161, loss = 0.53112360\n",
            "Iteration 162, loss = 0.53115639\n",
            "Iteration 163, loss = 0.53093468\n",
            "Iteration 164, loss = 0.53082080\n",
            "Iteration 165, loss = 0.53068017\n",
            "Iteration 166, loss = 0.53048262\n",
            "Iteration 167, loss = 0.53045526\n",
            "Iteration 168, loss = 0.53036890\n",
            "Iteration 169, loss = 0.53021732\n",
            "Iteration 170, loss = 0.53008398\n",
            "Iteration 171, loss = 0.52999110\n",
            "Iteration 172, loss = 0.52978625\n",
            "Iteration 173, loss = 0.52965636\n",
            "Iteration 174, loss = 0.52944824\n",
            "Iteration 175, loss = 0.52936214\n",
            "Iteration 176, loss = 0.52919847\n",
            "Iteration 177, loss = 0.52909706\n",
            "Iteration 178, loss = 0.52888302\n",
            "Iteration 179, loss = 0.52877652\n",
            "Iteration 180, loss = 0.52863237\n",
            "Iteration 181, loss = 0.52850506\n",
            "Iteration 182, loss = 0.52842043\n",
            "Iteration 183, loss = 0.52830067\n",
            "Iteration 184, loss = 0.52824107\n",
            "Iteration 185, loss = 0.52814023\n",
            "Iteration 186, loss = 0.52798196\n",
            "Iteration 187, loss = 0.52799705\n",
            "Iteration 188, loss = 0.52789158\n",
            "Iteration 189, loss = 0.52784611\n",
            "Iteration 190, loss = 0.52782159\n",
            "Iteration 191, loss = 0.52758277\n",
            "Iteration 192, loss = 0.52754709\n",
            "Iteration 193, loss = 0.52731328\n",
            "Iteration 194, loss = 0.52724128\n",
            "Iteration 195, loss = 0.52717716\n",
            "Iteration 196, loss = 0.52702696\n",
            "Iteration 197, loss = 0.52707108\n",
            "Iteration 198, loss = 0.52693569\n",
            "Iteration 199, loss = 0.52678816\n",
            "Iteration 200, loss = 0.52657006\n",
            "Iteration 1, loss = 0.86511432\n",
            "Iteration 2, loss = 0.83659966\n",
            "Iteration 3, loss = 0.80055969\n",
            "Iteration 4, loss = 0.75853561\n",
            "Iteration 5, loss = 0.72086132\n",
            "Iteration 6, loss = 0.68969566\n",
            "Iteration 7, loss = 0.66421861\n",
            "Iteration 8, loss = 0.64434004\n",
            "Iteration 9, loss = 0.63160029\n",
            "Iteration 10, loss = 0.62011071\n",
            "Iteration 11, loss = 0.61155999\n",
            "Iteration 12, loss = 0.60497986\n",
            "Iteration 13, loss = 0.59971010\n",
            "Iteration 14, loss = 0.59516710\n",
            "Iteration 15, loss = 0.59089636\n",
            "Iteration 16, loss = 0.58736900\n",
            "Iteration 17, loss = 0.58433935\n",
            "Iteration 18, loss = 0.58137018\n",
            "Iteration 19, loss = 0.57880625\n",
            "Iteration 20, loss = 0.57652473\n",
            "Iteration 21, loss = 0.57419227\n",
            "Iteration 22, loss = 0.57221919\n",
            "Iteration 23, loss = 0.57043440\n",
            "Iteration 24, loss = 0.56876358\n",
            "Iteration 25, loss = 0.56716867\n",
            "Iteration 26, loss = 0.56584442\n",
            "Iteration 27, loss = 0.56433635\n",
            "Iteration 28, loss = 0.56311521\n",
            "Iteration 29, loss = 0.56189909\n",
            "Iteration 30, loss = 0.56081779\n",
            "Iteration 31, loss = 0.55979345\n",
            "Iteration 32, loss = 0.55889623\n",
            "Iteration 33, loss = 0.55803597\n",
            "Iteration 34, loss = 0.55720608\n",
            "Iteration 35, loss = 0.55645261\n",
            "Iteration 36, loss = 0.55572053\n",
            "Iteration 37, loss = 0.55504676\n",
            "Iteration 38, loss = 0.55442651\n",
            "Iteration 39, loss = 0.55373834\n",
            "Iteration 40, loss = 0.55307272\n",
            "Iteration 41, loss = 0.55223474\n",
            "Iteration 42, loss = 0.55169607\n",
            "Iteration 43, loss = 0.55108669\n",
            "Iteration 44, loss = 0.55038771\n",
            "Iteration 45, loss = 0.54992049\n",
            "Iteration 46, loss = 0.54944630\n",
            "Iteration 47, loss = 0.54894736\n",
            "Iteration 48, loss = 0.54847299\n",
            "Iteration 49, loss = 0.54798903\n",
            "Iteration 50, loss = 0.54744079\n",
            "Iteration 51, loss = 0.54699818\n",
            "Iteration 52, loss = 0.54644728\n",
            "Iteration 53, loss = 0.54582215\n",
            "Iteration 54, loss = 0.54523601\n",
            "Iteration 55, loss = 0.54486145\n",
            "Iteration 56, loss = 0.54441470\n",
            "Iteration 57, loss = 0.54396780\n",
            "Iteration 58, loss = 0.54354432\n",
            "Iteration 59, loss = 0.54317617\n",
            "Iteration 60, loss = 0.54271414\n",
            "Iteration 61, loss = 0.54228241\n",
            "Iteration 62, loss = 0.54187862\n",
            "Iteration 63, loss = 0.54145488\n",
            "Iteration 64, loss = 0.54105818\n",
            "Iteration 65, loss = 0.54070902\n",
            "Iteration 66, loss = 0.54033059\n",
            "Iteration 67, loss = 0.53986242\n",
            "Iteration 68, loss = 0.53947009\n",
            "Iteration 69, loss = 0.53907710\n",
            "Iteration 70, loss = 0.53872273\n",
            "Iteration 71, loss = 0.53852677\n",
            "Iteration 72, loss = 0.53825027\n",
            "Iteration 73, loss = 0.53802444\n",
            "Iteration 74, loss = 0.53759156\n",
            "Iteration 75, loss = 0.53719528\n",
            "Iteration 76, loss = 0.53695689\n",
            "Iteration 77, loss = 0.53658347\n",
            "Iteration 78, loss = 0.53635432\n",
            "Iteration 79, loss = 0.53605595\n",
            "Iteration 80, loss = 0.53582474\n",
            "Iteration 81, loss = 0.53554773\n",
            "Iteration 82, loss = 0.53535155\n",
            "Iteration 83, loss = 0.53515822\n",
            "Iteration 84, loss = 0.53487830\n",
            "Iteration 85, loss = 0.53471838\n",
            "Iteration 86, loss = 0.53452936\n",
            "Iteration 87, loss = 0.53428679\n",
            "Iteration 88, loss = 0.53411485\n",
            "Iteration 89, loss = 0.53393016\n",
            "Iteration 90, loss = 0.53375943\n",
            "Iteration 91, loss = 0.53365426\n",
            "Iteration 92, loss = 0.53351596\n",
            "Iteration 93, loss = 0.53327774\n",
            "Iteration 94, loss = 0.53314514\n",
            "Iteration 95, loss = 0.53281641\n",
            "Iteration 96, loss = 0.53262599\n",
            "Iteration 97, loss = 0.53239384\n",
            "Iteration 98, loss = 0.53232582\n",
            "Iteration 99, loss = 0.53211350\n",
            "Iteration 100, loss = 0.53190029\n",
            "Iteration 101, loss = 0.53173212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 102, loss = 0.53161053\n",
            "Iteration 103, loss = 0.53145592\n",
            "Iteration 104, loss = 0.53128751\n",
            "Iteration 105, loss = 0.53120559\n",
            "Iteration 106, loss = 0.53110726\n",
            "Iteration 107, loss = 0.53099419\n",
            "Iteration 108, loss = 0.53085608\n",
            "Iteration 109, loss = 0.53079898\n",
            "Iteration 110, loss = 0.53075564\n",
            "Iteration 111, loss = 0.53057516\n",
            "Iteration 112, loss = 0.53049676\n",
            "Iteration 113, loss = 0.53038255\n",
            "Iteration 114, loss = 0.53021973\n",
            "Iteration 115, loss = 0.53023859\n",
            "Iteration 116, loss = 0.53017259\n",
            "Iteration 117, loss = 0.53007937\n",
            "Iteration 118, loss = 0.52994985\n",
            "Iteration 119, loss = 0.52982128\n",
            "Iteration 120, loss = 0.52974964\n",
            "Iteration 121, loss = 0.52960121\n",
            "Iteration 122, loss = 0.52940096\n",
            "Iteration 123, loss = 0.52934112\n",
            "Iteration 124, loss = 0.52931373\n",
            "Iteration 125, loss = 0.52915815\n",
            "Iteration 126, loss = 0.52909309\n",
            "Iteration 127, loss = 0.52895152\n",
            "Iteration 128, loss = 0.52882711\n",
            "Iteration 129, loss = 0.52876681\n",
            "Iteration 130, loss = 0.52866889\n",
            "Iteration 131, loss = 0.52865310\n",
            "Iteration 132, loss = 0.52858254\n",
            "Iteration 133, loss = 0.52862777\n",
            "Iteration 134, loss = 0.52852957\n",
            "Iteration 135, loss = 0.52859374\n",
            "Iteration 136, loss = 0.52849947\n",
            "Iteration 137, loss = 0.52844982\n",
            "Iteration 138, loss = 0.52852134\n",
            "Iteration 139, loss = 0.52836411\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.84459665\n",
            "Iteration 2, loss = 0.81870228\n",
            "Iteration 3, loss = 0.78595389\n",
            "Iteration 4, loss = 0.74703750\n",
            "Iteration 5, loss = 0.71258339\n",
            "Iteration 6, loss = 0.68531183\n",
            "Iteration 7, loss = 0.66229352\n",
            "Iteration 8, loss = 0.64435029\n",
            "Iteration 9, loss = 0.63245378\n",
            "Iteration 10, loss = 0.62203620\n",
            "Iteration 11, loss = 0.61422600\n",
            "Iteration 12, loss = 0.60819304\n",
            "Iteration 13, loss = 0.60394945\n",
            "Iteration 14, loss = 0.60001920\n",
            "Iteration 15, loss = 0.59682229\n",
            "Iteration 16, loss = 0.59409794\n",
            "Iteration 17, loss = 0.59187271\n",
            "Iteration 18, loss = 0.58971012\n",
            "Iteration 19, loss = 0.58782321\n",
            "Iteration 20, loss = 0.58622969\n",
            "Iteration 21, loss = 0.58456990\n",
            "Iteration 22, loss = 0.58308345\n",
            "Iteration 23, loss = 0.58175424\n",
            "Iteration 24, loss = 0.58053072\n",
            "Iteration 25, loss = 0.57943520\n",
            "Iteration 26, loss = 0.57847461\n",
            "Iteration 27, loss = 0.57744098\n",
            "Iteration 28, loss = 0.57649424\n",
            "Iteration 29, loss = 0.57561182\n",
            "Iteration 30, loss = 0.57472458\n",
            "Iteration 31, loss = 0.57386337\n",
            "Iteration 32, loss = 0.57320966\n",
            "Iteration 33, loss = 0.57256386\n",
            "Iteration 34, loss = 0.57190437\n",
            "Iteration 35, loss = 0.57129547\n",
            "Iteration 36, loss = 0.57075666\n",
            "Iteration 37, loss = 0.57013798\n",
            "Iteration 38, loss = 0.56964642\n",
            "Iteration 39, loss = 0.56913319\n",
            "Iteration 40, loss = 0.56867031\n",
            "Iteration 41, loss = 0.56805439\n",
            "Iteration 42, loss = 0.56757060\n",
            "Iteration 43, loss = 0.56705874\n",
            "Iteration 44, loss = 0.56652038\n",
            "Iteration 45, loss = 0.56613455\n",
            "Iteration 46, loss = 0.56564884\n",
            "Iteration 47, loss = 0.56532256\n",
            "Iteration 48, loss = 0.56486247\n",
            "Iteration 49, loss = 0.56442581\n",
            "Iteration 50, loss = 0.56400200\n",
            "Iteration 51, loss = 0.56363125\n",
            "Iteration 52, loss = 0.56319439\n",
            "Iteration 53, loss = 0.56270183\n",
            "Iteration 54, loss = 0.56225417\n",
            "Iteration 55, loss = 0.56195227\n",
            "Iteration 56, loss = 0.56154741\n",
            "Iteration 57, loss = 0.56111386\n",
            "Iteration 58, loss = 0.56077495\n",
            "Iteration 59, loss = 0.56040345\n",
            "Iteration 60, loss = 0.56005605\n",
            "Iteration 61, loss = 0.55961365\n",
            "Iteration 62, loss = 0.55924774\n",
            "Iteration 63, loss = 0.55888500\n",
            "Iteration 64, loss = 0.55855013\n",
            "Iteration 65, loss = 0.55820503\n",
            "Iteration 66, loss = 0.55783153\n",
            "Iteration 67, loss = 0.55740062\n",
            "Iteration 68, loss = 0.55713550\n",
            "Iteration 69, loss = 0.55676995\n",
            "Iteration 70, loss = 0.55640435\n",
            "Iteration 71, loss = 0.55623051\n",
            "Iteration 72, loss = 0.55586898\n",
            "Iteration 73, loss = 0.55551588\n",
            "Iteration 74, loss = 0.55517806\n",
            "Iteration 75, loss = 0.55474794\n",
            "Iteration 76, loss = 0.55447635\n",
            "Iteration 77, loss = 0.55405120\n",
            "Iteration 78, loss = 0.55367833\n",
            "Iteration 79, loss = 0.55337910\n",
            "Iteration 80, loss = 0.55296347\n",
            "Iteration 81, loss = 0.55258527\n",
            "Iteration 82, loss = 0.55233845\n",
            "Iteration 83, loss = 0.55204456\n",
            "Iteration 84, loss = 0.55171987\n",
            "Iteration 85, loss = 0.55152066\n",
            "Iteration 86, loss = 0.55125547\n",
            "Iteration 87, loss = 0.55102926\n",
            "Iteration 88, loss = 0.55078327\n",
            "Iteration 89, loss = 0.55049191\n",
            "Iteration 90, loss = 0.55022934\n",
            "Iteration 91, loss = 0.55002248\n",
            "Iteration 92, loss = 0.54970664\n",
            "Iteration 93, loss = 0.54939765\n",
            "Iteration 94, loss = 0.54915329\n",
            "Iteration 95, loss = 0.54878256\n",
            "Iteration 96, loss = 0.54848819\n",
            "Iteration 97, loss = 0.54816994\n",
            "Iteration 98, loss = 0.54797776\n",
            "Iteration 99, loss = 0.54764892\n",
            "Iteration 100, loss = 0.54741962\n",
            "Iteration 101, loss = 0.54716725\n",
            "Iteration 102, loss = 0.54694231\n",
            "Iteration 103, loss = 0.54675451\n",
            "Iteration 104, loss = 0.54660119\n",
            "Iteration 105, loss = 0.54638037\n",
            "Iteration 106, loss = 0.54624164\n",
            "Iteration 107, loss = 0.54596770\n",
            "Iteration 108, loss = 0.54578311\n",
            "Iteration 109, loss = 0.54553795\n",
            "Iteration 110, loss = 0.54538834\n",
            "Iteration 111, loss = 0.54519370\n",
            "Iteration 112, loss = 0.54493799\n",
            "Iteration 113, loss = 0.54472679\n",
            "Iteration 114, loss = 0.54447468\n",
            "Iteration 115, loss = 0.54438885\n",
            "Iteration 116, loss = 0.54411089\n",
            "Iteration 117, loss = 0.54395248\n",
            "Iteration 118, loss = 0.54370300\n",
            "Iteration 119, loss = 0.54354589\n",
            "Iteration 120, loss = 0.54332472\n",
            "Iteration 121, loss = 0.54318818\n",
            "Iteration 122, loss = 0.54290722\n",
            "Iteration 123, loss = 0.54273256\n",
            "Iteration 124, loss = 0.54249093\n",
            "Iteration 125, loss = 0.54230916\n",
            "Iteration 126, loss = 0.54212763\n",
            "Iteration 127, loss = 0.54181847\n",
            "Iteration 128, loss = 0.54165533\n",
            "Iteration 129, loss = 0.54141417\n",
            "Iteration 130, loss = 0.54112551\n",
            "Iteration 131, loss = 0.54102938\n",
            "Iteration 132, loss = 0.54077312\n",
            "Iteration 133, loss = 0.54061478\n",
            "Iteration 134, loss = 0.54037494\n",
            "Iteration 135, loss = 0.54008000\n",
            "Iteration 136, loss = 0.53980382\n",
            "Iteration 137, loss = 0.53957254\n",
            "Iteration 138, loss = 0.53941135\n",
            "Iteration 139, loss = 0.53915533\n",
            "Iteration 140, loss = 0.53879635\n",
            "Iteration 141, loss = 0.53861513\n",
            "Iteration 142, loss = 0.53826862\n",
            "Iteration 143, loss = 0.53797140\n",
            "Iteration 144, loss = 0.53767791\n",
            "Iteration 145, loss = 0.53758785\n",
            "Iteration 146, loss = 0.53735928\n",
            "Iteration 147, loss = 0.53707884\n",
            "Iteration 148, loss = 0.53685146\n",
            "Iteration 149, loss = 0.53667185\n",
            "Iteration 150, loss = 0.53646668\n",
            "Iteration 151, loss = 0.53629684\n",
            "Iteration 152, loss = 0.53600397\n",
            "Iteration 153, loss = 0.53586613\n",
            "Iteration 154, loss = 0.53557390\n",
            "Iteration 155, loss = 0.53541893\n",
            "Iteration 156, loss = 0.53522610\n",
            "Iteration 157, loss = 0.53501203\n",
            "Iteration 158, loss = 0.53479968\n",
            "Iteration 159, loss = 0.53467419\n",
            "Iteration 160, loss = 0.53446330\n",
            "Iteration 161, loss = 0.53430320\n",
            "Iteration 162, loss = 0.53421672\n",
            "Iteration 163, loss = 0.53408747\n",
            "Iteration 164, loss = 0.53395943\n",
            "Iteration 165, loss = 0.53381110\n",
            "Iteration 166, loss = 0.53367461\n",
            "Iteration 167, loss = 0.53350300\n",
            "Iteration 168, loss = 0.53333019\n",
            "Iteration 169, loss = 0.53329169\n",
            "Iteration 170, loss = 0.53306603\n",
            "Iteration 171, loss = 0.53293138\n",
            "Iteration 172, loss = 0.53273355\n",
            "Iteration 173, loss = 0.53262714\n",
            "Iteration 174, loss = 0.53249091\n",
            "Iteration 175, loss = 0.53225176\n",
            "Iteration 176, loss = 0.53202613\n",
            "Iteration 177, loss = 0.53182476\n",
            "Iteration 178, loss = 0.53166752\n",
            "Iteration 179, loss = 0.53141635\n",
            "Iteration 180, loss = 0.53127269\n",
            "Iteration 181, loss = 0.53098876\n",
            "Iteration 182, loss = 0.53085588\n",
            "Iteration 183, loss = 0.53072559\n",
            "Iteration 184, loss = 0.53057094\n",
            "Iteration 185, loss = 0.53053204\n",
            "Iteration 186, loss = 0.53037552\n",
            "Iteration 187, loss = 0.53028680\n",
            "Iteration 188, loss = 0.53008481\n",
            "Iteration 189, loss = 0.52999455\n",
            "Iteration 190, loss = 0.52980150\n",
            "Iteration 191, loss = 0.52961696\n",
            "Iteration 192, loss = 0.52946260\n",
            "Iteration 193, loss = 0.52926877\n",
            "Iteration 194, loss = 0.52909502\n",
            "Iteration 195, loss = 0.52890922\n",
            "Iteration 196, loss = 0.52879534\n",
            "Iteration 197, loss = 0.52872179\n",
            "Iteration 198, loss = 0.52852934\n",
            "Iteration 199, loss = 0.52837975\n",
            "Iteration 200, loss = 0.52821005\n",
            "Iteration 1, loss = 0.84006331\n",
            "Iteration 2, loss = 0.81376413\n",
            "Iteration 3, loss = 0.77923652\n",
            "Iteration 4, loss = 0.74192691\n",
            "Iteration 5, loss = 0.71081488\n",
            "Iteration 6, loss = 0.68542628\n",
            "Iteration 7, loss = 0.66332206\n",
            "Iteration 8, loss = 0.64635824\n",
            "Iteration 9, loss = 0.63430855\n",
            "Iteration 10, loss = 0.62333389\n",
            "Iteration 11, loss = 0.61592732\n",
            "Iteration 12, loss = 0.61059120\n",
            "Iteration 13, loss = 0.60572314\n",
            "Iteration 14, loss = 0.60162257\n",
            "Iteration 15, loss = 0.59818823\n",
            "Iteration 16, loss = 0.59510392\n",
            "Iteration 17, loss = 0.59243361\n",
            "Iteration 18, loss = 0.58999220\n",
            "Iteration 19, loss = 0.58775910\n",
            "Iteration 20, loss = 0.58570466\n",
            "Iteration 21, loss = 0.58382153\n",
            "Iteration 22, loss = 0.58221027\n",
            "Iteration 23, loss = 0.58074782\n",
            "Iteration 24, loss = 0.57942583\n",
            "Iteration 25, loss = 0.57825592\n",
            "Iteration 26, loss = 0.57718551\n",
            "Iteration 27, loss = 0.57609237\n",
            "Iteration 28, loss = 0.57502039\n",
            "Iteration 29, loss = 0.57416546\n",
            "Iteration 30, loss = 0.57336706\n",
            "Iteration 31, loss = 0.57254110\n",
            "Iteration 32, loss = 0.57178136\n",
            "Iteration 33, loss = 0.57121735\n",
            "Iteration 34, loss = 0.57065672\n",
            "Iteration 35, loss = 0.57011026\n",
            "Iteration 36, loss = 0.56962549\n",
            "Iteration 37, loss = 0.56905940\n",
            "Iteration 38, loss = 0.56870017\n",
            "Iteration 39, loss = 0.56841004\n",
            "Iteration 40, loss = 0.56813621\n",
            "Iteration 41, loss = 0.56767469\n",
            "Iteration 42, loss = 0.56735452\n",
            "Iteration 43, loss = 0.56692246\n",
            "Iteration 44, loss = 0.56655300\n",
            "Iteration 45, loss = 0.56632262\n",
            "Iteration 46, loss = 0.56592314\n",
            "Iteration 47, loss = 0.56566446\n",
            "Iteration 48, loss = 0.56536787\n",
            "Iteration 49, loss = 0.56507362\n",
            "Iteration 50, loss = 0.56477701\n",
            "Iteration 51, loss = 0.56458465\n",
            "Iteration 52, loss = 0.56430101\n",
            "Iteration 53, loss = 0.56397691\n",
            "Iteration 54, loss = 0.56383676\n",
            "Iteration 55, loss = 0.56355069\n",
            "Iteration 56, loss = 0.56333377\n",
            "Iteration 57, loss = 0.56300657\n",
            "Iteration 58, loss = 0.56280934\n",
            "Iteration 59, loss = 0.56259266\n",
            "Iteration 60, loss = 0.56235040\n",
            "Iteration 61, loss = 0.56206429\n",
            "Iteration 62, loss = 0.56184369\n",
            "Iteration 63, loss = 0.56162229\n",
            "Iteration 64, loss = 0.56138660\n",
            "Iteration 65, loss = 0.56115882\n",
            "Iteration 66, loss = 0.56092963\n",
            "Iteration 67, loss = 0.56062588\n",
            "Iteration 68, loss = 0.56041223\n",
            "Iteration 69, loss = 0.56023079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 70, loss = 0.55999148\n",
            "Iteration 71, loss = 0.55977984\n",
            "Iteration 72, loss = 0.55951684\n",
            "Iteration 73, loss = 0.55937821\n",
            "Iteration 74, loss = 0.55921986\n",
            "Iteration 75, loss = 0.55887053\n",
            "Iteration 76, loss = 0.55866763\n",
            "Iteration 77, loss = 0.55835573\n",
            "Iteration 78, loss = 0.55806711\n",
            "Iteration 79, loss = 0.55774697\n",
            "Iteration 80, loss = 0.55738107\n",
            "Iteration 81, loss = 0.55707906\n",
            "Iteration 82, loss = 0.55689875\n",
            "Iteration 83, loss = 0.55667676\n",
            "Iteration 84, loss = 0.55639916\n",
            "Iteration 85, loss = 0.55622890\n",
            "Iteration 86, loss = 0.55600852\n",
            "Iteration 87, loss = 0.55583013\n",
            "Iteration 88, loss = 0.55565230\n",
            "Iteration 89, loss = 0.55543481\n",
            "Iteration 90, loss = 0.55528561\n",
            "Iteration 91, loss = 0.55506069\n",
            "Iteration 92, loss = 0.55477530\n",
            "Iteration 93, loss = 0.55451576\n",
            "Iteration 94, loss = 0.55430681\n",
            "Iteration 95, loss = 0.55400776\n",
            "Iteration 96, loss = 0.55369259\n",
            "Iteration 97, loss = 0.55338130\n",
            "Iteration 98, loss = 0.55315037\n",
            "Iteration 99, loss = 0.55291589\n",
            "Iteration 100, loss = 0.55264640\n",
            "Iteration 101, loss = 0.55247134\n",
            "Iteration 102, loss = 0.55233386\n",
            "Iteration 103, loss = 0.55216016\n",
            "Iteration 104, loss = 0.55193526\n",
            "Iteration 105, loss = 0.55171716\n",
            "Iteration 106, loss = 0.55161165\n",
            "Iteration 107, loss = 0.55133595\n",
            "Iteration 108, loss = 0.55128412\n",
            "Iteration 109, loss = 0.55106182\n",
            "Iteration 110, loss = 0.55094599\n",
            "Iteration 111, loss = 0.55088210\n",
            "Iteration 112, loss = 0.55070015\n",
            "Iteration 113, loss = 0.55049384\n",
            "Iteration 114, loss = 0.55041983\n",
            "Iteration 115, loss = 0.55029610\n",
            "Iteration 116, loss = 0.55008228\n",
            "Iteration 117, loss = 0.55000776\n",
            "Iteration 118, loss = 0.54975533\n",
            "Iteration 119, loss = 0.54956440\n",
            "Iteration 120, loss = 0.54945570\n",
            "Iteration 121, loss = 0.54927225\n",
            "Iteration 122, loss = 0.54911288\n",
            "Iteration 123, loss = 0.54895665\n",
            "Iteration 124, loss = 0.54875025\n",
            "Iteration 125, loss = 0.54857615\n",
            "Iteration 126, loss = 0.54849266\n",
            "Iteration 127, loss = 0.54822139\n",
            "Iteration 128, loss = 0.54812265\n",
            "Iteration 129, loss = 0.54801617\n",
            "Iteration 130, loss = 0.54771625\n",
            "Iteration 131, loss = 0.54773805\n",
            "Iteration 132, loss = 0.54743297\n",
            "Iteration 133, loss = 0.54737811\n",
            "Iteration 134, loss = 0.54717746\n",
            "Iteration 135, loss = 0.54696117\n",
            "Iteration 136, loss = 0.54676712\n",
            "Iteration 137, loss = 0.54661184\n",
            "Iteration 138, loss = 0.54652299\n",
            "Iteration 139, loss = 0.54633499\n",
            "Iteration 140, loss = 0.54617776\n",
            "Iteration 141, loss = 0.54619609\n",
            "Iteration 142, loss = 0.54596633\n",
            "Iteration 143, loss = 0.54583598\n",
            "Iteration 144, loss = 0.54563675\n",
            "Iteration 145, loss = 0.54564187\n",
            "Iteration 146, loss = 0.54546484\n",
            "Iteration 147, loss = 0.54532720\n",
            "Iteration 148, loss = 0.54518740\n",
            "Iteration 149, loss = 0.54503367\n",
            "Iteration 150, loss = 0.54491398\n",
            "Iteration 151, loss = 0.54481154\n",
            "Iteration 152, loss = 0.54461989\n",
            "Iteration 153, loss = 0.54460808\n",
            "Iteration 154, loss = 0.54444292\n",
            "Iteration 155, loss = 0.54435592\n",
            "Iteration 156, loss = 0.54425112\n",
            "Iteration 157, loss = 0.54412352\n",
            "Iteration 158, loss = 0.54402035\n",
            "Iteration 159, loss = 0.54391572\n",
            "Iteration 160, loss = 0.54390491\n",
            "Iteration 161, loss = 0.54371448\n",
            "Iteration 162, loss = 0.54362302\n",
            "Iteration 163, loss = 0.54364602\n",
            "Iteration 164, loss = 0.54343237\n",
            "Iteration 165, loss = 0.54335945\n",
            "Iteration 166, loss = 0.54338430\n",
            "Iteration 167, loss = 0.54326254\n",
            "Iteration 168, loss = 0.54317258\n",
            "Iteration 169, loss = 0.54322700\n",
            "Iteration 170, loss = 0.54310808\n",
            "Iteration 171, loss = 0.54298395\n",
            "Iteration 172, loss = 0.54295708\n",
            "Iteration 173, loss = 0.54285906\n",
            "Iteration 174, loss = 0.54271957\n",
            "Iteration 175, loss = 0.54253791\n",
            "Iteration 176, loss = 0.54239259\n",
            "Iteration 177, loss = 0.54227054\n",
            "Iteration 178, loss = 0.54215037\n",
            "Iteration 179, loss = 0.54202403\n",
            "Iteration 180, loss = 0.54194543\n",
            "Iteration 181, loss = 0.54185852\n",
            "Iteration 182, loss = 0.54181154\n",
            "Iteration 183, loss = 0.54171741\n",
            "Iteration 184, loss = 0.54164549\n",
            "Iteration 185, loss = 0.54158963\n",
            "Iteration 186, loss = 0.54156451\n",
            "Iteration 187, loss = 0.54150780\n",
            "Iteration 188, loss = 0.54138977\n",
            "Iteration 189, loss = 0.54141423\n",
            "Iteration 190, loss = 0.54125364\n",
            "Iteration 191, loss = 0.54111794\n",
            "Iteration 192, loss = 0.54097003\n",
            "Iteration 193, loss = 0.54089889\n",
            "Iteration 194, loss = 0.54076714\n",
            "Iteration 195, loss = 0.54056800\n",
            "Iteration 196, loss = 0.54056338\n",
            "Iteration 197, loss = 0.54038133\n",
            "Iteration 198, loss = 0.54019930\n",
            "Iteration 199, loss = 0.54007844\n",
            "Iteration 200, loss = 0.53989106\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 200 and for layer number 3 : 0.7150000000000001\n",
            "Iteration 1, loss = 0.92540177\n",
            "Iteration 2, loss = 0.90606988\n",
            "Iteration 3, loss = 0.87636521\n",
            "Iteration 4, loss = 0.84453499\n",
            "Iteration 5, loss = 0.81069894\n",
            "Iteration 6, loss = 0.78049825\n",
            "Iteration 7, loss = 0.75458834\n",
            "Iteration 8, loss = 0.73221444\n",
            "Iteration 9, loss = 0.71299243\n",
            "Iteration 10, loss = 0.69629293\n",
            "Iteration 11, loss = 0.68190019\n",
            "Iteration 12, loss = 0.66823391\n",
            "Iteration 13, loss = 0.65677101\n",
            "Iteration 14, loss = 0.64656579\n",
            "Iteration 15, loss = 0.63780899\n",
            "Iteration 16, loss = 0.63058347\n",
            "Iteration 17, loss = 0.62420683\n",
            "Iteration 18, loss = 0.61911339\n",
            "Iteration 19, loss = 0.61402184\n",
            "Iteration 20, loss = 0.60966985\n",
            "Iteration 21, loss = 0.60588786\n",
            "Iteration 22, loss = 0.60253512\n",
            "Iteration 23, loss = 0.59952335\n",
            "Iteration 24, loss = 0.59636324\n",
            "Iteration 25, loss = 0.59415604\n",
            "Iteration 26, loss = 0.59185568\n",
            "Iteration 27, loss = 0.58962051\n",
            "Iteration 28, loss = 0.58753568\n",
            "Iteration 29, loss = 0.58565227\n",
            "Iteration 30, loss = 0.58359418\n",
            "Iteration 31, loss = 0.58153387\n",
            "Iteration 32, loss = 0.57977999\n",
            "Iteration 33, loss = 0.57793385\n",
            "Iteration 34, loss = 0.57632165\n",
            "Iteration 35, loss = 0.57508868\n",
            "Iteration 36, loss = 0.57386065\n",
            "Iteration 37, loss = 0.57284075\n",
            "Iteration 38, loss = 0.57151683\n",
            "Iteration 39, loss = 0.57035367\n",
            "Iteration 40, loss = 0.56936141\n",
            "Iteration 41, loss = 0.56829218\n",
            "Iteration 42, loss = 0.56718683\n",
            "Iteration 43, loss = 0.56628285\n",
            "Iteration 44, loss = 0.56540172\n",
            "Iteration 45, loss = 0.56450610\n",
            "Iteration 46, loss = 0.56359359\n",
            "Iteration 47, loss = 0.56274113\n",
            "Iteration 48, loss = 0.56205347\n",
            "Iteration 49, loss = 0.56141325\n",
            "Iteration 50, loss = 0.56076824\n",
            "Iteration 51, loss = 0.56010553\n",
            "Iteration 52, loss = 0.55955061\n",
            "Iteration 53, loss = 0.55909395\n",
            "Iteration 54, loss = 0.55851126\n",
            "Iteration 55, loss = 0.55829418\n",
            "Iteration 56, loss = 0.55769846\n",
            "Iteration 57, loss = 0.55725754\n",
            "Iteration 58, loss = 0.55675838\n",
            "Iteration 59, loss = 0.55638045\n",
            "Iteration 60, loss = 0.55586590\n",
            "Iteration 61, loss = 0.55559102\n",
            "Iteration 62, loss = 0.55526240\n",
            "Iteration 63, loss = 0.55501197\n",
            "Iteration 64, loss = 0.55472593\n",
            "Iteration 65, loss = 0.55441042\n",
            "Iteration 66, loss = 0.55417765\n",
            "Iteration 67, loss = 0.55391604\n",
            "Iteration 68, loss = 0.55367443\n",
            "Iteration 69, loss = 0.55341227\n",
            "Iteration 70, loss = 0.55306736\n",
            "Iteration 71, loss = 0.55291328\n",
            "Iteration 72, loss = 0.55263420\n",
            "Iteration 73, loss = 0.55247432\n",
            "Iteration 74, loss = 0.55217557\n",
            "Iteration 75, loss = 0.55197460\n",
            "Iteration 76, loss = 0.55176851\n",
            "Iteration 77, loss = 0.55160822\n",
            "Iteration 78, loss = 0.55124273\n",
            "Iteration 79, loss = 0.55130751\n",
            "Iteration 80, loss = 0.55101272\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 81, loss = 0.55080159\n",
            "Iteration 82, loss = 0.55055801\n",
            "Iteration 83, loss = 0.55028475\n",
            "Iteration 84, loss = 0.55008004\n",
            "Iteration 85, loss = 0.54978130\n",
            "Iteration 86, loss = 0.54946212\n",
            "Iteration 87, loss = 0.54921940\n",
            "Iteration 88, loss = 0.54905529\n",
            "Iteration 89, loss = 0.54883849\n",
            "Iteration 90, loss = 0.54866519\n",
            "Iteration 91, loss = 0.54847662\n",
            "Iteration 92, loss = 0.54822737\n",
            "Iteration 93, loss = 0.54809133\n",
            "Iteration 94, loss = 0.54783013\n",
            "Iteration 95, loss = 0.54764196\n",
            "Iteration 96, loss = 0.54742739\n",
            "Iteration 97, loss = 0.54731608\n",
            "Iteration 98, loss = 0.54716179\n",
            "Iteration 99, loss = 0.54712924\n",
            "Iteration 100, loss = 0.54680985\n",
            "Iteration 101, loss = 0.54673239\n",
            "Iteration 102, loss = 0.54647657\n",
            "Iteration 103, loss = 0.54624866\n",
            "Iteration 104, loss = 0.54612778\n",
            "Iteration 105, loss = 0.54598568\n",
            "Iteration 106, loss = 0.54591547\n",
            "Iteration 107, loss = 0.54585241\n",
            "Iteration 108, loss = 0.54591879\n",
            "Iteration 109, loss = 0.54577268\n",
            "Iteration 110, loss = 0.54575403\n",
            "Iteration 111, loss = 0.54565059\n",
            "Iteration 112, loss = 0.54569602\n",
            "Iteration 113, loss = 0.54556721\n",
            "Iteration 114, loss = 0.54536502\n",
            "Iteration 115, loss = 0.54530154\n",
            "Iteration 116, loss = 0.54523272\n",
            "Iteration 117, loss = 0.54506782\n",
            "Iteration 118, loss = 0.54500117\n",
            "Iteration 119, loss = 0.54493549\n",
            "Iteration 120, loss = 0.54482254\n",
            "Iteration 121, loss = 0.54475022\n",
            "Iteration 122, loss = 0.54464520\n",
            "Iteration 123, loss = 0.54462680\n",
            "Iteration 124, loss = 0.54460052\n",
            "Iteration 125, loss = 0.54447940\n",
            "Iteration 126, loss = 0.54456956\n",
            "Iteration 127, loss = 0.54425805\n",
            "Iteration 128, loss = 0.54413216\n",
            "Iteration 129, loss = 0.54384069\n",
            "Iteration 130, loss = 0.54371370\n",
            "Iteration 131, loss = 0.54352732\n",
            "Iteration 132, loss = 0.54335967\n",
            "Iteration 133, loss = 0.54329342\n",
            "Iteration 134, loss = 0.54322469\n",
            "Iteration 135, loss = 0.54321215\n",
            "Iteration 136, loss = 0.54299755\n",
            "Iteration 137, loss = 0.54285669\n",
            "Iteration 138, loss = 0.54275060\n",
            "Iteration 139, loss = 0.54247230\n",
            "Iteration 140, loss = 0.54230632\n",
            "Iteration 141, loss = 0.54220545\n",
            "Iteration 142, loss = 0.54214524\n",
            "Iteration 143, loss = 0.54205900\n",
            "Iteration 144, loss = 0.54184695\n",
            "Iteration 145, loss = 0.54182552\n",
            "Iteration 146, loss = 0.54157341\n",
            "Iteration 147, loss = 0.54145732\n",
            "Iteration 148, loss = 0.54126089\n",
            "Iteration 149, loss = 0.54101512\n",
            "Iteration 150, loss = 0.54097605\n",
            "Iteration 151, loss = 0.54093882\n",
            "Iteration 152, loss = 0.54102659\n",
            "Iteration 153, loss = 0.54061549\n",
            "Iteration 154, loss = 0.54054110\n",
            "Iteration 155, loss = 0.54014290\n",
            "Iteration 156, loss = 0.54012450\n",
            "Iteration 157, loss = 0.53982654\n",
            "Iteration 158, loss = 0.53964231\n",
            "Iteration 159, loss = 0.53956305\n",
            "Iteration 160, loss = 0.53944233\n",
            "Iteration 161, loss = 0.53931231\n",
            "Iteration 162, loss = 0.53930214\n",
            "Iteration 163, loss = 0.53903052\n",
            "Iteration 164, loss = 0.53877734\n",
            "Iteration 165, loss = 0.53864195\n",
            "Iteration 166, loss = 0.53845220\n",
            "Iteration 167, loss = 0.53851440\n",
            "Iteration 168, loss = 0.53822528\n",
            "Iteration 169, loss = 0.53828039\n",
            "Iteration 170, loss = 0.53811978\n",
            "Iteration 171, loss = 0.53793443\n",
            "Iteration 172, loss = 0.53770276\n",
            "Iteration 173, loss = 0.53757042\n",
            "Iteration 174, loss = 0.53746841\n",
            "Iteration 175, loss = 0.53744624\n",
            "Iteration 176, loss = 0.53732561\n",
            "Iteration 177, loss = 0.53692407\n",
            "Iteration 178, loss = 0.53683696\n",
            "Iteration 179, loss = 0.53647780\n",
            "Iteration 180, loss = 0.53624622\n",
            "Iteration 181, loss = 0.53593573\n",
            "Iteration 182, loss = 0.53577339\n",
            "Iteration 183, loss = 0.53568908\n",
            "Iteration 184, loss = 0.53555303\n",
            "Iteration 185, loss = 0.53556762\n",
            "Iteration 186, loss = 0.53560811\n",
            "Iteration 187, loss = 0.53555030\n",
            "Iteration 188, loss = 0.53519201\n",
            "Iteration 189, loss = 0.53517003\n",
            "Iteration 190, loss = 0.53473487\n",
            "Iteration 191, loss = 0.53489425\n",
            "Iteration 192, loss = 0.53493529\n",
            "Iteration 193, loss = 0.53468970\n",
            "Iteration 194, loss = 0.53444205\n",
            "Iteration 195, loss = 0.53429011\n",
            "Iteration 196, loss = 0.53407521\n",
            "Iteration 197, loss = 0.53398312\n",
            "Iteration 198, loss = 0.53374604\n",
            "Iteration 199, loss = 0.53368447\n",
            "Iteration 200, loss = 0.53341937\n",
            "Iteration 1, loss = 0.92236300\n",
            "Iteration 2, loss = 0.90372668\n",
            "Iteration 3, loss = 0.87500833\n",
            "Iteration 4, loss = 0.84285035\n",
            "Iteration 5, loss = 0.80913380\n",
            "Iteration 6, loss = 0.77972252\n",
            "Iteration 7, loss = 0.75423862\n",
            "Iteration 8, loss = 0.73244226\n",
            "Iteration 9, loss = 0.71337957\n",
            "Iteration 10, loss = 0.69694930\n",
            "Iteration 11, loss = 0.68235517\n",
            "Iteration 12, loss = 0.66874001\n",
            "Iteration 13, loss = 0.65701897\n",
            "Iteration 14, loss = 0.64666486\n",
            "Iteration 15, loss = 0.63680422\n",
            "Iteration 16, loss = 0.62903974\n",
            "Iteration 17, loss = 0.62282581\n",
            "Iteration 18, loss = 0.61753655\n",
            "Iteration 19, loss = 0.61255961\n",
            "Iteration 20, loss = 0.60822250\n",
            "Iteration 21, loss = 0.60455223\n",
            "Iteration 22, loss = 0.60093557\n",
            "Iteration 23, loss = 0.59766462\n",
            "Iteration 24, loss = 0.59443533\n",
            "Iteration 25, loss = 0.59186429\n",
            "Iteration 26, loss = 0.58954458\n",
            "Iteration 27, loss = 0.58722957\n",
            "Iteration 28, loss = 0.58504645\n",
            "Iteration 29, loss = 0.58302508\n",
            "Iteration 30, loss = 0.58092002\n",
            "Iteration 31, loss = 0.57890737\n",
            "Iteration 32, loss = 0.57713204\n",
            "Iteration 33, loss = 0.57553550\n",
            "Iteration 34, loss = 0.57399110\n",
            "Iteration 35, loss = 0.57258257\n",
            "Iteration 36, loss = 0.57132345\n",
            "Iteration 37, loss = 0.57036989\n",
            "Iteration 38, loss = 0.56919804\n",
            "Iteration 39, loss = 0.56804072\n",
            "Iteration 40, loss = 0.56720233\n",
            "Iteration 41, loss = 0.56616219\n",
            "Iteration 42, loss = 0.56523123\n",
            "Iteration 43, loss = 0.56418669\n",
            "Iteration 44, loss = 0.56340104\n",
            "Iteration 45, loss = 0.56229242\n",
            "Iteration 46, loss = 0.56136004\n",
            "Iteration 47, loss = 0.56050574\n",
            "Iteration 48, loss = 0.55975555\n",
            "Iteration 49, loss = 0.55893402\n",
            "Iteration 50, loss = 0.55825810\n",
            "Iteration 51, loss = 0.55750539\n",
            "Iteration 52, loss = 0.55689701\n",
            "Iteration 53, loss = 0.55630534\n",
            "Iteration 54, loss = 0.55554960\n",
            "Iteration 55, loss = 0.55530925\n",
            "Iteration 56, loss = 0.55458557\n",
            "Iteration 57, loss = 0.55417055\n",
            "Iteration 58, loss = 0.55375697\n",
            "Iteration 59, loss = 0.55343024\n",
            "Iteration 60, loss = 0.55303828\n",
            "Iteration 61, loss = 0.55275448\n",
            "Iteration 62, loss = 0.55246826\n",
            "Iteration 63, loss = 0.55210971\n",
            "Iteration 64, loss = 0.55175282\n",
            "Iteration 65, loss = 0.55137494\n",
            "Iteration 66, loss = 0.55113308\n",
            "Iteration 67, loss = 0.55081752\n",
            "Iteration 68, loss = 0.55067280\n",
            "Iteration 69, loss = 0.55037239\n",
            "Iteration 70, loss = 0.55015762\n",
            "Iteration 71, loss = 0.54984322\n",
            "Iteration 72, loss = 0.54962258\n",
            "Iteration 73, loss = 0.54941433\n",
            "Iteration 74, loss = 0.54910659\n",
            "Iteration 75, loss = 0.54882617\n",
            "Iteration 76, loss = 0.54858009\n",
            "Iteration 77, loss = 0.54842549\n",
            "Iteration 78, loss = 0.54826356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 79, loss = 0.54817975\n",
            "Iteration 80, loss = 0.54779764\n",
            "Iteration 81, loss = 0.54763140\n",
            "Iteration 82, loss = 0.54736580\n",
            "Iteration 83, loss = 0.54705013\n",
            "Iteration 84, loss = 0.54689619\n",
            "Iteration 85, loss = 0.54652246\n",
            "Iteration 86, loss = 0.54618831\n",
            "Iteration 87, loss = 0.54586111\n",
            "Iteration 88, loss = 0.54563663\n",
            "Iteration 89, loss = 0.54532937\n",
            "Iteration 90, loss = 0.54516871\n",
            "Iteration 91, loss = 0.54484111\n",
            "Iteration 92, loss = 0.54457027\n",
            "Iteration 93, loss = 0.54439667\n",
            "Iteration 94, loss = 0.54405331\n",
            "Iteration 95, loss = 0.54382871\n",
            "Iteration 96, loss = 0.54356907\n",
            "Iteration 97, loss = 0.54331510\n",
            "Iteration 98, loss = 0.54317759\n",
            "Iteration 99, loss = 0.54304768\n",
            "Iteration 100, loss = 0.54279147\n",
            "Iteration 101, loss = 0.54261840\n",
            "Iteration 102, loss = 0.54245119\n",
            "Iteration 103, loss = 0.54230165\n",
            "Iteration 104, loss = 0.54201321\n",
            "Iteration 105, loss = 0.54196650\n",
            "Iteration 106, loss = 0.54187452\n",
            "Iteration 107, loss = 0.54182689\n",
            "Iteration 108, loss = 0.54186328\n",
            "Iteration 109, loss = 0.54161841\n",
            "Iteration 110, loss = 0.54159098\n",
            "Iteration 111, loss = 0.54140119\n",
            "Iteration 112, loss = 0.54132090\n",
            "Iteration 113, loss = 0.54133298\n",
            "Iteration 114, loss = 0.54113774\n",
            "Iteration 115, loss = 0.54102169\n",
            "Iteration 116, loss = 0.54094796\n",
            "Iteration 117, loss = 0.54082351\n",
            "Iteration 118, loss = 0.54077253\n",
            "Iteration 119, loss = 0.54069632\n",
            "Iteration 120, loss = 0.54060634\n",
            "Iteration 121, loss = 0.54040797\n",
            "Iteration 122, loss = 0.54024138\n",
            "Iteration 123, loss = 0.54028824\n",
            "Iteration 124, loss = 0.54029877\n",
            "Iteration 125, loss = 0.54022868\n",
            "Iteration 126, loss = 0.54001368\n",
            "Iteration 127, loss = 0.53985196\n",
            "Iteration 128, loss = 0.53976040\n",
            "Iteration 129, loss = 0.53951986\n",
            "Iteration 130, loss = 0.53942221\n",
            "Iteration 131, loss = 0.53929777\n",
            "Iteration 132, loss = 0.53916102\n",
            "Iteration 133, loss = 0.53903189\n",
            "Iteration 134, loss = 0.53910032\n",
            "Iteration 135, loss = 0.53900365\n",
            "Iteration 136, loss = 0.53908288\n",
            "Iteration 137, loss = 0.53888749\n",
            "Iteration 138, loss = 0.53882052\n",
            "Iteration 139, loss = 0.53874478\n",
            "Iteration 140, loss = 0.53866072\n",
            "Iteration 141, loss = 0.53861333\n",
            "Iteration 142, loss = 0.53871510\n",
            "Iteration 143, loss = 0.53874044\n",
            "Iteration 144, loss = 0.53861559\n",
            "Iteration 145, loss = 0.53866957\n",
            "Iteration 146, loss = 0.53851373\n",
            "Iteration 147, loss = 0.53844718\n",
            "Iteration 148, loss = 0.53822626\n",
            "Iteration 149, loss = 0.53814144\n",
            "Iteration 150, loss = 0.53820563\n",
            "Iteration 151, loss = 0.53810266\n",
            "Iteration 152, loss = 0.53809679\n",
            "Iteration 153, loss = 0.53803421\n",
            "Iteration 154, loss = 0.53800440\n",
            "Iteration 155, loss = 0.53790815\n",
            "Iteration 156, loss = 0.53807663\n",
            "Iteration 157, loss = 0.53775598\n",
            "Iteration 158, loss = 0.53751940\n",
            "Iteration 159, loss = 0.53736868\n",
            "Iteration 160, loss = 0.53736605\n",
            "Iteration 161, loss = 0.53731247\n",
            "Iteration 162, loss = 0.53737783\n",
            "Iteration 163, loss = 0.53721698\n",
            "Iteration 164, loss = 0.53703478\n",
            "Iteration 165, loss = 0.53714732\n",
            "Iteration 166, loss = 0.53695543\n",
            "Iteration 167, loss = 0.53690205\n",
            "Iteration 168, loss = 0.53686330\n",
            "Iteration 169, loss = 0.53704083\n",
            "Iteration 170, loss = 0.53699350\n",
            "Iteration 171, loss = 0.53698387\n",
            "Iteration 172, loss = 0.53692939\n",
            "Iteration 173, loss = 0.53686512\n",
            "Iteration 174, loss = 0.53672144\n",
            "Iteration 175, loss = 0.53683777\n",
            "Iteration 176, loss = 0.53682227\n",
            "Iteration 177, loss = 0.53652605\n",
            "Iteration 178, loss = 0.53659008\n",
            "Iteration 179, loss = 0.53644548\n",
            "Iteration 180, loss = 0.53624399\n",
            "Iteration 181, loss = 0.53602806\n",
            "Iteration 182, loss = 0.53601239\n",
            "Iteration 183, loss = 0.53581716\n",
            "Iteration 184, loss = 0.53575172\n",
            "Iteration 185, loss = 0.53569976\n",
            "Iteration 186, loss = 0.53569924\n",
            "Iteration 187, loss = 0.53579638\n",
            "Iteration 188, loss = 0.53554040\n",
            "Iteration 189, loss = 0.53549239\n",
            "Iteration 190, loss = 0.53542215\n",
            "Iteration 191, loss = 0.53555390\n",
            "Iteration 192, loss = 0.53551448\n",
            "Iteration 193, loss = 0.53545647\n",
            "Iteration 194, loss = 0.53530799\n",
            "Iteration 195, loss = 0.53528437\n",
            "Iteration 196, loss = 0.53529348\n",
            "Iteration 197, loss = 0.53516088\n",
            "Iteration 198, loss = 0.53503815\n",
            "Iteration 199, loss = 0.53505396\n",
            "Iteration 200, loss = 0.53501743\n",
            "Iteration 1, loss = 0.93166195\n",
            "Iteration 2, loss = 0.91234492\n",
            "Iteration 3, loss = 0.88372313\n",
            "Iteration 4, loss = 0.84961251\n",
            "Iteration 5, loss = 0.81641210\n",
            "Iteration 6, loss = 0.78527885\n",
            "Iteration 7, loss = 0.75872117\n",
            "Iteration 8, loss = 0.73530918\n",
            "Iteration 9, loss = 0.71535645\n",
            "Iteration 10, loss = 0.69760505\n",
            "Iteration 11, loss = 0.68188671\n",
            "Iteration 12, loss = 0.66752288\n",
            "Iteration 13, loss = 0.65465085\n",
            "Iteration 14, loss = 0.64368593\n",
            "Iteration 15, loss = 0.63335085\n",
            "Iteration 16, loss = 0.62497229\n",
            "Iteration 17, loss = 0.61851546\n",
            "Iteration 18, loss = 0.61250769\n",
            "Iteration 19, loss = 0.60725642\n",
            "Iteration 20, loss = 0.60264352\n",
            "Iteration 21, loss = 0.59848665\n",
            "Iteration 22, loss = 0.59465851\n",
            "Iteration 23, loss = 0.59134538\n",
            "Iteration 24, loss = 0.58812798\n",
            "Iteration 25, loss = 0.58555756\n",
            "Iteration 26, loss = 0.58347267\n",
            "Iteration 27, loss = 0.58105626\n",
            "Iteration 28, loss = 0.57898547\n",
            "Iteration 29, loss = 0.57707857\n",
            "Iteration 30, loss = 0.57510602\n",
            "Iteration 31, loss = 0.57303366\n",
            "Iteration 32, loss = 0.57121707\n",
            "Iteration 33, loss = 0.56956037\n",
            "Iteration 34, loss = 0.56781174\n",
            "Iteration 35, loss = 0.56635167\n",
            "Iteration 36, loss = 0.56496659\n",
            "Iteration 37, loss = 0.56388825\n",
            "Iteration 38, loss = 0.56265033\n",
            "Iteration 39, loss = 0.56150979\n",
            "Iteration 40, loss = 0.56060243\n",
            "Iteration 41, loss = 0.55974129\n",
            "Iteration 42, loss = 0.55864499\n",
            "Iteration 43, loss = 0.55766563\n",
            "Iteration 44, loss = 0.55679573\n",
            "Iteration 45, loss = 0.55585704\n",
            "Iteration 46, loss = 0.55484301\n",
            "Iteration 47, loss = 0.55397414\n",
            "Iteration 48, loss = 0.55323994\n",
            "Iteration 49, loss = 0.55246703\n",
            "Iteration 50, loss = 0.55180371\n",
            "Iteration 51, loss = 0.55112381\n",
            "Iteration 52, loss = 0.55041014\n",
            "Iteration 53, loss = 0.54992329\n",
            "Iteration 54, loss = 0.54928756\n",
            "Iteration 55, loss = 0.54895870\n",
            "Iteration 56, loss = 0.54836925\n",
            "Iteration 57, loss = 0.54789274\n",
            "Iteration 58, loss = 0.54743070\n",
            "Iteration 59, loss = 0.54683477\n",
            "Iteration 60, loss = 0.54645133\n",
            "Iteration 61, loss = 0.54598155\n",
            "Iteration 62, loss = 0.54552238\n",
            "Iteration 63, loss = 0.54515072\n",
            "Iteration 64, loss = 0.54483821\n",
            "Iteration 65, loss = 0.54450111\n",
            "Iteration 66, loss = 0.54421242\n",
            "Iteration 67, loss = 0.54376193\n",
            "Iteration 68, loss = 0.54363452\n",
            "Iteration 69, loss = 0.54321728\n",
            "Iteration 70, loss = 0.54288609\n",
            "Iteration 71, loss = 0.54252895\n",
            "Iteration 72, loss = 0.54228397\n",
            "Iteration 73, loss = 0.54204462\n",
            "Iteration 74, loss = 0.54173786\n",
            "Iteration 75, loss = 0.54144168\n",
            "Iteration 76, loss = 0.54119500\n",
            "Iteration 77, loss = 0.54091047\n",
            "Iteration 78, loss = 0.54069623\n",
            "Iteration 79, loss = 0.54067683\n",
            "Iteration 80, loss = 0.54013276\n",
            "Iteration 81, loss = 0.53989053\n",
            "Iteration 82, loss = 0.53959924\n",
            "Iteration 83, loss = 0.53936832\n",
            "Iteration 84, loss = 0.53909921\n",
            "Iteration 85, loss = 0.53901869\n",
            "Iteration 86, loss = 0.53886630\n",
            "Iteration 87, loss = 0.53863528\n",
            "Iteration 88, loss = 0.53844614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 89, loss = 0.53823882\n",
            "Iteration 90, loss = 0.53804482\n",
            "Iteration 91, loss = 0.53777906\n",
            "Iteration 92, loss = 0.53750544\n",
            "Iteration 93, loss = 0.53723586\n",
            "Iteration 94, loss = 0.53706081\n",
            "Iteration 95, loss = 0.53680307\n",
            "Iteration 96, loss = 0.53657594\n",
            "Iteration 97, loss = 0.53648131\n",
            "Iteration 98, loss = 0.53633934\n",
            "Iteration 99, loss = 0.53616061\n",
            "Iteration 100, loss = 0.53588690\n",
            "Iteration 101, loss = 0.53572652\n",
            "Iteration 102, loss = 0.53554122\n",
            "Iteration 103, loss = 0.53541845\n",
            "Iteration 104, loss = 0.53524751\n",
            "Iteration 105, loss = 0.53511494\n",
            "Iteration 106, loss = 0.53496457\n",
            "Iteration 107, loss = 0.53490532\n",
            "Iteration 108, loss = 0.53475815\n",
            "Iteration 109, loss = 0.53455697\n",
            "Iteration 110, loss = 0.53447970\n",
            "Iteration 111, loss = 0.53441300\n",
            "Iteration 112, loss = 0.53425956\n",
            "Iteration 113, loss = 0.53414518\n",
            "Iteration 114, loss = 0.53402485\n",
            "Iteration 115, loss = 0.53396639\n",
            "Iteration 116, loss = 0.53392574\n",
            "Iteration 117, loss = 0.53373909\n",
            "Iteration 118, loss = 0.53377461\n",
            "Iteration 119, loss = 0.53363294\n",
            "Iteration 120, loss = 0.53362123\n",
            "Iteration 121, loss = 0.53359790\n",
            "Iteration 122, loss = 0.53336937\n",
            "Iteration 123, loss = 0.53322864\n",
            "Iteration 124, loss = 0.53314603\n",
            "Iteration 125, loss = 0.53310922\n",
            "Iteration 126, loss = 0.53300059\n",
            "Iteration 127, loss = 0.53300315\n",
            "Iteration 128, loss = 0.53294813\n",
            "Iteration 129, loss = 0.53288857\n",
            "Iteration 130, loss = 0.53290528\n",
            "Iteration 131, loss = 0.53289451\n",
            "Iteration 132, loss = 0.53291395\n",
            "Iteration 133, loss = 0.53283436\n",
            "Iteration 134, loss = 0.53261960\n",
            "Iteration 135, loss = 0.53265552\n",
            "Iteration 136, loss = 0.53270260\n",
            "Iteration 137, loss = 0.53254937\n",
            "Iteration 138, loss = 0.53253935\n",
            "Iteration 139, loss = 0.53249355\n",
            "Iteration 140, loss = 0.53237285\n",
            "Iteration 141, loss = 0.53230035\n",
            "Iteration 142, loss = 0.53216267\n",
            "Iteration 143, loss = 0.53203890\n",
            "Iteration 144, loss = 0.53195989\n",
            "Iteration 145, loss = 0.53192892\n",
            "Iteration 146, loss = 0.53178198\n",
            "Iteration 147, loss = 0.53173814\n",
            "Iteration 148, loss = 0.53150166\n",
            "Iteration 149, loss = 0.53143147\n",
            "Iteration 150, loss = 0.53135531\n",
            "Iteration 151, loss = 0.53124938\n",
            "Iteration 152, loss = 0.53113380\n",
            "Iteration 153, loss = 0.53095503\n",
            "Iteration 154, loss = 0.53089162\n",
            "Iteration 155, loss = 0.53086375\n",
            "Iteration 156, loss = 0.53085149\n",
            "Iteration 157, loss = 0.53059771\n",
            "Iteration 158, loss = 0.53048593\n",
            "Iteration 159, loss = 0.53072175\n",
            "Iteration 160, loss = 0.53072225\n",
            "Iteration 161, loss = 0.53057661\n",
            "Iteration 162, loss = 0.53070197\n",
            "Iteration 163, loss = 0.53037790\n",
            "Iteration 164, loss = 0.53035044\n",
            "Iteration 165, loss = 0.53034699\n",
            "Iteration 166, loss = 0.53020613\n",
            "Iteration 167, loss = 0.53014485\n",
            "Iteration 168, loss = 0.52999594\n",
            "Iteration 169, loss = 0.53015019\n",
            "Iteration 170, loss = 0.53001537\n",
            "Iteration 171, loss = 0.53002088\n",
            "Iteration 172, loss = 0.53011873\n",
            "Iteration 173, loss = 0.53007490\n",
            "Iteration 174, loss = 0.52987198\n",
            "Iteration 175, loss = 0.52981520\n",
            "Iteration 176, loss = 0.52956081\n",
            "Iteration 177, loss = 0.52937591\n",
            "Iteration 178, loss = 0.52945228\n",
            "Iteration 179, loss = 0.52920721\n",
            "Iteration 180, loss = 0.52921958\n",
            "Iteration 181, loss = 0.52890810\n",
            "Iteration 182, loss = 0.52894055\n",
            "Iteration 183, loss = 0.52884664\n",
            "Iteration 184, loss = 0.52901047\n",
            "Iteration 185, loss = 0.52883121\n",
            "Iteration 186, loss = 0.52877123\n",
            "Iteration 187, loss = 0.52880530\n",
            "Iteration 188, loss = 0.52865693\n",
            "Iteration 189, loss = 0.52864742\n",
            "Iteration 190, loss = 0.52861805\n",
            "Iteration 191, loss = 0.52857983\n",
            "Iteration 192, loss = 0.52866023\n",
            "Iteration 193, loss = 0.52858115\n",
            "Iteration 194, loss = 0.52848561\n",
            "Iteration 195, loss = 0.52844125\n",
            "Iteration 196, loss = 0.52853848\n",
            "Iteration 197, loss = 0.52836810\n",
            "Iteration 198, loss = 0.52836734\n",
            "Iteration 199, loss = 0.52842305\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.93004949\n",
            "Iteration 2, loss = 0.90996806\n",
            "Iteration 3, loss = 0.88055072\n",
            "Iteration 4, loss = 0.84659018\n",
            "Iteration 5, loss = 0.81298067\n",
            "Iteration 6, loss = 0.78206991\n",
            "Iteration 7, loss = 0.75624473\n",
            "Iteration 8, loss = 0.73305263\n",
            "Iteration 9, loss = 0.71302062\n",
            "Iteration 10, loss = 0.69627033\n",
            "Iteration 11, loss = 0.68068281\n",
            "Iteration 12, loss = 0.66784432\n",
            "Iteration 13, loss = 0.65616483\n",
            "Iteration 14, loss = 0.64623851\n",
            "Iteration 15, loss = 0.63710321\n",
            "Iteration 16, loss = 0.62947245\n",
            "Iteration 17, loss = 0.62332210\n",
            "Iteration 18, loss = 0.61772357\n",
            "Iteration 19, loss = 0.61296514\n",
            "Iteration 20, loss = 0.60893825\n",
            "Iteration 21, loss = 0.60537285\n",
            "Iteration 22, loss = 0.60197233\n",
            "Iteration 23, loss = 0.59888836\n",
            "Iteration 24, loss = 0.59592724\n",
            "Iteration 25, loss = 0.59330154\n",
            "Iteration 26, loss = 0.59119156\n",
            "Iteration 27, loss = 0.58870339\n",
            "Iteration 28, loss = 0.58650396\n",
            "Iteration 29, loss = 0.58465794\n",
            "Iteration 30, loss = 0.58280816\n",
            "Iteration 31, loss = 0.58096325\n",
            "Iteration 32, loss = 0.57914326\n",
            "Iteration 33, loss = 0.57759947\n",
            "Iteration 34, loss = 0.57596723\n",
            "Iteration 35, loss = 0.57461254\n",
            "Iteration 36, loss = 0.57309404\n",
            "Iteration 37, loss = 0.57195132\n",
            "Iteration 38, loss = 0.57082853\n",
            "Iteration 39, loss = 0.56968276\n",
            "Iteration 40, loss = 0.56876378\n",
            "Iteration 41, loss = 0.56795050\n",
            "Iteration 42, loss = 0.56670730\n",
            "Iteration 43, loss = 0.56589047\n",
            "Iteration 44, loss = 0.56489212\n",
            "Iteration 45, loss = 0.56408490\n",
            "Iteration 46, loss = 0.56314279\n",
            "Iteration 47, loss = 0.56221700\n",
            "Iteration 48, loss = 0.56123991\n",
            "Iteration 49, loss = 0.56021024\n",
            "Iteration 50, loss = 0.55950861\n",
            "Iteration 51, loss = 0.55881625\n",
            "Iteration 52, loss = 0.55804567\n",
            "Iteration 53, loss = 0.55743272\n",
            "Iteration 54, loss = 0.55675730\n",
            "Iteration 55, loss = 0.55651589\n",
            "Iteration 56, loss = 0.55608563\n",
            "Iteration 57, loss = 0.55564424\n",
            "Iteration 58, loss = 0.55526565\n",
            "Iteration 59, loss = 0.55479143\n",
            "Iteration 60, loss = 0.55430114\n",
            "Iteration 61, loss = 0.55392714\n",
            "Iteration 62, loss = 0.55358673\n",
            "Iteration 63, loss = 0.55323272\n",
            "Iteration 64, loss = 0.55316209\n",
            "Iteration 65, loss = 0.55273953\n",
            "Iteration 66, loss = 0.55263798\n",
            "Iteration 67, loss = 0.55236876\n",
            "Iteration 68, loss = 0.55214964\n",
            "Iteration 69, loss = 0.55181669\n",
            "Iteration 70, loss = 0.55156614\n",
            "Iteration 71, loss = 0.55140596\n",
            "Iteration 72, loss = 0.55129497\n",
            "Iteration 73, loss = 0.55112913\n",
            "Iteration 74, loss = 0.55095720\n",
            "Iteration 75, loss = 0.55084412\n",
            "Iteration 76, loss = 0.55071812\n",
            "Iteration 77, loss = 0.55048781\n",
            "Iteration 78, loss = 0.55036839\n",
            "Iteration 79, loss = 0.55043769\n",
            "Iteration 80, loss = 0.55008919\n",
            "Iteration 81, loss = 0.54996789\n",
            "Iteration 82, loss = 0.54984336\n",
            "Iteration 83, loss = 0.54956273\n",
            "Iteration 84, loss = 0.54928818\n",
            "Iteration 85, loss = 0.54921401\n",
            "Iteration 86, loss = 0.54901825\n",
            "Iteration 87, loss = 0.54890758\n",
            "Iteration 88, loss = 0.54882581\n",
            "Iteration 89, loss = 0.54877306\n",
            "Iteration 90, loss = 0.54856873\n",
            "Iteration 91, loss = 0.54851207\n",
            "Iteration 92, loss = 0.54835563\n",
            "Iteration 93, loss = 0.54816557\n",
            "Iteration 94, loss = 0.54804833\n",
            "Iteration 95, loss = 0.54792993\n",
            "Iteration 96, loss = 0.54784983\n",
            "Iteration 97, loss = 0.54774266\n",
            "Iteration 98, loss = 0.54772164\n",
            "Iteration 99, loss = 0.54760421\n",
            "Iteration 100, loss = 0.54742928\n",
            "Iteration 101, loss = 0.54728446\n",
            "Iteration 102, loss = 0.54712408\n",
            "Iteration 103, loss = 0.54705767\n",
            "Iteration 104, loss = 0.54683412\n",
            "Iteration 105, loss = 0.54670329\n",
            "Iteration 106, loss = 0.54657429\n",
            "Iteration 107, loss = 0.54647062\n",
            "Iteration 108, loss = 0.54652613\n",
            "Iteration 109, loss = 0.54649605\n",
            "Iteration 110, loss = 0.54649798\n",
            "Iteration 111, loss = 0.54647283\n",
            "Iteration 112, loss = 0.54633499\n",
            "Iteration 113, loss = 0.54633940\n",
            "Iteration 114, loss = 0.54621142\n",
            "Iteration 115, loss = 0.54618250\n",
            "Iteration 116, loss = 0.54615024\n",
            "Iteration 117, loss = 0.54607173\n",
            "Iteration 118, loss = 0.54600383\n",
            "Iteration 119, loss = 0.54598453\n",
            "Iteration 120, loss = 0.54603966\n",
            "Iteration 121, loss = 0.54605032\n",
            "Iteration 122, loss = 0.54588161\n",
            "Iteration 123, loss = 0.54578006\n",
            "Iteration 124, loss = 0.54578094\n",
            "Iteration 125, loss = 0.54571232\n",
            "Iteration 126, loss = 0.54566232\n",
            "Iteration 127, loss = 0.54577442\n",
            "Iteration 128, loss = 0.54571461\n",
            "Iteration 129, loss = 0.54566955\n",
            "Iteration 130, loss = 0.54569243\n",
            "Iteration 131, loss = 0.54551785\n",
            "Iteration 132, loss = 0.54545224\n",
            "Iteration 133, loss = 0.54541232\n",
            "Iteration 134, loss = 0.54536686\n",
            "Iteration 135, loss = 0.54543813\n",
            "Iteration 136, loss = 0.54543235\n",
            "Iteration 137, loss = 0.54534983\n",
            "Iteration 138, loss = 0.54535801\n",
            "Iteration 139, loss = 0.54530951\n",
            "Iteration 140, loss = 0.54522699\n",
            "Iteration 141, loss = 0.54519488\n",
            "Iteration 142, loss = 0.54516024\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.93650373\n",
            "Iteration 2, loss = 0.91614372\n",
            "Iteration 3, loss = 0.88631046\n",
            "Iteration 4, loss = 0.85327913\n",
            "Iteration 5, loss = 0.82138194\n",
            "Iteration 6, loss = 0.79229617\n",
            "Iteration 7, loss = 0.76753456\n",
            "Iteration 8, loss = 0.74573124\n",
            "Iteration 9, loss = 0.72648054\n",
            "Iteration 10, loss = 0.71056910\n",
            "Iteration 11, loss = 0.69588855\n",
            "Iteration 12, loss = 0.68427106\n",
            "Iteration 13, loss = 0.67305324\n",
            "Iteration 14, loss = 0.66346976\n",
            "Iteration 15, loss = 0.65484224\n",
            "Iteration 16, loss = 0.64765771\n",
            "Iteration 17, loss = 0.64160092\n",
            "Iteration 18, loss = 0.63596695\n",
            "Iteration 19, loss = 0.63145300\n",
            "Iteration 20, loss = 0.62738962\n",
            "Iteration 21, loss = 0.62388968\n",
            "Iteration 22, loss = 0.62051025\n",
            "Iteration 23, loss = 0.61719150\n",
            "Iteration 24, loss = 0.61415457\n",
            "Iteration 25, loss = 0.61154190\n",
            "Iteration 26, loss = 0.60921490\n",
            "Iteration 27, loss = 0.60680333\n",
            "Iteration 28, loss = 0.60463037\n",
            "Iteration 29, loss = 0.60261868\n",
            "Iteration 30, loss = 0.60053790\n",
            "Iteration 31, loss = 0.59861214\n",
            "Iteration 32, loss = 0.59687878\n",
            "Iteration 33, loss = 0.59521436\n",
            "Iteration 34, loss = 0.59373168\n",
            "Iteration 35, loss = 0.59231031\n",
            "Iteration 36, loss = 0.59089091\n",
            "Iteration 37, loss = 0.58975222\n",
            "Iteration 38, loss = 0.58844761\n",
            "Iteration 39, loss = 0.58743580\n",
            "Iteration 40, loss = 0.58644674\n",
            "Iteration 41, loss = 0.58550410\n",
            "Iteration 42, loss = 0.58440164\n",
            "Iteration 43, loss = 0.58328570\n",
            "Iteration 44, loss = 0.58228471\n",
            "Iteration 45, loss = 0.58128176\n",
            "Iteration 46, loss = 0.58040523\n",
            "Iteration 47, loss = 0.57952866\n",
            "Iteration 48, loss = 0.57856752\n",
            "Iteration 49, loss = 0.57759396\n",
            "Iteration 50, loss = 0.57673645\n",
            "Iteration 51, loss = 0.57577758\n",
            "Iteration 52, loss = 0.57477461\n",
            "Iteration 53, loss = 0.57391312\n",
            "Iteration 54, loss = 0.57295768\n",
            "Iteration 55, loss = 0.57235619\n",
            "Iteration 56, loss = 0.57150601\n",
            "Iteration 57, loss = 0.57068862\n",
            "Iteration 58, loss = 0.56995885\n",
            "Iteration 59, loss = 0.56926414\n",
            "Iteration 60, loss = 0.56848196\n",
            "Iteration 61, loss = 0.56771898\n",
            "Iteration 62, loss = 0.56713732\n",
            "Iteration 63, loss = 0.56642712\n",
            "Iteration 64, loss = 0.56596670\n",
            "Iteration 65, loss = 0.56543547\n",
            "Iteration 66, loss = 0.56509810\n",
            "Iteration 67, loss = 0.56455514\n",
            "Iteration 68, loss = 0.56433789\n",
            "Iteration 69, loss = 0.56375333\n",
            "Iteration 70, loss = 0.56327808\n",
            "Iteration 71, loss = 0.56292281\n",
            "Iteration 72, loss = 0.56257013\n",
            "Iteration 73, loss = 0.56215551\n",
            "Iteration 74, loss = 0.56183300\n",
            "Iteration 75, loss = 0.56170533\n",
            "Iteration 76, loss = 0.56121154\n",
            "Iteration 77, loss = 0.56080663\n",
            "Iteration 78, loss = 0.56057794\n",
            "Iteration 79, loss = 0.56030054\n",
            "Iteration 80, loss = 0.56000938\n",
            "Iteration 81, loss = 0.55969876\n",
            "Iteration 82, loss = 0.55937165\n",
            "Iteration 83, loss = 0.55913175\n",
            "Iteration 84, loss = 0.55876924\n",
            "Iteration 85, loss = 0.55859709\n",
            "Iteration 86, loss = 0.55830636\n",
            "Iteration 87, loss = 0.55803344\n",
            "Iteration 88, loss = 0.55769157\n",
            "Iteration 89, loss = 0.55763339\n",
            "Iteration 90, loss = 0.55740669\n",
            "Iteration 91, loss = 0.55726678\n",
            "Iteration 92, loss = 0.55707046\n",
            "Iteration 93, loss = 0.55675904\n",
            "Iteration 94, loss = 0.55659666\n",
            "Iteration 95, loss = 0.55647031\n",
            "Iteration 96, loss = 0.55623035\n",
            "Iteration 97, loss = 0.55605166\n",
            "Iteration 98, loss = 0.55588317\n",
            "Iteration 99, loss = 0.55568320\n",
            "Iteration 100, loss = 0.55547851\n",
            "Iteration 101, loss = 0.55526354\n",
            "Iteration 102, loss = 0.55509953\n",
            "Iteration 103, loss = 0.55498294\n",
            "Iteration 104, loss = 0.55484102\n",
            "Iteration 105, loss = 0.55457174\n",
            "Iteration 106, loss = 0.55451573\n",
            "Iteration 107, loss = 0.55438375\n",
            "Iteration 108, loss = 0.55428149\n",
            "Iteration 109, loss = 0.55413290\n",
            "Iteration 110, loss = 0.55398899\n",
            "Iteration 111, loss = 0.55399261\n",
            "Iteration 112, loss = 0.55390406\n",
            "Iteration 113, loss = 0.55378590\n",
            "Iteration 114, loss = 0.55384290\n",
            "Iteration 115, loss = 0.55370452\n",
            "Iteration 116, loss = 0.55361862\n",
            "Iteration 117, loss = 0.55354360\n",
            "Iteration 118, loss = 0.55337221\n",
            "Iteration 119, loss = 0.55329853\n",
            "Iteration 120, loss = 0.55319982\n",
            "Iteration 121, loss = 0.55327211\n",
            "Iteration 122, loss = 0.55295736\n",
            "Iteration 123, loss = 0.55293288\n",
            "Iteration 124, loss = 0.55279517\n",
            "Iteration 125, loss = 0.55270083\n",
            "Iteration 126, loss = 0.55261500\n",
            "Iteration 127, loss = 0.55259929\n",
            "Iteration 128, loss = 0.55243765\n",
            "Iteration 129, loss = 0.55236737\n",
            "Iteration 130, loss = 0.55227829\n",
            "Iteration 131, loss = 0.55212700\n",
            "Iteration 132, loss = 0.55216285\n",
            "Iteration 133, loss = 0.55201594\n",
            "Iteration 134, loss = 0.55203048\n",
            "Iteration 135, loss = 0.55192038\n",
            "Iteration 136, loss = 0.55194002\n",
            "Iteration 137, loss = 0.55188866\n",
            "Iteration 138, loss = 0.55194771\n",
            "Iteration 139, loss = 0.55176543\n",
            "Iteration 140, loss = 0.55170046\n",
            "Iteration 141, loss = 0.55163516\n",
            "Iteration 142, loss = 0.55161189\n",
            "Iteration 143, loss = 0.55169968\n",
            "Iteration 144, loss = 0.55153053\n",
            "Iteration 145, loss = 0.55147029\n",
            "Iteration 146, loss = 0.55144082\n",
            "Iteration 147, loss = 0.55136457\n",
            "Iteration 148, loss = 0.55126246\n",
            "Iteration 149, loss = 0.55128184\n",
            "Iteration 150, loss = 0.55115483\n",
            "Iteration 151, loss = 0.55122952\n",
            "Iteration 152, loss = 0.55126025\n",
            "Iteration 153, loss = 0.55117985\n",
            "Iteration 154, loss = 0.55112526\n",
            "Iteration 155, loss = 0.55106347\n",
            "Iteration 156, loss = 0.55121775\n",
            "Iteration 157, loss = 0.55099820\n",
            "Iteration 158, loss = 0.55091012\n",
            "Iteration 159, loss = 0.55097006\n",
            "Iteration 160, loss = 0.55081055\n",
            "Iteration 161, loss = 0.55066081\n",
            "Iteration 162, loss = 0.55084435\n",
            "Iteration 163, loss = 0.55097819\n",
            "Iteration 164, loss = 0.55107552\n",
            "Iteration 165, loss = 0.55096152\n",
            "Iteration 166, loss = 0.55085310\n",
            "Iteration 167, loss = 0.55076802\n",
            "Iteration 168, loss = 0.55078911\n",
            "Iteration 169, loss = 0.55068542\n",
            "Iteration 170, loss = 0.55048435\n",
            "Iteration 171, loss = 0.55039659\n",
            "Iteration 172, loss = 0.55029417\n",
            "Iteration 173, loss = 0.55031129\n",
            "Iteration 174, loss = 0.55039791\n",
            "Iteration 175, loss = 0.55037367\n",
            "Iteration 176, loss = 0.55029732\n",
            "Iteration 177, loss = 0.55030308\n",
            "Iteration 178, loss = 0.55019860\n",
            "Iteration 179, loss = 0.55021757\n",
            "Iteration 180, loss = 0.55028861\n",
            "Iteration 181, loss = 0.55024809\n",
            "Iteration 182, loss = 0.55022688\n",
            "Iteration 183, loss = 0.55017198\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 200 and for layer number 4 : 0.7075\n",
            "Iteration 1, loss = 0.88336594\n",
            "Iteration 2, loss = 0.85371585\n",
            "Iteration 3, loss = 0.81114998\n",
            "Iteration 4, loss = 0.76878568\n",
            "Iteration 5, loss = 0.72891304\n",
            "Iteration 6, loss = 0.69812822\n",
            "Iteration 7, loss = 0.67784648\n",
            "Iteration 8, loss = 0.65873975\n",
            "Iteration 9, loss = 0.64585588\n",
            "Iteration 10, loss = 0.63646662\n",
            "Iteration 11, loss = 0.62908658\n",
            "Iteration 12, loss = 0.62375837\n",
            "Iteration 13, loss = 0.61915746\n",
            "Iteration 14, loss = 0.61496245\n",
            "Iteration 15, loss = 0.61163021\n",
            "Iteration 16, loss = 0.60865001\n",
            "Iteration 17, loss = 0.60613154\n",
            "Iteration 18, loss = 0.60378173\n",
            "Iteration 19, loss = 0.60170407\n",
            "Iteration 20, loss = 0.59968740\n",
            "Iteration 21, loss = 0.59787396\n",
            "Iteration 22, loss = 0.59617398\n",
            "Iteration 23, loss = 0.59435058\n",
            "Iteration 24, loss = 0.59263873\n",
            "Iteration 25, loss = 0.59096900\n",
            "Iteration 26, loss = 0.58936046\n",
            "Iteration 27, loss = 0.58778572\n",
            "Iteration 28, loss = 0.58636900\n",
            "Iteration 29, loss = 0.58490469\n",
            "Iteration 30, loss = 0.58355581\n",
            "Iteration 31, loss = 0.58232012\n",
            "Iteration 32, loss = 0.58092064\n",
            "Iteration 33, loss = 0.57974507\n",
            "Iteration 34, loss = 0.57855058\n",
            "Iteration 35, loss = 0.57753596\n",
            "Iteration 36, loss = 0.57642234\n",
            "Iteration 37, loss = 0.57550100\n",
            "Iteration 38, loss = 0.57458003\n",
            "Iteration 39, loss = 0.57363445\n",
            "Iteration 40, loss = 0.57283919\n",
            "Iteration 41, loss = 0.57182274\n",
            "Iteration 42, loss = 0.57091657\n",
            "Iteration 43, loss = 0.57011771\n",
            "Iteration 44, loss = 0.56937148\n",
            "Iteration 45, loss = 0.56858382\n",
            "Iteration 46, loss = 0.56782347\n",
            "Iteration 47, loss = 0.56719403\n",
            "Iteration 48, loss = 0.56660501\n",
            "Iteration 49, loss = 0.56591930\n",
            "Iteration 50, loss = 0.56508853\n",
            "Iteration 51, loss = 0.56449271\n",
            "Iteration 52, loss = 0.56368499\n",
            "Iteration 53, loss = 0.56297676\n",
            "Iteration 54, loss = 0.56236424\n",
            "Iteration 55, loss = 0.56186295\n",
            "Iteration 56, loss = 0.56108268\n",
            "Iteration 57, loss = 0.56055555\n",
            "Iteration 58, loss = 0.55999247\n",
            "Iteration 59, loss = 0.55958193\n",
            "Iteration 60, loss = 0.55901068\n",
            "Iteration 61, loss = 0.55863943\n",
            "Iteration 62, loss = 0.55821083\n",
            "Iteration 63, loss = 0.55788734\n",
            "Iteration 64, loss = 0.55735403\n",
            "Iteration 65, loss = 0.55691118\n",
            "Iteration 66, loss = 0.55652812\n",
            "Iteration 67, loss = 0.55606649\n",
            "Iteration 68, loss = 0.55577844\n",
            "Iteration 69, loss = 0.55533429\n",
            "Iteration 70, loss = 0.55504655\n",
            "Iteration 71, loss = 0.55474930\n",
            "Iteration 72, loss = 0.55424172\n",
            "Iteration 73, loss = 0.55390727\n",
            "Iteration 74, loss = 0.55356165\n",
            "Iteration 75, loss = 0.55313879\n",
            "Iteration 76, loss = 0.55269980\n",
            "Iteration 77, loss = 0.55238251\n",
            "Iteration 78, loss = 0.55204490\n",
            "Iteration 79, loss = 0.55161358\n",
            "Iteration 80, loss = 0.55135349\n",
            "Iteration 81, loss = 0.55094036\n",
            "Iteration 82, loss = 0.55070037\n",
            "Iteration 83, loss = 0.55039199\n",
            "Iteration 84, loss = 0.54990062\n",
            "Iteration 85, loss = 0.54951022\n",
            "Iteration 86, loss = 0.54923000\n",
            "Iteration 87, loss = 0.54888291\n",
            "Iteration 88, loss = 0.54852080\n",
            "Iteration 89, loss = 0.54819029\n",
            "Iteration 90, loss = 0.54788580\n",
            "Iteration 91, loss = 0.54753336\n",
            "Iteration 92, loss = 0.54723242\n",
            "Iteration 93, loss = 0.54696185\n",
            "Iteration 94, loss = 0.54663047\n",
            "Iteration 95, loss = 0.54642423\n",
            "Iteration 96, loss = 0.54612106\n",
            "Iteration 97, loss = 0.54588052\n",
            "Iteration 98, loss = 0.54566276\n",
            "Iteration 99, loss = 0.54547018\n",
            "Iteration 100, loss = 0.54522125\n",
            "Iteration 101, loss = 0.54497855\n",
            "Iteration 102, loss = 0.54484424\n",
            "Iteration 103, loss = 0.54454975\n",
            "Iteration 104, loss = 0.54432694\n",
            "Iteration 105, loss = 0.54405871\n",
            "Iteration 106, loss = 0.54400529\n",
            "Iteration 107, loss = 0.54365645\n",
            "Iteration 108, loss = 0.54342444\n",
            "Iteration 109, loss = 0.54319360\n",
            "Iteration 110, loss = 0.54296398\n",
            "Iteration 111, loss = 0.54259789\n",
            "Iteration 112, loss = 0.54246263\n",
            "Iteration 113, loss = 0.54222654\n",
            "Iteration 114, loss = 0.54204101\n",
            "Iteration 115, loss = 0.54189419\n",
            "Iteration 116, loss = 0.54166685\n",
            "Iteration 117, loss = 0.54157145\n",
            "Iteration 118, loss = 0.54127755\n",
            "Iteration 119, loss = 0.54110824\n",
            "Iteration 120, loss = 0.54091335\n",
            "Iteration 121, loss = 0.54080406\n",
            "Iteration 122, loss = 0.54057483\n",
            "Iteration 123, loss = 0.54047833\n",
            "Iteration 124, loss = 0.54036178\n",
            "Iteration 125, loss = 0.54025044\n",
            "Iteration 126, loss = 0.54020928\n",
            "Iteration 127, loss = 0.53992647\n",
            "Iteration 128, loss = 0.53984166\n",
            "Iteration 129, loss = 0.53959034\n",
            "Iteration 130, loss = 0.53946540\n",
            "Iteration 131, loss = 0.53925245\n",
            "Iteration 132, loss = 0.53923818\n",
            "Iteration 133, loss = 0.53914175\n",
            "Iteration 134, loss = 0.53886620\n",
            "Iteration 135, loss = 0.53879700\n",
            "Iteration 136, loss = 0.53871831\n",
            "Iteration 137, loss = 0.53870791\n",
            "Iteration 138, loss = 0.53862767\n",
            "Iteration 139, loss = 0.53858188\n",
            "Iteration 140, loss = 0.53852717\n",
            "Iteration 141, loss = 0.53850989\n",
            "Iteration 142, loss = 0.53834784\n",
            "Iteration 143, loss = 0.53825307\n",
            "Iteration 144, loss = 0.53806887\n",
            "Iteration 145, loss = 0.53794010\n",
            "Iteration 146, loss = 0.53774559\n",
            "Iteration 147, loss = 0.53758361\n",
            "Iteration 148, loss = 0.53737071\n",
            "Iteration 149, loss = 0.53726470\n",
            "Iteration 150, loss = 0.53734451\n",
            "Iteration 151, loss = 0.53726507\n",
            "Iteration 152, loss = 0.53731548\n",
            "Iteration 153, loss = 0.53736934\n",
            "Iteration 154, loss = 0.53745739\n",
            "Iteration 155, loss = 0.53725930\n",
            "Iteration 156, loss = 0.53704189\n",
            "Iteration 157, loss = 0.53672173\n",
            "Iteration 158, loss = 0.53673070\n",
            "Iteration 159, loss = 0.53640430\n",
            "Iteration 160, loss = 0.53636990\n",
            "Iteration 161, loss = 0.53624303\n",
            "Iteration 162, loss = 0.53638501\n",
            "Iteration 163, loss = 0.53618847\n",
            "Iteration 164, loss = 0.53614041\n",
            "Iteration 165, loss = 0.53609172\n",
            "Iteration 166, loss = 0.53597557\n",
            "Iteration 167, loss = 0.53581952\n",
            "Iteration 168, loss = 0.53576021\n",
            "Iteration 169, loss = 0.53565892\n",
            "Iteration 170, loss = 0.53548493\n",
            "Iteration 171, loss = 0.53525189\n",
            "Iteration 172, loss = 0.53522615\n",
            "Iteration 173, loss = 0.53491178\n",
            "Iteration 174, loss = 0.53476777\n",
            "Iteration 175, loss = 0.53479530\n",
            "Iteration 176, loss = 0.53470860\n",
            "Iteration 177, loss = 0.53470846\n",
            "Iteration 178, loss = 0.53456213\n",
            "Iteration 179, loss = 0.53433062\n",
            "Iteration 180, loss = 0.53415476\n",
            "Iteration 181, loss = 0.53397479\n",
            "Iteration 182, loss = 0.53395149\n",
            "Iteration 183, loss = 0.53400429\n",
            "Iteration 184, loss = 0.53388538\n",
            "Iteration 185, loss = 0.53376509\n",
            "Iteration 186, loss = 0.53369759\n",
            "Iteration 187, loss = 0.53354538\n",
            "Iteration 188, loss = 0.53325988\n",
            "Iteration 189, loss = 0.53317403\n",
            "Iteration 190, loss = 0.53309107\n",
            "Iteration 191, loss = 0.53300766\n",
            "Iteration 192, loss = 0.53301044\n",
            "Iteration 193, loss = 0.53292291\n",
            "Iteration 194, loss = 0.53275166\n",
            "Iteration 195, loss = 0.53278188\n",
            "Iteration 196, loss = 0.53277496\n",
            "Iteration 197, loss = 0.53271782\n",
            "Iteration 198, loss = 0.53257775\n",
            "Iteration 199, loss = 0.53244531\n",
            "Iteration 200, loss = 0.53229277\n",
            "Iteration 1, loss = 0.88215379\n",
            "Iteration 2, loss = 0.85222359\n",
            "Iteration 3, loss = 0.81083995\n",
            "Iteration 4, loss = 0.76741428\n",
            "Iteration 5, loss = 0.72754261\n",
            "Iteration 6, loss = 0.69573379\n",
            "Iteration 7, loss = 0.67445780\n",
            "Iteration 8, loss = 0.65527871\n",
            "Iteration 9, loss = 0.64178806\n",
            "Iteration 10, loss = 0.63239091\n",
            "Iteration 11, loss = 0.62427738\n",
            "Iteration 12, loss = 0.61862901\n",
            "Iteration 13, loss = 0.61370662\n",
            "Iteration 14, loss = 0.60941738\n",
            "Iteration 15, loss = 0.60603175\n",
            "Iteration 16, loss = 0.60286680\n",
            "Iteration 17, loss = 0.60003633\n",
            "Iteration 18, loss = 0.59747356\n",
            "Iteration 19, loss = 0.59516755\n",
            "Iteration 20, loss = 0.59302637\n",
            "Iteration 21, loss = 0.59102069\n",
            "Iteration 22, loss = 0.58906081\n",
            "Iteration 23, loss = 0.58713216\n",
            "Iteration 24, loss = 0.58522686\n",
            "Iteration 25, loss = 0.58343551\n",
            "Iteration 26, loss = 0.58140821\n",
            "Iteration 27, loss = 0.57962191\n",
            "Iteration 28, loss = 0.57773768\n",
            "Iteration 29, loss = 0.57599283\n",
            "Iteration 30, loss = 0.57440911\n",
            "Iteration 31, loss = 0.57292679\n",
            "Iteration 32, loss = 0.57134210\n",
            "Iteration 33, loss = 0.57005743\n",
            "Iteration 34, loss = 0.56866518\n",
            "Iteration 35, loss = 0.56746551\n",
            "Iteration 36, loss = 0.56631215\n",
            "Iteration 37, loss = 0.56506011\n",
            "Iteration 38, loss = 0.56388150\n",
            "Iteration 39, loss = 0.56266819\n",
            "Iteration 40, loss = 0.56150018\n",
            "Iteration 41, loss = 0.56027643\n",
            "Iteration 42, loss = 0.55923365\n",
            "Iteration 43, loss = 0.55839916\n",
            "Iteration 44, loss = 0.55764037\n",
            "Iteration 45, loss = 0.55671588\n",
            "Iteration 46, loss = 0.55603532\n",
            "Iteration 47, loss = 0.55539730\n",
            "Iteration 48, loss = 0.55489305\n",
            "Iteration 49, loss = 0.55430102\n",
            "Iteration 50, loss = 0.55366618\n",
            "Iteration 51, loss = 0.55308823\n",
            "Iteration 52, loss = 0.55248338\n",
            "Iteration 53, loss = 0.55180195\n",
            "Iteration 54, loss = 0.55138032\n",
            "Iteration 55, loss = 0.55064507\n",
            "Iteration 56, loss = 0.55004513\n",
            "Iteration 57, loss = 0.54956125\n",
            "Iteration 58, loss = 0.54905056\n",
            "Iteration 59, loss = 0.54869579\n",
            "Iteration 60, loss = 0.54806567\n",
            "Iteration 61, loss = 0.54766645\n",
            "Iteration 62, loss = 0.54719837\n",
            "Iteration 63, loss = 0.54672621\n",
            "Iteration 64, loss = 0.54622130\n",
            "Iteration 65, loss = 0.54567083\n",
            "Iteration 66, loss = 0.54529271\n",
            "Iteration 67, loss = 0.54475088\n",
            "Iteration 68, loss = 0.54431846\n",
            "Iteration 69, loss = 0.54386110\n",
            "Iteration 70, loss = 0.54360986\n",
            "Iteration 71, loss = 0.54328716\n",
            "Iteration 72, loss = 0.54281576\n",
            "Iteration 73, loss = 0.54245138\n",
            "Iteration 74, loss = 0.54203901\n",
            "Iteration 75, loss = 0.54159343\n",
            "Iteration 76, loss = 0.54118515\n",
            "Iteration 77, loss = 0.54086813\n",
            "Iteration 78, loss = 0.54042066\n",
            "Iteration 79, loss = 0.53996870\n",
            "Iteration 80, loss = 0.53966351\n",
            "Iteration 81, loss = 0.53922575\n",
            "Iteration 82, loss = 0.53883012\n",
            "Iteration 83, loss = 0.53845198\n",
            "Iteration 84, loss = 0.53801889\n",
            "Iteration 85, loss = 0.53775136\n",
            "Iteration 86, loss = 0.53734075\n",
            "Iteration 87, loss = 0.53694553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 88, loss = 0.53650020\n",
            "Iteration 89, loss = 0.53610993\n",
            "Iteration 90, loss = 0.53573081\n",
            "Iteration 91, loss = 0.53537613\n",
            "Iteration 92, loss = 0.53504234\n",
            "Iteration 93, loss = 0.53466072\n",
            "Iteration 94, loss = 0.53441641\n",
            "Iteration 95, loss = 0.53430791\n",
            "Iteration 96, loss = 0.53402009\n",
            "Iteration 97, loss = 0.53367372\n",
            "Iteration 98, loss = 0.53342543\n",
            "Iteration 99, loss = 0.53312309\n",
            "Iteration 100, loss = 0.53285196\n",
            "Iteration 101, loss = 0.53257083\n",
            "Iteration 102, loss = 0.53226454\n",
            "Iteration 103, loss = 0.53199796\n",
            "Iteration 104, loss = 0.53182985\n",
            "Iteration 105, loss = 0.53158153\n",
            "Iteration 106, loss = 0.53145871\n",
            "Iteration 107, loss = 0.53126859\n",
            "Iteration 108, loss = 0.53101534\n",
            "Iteration 109, loss = 0.53072079\n",
            "Iteration 110, loss = 0.53037952\n",
            "Iteration 111, loss = 0.53005599\n",
            "Iteration 112, loss = 0.52990832\n",
            "Iteration 113, loss = 0.52973400\n",
            "Iteration 114, loss = 0.52956077\n",
            "Iteration 115, loss = 0.52935312\n",
            "Iteration 116, loss = 0.52936955\n",
            "Iteration 117, loss = 0.52935374\n",
            "Iteration 118, loss = 0.52917633\n",
            "Iteration 119, loss = 0.52910919\n",
            "Iteration 120, loss = 0.52891145\n",
            "Iteration 121, loss = 0.52874484\n",
            "Iteration 122, loss = 0.52855772\n",
            "Iteration 123, loss = 0.52843582\n",
            "Iteration 124, loss = 0.52818377\n",
            "Iteration 125, loss = 0.52808388\n",
            "Iteration 126, loss = 0.52787556\n",
            "Iteration 127, loss = 0.52750644\n",
            "Iteration 128, loss = 0.52727445\n",
            "Iteration 129, loss = 0.52683233\n",
            "Iteration 130, loss = 0.52675250\n",
            "Iteration 131, loss = 0.52643472\n",
            "Iteration 132, loss = 0.52630503\n",
            "Iteration 133, loss = 0.52617970\n",
            "Iteration 134, loss = 0.52593767\n",
            "Iteration 135, loss = 0.52584011\n",
            "Iteration 136, loss = 0.52570856\n",
            "Iteration 137, loss = 0.52566357\n",
            "Iteration 138, loss = 0.52552836\n",
            "Iteration 139, loss = 0.52532887\n",
            "Iteration 140, loss = 0.52528387\n",
            "Iteration 141, loss = 0.52526329\n",
            "Iteration 142, loss = 0.52515066\n",
            "Iteration 143, loss = 0.52518009\n",
            "Iteration 144, loss = 0.52495126\n",
            "Iteration 145, loss = 0.52484589\n",
            "Iteration 146, loss = 0.52458742\n",
            "Iteration 147, loss = 0.52429887\n",
            "Iteration 148, loss = 0.52416796\n",
            "Iteration 149, loss = 0.52398389\n",
            "Iteration 150, loss = 0.52379132\n",
            "Iteration 151, loss = 0.52378260\n",
            "Iteration 152, loss = 0.52382418\n",
            "Iteration 153, loss = 0.52381207\n",
            "Iteration 154, loss = 0.52367169\n",
            "Iteration 155, loss = 0.52341721\n",
            "Iteration 156, loss = 0.52313430\n",
            "Iteration 157, loss = 0.52274374\n",
            "Iteration 158, loss = 0.52263986\n",
            "Iteration 159, loss = 0.52240583\n",
            "Iteration 160, loss = 0.52218727\n",
            "Iteration 161, loss = 0.52206031\n",
            "Iteration 162, loss = 0.52206735\n",
            "Iteration 163, loss = 0.52195280\n",
            "Iteration 164, loss = 0.52169895\n",
            "Iteration 165, loss = 0.52151962\n",
            "Iteration 166, loss = 0.52134267\n",
            "Iteration 167, loss = 0.52118841\n",
            "Iteration 168, loss = 0.52097498\n",
            "Iteration 169, loss = 0.52083776\n",
            "Iteration 170, loss = 0.52083916\n",
            "Iteration 171, loss = 0.52055082\n",
            "Iteration 172, loss = 0.52041256\n",
            "Iteration 173, loss = 0.52028835\n",
            "Iteration 174, loss = 0.52022625\n",
            "Iteration 175, loss = 0.52011479\n",
            "Iteration 176, loss = 0.51996176\n",
            "Iteration 177, loss = 0.51984145\n",
            "Iteration 178, loss = 0.51974168\n",
            "Iteration 179, loss = 0.51959673\n",
            "Iteration 180, loss = 0.51946965\n",
            "Iteration 181, loss = 0.51924536\n",
            "Iteration 182, loss = 0.51917335\n",
            "Iteration 183, loss = 0.51939132\n",
            "Iteration 184, loss = 0.51957364\n",
            "Iteration 185, loss = 0.51943820\n",
            "Iteration 186, loss = 0.51930438\n",
            "Iteration 187, loss = 0.51891515\n",
            "Iteration 188, loss = 0.51846847\n",
            "Iteration 189, loss = 0.51813664\n",
            "Iteration 190, loss = 0.51797320\n",
            "Iteration 191, loss = 0.51799348\n",
            "Iteration 192, loss = 0.51797720\n",
            "Iteration 193, loss = 0.51803809\n",
            "Iteration 194, loss = 0.51791912\n",
            "Iteration 195, loss = 0.51799471\n",
            "Iteration 196, loss = 0.51802324\n",
            "Iteration 197, loss = 0.51777781\n",
            "Iteration 198, loss = 0.51761784\n",
            "Iteration 199, loss = 0.51725642\n",
            "Iteration 200, loss = 0.51724369\n",
            "Iteration 1, loss = 0.88505012\n",
            "Iteration 2, loss = 0.85225471\n",
            "Iteration 3, loss = 0.80692705\n",
            "Iteration 4, loss = 0.76187066\n",
            "Iteration 5, loss = 0.72125962\n",
            "Iteration 6, loss = 0.69001636\n",
            "Iteration 7, loss = 0.66839879\n",
            "Iteration 8, loss = 0.65016370\n",
            "Iteration 9, loss = 0.63775462\n",
            "Iteration 10, loss = 0.62892995\n",
            "Iteration 11, loss = 0.62138454\n",
            "Iteration 12, loss = 0.61613777\n",
            "Iteration 13, loss = 0.61152308\n",
            "Iteration 14, loss = 0.60716429\n",
            "Iteration 15, loss = 0.60391754\n",
            "Iteration 16, loss = 0.60054268\n",
            "Iteration 17, loss = 0.59793934\n",
            "Iteration 18, loss = 0.59555825\n",
            "Iteration 19, loss = 0.59346251\n",
            "Iteration 20, loss = 0.59155139\n",
            "Iteration 21, loss = 0.58951223\n",
            "Iteration 22, loss = 0.58759183\n",
            "Iteration 23, loss = 0.58570953\n",
            "Iteration 24, loss = 0.58384280\n",
            "Iteration 25, loss = 0.58210152\n",
            "Iteration 26, loss = 0.58016830\n",
            "Iteration 27, loss = 0.57837726\n",
            "Iteration 28, loss = 0.57656482\n",
            "Iteration 29, loss = 0.57466052\n",
            "Iteration 30, loss = 0.57280573\n",
            "Iteration 31, loss = 0.57095358\n",
            "Iteration 32, loss = 0.56918098\n",
            "Iteration 33, loss = 0.56766041\n",
            "Iteration 34, loss = 0.56606900\n",
            "Iteration 35, loss = 0.56460579\n",
            "Iteration 36, loss = 0.56336991\n",
            "Iteration 37, loss = 0.56205355\n",
            "Iteration 38, loss = 0.56082783\n",
            "Iteration 39, loss = 0.55947813\n",
            "Iteration 40, loss = 0.55812098\n",
            "Iteration 41, loss = 0.55681742\n",
            "Iteration 42, loss = 0.55563897\n",
            "Iteration 43, loss = 0.55440444\n",
            "Iteration 44, loss = 0.55338679\n",
            "Iteration 45, loss = 0.55235047\n",
            "Iteration 46, loss = 0.55162732\n",
            "Iteration 47, loss = 0.55084266\n",
            "Iteration 48, loss = 0.55021080\n",
            "Iteration 49, loss = 0.54960170\n",
            "Iteration 50, loss = 0.54880901\n",
            "Iteration 51, loss = 0.54801736\n",
            "Iteration 52, loss = 0.54727104\n",
            "Iteration 53, loss = 0.54650042\n",
            "Iteration 54, loss = 0.54590923\n",
            "Iteration 55, loss = 0.54515730\n",
            "Iteration 56, loss = 0.54447942\n",
            "Iteration 57, loss = 0.54393684\n",
            "Iteration 58, loss = 0.54339839\n",
            "Iteration 59, loss = 0.54306135\n",
            "Iteration 60, loss = 0.54242856\n",
            "Iteration 61, loss = 0.54192597\n",
            "Iteration 62, loss = 0.54135690\n",
            "Iteration 63, loss = 0.54087972\n",
            "Iteration 64, loss = 0.54047878\n",
            "Iteration 65, loss = 0.53981990\n",
            "Iteration 66, loss = 0.53953724\n",
            "Iteration 67, loss = 0.53893992\n",
            "Iteration 68, loss = 0.53851477\n",
            "Iteration 69, loss = 0.53807490\n",
            "Iteration 70, loss = 0.53759873\n",
            "Iteration 71, loss = 0.53721866\n",
            "Iteration 72, loss = 0.53669204\n",
            "Iteration 73, loss = 0.53617939\n",
            "Iteration 74, loss = 0.53568681\n",
            "Iteration 75, loss = 0.53513738\n",
            "Iteration 76, loss = 0.53456490\n",
            "Iteration 77, loss = 0.53422560\n",
            "Iteration 78, loss = 0.53376142\n",
            "Iteration 79, loss = 0.53334794\n",
            "Iteration 80, loss = 0.53301828\n",
            "Iteration 81, loss = 0.53262479\n",
            "Iteration 82, loss = 0.53209680\n",
            "Iteration 83, loss = 0.53182712\n",
            "Iteration 84, loss = 0.53126292\n",
            "Iteration 85, loss = 0.53094452\n",
            "Iteration 86, loss = 0.53046918\n",
            "Iteration 87, loss = 0.53009524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 88, loss = 0.52966948\n",
            "Iteration 89, loss = 0.52938844\n",
            "Iteration 90, loss = 0.52908594\n",
            "Iteration 91, loss = 0.52882262\n",
            "Iteration 92, loss = 0.52844590\n",
            "Iteration 93, loss = 0.52818897\n",
            "Iteration 94, loss = 0.52793869\n",
            "Iteration 95, loss = 0.52779602\n",
            "Iteration 96, loss = 0.52731289\n",
            "Iteration 97, loss = 0.52709549\n",
            "Iteration 98, loss = 0.52691321\n",
            "Iteration 99, loss = 0.52654699\n",
            "Iteration 100, loss = 0.52629573\n",
            "Iteration 101, loss = 0.52601449\n",
            "Iteration 102, loss = 0.52594261\n",
            "Iteration 103, loss = 0.52562273\n",
            "Iteration 104, loss = 0.52541581\n",
            "Iteration 105, loss = 0.52523782\n",
            "Iteration 106, loss = 0.52512014\n",
            "Iteration 107, loss = 0.52499645\n",
            "Iteration 108, loss = 0.52478098\n",
            "Iteration 109, loss = 0.52445755\n",
            "Iteration 110, loss = 0.52428922\n",
            "Iteration 111, loss = 0.52412815\n",
            "Iteration 112, loss = 0.52393271\n",
            "Iteration 113, loss = 0.52375826\n",
            "Iteration 114, loss = 0.52350129\n",
            "Iteration 115, loss = 0.52330822\n",
            "Iteration 116, loss = 0.52333016\n",
            "Iteration 117, loss = 0.52320371\n",
            "Iteration 118, loss = 0.52315890\n",
            "Iteration 119, loss = 0.52315360\n",
            "Iteration 120, loss = 0.52334969\n",
            "Iteration 121, loss = 0.52307553\n",
            "Iteration 122, loss = 0.52281812\n",
            "Iteration 123, loss = 0.52249253\n",
            "Iteration 124, loss = 0.52219434\n",
            "Iteration 125, loss = 0.52201704\n",
            "Iteration 126, loss = 0.52175775\n",
            "Iteration 127, loss = 0.52149214\n",
            "Iteration 128, loss = 0.52131010\n",
            "Iteration 129, loss = 0.52120647\n",
            "Iteration 130, loss = 0.52107551\n",
            "Iteration 131, loss = 0.52088518\n",
            "Iteration 132, loss = 0.52069858\n",
            "Iteration 133, loss = 0.52066429\n",
            "Iteration 134, loss = 0.52056682\n",
            "Iteration 135, loss = 0.52044684\n",
            "Iteration 136, loss = 0.52035116\n",
            "Iteration 137, loss = 0.52039273\n",
            "Iteration 138, loss = 0.52031628\n",
            "Iteration 139, loss = 0.52016205\n",
            "Iteration 140, loss = 0.52018246\n",
            "Iteration 141, loss = 0.52019436\n",
            "Iteration 142, loss = 0.52010632\n",
            "Iteration 143, loss = 0.52026974\n",
            "Iteration 144, loss = 0.52010303\n",
            "Iteration 145, loss = 0.51996273\n",
            "Iteration 146, loss = 0.51972732\n",
            "Iteration 147, loss = 0.51938107\n",
            "Iteration 148, loss = 0.51915831\n",
            "Iteration 149, loss = 0.51880936\n",
            "Iteration 150, loss = 0.51883335\n",
            "Iteration 151, loss = 0.51890969\n",
            "Iteration 152, loss = 0.51903890\n",
            "Iteration 153, loss = 0.51904551\n",
            "Iteration 154, loss = 0.51885134\n",
            "Iteration 155, loss = 0.51868661\n",
            "Iteration 156, loss = 0.51862434\n",
            "Iteration 157, loss = 0.51832945\n",
            "Iteration 158, loss = 0.51823184\n",
            "Iteration 159, loss = 0.51809408\n",
            "Iteration 160, loss = 0.51797490\n",
            "Iteration 161, loss = 0.51798341\n",
            "Iteration 162, loss = 0.51796662\n",
            "Iteration 163, loss = 0.51788220\n",
            "Iteration 164, loss = 0.51773377\n",
            "Iteration 165, loss = 0.51762954\n",
            "Iteration 166, loss = 0.51757075\n",
            "Iteration 167, loss = 0.51758254\n",
            "Iteration 168, loss = 0.51731918\n",
            "Iteration 169, loss = 0.51733879\n",
            "Iteration 170, loss = 0.51757394\n",
            "Iteration 171, loss = 0.51728112\n",
            "Iteration 172, loss = 0.51717302\n",
            "Iteration 173, loss = 0.51709239\n",
            "Iteration 174, loss = 0.51688080\n",
            "Iteration 175, loss = 0.51693992\n",
            "Iteration 176, loss = 0.51677494\n",
            "Iteration 177, loss = 0.51658956\n",
            "Iteration 178, loss = 0.51642716\n",
            "Iteration 179, loss = 0.51634552\n",
            "Iteration 180, loss = 0.51637389\n",
            "Iteration 181, loss = 0.51619640\n",
            "Iteration 182, loss = 0.51612988\n",
            "Iteration 183, loss = 0.51621726\n",
            "Iteration 184, loss = 0.51622963\n",
            "Iteration 185, loss = 0.51610881\n",
            "Iteration 186, loss = 0.51599573\n",
            "Iteration 187, loss = 0.51587262\n",
            "Iteration 188, loss = 0.51571481\n",
            "Iteration 189, loss = 0.51569069\n",
            "Iteration 190, loss = 0.51563333\n",
            "Iteration 191, loss = 0.51556212\n",
            "Iteration 192, loss = 0.51552936\n",
            "Iteration 193, loss = 0.51558506\n",
            "Iteration 194, loss = 0.51561893\n",
            "Iteration 195, loss = 0.51557442\n",
            "Iteration 196, loss = 0.51545020\n",
            "Iteration 197, loss = 0.51531570\n",
            "Iteration 198, loss = 0.51522442\n",
            "Iteration 199, loss = 0.51512491\n",
            "Iteration 200, loss = 0.51526171\n",
            "Iteration 1, loss = 0.87868239\n",
            "Iteration 2, loss = 0.84534527\n",
            "Iteration 3, loss = 0.80264634\n",
            "Iteration 4, loss = 0.75856716\n",
            "Iteration 5, loss = 0.72047810\n",
            "Iteration 6, loss = 0.69020470\n",
            "Iteration 7, loss = 0.66899651\n",
            "Iteration 8, loss = 0.65325101\n",
            "Iteration 9, loss = 0.64084348\n",
            "Iteration 10, loss = 0.63256251\n",
            "Iteration 11, loss = 0.62593344\n",
            "Iteration 12, loss = 0.62085925\n",
            "Iteration 13, loss = 0.61668379\n",
            "Iteration 14, loss = 0.61298618\n",
            "Iteration 15, loss = 0.61027972\n",
            "Iteration 16, loss = 0.60738093\n",
            "Iteration 17, loss = 0.60519015\n",
            "Iteration 18, loss = 0.60314661\n",
            "Iteration 19, loss = 0.60138211\n",
            "Iteration 20, loss = 0.59990388\n",
            "Iteration 21, loss = 0.59816570\n",
            "Iteration 22, loss = 0.59667437\n",
            "Iteration 23, loss = 0.59523266\n",
            "Iteration 24, loss = 0.59383865\n",
            "Iteration 25, loss = 0.59244065\n",
            "Iteration 26, loss = 0.59109045\n",
            "Iteration 27, loss = 0.58969415\n",
            "Iteration 28, loss = 0.58839863\n",
            "Iteration 29, loss = 0.58706673\n",
            "Iteration 30, loss = 0.58587255\n",
            "Iteration 31, loss = 0.58455172\n",
            "Iteration 32, loss = 0.58331959\n",
            "Iteration 33, loss = 0.58212022\n",
            "Iteration 34, loss = 0.58094719\n",
            "Iteration 35, loss = 0.57976950\n",
            "Iteration 36, loss = 0.57873844\n",
            "Iteration 37, loss = 0.57759582\n",
            "Iteration 38, loss = 0.57646096\n",
            "Iteration 39, loss = 0.57532754\n",
            "Iteration 40, loss = 0.57432229\n",
            "Iteration 41, loss = 0.57318304\n",
            "Iteration 42, loss = 0.57217843\n",
            "Iteration 43, loss = 0.57098995\n",
            "Iteration 44, loss = 0.56998664\n",
            "Iteration 45, loss = 0.56894697\n",
            "Iteration 46, loss = 0.56822469\n",
            "Iteration 47, loss = 0.56721476\n",
            "Iteration 48, loss = 0.56646854\n",
            "Iteration 49, loss = 0.56570712\n",
            "Iteration 50, loss = 0.56480305\n",
            "Iteration 51, loss = 0.56388092\n",
            "Iteration 52, loss = 0.56300559\n",
            "Iteration 53, loss = 0.56215389\n",
            "Iteration 54, loss = 0.56146918\n",
            "Iteration 55, loss = 0.56066012\n",
            "Iteration 56, loss = 0.56010518\n",
            "Iteration 57, loss = 0.55936017\n",
            "Iteration 58, loss = 0.55867960\n",
            "Iteration 59, loss = 0.55806008\n",
            "Iteration 60, loss = 0.55745688\n",
            "Iteration 61, loss = 0.55683435\n",
            "Iteration 62, loss = 0.55617745\n",
            "Iteration 63, loss = 0.55567840\n",
            "Iteration 64, loss = 0.55522448\n",
            "Iteration 65, loss = 0.55451282\n",
            "Iteration 66, loss = 0.55408871\n",
            "Iteration 67, loss = 0.55340767\n",
            "Iteration 68, loss = 0.55308825\n",
            "Iteration 69, loss = 0.55248680\n",
            "Iteration 70, loss = 0.55197835\n",
            "Iteration 71, loss = 0.55151704\n",
            "Iteration 72, loss = 0.55094640\n",
            "Iteration 73, loss = 0.55043214\n",
            "Iteration 74, loss = 0.54999353\n",
            "Iteration 75, loss = 0.54958755\n",
            "Iteration 76, loss = 0.54921556\n",
            "Iteration 77, loss = 0.54881768\n",
            "Iteration 78, loss = 0.54841853\n",
            "Iteration 79, loss = 0.54806308\n",
            "Iteration 80, loss = 0.54755768\n",
            "Iteration 81, loss = 0.54715637\n",
            "Iteration 82, loss = 0.54664993\n",
            "Iteration 83, loss = 0.54633178\n",
            "Iteration 84, loss = 0.54586308\n",
            "Iteration 85, loss = 0.54531629\n",
            "Iteration 86, loss = 0.54479784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 87, loss = 0.54431345\n",
            "Iteration 88, loss = 0.54395825\n",
            "Iteration 89, loss = 0.54347640\n",
            "Iteration 90, loss = 0.54309495\n",
            "Iteration 91, loss = 0.54277762\n",
            "Iteration 92, loss = 0.54228906\n",
            "Iteration 93, loss = 0.54182477\n",
            "Iteration 94, loss = 0.54136635\n",
            "Iteration 95, loss = 0.54103392\n",
            "Iteration 96, loss = 0.54055500\n",
            "Iteration 97, loss = 0.54020524\n",
            "Iteration 98, loss = 0.53990612\n",
            "Iteration 99, loss = 0.53946956\n",
            "Iteration 100, loss = 0.53922342\n",
            "Iteration 101, loss = 0.53886199\n",
            "Iteration 102, loss = 0.53863403\n",
            "Iteration 103, loss = 0.53835363\n",
            "Iteration 104, loss = 0.53807245\n",
            "Iteration 105, loss = 0.53787585\n",
            "Iteration 106, loss = 0.53772048\n",
            "Iteration 107, loss = 0.53739586\n",
            "Iteration 108, loss = 0.53703830\n",
            "Iteration 109, loss = 0.53674683\n",
            "Iteration 110, loss = 0.53654103\n",
            "Iteration 111, loss = 0.53635130\n",
            "Iteration 112, loss = 0.53605854\n",
            "Iteration 113, loss = 0.53586204\n",
            "Iteration 114, loss = 0.53548591\n",
            "Iteration 115, loss = 0.53527808\n",
            "Iteration 116, loss = 0.53506597\n",
            "Iteration 117, loss = 0.53496093\n",
            "Iteration 118, loss = 0.53466493\n",
            "Iteration 119, loss = 0.53449262\n",
            "Iteration 120, loss = 0.53439704\n",
            "Iteration 121, loss = 0.53412809\n",
            "Iteration 122, loss = 0.53385286\n",
            "Iteration 123, loss = 0.53358210\n",
            "Iteration 124, loss = 0.53332720\n",
            "Iteration 125, loss = 0.53317083\n",
            "Iteration 126, loss = 0.53286274\n",
            "Iteration 127, loss = 0.53277989\n",
            "Iteration 128, loss = 0.53248450\n",
            "Iteration 129, loss = 0.53231624\n",
            "Iteration 130, loss = 0.53223747\n",
            "Iteration 131, loss = 0.53207475\n",
            "Iteration 132, loss = 0.53183436\n",
            "Iteration 133, loss = 0.53166183\n",
            "Iteration 134, loss = 0.53146902\n",
            "Iteration 135, loss = 0.53131621\n",
            "Iteration 136, loss = 0.53113035\n",
            "Iteration 137, loss = 0.53099015\n",
            "Iteration 138, loss = 0.53085964\n",
            "Iteration 139, loss = 0.53063896\n",
            "Iteration 140, loss = 0.53049519\n",
            "Iteration 141, loss = 0.53028788\n",
            "Iteration 142, loss = 0.53009605\n",
            "Iteration 143, loss = 0.53001063\n",
            "Iteration 144, loss = 0.52968939\n",
            "Iteration 145, loss = 0.52955078\n",
            "Iteration 146, loss = 0.52932441\n",
            "Iteration 147, loss = 0.52903402\n",
            "Iteration 148, loss = 0.52892468\n",
            "Iteration 149, loss = 0.52868529\n",
            "Iteration 150, loss = 0.52853744\n",
            "Iteration 151, loss = 0.52837997\n",
            "Iteration 152, loss = 0.52821335\n",
            "Iteration 153, loss = 0.52816267\n",
            "Iteration 154, loss = 0.52807413\n",
            "Iteration 155, loss = 0.52796213\n",
            "Iteration 156, loss = 0.52782152\n",
            "Iteration 157, loss = 0.52746300\n",
            "Iteration 158, loss = 0.52737167\n",
            "Iteration 159, loss = 0.52717787\n",
            "Iteration 160, loss = 0.52705787\n",
            "Iteration 161, loss = 0.52695369\n",
            "Iteration 162, loss = 0.52680928\n",
            "Iteration 163, loss = 0.52659861\n",
            "Iteration 164, loss = 0.52651859\n",
            "Iteration 165, loss = 0.52635064\n",
            "Iteration 166, loss = 0.52624605\n",
            "Iteration 167, loss = 0.52621231\n",
            "Iteration 168, loss = 0.52607817\n",
            "Iteration 169, loss = 0.52594390\n",
            "Iteration 170, loss = 0.52606914\n",
            "Iteration 171, loss = 0.52601254\n",
            "Iteration 172, loss = 0.52575548\n",
            "Iteration 173, loss = 0.52557508\n",
            "Iteration 174, loss = 0.52543777\n",
            "Iteration 175, loss = 0.52531886\n",
            "Iteration 176, loss = 0.52521512\n",
            "Iteration 177, loss = 0.52508019\n",
            "Iteration 178, loss = 0.52499191\n",
            "Iteration 179, loss = 0.52490653\n",
            "Iteration 180, loss = 0.52478525\n",
            "Iteration 181, loss = 0.52471523\n",
            "Iteration 182, loss = 0.52462331\n",
            "Iteration 183, loss = 0.52458967\n",
            "Iteration 184, loss = 0.52459366\n",
            "Iteration 185, loss = 0.52449418\n",
            "Iteration 186, loss = 0.52445561\n",
            "Iteration 187, loss = 0.52407186\n",
            "Iteration 188, loss = 0.52407868\n",
            "Iteration 189, loss = 0.52398571\n",
            "Iteration 190, loss = 0.52396216\n",
            "Iteration 191, loss = 0.52382399\n",
            "Iteration 192, loss = 0.52370536\n",
            "Iteration 193, loss = 0.52360539\n",
            "Iteration 194, loss = 0.52357143\n",
            "Iteration 195, loss = 0.52360753\n",
            "Iteration 196, loss = 0.52332614\n",
            "Iteration 197, loss = 0.52319019\n",
            "Iteration 198, loss = 0.52307208\n",
            "Iteration 199, loss = 0.52295488\n",
            "Iteration 200, loss = 0.52297903\n",
            "Iteration 1, loss = 0.88420876\n",
            "Iteration 2, loss = 0.85029778\n",
            "Iteration 3, loss = 0.80666346\n",
            "Iteration 4, loss = 0.76251740\n",
            "Iteration 5, loss = 0.72239930\n",
            "Iteration 6, loss = 0.69102003\n",
            "Iteration 7, loss = 0.66914065\n",
            "Iteration 8, loss = 0.65366285\n",
            "Iteration 9, loss = 0.64129681\n",
            "Iteration 10, loss = 0.63364634\n",
            "Iteration 11, loss = 0.62684979\n",
            "Iteration 12, loss = 0.62195144\n",
            "Iteration 13, loss = 0.61783002\n",
            "Iteration 14, loss = 0.61387722\n",
            "Iteration 15, loss = 0.61079756\n",
            "Iteration 16, loss = 0.60786781\n",
            "Iteration 17, loss = 0.60543311\n",
            "Iteration 18, loss = 0.60300274\n",
            "Iteration 19, loss = 0.60112791\n",
            "Iteration 20, loss = 0.59925533\n",
            "Iteration 21, loss = 0.59731542\n",
            "Iteration 22, loss = 0.59563197\n",
            "Iteration 23, loss = 0.59403794\n",
            "Iteration 24, loss = 0.59252695\n",
            "Iteration 25, loss = 0.59087718\n",
            "Iteration 26, loss = 0.58932431\n",
            "Iteration 27, loss = 0.58783735\n",
            "Iteration 28, loss = 0.58652909\n",
            "Iteration 29, loss = 0.58526168\n",
            "Iteration 30, loss = 0.58384581\n",
            "Iteration 31, loss = 0.58249022\n",
            "Iteration 32, loss = 0.58107302\n",
            "Iteration 33, loss = 0.57968523\n",
            "Iteration 34, loss = 0.57826974\n",
            "Iteration 35, loss = 0.57685409\n",
            "Iteration 36, loss = 0.57570653\n",
            "Iteration 37, loss = 0.57454359\n",
            "Iteration 38, loss = 0.57337072\n",
            "Iteration 39, loss = 0.57236750\n",
            "Iteration 40, loss = 0.57151315\n",
            "Iteration 41, loss = 0.57056749\n",
            "Iteration 42, loss = 0.56961456\n",
            "Iteration 43, loss = 0.56866093\n",
            "Iteration 44, loss = 0.56779404\n",
            "Iteration 45, loss = 0.56702039\n",
            "Iteration 46, loss = 0.56636969\n",
            "Iteration 47, loss = 0.56567263\n",
            "Iteration 48, loss = 0.56506709\n",
            "Iteration 49, loss = 0.56456720\n",
            "Iteration 50, loss = 0.56395569\n",
            "Iteration 51, loss = 0.56323524\n",
            "Iteration 52, loss = 0.56254141\n",
            "Iteration 53, loss = 0.56184053\n",
            "Iteration 54, loss = 0.56130169\n",
            "Iteration 55, loss = 0.56068746\n",
            "Iteration 56, loss = 0.56027446\n",
            "Iteration 57, loss = 0.55964821\n",
            "Iteration 58, loss = 0.55911947\n",
            "Iteration 59, loss = 0.55863265\n",
            "Iteration 60, loss = 0.55817650\n",
            "Iteration 61, loss = 0.55778245\n",
            "Iteration 62, loss = 0.55727190\n",
            "Iteration 63, loss = 0.55689895\n",
            "Iteration 64, loss = 0.55664518\n",
            "Iteration 65, loss = 0.55617918\n",
            "Iteration 66, loss = 0.55581549\n",
            "Iteration 67, loss = 0.55536771\n",
            "Iteration 68, loss = 0.55518132\n",
            "Iteration 69, loss = 0.55482123\n",
            "Iteration 70, loss = 0.55467827\n",
            "Iteration 71, loss = 0.55436780\n",
            "Iteration 72, loss = 0.55404453\n",
            "Iteration 73, loss = 0.55373464\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 74, loss = 0.55344119\n",
            "Iteration 75, loss = 0.55320977\n",
            "Iteration 76, loss = 0.55299683\n",
            "Iteration 77, loss = 0.55278411\n",
            "Iteration 78, loss = 0.55251193\n",
            "Iteration 79, loss = 0.55228212\n",
            "Iteration 80, loss = 0.55204654\n",
            "Iteration 81, loss = 0.55182542\n",
            "Iteration 82, loss = 0.55150055\n",
            "Iteration 83, loss = 0.55129268\n",
            "Iteration 84, loss = 0.55103820\n",
            "Iteration 85, loss = 0.55075821\n",
            "Iteration 86, loss = 0.55045633\n",
            "Iteration 87, loss = 0.55023268\n",
            "Iteration 88, loss = 0.55010605\n",
            "Iteration 89, loss = 0.54982742\n",
            "Iteration 90, loss = 0.54971483\n",
            "Iteration 91, loss = 0.54972757\n",
            "Iteration 92, loss = 0.54939568\n",
            "Iteration 93, loss = 0.54898982\n",
            "Iteration 94, loss = 0.54881902\n",
            "Iteration 95, loss = 0.54858255\n",
            "Iteration 96, loss = 0.54831362\n",
            "Iteration 97, loss = 0.54811557\n",
            "Iteration 98, loss = 0.54799809\n",
            "Iteration 99, loss = 0.54778299\n",
            "Iteration 100, loss = 0.54765785\n",
            "Iteration 101, loss = 0.54740884\n",
            "Iteration 102, loss = 0.54725663\n",
            "Iteration 103, loss = 0.54706772\n",
            "Iteration 104, loss = 0.54692220\n",
            "Iteration 105, loss = 0.54679752\n",
            "Iteration 106, loss = 0.54662427\n",
            "Iteration 107, loss = 0.54639225\n",
            "Iteration 108, loss = 0.54621099\n",
            "Iteration 109, loss = 0.54610797\n",
            "Iteration 110, loss = 0.54593550\n",
            "Iteration 111, loss = 0.54586041\n",
            "Iteration 112, loss = 0.54564492\n",
            "Iteration 113, loss = 0.54554842\n",
            "Iteration 114, loss = 0.54546008\n",
            "Iteration 115, loss = 0.54524899\n",
            "Iteration 116, loss = 0.54515192\n",
            "Iteration 117, loss = 0.54505288\n",
            "Iteration 118, loss = 0.54479587\n",
            "Iteration 119, loss = 0.54463250\n",
            "Iteration 120, loss = 0.54452734\n",
            "Iteration 121, loss = 0.54425207\n",
            "Iteration 122, loss = 0.54408486\n",
            "Iteration 123, loss = 0.54394875\n",
            "Iteration 124, loss = 0.54371186\n",
            "Iteration 125, loss = 0.54363433\n",
            "Iteration 126, loss = 0.54331513\n",
            "Iteration 127, loss = 0.54320696\n",
            "Iteration 128, loss = 0.54298310\n",
            "Iteration 129, loss = 0.54284629\n",
            "Iteration 130, loss = 0.54268682\n",
            "Iteration 131, loss = 0.54248868\n",
            "Iteration 132, loss = 0.54230650\n",
            "Iteration 133, loss = 0.54214827\n",
            "Iteration 134, loss = 0.54197261\n",
            "Iteration 135, loss = 0.54181534\n",
            "Iteration 136, loss = 0.54167607\n",
            "Iteration 137, loss = 0.54167254\n",
            "Iteration 138, loss = 0.54154016\n",
            "Iteration 139, loss = 0.54135835\n",
            "Iteration 140, loss = 0.54119593\n",
            "Iteration 141, loss = 0.54097055\n",
            "Iteration 142, loss = 0.54079159\n",
            "Iteration 143, loss = 0.54062195\n",
            "Iteration 144, loss = 0.54035561\n",
            "Iteration 145, loss = 0.54022586\n",
            "Iteration 146, loss = 0.54003420\n",
            "Iteration 147, loss = 0.53985661\n",
            "Iteration 148, loss = 0.53970262\n",
            "Iteration 149, loss = 0.53956974\n",
            "Iteration 150, loss = 0.53951964\n",
            "Iteration 151, loss = 0.53938195\n",
            "Iteration 152, loss = 0.53918085\n",
            "Iteration 153, loss = 0.53913227\n",
            "Iteration 154, loss = 0.53897941\n",
            "Iteration 155, loss = 0.53883506\n",
            "Iteration 156, loss = 0.53863939\n",
            "Iteration 157, loss = 0.53846476\n",
            "Iteration 158, loss = 0.53820396\n",
            "Iteration 159, loss = 0.53802165\n",
            "Iteration 160, loss = 0.53787899\n",
            "Iteration 161, loss = 0.53772227\n",
            "Iteration 162, loss = 0.53753486\n",
            "Iteration 163, loss = 0.53725080\n",
            "Iteration 164, loss = 0.53711853\n",
            "Iteration 165, loss = 0.53691431\n",
            "Iteration 166, loss = 0.53675786\n",
            "Iteration 167, loss = 0.53657283\n",
            "Iteration 168, loss = 0.53640559\n",
            "Iteration 169, loss = 0.53626383\n",
            "Iteration 170, loss = 0.53634432\n",
            "Iteration 171, loss = 0.53637854\n",
            "Iteration 172, loss = 0.53625644\n",
            "Iteration 173, loss = 0.53608987\n",
            "Iteration 174, loss = 0.53568733\n",
            "Iteration 175, loss = 0.53551559\n",
            "Iteration 176, loss = 0.53531452\n",
            "Iteration 177, loss = 0.53505349\n",
            "Iteration 178, loss = 0.53490349\n",
            "Iteration 179, loss = 0.53485469\n",
            "Iteration 180, loss = 0.53470325\n",
            "Iteration 181, loss = 0.53455529\n",
            "Iteration 182, loss = 0.53447901\n",
            "Iteration 183, loss = 0.53461682\n",
            "Iteration 184, loss = 0.53429817\n",
            "Iteration 185, loss = 0.53407708\n",
            "Iteration 186, loss = 0.53409191\n",
            "Iteration 187, loss = 0.53385466\n",
            "Iteration 188, loss = 0.53385963\n",
            "Iteration 189, loss = 0.53368813\n",
            "Iteration 190, loss = 0.53366462\n",
            "Iteration 191, loss = 0.53347692\n",
            "Iteration 192, loss = 0.53331859\n",
            "Iteration 193, loss = 0.53321726\n",
            "Iteration 194, loss = 0.53314220\n",
            "Iteration 195, loss = 0.53319821\n",
            "Iteration 196, loss = 0.53292876\n",
            "Iteration 197, loss = 0.53280856\n",
            "Iteration 198, loss = 0.53262628\n",
            "Iteration 199, loss = 0.53247102\n",
            "Iteration 200, loss = 0.53240201\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 200 and for layer number 5 : 0.7150000000000001\n",
            "Iteration 1, loss = 1.36577264\n",
            "Iteration 2, loss = 1.21631564\n",
            "Iteration 3, loss = 1.02778964\n",
            "Iteration 4, loss = 0.86161348\n",
            "Iteration 5, loss = 0.75517608\n",
            "Iteration 6, loss = 0.69677016\n",
            "Iteration 7, loss = 0.67054166\n",
            "Iteration 8, loss = 0.65696349\n",
            "Iteration 9, loss = 0.64924227\n",
            "Iteration 10, loss = 0.64200040\n",
            "Iteration 11, loss = 0.63502000\n",
            "Iteration 12, loss = 0.62804496\n",
            "Iteration 13, loss = 0.62121423\n",
            "Iteration 14, loss = 0.61544081\n",
            "Iteration 15, loss = 0.60966913\n",
            "Iteration 16, loss = 0.60458321\n",
            "Iteration 17, loss = 0.60053459\n",
            "Iteration 18, loss = 0.59669563\n",
            "Iteration 19, loss = 0.59365093\n",
            "Iteration 20, loss = 0.59065223\n",
            "Iteration 21, loss = 0.58804416\n",
            "Iteration 22, loss = 0.58582895\n",
            "Iteration 23, loss = 0.58349854\n",
            "Iteration 24, loss = 0.58165816\n",
            "Iteration 25, loss = 0.57966754\n",
            "Iteration 26, loss = 0.57764872\n",
            "Iteration 27, loss = 0.57592592\n",
            "Iteration 28, loss = 0.57428512\n",
            "Iteration 29, loss = 0.57254934\n",
            "Iteration 30, loss = 0.57118603\n",
            "Iteration 31, loss = 0.56960122\n",
            "Iteration 32, loss = 0.56838182\n",
            "Iteration 33, loss = 0.56685777\n",
            "Iteration 34, loss = 0.56551142\n",
            "Iteration 35, loss = 0.56440679\n",
            "Iteration 36, loss = 0.56326099\n",
            "Iteration 37, loss = 0.56204087\n",
            "Iteration 38, loss = 0.56109031\n",
            "Iteration 39, loss = 0.55986212\n",
            "Iteration 40, loss = 0.55891318\n",
            "Iteration 41, loss = 0.55804948\n",
            "Iteration 42, loss = 0.55708778\n",
            "Iteration 43, loss = 0.55612363\n",
            "Iteration 44, loss = 0.55520370\n",
            "Iteration 45, loss = 0.55433500\n",
            "Iteration 46, loss = 0.55324730\n",
            "Iteration 47, loss = 0.55239553\n",
            "Iteration 48, loss = 0.55173779\n",
            "Iteration 49, loss = 0.55101393\n",
            "Iteration 50, loss = 0.55038937\n",
            "Iteration 51, loss = 0.54971687\n",
            "Iteration 52, loss = 0.54916416\n",
            "Iteration 53, loss = 0.54852380\n",
            "Iteration 54, loss = 0.54794764\n",
            "Iteration 55, loss = 0.54748227\n",
            "Iteration 56, loss = 0.54684467\n",
            "Iteration 57, loss = 0.54634707\n",
            "Iteration 58, loss = 0.54580420\n",
            "Iteration 59, loss = 0.54519260\n",
            "Iteration 60, loss = 0.54468122\n",
            "Iteration 61, loss = 0.54419114\n",
            "Iteration 62, loss = 0.54359535\n",
            "Iteration 63, loss = 0.54298356\n",
            "Iteration 64, loss = 0.54246837\n",
            "Iteration 65, loss = 0.54181965\n",
            "Iteration 66, loss = 0.54124425\n",
            "Iteration 67, loss = 0.54066048\n",
            "Iteration 68, loss = 0.54002205\n",
            "Iteration 69, loss = 0.53948822\n",
            "Iteration 70, loss = 0.53892974\n",
            "Iteration 71, loss = 0.53834934\n",
            "Iteration 72, loss = 0.53786761\n",
            "Iteration 73, loss = 0.53739308\n",
            "Iteration 74, loss = 0.53716736\n",
            "Iteration 75, loss = 0.53641757\n",
            "Iteration 76, loss = 0.53606271\n",
            "Iteration 77, loss = 0.53557915\n",
            "Iteration 78, loss = 0.53511829\n",
            "Iteration 79, loss = 0.53464439\n",
            "Iteration 80, loss = 0.53433061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 81, loss = 0.53404424\n",
            "Iteration 82, loss = 0.53369755\n",
            "Iteration 83, loss = 0.53324430\n",
            "Iteration 84, loss = 0.53294977\n",
            "Iteration 85, loss = 0.53250768\n",
            "Iteration 86, loss = 0.53216655\n",
            "Iteration 87, loss = 0.53175103\n",
            "Iteration 88, loss = 0.53142669\n",
            "Iteration 89, loss = 0.53126093\n",
            "Iteration 90, loss = 0.53081181\n",
            "Iteration 91, loss = 0.53051382\n",
            "Iteration 92, loss = 0.53014209\n",
            "Iteration 93, loss = 0.52983351\n",
            "Iteration 94, loss = 0.52955779\n",
            "Iteration 95, loss = 0.52924251\n",
            "Iteration 96, loss = 0.52894063\n",
            "Iteration 97, loss = 0.52859632\n",
            "Iteration 98, loss = 0.52836618\n",
            "Iteration 99, loss = 0.52816789\n",
            "Iteration 100, loss = 0.52777562\n",
            "Iteration 101, loss = 0.52736082\n",
            "Iteration 102, loss = 0.52689572\n",
            "Iteration 103, loss = 0.52655346\n",
            "Iteration 104, loss = 0.52632362\n",
            "Iteration 105, loss = 0.52602093\n",
            "Iteration 106, loss = 0.52565738\n",
            "Iteration 107, loss = 0.52529612\n",
            "Iteration 108, loss = 0.52513463\n",
            "Iteration 109, loss = 0.52454681\n",
            "Iteration 110, loss = 0.52419489\n",
            "Iteration 111, loss = 0.52393741\n",
            "Iteration 112, loss = 0.52364714\n",
            "Iteration 113, loss = 0.52336835\n",
            "Iteration 114, loss = 0.52292347\n",
            "Iteration 115, loss = 0.52261625\n",
            "Iteration 116, loss = 0.52216289\n",
            "Iteration 117, loss = 0.52191115\n",
            "Iteration 118, loss = 0.52167411\n",
            "Iteration 119, loss = 0.52129701\n",
            "Iteration 120, loss = 0.52086030\n",
            "Iteration 121, loss = 0.52065410\n",
            "Iteration 122, loss = 0.52036663\n",
            "Iteration 123, loss = 0.52007206\n",
            "Iteration 124, loss = 0.51971518\n",
            "Iteration 125, loss = 0.51938290\n",
            "Iteration 126, loss = 0.51906406\n",
            "Iteration 127, loss = 0.51878832\n",
            "Iteration 128, loss = 0.51843197\n",
            "Iteration 129, loss = 0.51804059\n",
            "Iteration 130, loss = 0.51767480\n",
            "Iteration 131, loss = 0.51750205\n",
            "Iteration 132, loss = 0.51704063\n",
            "Iteration 133, loss = 0.51670742\n",
            "Iteration 134, loss = 0.51652978\n",
            "Iteration 135, loss = 0.51621894\n",
            "Iteration 136, loss = 0.51597440\n",
            "Iteration 137, loss = 0.51563672\n",
            "Iteration 138, loss = 0.51525275\n",
            "Iteration 139, loss = 0.51503894\n",
            "Iteration 140, loss = 0.51466413\n",
            "Iteration 141, loss = 0.51433617\n",
            "Iteration 142, loss = 0.51414569\n",
            "Iteration 143, loss = 0.51380601\n",
            "Iteration 144, loss = 0.51343096\n",
            "Iteration 145, loss = 0.51312277\n",
            "Iteration 146, loss = 0.51284898\n",
            "Iteration 147, loss = 0.51248492\n",
            "Iteration 148, loss = 0.51225234\n",
            "Iteration 149, loss = 0.51185209\n",
            "Iteration 150, loss = 0.51170754\n",
            "Iteration 151, loss = 0.51134963\n",
            "Iteration 152, loss = 0.51111595\n",
            "Iteration 153, loss = 0.51070866\n",
            "Iteration 154, loss = 0.51042514\n",
            "Iteration 155, loss = 0.50993130\n",
            "Iteration 156, loss = 0.50975291\n",
            "Iteration 157, loss = 0.50953039\n",
            "Iteration 158, loss = 0.50905971\n",
            "Iteration 159, loss = 0.50877156\n",
            "Iteration 160, loss = 0.50845924\n",
            "Iteration 161, loss = 0.50826456\n",
            "Iteration 162, loss = 0.50795596\n",
            "Iteration 163, loss = 0.50773307\n",
            "Iteration 164, loss = 0.50741636\n",
            "Iteration 165, loss = 0.50715164\n",
            "Iteration 166, loss = 0.50680300\n",
            "Iteration 167, loss = 0.50650680\n",
            "Iteration 168, loss = 0.50637832\n",
            "Iteration 169, loss = 0.50608983\n",
            "Iteration 170, loss = 0.50568664\n",
            "Iteration 171, loss = 0.50540610\n",
            "Iteration 172, loss = 0.50507140\n",
            "Iteration 173, loss = 0.50475273\n",
            "Iteration 174, loss = 0.50461285\n",
            "Iteration 175, loss = 0.50421829\n",
            "Iteration 176, loss = 0.50387057\n",
            "Iteration 177, loss = 0.50357414\n",
            "Iteration 178, loss = 0.50340073\n",
            "Iteration 179, loss = 0.50306695\n",
            "Iteration 180, loss = 0.50295475\n",
            "Iteration 181, loss = 0.50270728\n",
            "Iteration 182, loss = 0.50260136\n",
            "Iteration 183, loss = 0.50226492\n",
            "Iteration 184, loss = 0.50195030\n",
            "Iteration 185, loss = 0.50177161\n",
            "Iteration 186, loss = 0.50149373\n",
            "Iteration 187, loss = 0.50122689\n",
            "Iteration 188, loss = 0.50086275\n",
            "Iteration 189, loss = 0.50069897\n",
            "Iteration 190, loss = 0.50061594\n",
            "Iteration 191, loss = 0.50059219\n",
            "Iteration 192, loss = 0.50036108\n",
            "Iteration 193, loss = 0.49995113\n",
            "Iteration 194, loss = 0.49973413\n",
            "Iteration 195, loss = 0.49936805\n",
            "Iteration 196, loss = 0.49916302\n",
            "Iteration 197, loss = 0.49881444\n",
            "Iteration 198, loss = 0.49857613\n",
            "Iteration 199, loss = 0.49853651\n",
            "Iteration 200, loss = 0.49807248\n",
            "Iteration 1, loss = 1.35514429\n",
            "Iteration 2, loss = 1.20785553\n",
            "Iteration 3, loss = 1.02088892\n",
            "Iteration 4, loss = 0.85738136\n",
            "Iteration 5, loss = 0.75279440\n",
            "Iteration 6, loss = 0.69511781\n",
            "Iteration 7, loss = 0.66830518\n",
            "Iteration 8, loss = 0.65594188\n",
            "Iteration 9, loss = 0.64756070\n",
            "Iteration 10, loss = 0.63958029\n",
            "Iteration 11, loss = 0.63226591\n",
            "Iteration 12, loss = 0.62537086\n",
            "Iteration 13, loss = 0.61859528\n",
            "Iteration 14, loss = 0.61262202\n",
            "Iteration 15, loss = 0.60683866\n",
            "Iteration 16, loss = 0.60192057\n",
            "Iteration 17, loss = 0.59769403\n",
            "Iteration 18, loss = 0.59407468\n",
            "Iteration 19, loss = 0.59087969\n",
            "Iteration 20, loss = 0.58787421\n",
            "Iteration 21, loss = 0.58490220\n",
            "Iteration 22, loss = 0.58260316\n",
            "Iteration 23, loss = 0.57994422\n",
            "Iteration 24, loss = 0.57770562\n",
            "Iteration 25, loss = 0.57568745\n",
            "Iteration 26, loss = 0.57360864\n",
            "Iteration 27, loss = 0.57185682\n",
            "Iteration 28, loss = 0.57009674\n",
            "Iteration 29, loss = 0.56836102\n",
            "Iteration 30, loss = 0.56690076\n",
            "Iteration 31, loss = 0.56525335\n",
            "Iteration 32, loss = 0.56398245\n",
            "Iteration 33, loss = 0.56232638\n",
            "Iteration 34, loss = 0.56101582\n",
            "Iteration 35, loss = 0.55982524\n",
            "Iteration 36, loss = 0.55856738\n",
            "Iteration 37, loss = 0.55735555\n",
            "Iteration 38, loss = 0.55627713\n",
            "Iteration 39, loss = 0.55510543\n",
            "Iteration 40, loss = 0.55407973\n",
            "Iteration 41, loss = 0.55330870\n",
            "Iteration 42, loss = 0.55233209\n",
            "Iteration 43, loss = 0.55132070\n",
            "Iteration 44, loss = 0.55036474\n",
            "Iteration 45, loss = 0.54944197\n",
            "Iteration 46, loss = 0.54837056\n",
            "Iteration 47, loss = 0.54738358\n",
            "Iteration 48, loss = 0.54652592\n",
            "Iteration 49, loss = 0.54571200\n",
            "Iteration 50, loss = 0.54485943\n",
            "Iteration 51, loss = 0.54400365\n",
            "Iteration 52, loss = 0.54341597\n",
            "Iteration 53, loss = 0.54259256\n",
            "Iteration 54, loss = 0.54195325\n",
            "Iteration 55, loss = 0.54135470\n",
            "Iteration 56, loss = 0.54070979\n",
            "Iteration 57, loss = 0.54016796\n",
            "Iteration 58, loss = 0.53969689\n",
            "Iteration 59, loss = 0.53917235\n",
            "Iteration 60, loss = 0.53869064\n",
            "Iteration 61, loss = 0.53838360\n",
            "Iteration 62, loss = 0.53768739\n",
            "Iteration 63, loss = 0.53703995\n",
            "Iteration 64, loss = 0.53649471\n",
            "Iteration 65, loss = 0.53589237\n",
            "Iteration 66, loss = 0.53544138\n",
            "Iteration 67, loss = 0.53490436\n",
            "Iteration 68, loss = 0.53439025\n",
            "Iteration 69, loss = 0.53396900\n",
            "Iteration 70, loss = 0.53351046\n",
            "Iteration 71, loss = 0.53296450\n",
            "Iteration 72, loss = 0.53245616\n",
            "Iteration 73, loss = 0.53198883\n",
            "Iteration 74, loss = 0.53171321\n",
            "Iteration 75, loss = 0.53101324\n",
            "Iteration 76, loss = 0.53067590\n",
            "Iteration 77, loss = 0.53026639\n",
            "Iteration 78, loss = 0.52987296\n",
            "Iteration 79, loss = 0.52957369\n",
            "Iteration 80, loss = 0.52923171\n",
            "Iteration 81, loss = 0.52877918\n",
            "Iteration 82, loss = 0.52845922\n",
            "Iteration 83, loss = 0.52806888\n",
            "Iteration 84, loss = 0.52770694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 85, loss = 0.52724198\n",
            "Iteration 86, loss = 0.52678667\n",
            "Iteration 87, loss = 0.52638671\n",
            "Iteration 88, loss = 0.52591062\n",
            "Iteration 89, loss = 0.52570284\n",
            "Iteration 90, loss = 0.52524291\n",
            "Iteration 91, loss = 0.52484179\n",
            "Iteration 92, loss = 0.52438492\n",
            "Iteration 93, loss = 0.52405214\n",
            "Iteration 94, loss = 0.52381102\n",
            "Iteration 95, loss = 0.52339081\n",
            "Iteration 96, loss = 0.52297938\n",
            "Iteration 97, loss = 0.52261205\n",
            "Iteration 98, loss = 0.52217822\n",
            "Iteration 99, loss = 0.52179697\n",
            "Iteration 100, loss = 0.52129746\n",
            "Iteration 101, loss = 0.52101927\n",
            "Iteration 102, loss = 0.52061005\n",
            "Iteration 103, loss = 0.52034454\n",
            "Iteration 104, loss = 0.52015692\n",
            "Iteration 105, loss = 0.51979616\n",
            "Iteration 106, loss = 0.51940073\n",
            "Iteration 107, loss = 0.51912936\n",
            "Iteration 108, loss = 0.51893490\n",
            "Iteration 109, loss = 0.51845004\n",
            "Iteration 110, loss = 0.51820597\n",
            "Iteration 111, loss = 0.51794268\n",
            "Iteration 112, loss = 0.51767564\n",
            "Iteration 113, loss = 0.51748003\n",
            "Iteration 114, loss = 0.51712466\n",
            "Iteration 115, loss = 0.51692308\n",
            "Iteration 116, loss = 0.51660287\n",
            "Iteration 117, loss = 0.51631498\n",
            "Iteration 118, loss = 0.51610752\n",
            "Iteration 119, loss = 0.51573181\n",
            "Iteration 120, loss = 0.51538704\n",
            "Iteration 121, loss = 0.51495356\n",
            "Iteration 122, loss = 0.51486355\n",
            "Iteration 123, loss = 0.51441849\n",
            "Iteration 124, loss = 0.51415901\n",
            "Iteration 125, loss = 0.51390740\n",
            "Iteration 126, loss = 0.51359845\n",
            "Iteration 127, loss = 0.51340911\n",
            "Iteration 128, loss = 0.51297929\n",
            "Iteration 129, loss = 0.51269676\n",
            "Iteration 130, loss = 0.51232668\n",
            "Iteration 131, loss = 0.51219657\n",
            "Iteration 132, loss = 0.51169586\n",
            "Iteration 133, loss = 0.51136207\n",
            "Iteration 134, loss = 0.51109099\n",
            "Iteration 135, loss = 0.51087480\n",
            "Iteration 136, loss = 0.51056979\n",
            "Iteration 137, loss = 0.51032456\n",
            "Iteration 138, loss = 0.51005568\n",
            "Iteration 139, loss = 0.50985310\n",
            "Iteration 140, loss = 0.50955879\n",
            "Iteration 141, loss = 0.50929265\n",
            "Iteration 142, loss = 0.50926849\n",
            "Iteration 143, loss = 0.50876538\n",
            "Iteration 144, loss = 0.50839873\n",
            "Iteration 145, loss = 0.50811519\n",
            "Iteration 146, loss = 0.50771953\n",
            "Iteration 147, loss = 0.50735215\n",
            "Iteration 148, loss = 0.50708792\n",
            "Iteration 149, loss = 0.50679498\n",
            "Iteration 150, loss = 0.50649485\n",
            "Iteration 151, loss = 0.50609960\n",
            "Iteration 152, loss = 0.50582013\n",
            "Iteration 153, loss = 0.50551546\n",
            "Iteration 154, loss = 0.50512894\n",
            "Iteration 155, loss = 0.50488042\n",
            "Iteration 156, loss = 0.50460068\n",
            "Iteration 157, loss = 0.50429551\n",
            "Iteration 158, loss = 0.50397380\n",
            "Iteration 159, loss = 0.50382683\n",
            "Iteration 160, loss = 0.50357294\n",
            "Iteration 161, loss = 0.50339565\n",
            "Iteration 162, loss = 0.50329490\n",
            "Iteration 163, loss = 0.50309448\n",
            "Iteration 164, loss = 0.50284658\n",
            "Iteration 165, loss = 0.50264631\n",
            "Iteration 166, loss = 0.50236569\n",
            "Iteration 167, loss = 0.50202179\n",
            "Iteration 168, loss = 0.50191047\n",
            "Iteration 169, loss = 0.50163282\n",
            "Iteration 170, loss = 0.50136437\n",
            "Iteration 171, loss = 0.50111850\n",
            "Iteration 172, loss = 0.50075805\n",
            "Iteration 173, loss = 0.50041021\n",
            "Iteration 174, loss = 0.50011290\n",
            "Iteration 175, loss = 0.49998879\n",
            "Iteration 176, loss = 0.49973993\n",
            "Iteration 177, loss = 0.49940391\n",
            "Iteration 178, loss = 0.49927179\n",
            "Iteration 179, loss = 0.49890769\n",
            "Iteration 180, loss = 0.49867277\n",
            "Iteration 181, loss = 0.49827444\n",
            "Iteration 182, loss = 0.49837573\n",
            "Iteration 183, loss = 0.49797115\n",
            "Iteration 184, loss = 0.49782260\n",
            "Iteration 185, loss = 0.49764770\n",
            "Iteration 186, loss = 0.49730963\n",
            "Iteration 187, loss = 0.49719445\n",
            "Iteration 188, loss = 0.49683310\n",
            "Iteration 189, loss = 0.49657023\n",
            "Iteration 190, loss = 0.49651669\n",
            "Iteration 191, loss = 0.49646990\n",
            "Iteration 192, loss = 0.49613116\n",
            "Iteration 193, loss = 0.49595627\n",
            "Iteration 194, loss = 0.49580219\n",
            "Iteration 195, loss = 0.49540949\n",
            "Iteration 196, loss = 0.49529174\n",
            "Iteration 197, loss = 0.49509679\n",
            "Iteration 198, loss = 0.49510256\n",
            "Iteration 199, loss = 0.49494341\n",
            "Iteration 200, loss = 0.49472225\n",
            "Iteration 1, loss = 1.35569250\n",
            "Iteration 2, loss = 1.20977855\n",
            "Iteration 3, loss = 1.02421468\n",
            "Iteration 4, loss = 0.86114766\n",
            "Iteration 5, loss = 0.75352175\n",
            "Iteration 6, loss = 0.69589482\n",
            "Iteration 7, loss = 0.66854088\n",
            "Iteration 8, loss = 0.65502259\n",
            "Iteration 9, loss = 0.64663631\n",
            "Iteration 10, loss = 0.63885391\n",
            "Iteration 11, loss = 0.63126598\n",
            "Iteration 12, loss = 0.62391326\n",
            "Iteration 13, loss = 0.61710692\n",
            "Iteration 14, loss = 0.61051424\n",
            "Iteration 15, loss = 0.60440264\n",
            "Iteration 16, loss = 0.59904249\n",
            "Iteration 17, loss = 0.59421694\n",
            "Iteration 18, loss = 0.58983593\n",
            "Iteration 19, loss = 0.58606930\n",
            "Iteration 20, loss = 0.58266201\n",
            "Iteration 21, loss = 0.57939851\n",
            "Iteration 22, loss = 0.57651751\n",
            "Iteration 23, loss = 0.57331381\n",
            "Iteration 24, loss = 0.57061743\n",
            "Iteration 25, loss = 0.56816832\n",
            "Iteration 26, loss = 0.56558906\n",
            "Iteration 27, loss = 0.56354880\n",
            "Iteration 28, loss = 0.56127377\n",
            "Iteration 29, loss = 0.55916195\n",
            "Iteration 30, loss = 0.55726156\n",
            "Iteration 31, loss = 0.55534880\n",
            "Iteration 32, loss = 0.55363341\n",
            "Iteration 33, loss = 0.55176312\n",
            "Iteration 34, loss = 0.55028963\n",
            "Iteration 35, loss = 0.54884212\n",
            "Iteration 36, loss = 0.54733604\n",
            "Iteration 37, loss = 0.54583440\n",
            "Iteration 38, loss = 0.54439355\n",
            "Iteration 39, loss = 0.54300394\n",
            "Iteration 40, loss = 0.54174970\n",
            "Iteration 41, loss = 0.54070347\n",
            "Iteration 42, loss = 0.53965873\n",
            "Iteration 43, loss = 0.53854609\n",
            "Iteration 44, loss = 0.53749954\n",
            "Iteration 45, loss = 0.53666066\n",
            "Iteration 46, loss = 0.53548315\n",
            "Iteration 47, loss = 0.53456246\n",
            "Iteration 48, loss = 0.53368647\n",
            "Iteration 49, loss = 0.53288963\n",
            "Iteration 50, loss = 0.53210325\n",
            "Iteration 51, loss = 0.53127099\n",
            "Iteration 52, loss = 0.53064426\n",
            "Iteration 53, loss = 0.52966743\n",
            "Iteration 54, loss = 0.52903463\n",
            "Iteration 55, loss = 0.52829084\n",
            "Iteration 56, loss = 0.52752085\n",
            "Iteration 57, loss = 0.52692793\n",
            "Iteration 58, loss = 0.52620528\n",
            "Iteration 59, loss = 0.52564618\n",
            "Iteration 60, loss = 0.52501302\n",
            "Iteration 61, loss = 0.52461704\n",
            "Iteration 62, loss = 0.52397652\n",
            "Iteration 63, loss = 0.52328006\n",
            "Iteration 64, loss = 0.52274315\n",
            "Iteration 65, loss = 0.52209173\n",
            "Iteration 66, loss = 0.52161141\n",
            "Iteration 67, loss = 0.52102971\n",
            "Iteration 68, loss = 0.52051457\n",
            "Iteration 69, loss = 0.51994342\n",
            "Iteration 70, loss = 0.51949139\n",
            "Iteration 71, loss = 0.51894718\n",
            "Iteration 72, loss = 0.51834461\n",
            "Iteration 73, loss = 0.51783013\n",
            "Iteration 74, loss = 0.51731223\n",
            "Iteration 75, loss = 0.51688740\n",
            "Iteration 76, loss = 0.51646968\n",
            "Iteration 77, loss = 0.51623903\n",
            "Iteration 78, loss = 0.51580940\n",
            "Iteration 79, loss = 0.51535095\n",
            "Iteration 80, loss = 0.51513440\n",
            "Iteration 81, loss = 0.51460954\n",
            "Iteration 82, loss = 0.51437374\n",
            "Iteration 83, loss = 0.51399374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 84, loss = 0.51366061\n",
            "Iteration 85, loss = 0.51329859\n",
            "Iteration 86, loss = 0.51284274\n",
            "Iteration 87, loss = 0.51240931\n",
            "Iteration 88, loss = 0.51200998\n",
            "Iteration 89, loss = 0.51163393\n",
            "Iteration 90, loss = 0.51123178\n",
            "Iteration 91, loss = 0.51078401\n",
            "Iteration 92, loss = 0.51037211\n",
            "Iteration 93, loss = 0.51000532\n",
            "Iteration 94, loss = 0.50948955\n",
            "Iteration 95, loss = 0.50923813\n",
            "Iteration 96, loss = 0.50879173\n",
            "Iteration 97, loss = 0.50827459\n",
            "Iteration 98, loss = 0.50788042\n",
            "Iteration 99, loss = 0.50765154\n",
            "Iteration 100, loss = 0.50721205\n",
            "Iteration 101, loss = 0.50691786\n",
            "Iteration 102, loss = 0.50651989\n",
            "Iteration 103, loss = 0.50615824\n",
            "Iteration 104, loss = 0.50586519\n",
            "Iteration 105, loss = 0.50556665\n",
            "Iteration 106, loss = 0.50509417\n",
            "Iteration 107, loss = 0.50485705\n",
            "Iteration 108, loss = 0.50463847\n",
            "Iteration 109, loss = 0.50429215\n",
            "Iteration 110, loss = 0.50390829\n",
            "Iteration 111, loss = 0.50366847\n",
            "Iteration 112, loss = 0.50339423\n",
            "Iteration 113, loss = 0.50308668\n",
            "Iteration 114, loss = 0.50277058\n",
            "Iteration 115, loss = 0.50258803\n",
            "Iteration 116, loss = 0.50232659\n",
            "Iteration 117, loss = 0.50196972\n",
            "Iteration 118, loss = 0.50175203\n",
            "Iteration 119, loss = 0.50146362\n",
            "Iteration 120, loss = 0.50133419\n",
            "Iteration 121, loss = 0.50107893\n",
            "Iteration 122, loss = 0.50094707\n",
            "Iteration 123, loss = 0.50063183\n",
            "Iteration 124, loss = 0.50039005\n",
            "Iteration 125, loss = 0.50015109\n",
            "Iteration 126, loss = 0.50000054\n",
            "Iteration 127, loss = 0.49987877\n",
            "Iteration 128, loss = 0.49961840\n",
            "Iteration 129, loss = 0.49932302\n",
            "Iteration 130, loss = 0.49909672\n",
            "Iteration 131, loss = 0.49906614\n",
            "Iteration 132, loss = 0.49867396\n",
            "Iteration 133, loss = 0.49839738\n",
            "Iteration 134, loss = 0.49828698\n",
            "Iteration 135, loss = 0.49812284\n",
            "Iteration 136, loss = 0.49781359\n",
            "Iteration 137, loss = 0.49766731\n",
            "Iteration 138, loss = 0.49742966\n",
            "Iteration 139, loss = 0.49729896\n",
            "Iteration 140, loss = 0.49693897\n",
            "Iteration 141, loss = 0.49679213\n",
            "Iteration 142, loss = 0.49665055\n",
            "Iteration 143, loss = 0.49641770\n",
            "Iteration 144, loss = 0.49617866\n",
            "Iteration 145, loss = 0.49592519\n",
            "Iteration 146, loss = 0.49574596\n",
            "Iteration 147, loss = 0.49556817\n",
            "Iteration 148, loss = 0.49543211\n",
            "Iteration 149, loss = 0.49532935\n",
            "Iteration 150, loss = 0.49496914\n",
            "Iteration 151, loss = 0.49473017\n",
            "Iteration 152, loss = 0.49460200\n",
            "Iteration 153, loss = 0.49440604\n",
            "Iteration 154, loss = 0.49417629\n",
            "Iteration 155, loss = 0.49392635\n",
            "Iteration 156, loss = 0.49369083\n",
            "Iteration 157, loss = 0.49356758\n",
            "Iteration 158, loss = 0.49324968\n",
            "Iteration 159, loss = 0.49304637\n",
            "Iteration 160, loss = 0.49288787\n",
            "Iteration 161, loss = 0.49260723\n",
            "Iteration 162, loss = 0.49238571\n",
            "Iteration 163, loss = 0.49234288\n",
            "Iteration 164, loss = 0.49187799\n",
            "Iteration 165, loss = 0.49160123\n",
            "Iteration 166, loss = 0.49144122\n",
            "Iteration 167, loss = 0.49109575\n",
            "Iteration 168, loss = 0.49072733\n",
            "Iteration 169, loss = 0.49058246\n",
            "Iteration 170, loss = 0.49025595\n",
            "Iteration 171, loss = 0.49008105\n",
            "Iteration 172, loss = 0.48979297\n",
            "Iteration 173, loss = 0.48959766\n",
            "Iteration 174, loss = 0.48930297\n",
            "Iteration 175, loss = 0.48903951\n",
            "Iteration 176, loss = 0.48874531\n",
            "Iteration 177, loss = 0.48837297\n",
            "Iteration 178, loss = 0.48821016\n",
            "Iteration 179, loss = 0.48794466\n",
            "Iteration 180, loss = 0.48757867\n",
            "Iteration 181, loss = 0.48739663\n",
            "Iteration 182, loss = 0.48737866\n",
            "Iteration 183, loss = 0.48719382\n",
            "Iteration 184, loss = 0.48689394\n",
            "Iteration 185, loss = 0.48664788\n",
            "Iteration 186, loss = 0.48637122\n",
            "Iteration 187, loss = 0.48621657\n",
            "Iteration 188, loss = 0.48600749\n",
            "Iteration 189, loss = 0.48552255\n",
            "Iteration 190, loss = 0.48556991\n",
            "Iteration 191, loss = 0.48535921\n",
            "Iteration 192, loss = 0.48497296\n",
            "Iteration 193, loss = 0.48480282\n",
            "Iteration 194, loss = 0.48450497\n",
            "Iteration 195, loss = 0.48440554\n",
            "Iteration 196, loss = 0.48412461\n",
            "Iteration 197, loss = 0.48400514\n",
            "Iteration 198, loss = 0.48384378\n",
            "Iteration 199, loss = 0.48370706\n",
            "Iteration 200, loss = 0.48348180\n",
            "Iteration 1, loss = 1.36723929\n",
            "Iteration 2, loss = 1.21905982\n",
            "Iteration 3, loss = 1.03107889\n",
            "Iteration 4, loss = 0.86195746\n",
            "Iteration 5, loss = 0.75246019\n",
            "Iteration 6, loss = 0.69773006\n",
            "Iteration 7, loss = 0.67069081\n",
            "Iteration 8, loss = 0.65769343\n",
            "Iteration 9, loss = 0.64888341\n",
            "Iteration 10, loss = 0.64099600\n",
            "Iteration 11, loss = 0.63323861\n",
            "Iteration 12, loss = 0.62561845\n",
            "Iteration 13, loss = 0.61893797\n",
            "Iteration 14, loss = 0.61269725\n",
            "Iteration 15, loss = 0.60721156\n",
            "Iteration 16, loss = 0.60205105\n",
            "Iteration 17, loss = 0.59786319\n",
            "Iteration 18, loss = 0.59406231\n",
            "Iteration 19, loss = 0.59071031\n",
            "Iteration 20, loss = 0.58803417\n",
            "Iteration 21, loss = 0.58526381\n",
            "Iteration 22, loss = 0.58253915\n",
            "Iteration 23, loss = 0.57958451\n",
            "Iteration 24, loss = 0.57694386\n",
            "Iteration 25, loss = 0.57480505\n",
            "Iteration 26, loss = 0.57235466\n",
            "Iteration 27, loss = 0.57040575\n",
            "Iteration 28, loss = 0.56840383\n",
            "Iteration 29, loss = 0.56663716\n",
            "Iteration 30, loss = 0.56492198\n",
            "Iteration 31, loss = 0.56329133\n",
            "Iteration 32, loss = 0.56191467\n",
            "Iteration 33, loss = 0.56045268\n",
            "Iteration 34, loss = 0.55912756\n",
            "Iteration 35, loss = 0.55794052\n",
            "Iteration 36, loss = 0.55654732\n",
            "Iteration 37, loss = 0.55516411\n",
            "Iteration 38, loss = 0.55392465\n",
            "Iteration 39, loss = 0.55276870\n",
            "Iteration 40, loss = 0.55161758\n",
            "Iteration 41, loss = 0.55055639\n",
            "Iteration 42, loss = 0.54954353\n",
            "Iteration 43, loss = 0.54854303\n",
            "Iteration 44, loss = 0.54750768\n",
            "Iteration 45, loss = 0.54666345\n",
            "Iteration 46, loss = 0.54566846\n",
            "Iteration 47, loss = 0.54496082\n",
            "Iteration 48, loss = 0.54419184\n",
            "Iteration 49, loss = 0.54348414\n",
            "Iteration 50, loss = 0.54278759\n",
            "Iteration 51, loss = 0.54212916\n",
            "Iteration 52, loss = 0.54153784\n",
            "Iteration 53, loss = 0.54082423\n",
            "Iteration 54, loss = 0.54022182\n",
            "Iteration 55, loss = 0.53954595\n",
            "Iteration 56, loss = 0.53892056\n",
            "Iteration 57, loss = 0.53836775\n",
            "Iteration 58, loss = 0.53769234\n",
            "Iteration 59, loss = 0.53724987\n",
            "Iteration 60, loss = 0.53663885\n",
            "Iteration 61, loss = 0.53612754\n",
            "Iteration 62, loss = 0.53547468\n",
            "Iteration 63, loss = 0.53504210\n",
            "Iteration 64, loss = 0.53427768\n",
            "Iteration 65, loss = 0.53366504\n",
            "Iteration 66, loss = 0.53303150\n",
            "Iteration 67, loss = 0.53238321\n",
            "Iteration 68, loss = 0.53194125\n",
            "Iteration 69, loss = 0.53133059\n",
            "Iteration 70, loss = 0.53074222\n",
            "Iteration 71, loss = 0.53014142\n",
            "Iteration 72, loss = 0.52957523\n",
            "Iteration 73, loss = 0.52908565\n",
            "Iteration 74, loss = 0.52854630\n",
            "Iteration 75, loss = 0.52804022\n",
            "Iteration 76, loss = 0.52757641\n",
            "Iteration 77, loss = 0.52717378\n",
            "Iteration 78, loss = 0.52668493\n",
            "Iteration 79, loss = 0.52635280\n",
            "Iteration 80, loss = 0.52604054\n",
            "Iteration 81, loss = 0.52543617\n",
            "Iteration 82, loss = 0.52503550\n",
            "Iteration 83, loss = 0.52449942\n",
            "Iteration 84, loss = 0.52396905\n",
            "Iteration 85, loss = 0.52358350\n",
            "Iteration 86, loss = 0.52302113\n",
            "Iteration 87, loss = 0.52252843\n",
            "Iteration 88, loss = 0.52205340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 89, loss = 0.52165514\n",
            "Iteration 90, loss = 0.52115976\n",
            "Iteration 91, loss = 0.52074615\n",
            "Iteration 92, loss = 0.52025791\n",
            "Iteration 93, loss = 0.51985299\n",
            "Iteration 94, loss = 0.51946563\n",
            "Iteration 95, loss = 0.51903519\n",
            "Iteration 96, loss = 0.51853386\n",
            "Iteration 97, loss = 0.51813098\n",
            "Iteration 98, loss = 0.51771788\n",
            "Iteration 99, loss = 0.51733601\n",
            "Iteration 100, loss = 0.51689669\n",
            "Iteration 101, loss = 0.51656435\n",
            "Iteration 102, loss = 0.51608763\n",
            "Iteration 103, loss = 0.51582360\n",
            "Iteration 104, loss = 0.51553747\n",
            "Iteration 105, loss = 0.51525748\n",
            "Iteration 106, loss = 0.51499401\n",
            "Iteration 107, loss = 0.51478558\n",
            "Iteration 108, loss = 0.51448378\n",
            "Iteration 109, loss = 0.51413283\n",
            "Iteration 110, loss = 0.51371076\n",
            "Iteration 111, loss = 0.51337152\n",
            "Iteration 112, loss = 0.51291200\n",
            "Iteration 113, loss = 0.51249863\n",
            "Iteration 114, loss = 0.51205963\n",
            "Iteration 115, loss = 0.51173540\n",
            "Iteration 116, loss = 0.51146686\n",
            "Iteration 117, loss = 0.51119443\n",
            "Iteration 118, loss = 0.51078437\n",
            "Iteration 119, loss = 0.51062081\n",
            "Iteration 120, loss = 0.51031826\n",
            "Iteration 121, loss = 0.50998651\n",
            "Iteration 122, loss = 0.50978558\n",
            "Iteration 123, loss = 0.50941361\n",
            "Iteration 124, loss = 0.50913029\n",
            "Iteration 125, loss = 0.50883698\n",
            "Iteration 126, loss = 0.50869105\n",
            "Iteration 127, loss = 0.50858617\n",
            "Iteration 128, loss = 0.50816938\n",
            "Iteration 129, loss = 0.50793804\n",
            "Iteration 130, loss = 0.50764031\n",
            "Iteration 131, loss = 0.50742087\n",
            "Iteration 132, loss = 0.50712230\n",
            "Iteration 133, loss = 0.50692221\n",
            "Iteration 134, loss = 0.50678377\n",
            "Iteration 135, loss = 0.50639626\n",
            "Iteration 136, loss = 0.50603770\n",
            "Iteration 137, loss = 0.50586967\n",
            "Iteration 138, loss = 0.50564069\n",
            "Iteration 139, loss = 0.50535606\n",
            "Iteration 140, loss = 0.50516058\n",
            "Iteration 141, loss = 0.50497520\n",
            "Iteration 142, loss = 0.50468096\n",
            "Iteration 143, loss = 0.50442426\n",
            "Iteration 144, loss = 0.50413265\n",
            "Iteration 145, loss = 0.50386107\n",
            "Iteration 146, loss = 0.50353061\n",
            "Iteration 147, loss = 0.50334634\n",
            "Iteration 148, loss = 0.50311783\n",
            "Iteration 149, loss = 0.50288664\n",
            "Iteration 150, loss = 0.50236505\n",
            "Iteration 151, loss = 0.50208089\n",
            "Iteration 152, loss = 0.50173600\n",
            "Iteration 153, loss = 0.50143136\n",
            "Iteration 154, loss = 0.50119702\n",
            "Iteration 155, loss = 0.50081917\n",
            "Iteration 156, loss = 0.50057737\n",
            "Iteration 157, loss = 0.50038199\n",
            "Iteration 158, loss = 0.50010093\n",
            "Iteration 159, loss = 0.49982835\n",
            "Iteration 160, loss = 0.49967910\n",
            "Iteration 161, loss = 0.49938863\n",
            "Iteration 162, loss = 0.49913924\n",
            "Iteration 163, loss = 0.49901202\n",
            "Iteration 164, loss = 0.49852383\n",
            "Iteration 165, loss = 0.49828099\n",
            "Iteration 166, loss = 0.49812412\n",
            "Iteration 167, loss = 0.49781615\n",
            "Iteration 168, loss = 0.49758333\n",
            "Iteration 169, loss = 0.49753652\n",
            "Iteration 170, loss = 0.49729156\n",
            "Iteration 171, loss = 0.49696199\n",
            "Iteration 172, loss = 0.49669771\n",
            "Iteration 173, loss = 0.49667868\n",
            "Iteration 174, loss = 0.49652562\n",
            "Iteration 175, loss = 0.49617646\n",
            "Iteration 176, loss = 0.49608681\n",
            "Iteration 177, loss = 0.49566038\n",
            "Iteration 178, loss = 0.49554962\n",
            "Iteration 179, loss = 0.49531796\n",
            "Iteration 180, loss = 0.49493488\n",
            "Iteration 181, loss = 0.49472215\n",
            "Iteration 182, loss = 0.49475523\n",
            "Iteration 183, loss = 0.49468497\n",
            "Iteration 184, loss = 0.49442485\n",
            "Iteration 185, loss = 0.49419867\n",
            "Iteration 186, loss = 0.49386104\n",
            "Iteration 187, loss = 0.49372019\n",
            "Iteration 188, loss = 0.49365549\n",
            "Iteration 189, loss = 0.49331288\n",
            "Iteration 190, loss = 0.49342631\n",
            "Iteration 191, loss = 0.49313070\n",
            "Iteration 192, loss = 0.49267346\n",
            "Iteration 193, loss = 0.49240412\n",
            "Iteration 194, loss = 0.49207607\n",
            "Iteration 195, loss = 0.49183511\n",
            "Iteration 196, loss = 0.49169131\n",
            "Iteration 197, loss = 0.49153235\n",
            "Iteration 198, loss = 0.49134857\n",
            "Iteration 199, loss = 0.49116338\n",
            "Iteration 200, loss = 0.49094298\n",
            "Iteration 1, loss = 1.34182386\n",
            "Iteration 2, loss = 1.20486792\n",
            "Iteration 3, loss = 1.03052691\n",
            "Iteration 4, loss = 0.87573110\n",
            "Iteration 5, loss = 0.76767102\n",
            "Iteration 6, loss = 0.71005122\n",
            "Iteration 7, loss = 0.68025523\n",
            "Iteration 8, loss = 0.66452169\n",
            "Iteration 9, loss = 0.65582803\n",
            "Iteration 10, loss = 0.64798558\n",
            "Iteration 11, loss = 0.64058304\n",
            "Iteration 12, loss = 0.63342237\n",
            "Iteration 13, loss = 0.62701482\n",
            "Iteration 14, loss = 0.62068021\n",
            "Iteration 15, loss = 0.61547319\n",
            "Iteration 16, loss = 0.61076412\n",
            "Iteration 17, loss = 0.60674878\n",
            "Iteration 18, loss = 0.60287881\n",
            "Iteration 19, loss = 0.59941555\n",
            "Iteration 20, loss = 0.59658795\n",
            "Iteration 21, loss = 0.59379273\n",
            "Iteration 22, loss = 0.59137819\n",
            "Iteration 23, loss = 0.58868711\n",
            "Iteration 24, loss = 0.58623099\n",
            "Iteration 25, loss = 0.58407023\n",
            "Iteration 26, loss = 0.58184564\n",
            "Iteration 27, loss = 0.57995834\n",
            "Iteration 28, loss = 0.57811124\n",
            "Iteration 29, loss = 0.57634992\n",
            "Iteration 30, loss = 0.57458462\n",
            "Iteration 31, loss = 0.57298054\n",
            "Iteration 32, loss = 0.57144934\n",
            "Iteration 33, loss = 0.56999777\n",
            "Iteration 34, loss = 0.56847783\n",
            "Iteration 35, loss = 0.56719457\n",
            "Iteration 36, loss = 0.56571401\n",
            "Iteration 37, loss = 0.56429184\n",
            "Iteration 38, loss = 0.56282301\n",
            "Iteration 39, loss = 0.56135988\n",
            "Iteration 40, loss = 0.55987407\n",
            "Iteration 41, loss = 0.55831365\n",
            "Iteration 42, loss = 0.55708077\n",
            "Iteration 43, loss = 0.55575808\n",
            "Iteration 44, loss = 0.55445939\n",
            "Iteration 45, loss = 0.55327371\n",
            "Iteration 46, loss = 0.55209696\n",
            "Iteration 47, loss = 0.55130947\n",
            "Iteration 48, loss = 0.55024949\n",
            "Iteration 49, loss = 0.54932537\n",
            "Iteration 50, loss = 0.54833315\n",
            "Iteration 51, loss = 0.54736602\n",
            "Iteration 52, loss = 0.54661875\n",
            "Iteration 53, loss = 0.54587476\n",
            "Iteration 54, loss = 0.54513703\n",
            "Iteration 55, loss = 0.54429737\n",
            "Iteration 56, loss = 0.54357680\n",
            "Iteration 57, loss = 0.54285780\n",
            "Iteration 58, loss = 0.54213330\n",
            "Iteration 59, loss = 0.54160968\n",
            "Iteration 60, loss = 0.54088100\n",
            "Iteration 61, loss = 0.54034655\n",
            "Iteration 62, loss = 0.53973508\n",
            "Iteration 63, loss = 0.53941835\n",
            "Iteration 64, loss = 0.53872499\n",
            "Iteration 65, loss = 0.53810126\n",
            "Iteration 66, loss = 0.53764695\n",
            "Iteration 67, loss = 0.53703482\n",
            "Iteration 68, loss = 0.53660654\n",
            "Iteration 69, loss = 0.53600849\n",
            "Iteration 70, loss = 0.53553453\n",
            "Iteration 71, loss = 0.53495367\n",
            "Iteration 72, loss = 0.53445071\n",
            "Iteration 73, loss = 0.53399473\n",
            "Iteration 74, loss = 0.53354493\n",
            "Iteration 75, loss = 0.53323114\n",
            "Iteration 76, loss = 0.53281414\n",
            "Iteration 77, loss = 0.53234541\n",
            "Iteration 78, loss = 0.53195831\n",
            "Iteration 79, loss = 0.53161136\n",
            "Iteration 80, loss = 0.53126047\n",
            "Iteration 81, loss = 0.53085038\n",
            "Iteration 82, loss = 0.53049602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 83, loss = 0.53010471\n",
            "Iteration 84, loss = 0.52972940\n",
            "Iteration 85, loss = 0.52942890\n",
            "Iteration 86, loss = 0.52903268\n",
            "Iteration 87, loss = 0.52860258\n",
            "Iteration 88, loss = 0.52823206\n",
            "Iteration 89, loss = 0.52794637\n",
            "Iteration 90, loss = 0.52760289\n",
            "Iteration 91, loss = 0.52728967\n",
            "Iteration 92, loss = 0.52693172\n",
            "Iteration 93, loss = 0.52664655\n",
            "Iteration 94, loss = 0.52628103\n",
            "Iteration 95, loss = 0.52585124\n",
            "Iteration 96, loss = 0.52557953\n",
            "Iteration 97, loss = 0.52530700\n",
            "Iteration 98, loss = 0.52495444\n",
            "Iteration 99, loss = 0.52463324\n",
            "Iteration 100, loss = 0.52435696\n",
            "Iteration 101, loss = 0.52406326\n",
            "Iteration 102, loss = 0.52373880\n",
            "Iteration 103, loss = 0.52357136\n",
            "Iteration 104, loss = 0.52330951\n",
            "Iteration 105, loss = 0.52302206\n",
            "Iteration 106, loss = 0.52279563\n",
            "Iteration 107, loss = 0.52267320\n",
            "Iteration 108, loss = 0.52232405\n",
            "Iteration 109, loss = 0.52213232\n",
            "Iteration 110, loss = 0.52180247\n",
            "Iteration 111, loss = 0.52145225\n",
            "Iteration 112, loss = 0.52126538\n",
            "Iteration 113, loss = 0.52094972\n",
            "Iteration 114, loss = 0.52063710\n",
            "Iteration 115, loss = 0.52041963\n",
            "Iteration 116, loss = 0.52016501\n",
            "Iteration 117, loss = 0.51994950\n",
            "Iteration 118, loss = 0.51968411\n",
            "Iteration 119, loss = 0.51963826\n",
            "Iteration 120, loss = 0.51927120\n",
            "Iteration 121, loss = 0.51907988\n",
            "Iteration 122, loss = 0.51882328\n",
            "Iteration 123, loss = 0.51856471\n",
            "Iteration 124, loss = 0.51839948\n",
            "Iteration 125, loss = 0.51809568\n",
            "Iteration 126, loss = 0.51792599\n",
            "Iteration 127, loss = 0.51780522\n",
            "Iteration 128, loss = 0.51755738\n",
            "Iteration 129, loss = 0.51728052\n",
            "Iteration 130, loss = 0.51698920\n",
            "Iteration 131, loss = 0.51668455\n",
            "Iteration 132, loss = 0.51661627\n",
            "Iteration 133, loss = 0.51626315\n",
            "Iteration 134, loss = 0.51614263\n",
            "Iteration 135, loss = 0.51589745\n",
            "Iteration 136, loss = 0.51566872\n",
            "Iteration 137, loss = 0.51559520\n",
            "Iteration 138, loss = 0.51540490\n",
            "Iteration 139, loss = 0.51520248\n",
            "Iteration 140, loss = 0.51494708\n",
            "Iteration 141, loss = 0.51480726\n",
            "Iteration 142, loss = 0.51462756\n",
            "Iteration 143, loss = 0.51451956\n",
            "Iteration 144, loss = 0.51423886\n",
            "Iteration 145, loss = 0.51421063\n",
            "Iteration 146, loss = 0.51394809\n",
            "Iteration 147, loss = 0.51387299\n",
            "Iteration 148, loss = 0.51367021\n",
            "Iteration 149, loss = 0.51357374\n",
            "Iteration 150, loss = 0.51334572\n",
            "Iteration 151, loss = 0.51322501\n",
            "Iteration 152, loss = 0.51302900\n",
            "Iteration 153, loss = 0.51287177\n",
            "Iteration 154, loss = 0.51277825\n",
            "Iteration 155, loss = 0.51278512\n",
            "Iteration 156, loss = 0.51276972\n",
            "Iteration 157, loss = 0.51260030\n",
            "Iteration 158, loss = 0.51236373\n",
            "Iteration 159, loss = 0.51222677\n",
            "Iteration 160, loss = 0.51197345\n",
            "Iteration 161, loss = 0.51174980\n",
            "Iteration 162, loss = 0.51166179\n",
            "Iteration 163, loss = 0.51158052\n",
            "Iteration 164, loss = 0.51125207\n",
            "Iteration 165, loss = 0.51108281\n",
            "Iteration 166, loss = 0.51085771\n",
            "Iteration 167, loss = 0.51076873\n",
            "Iteration 168, loss = 0.51063824\n",
            "Iteration 169, loss = 0.51051966\n",
            "Iteration 170, loss = 0.51048734\n",
            "Iteration 171, loss = 0.51024973\n",
            "Iteration 172, loss = 0.50997772\n",
            "Iteration 173, loss = 0.50993956\n",
            "Iteration 174, loss = 0.50999497\n",
            "Iteration 175, loss = 0.50983296\n",
            "Iteration 176, loss = 0.50957546\n",
            "Iteration 177, loss = 0.50948717\n",
            "Iteration 178, loss = 0.50928462\n",
            "Iteration 179, loss = 0.50924133\n",
            "Iteration 180, loss = 0.50895414\n",
            "Iteration 181, loss = 0.50887183\n",
            "Iteration 182, loss = 0.50872777\n",
            "Iteration 183, loss = 0.50877802\n",
            "Iteration 184, loss = 0.50865577\n",
            "Iteration 185, loss = 0.50845977\n",
            "Iteration 186, loss = 0.50836608\n",
            "Iteration 187, loss = 0.50818923\n",
            "Iteration 188, loss = 0.50803508\n",
            "Iteration 189, loss = 0.50782959\n",
            "Iteration 190, loss = 0.50772157\n",
            "Iteration 191, loss = 0.50755464\n",
            "Iteration 192, loss = 0.50738673\n",
            "Iteration 193, loss = 0.50722245\n",
            "Iteration 194, loss = 0.50689474\n",
            "Iteration 195, loss = 0.50671747\n",
            "Iteration 196, loss = 0.50655550\n",
            "Iteration 197, loss = 0.50644272\n",
            "Iteration 198, loss = 0.50630171\n",
            "Iteration 199, loss = 0.50614826\n",
            "Iteration 200, loss = 0.50601847\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 200 and for layer number 6 : 0.71\n",
            "Iteration 1, loss = 0.61686348\n",
            "Iteration 2, loss = 0.61615212\n",
            "Iteration 3, loss = 0.61509889\n",
            "Iteration 4, loss = 0.61393355\n",
            "Iteration 5, loss = 0.61267100\n",
            "Iteration 6, loss = 0.61156262"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 7, loss = 0.61039793\n",
            "Iteration 8, loss = 0.60920464\n",
            "Iteration 9, loss = 0.60819776\n",
            "Iteration 10, loss = 0.60727368\n",
            "Iteration 11, loss = 0.60643854\n",
            "Iteration 12, loss = 0.60574605\n",
            "Iteration 13, loss = 0.60503921\n",
            "Iteration 14, loss = 0.60439293\n",
            "Iteration 15, loss = 0.60372900\n",
            "Iteration 16, loss = 0.60312998\n",
            "Iteration 17, loss = 0.60257188\n",
            "Iteration 18, loss = 0.60207421\n",
            "Iteration 19, loss = 0.60156745\n",
            "Iteration 20, loss = 0.60103417\n",
            "Iteration 21, loss = 0.60046005\n",
            "Iteration 22, loss = 0.60001837\n",
            "Iteration 23, loss = 0.59949961\n",
            "Iteration 24, loss = 0.59907076\n",
            "Iteration 25, loss = 0.59862261\n",
            "Iteration 26, loss = 0.59821262\n",
            "Iteration 27, loss = 0.59768043\n",
            "Iteration 28, loss = 0.59727575\n",
            "Iteration 29, loss = 0.59686577\n",
            "Iteration 30, loss = 0.59634456\n",
            "Iteration 31, loss = 0.59592913\n",
            "Iteration 32, loss = 0.59551543\n",
            "Iteration 33, loss = 0.59511982\n",
            "Iteration 34, loss = 0.59488057\n",
            "Iteration 35, loss = 0.59437990\n",
            "Iteration 36, loss = 0.59393766\n",
            "Iteration 37, loss = 0.59343460\n",
            "Iteration 38, loss = 0.59299992\n",
            "Iteration 39, loss = 0.59251347\n",
            "Iteration 40, loss = 0.59210013\n",
            "Iteration 41, loss = 0.59158753\n",
            "Iteration 42, loss = 0.59116433\n",
            "Iteration 43, loss = 0.59067977\n",
            "Iteration 44, loss = 0.59030424\n",
            "Iteration 45, loss = 0.58985025\n",
            "Iteration 46, loss = 0.58934126\n",
            "Iteration 47, loss = 0.58893644\n",
            "Iteration 48, loss = 0.58838509\n",
            "Iteration 49, loss = 0.58789781\n",
            "Iteration 50, loss = 0.58745034\n",
            "Iteration 51, loss = 0.58705463\n",
            "Iteration 52, loss = 0.58659520\n",
            "Iteration 53, loss = 0.58621788\n",
            "Iteration 54, loss = 0.58582979\n",
            "Iteration 55, loss = 0.58542994\n",
            "Iteration 56, loss = 0.58504137\n",
            "Iteration 57, loss = 0.58463168\n",
            "Iteration 58, loss = 0.58429345\n",
            "Iteration 59, loss = 0.58388301\n",
            "Iteration 60, loss = 0.58354039\n",
            "Iteration 61, loss = 0.58310095\n",
            "Iteration 62, loss = 0.58279069\n",
            "Iteration 63, loss = 0.58237879\n",
            "Iteration 64, loss = 0.58204179\n",
            "Iteration 65, loss = 0.58167540\n",
            "Iteration 66, loss = 0.58131958\n",
            "Iteration 67, loss = 0.58107527\n",
            "Iteration 68, loss = 0.58075012\n",
            "Iteration 69, loss = 0.58043709\n",
            "Iteration 70, loss = 0.58009698\n",
            "Iteration 71, loss = 0.57979144\n",
            "Iteration 72, loss = 0.57952207\n",
            "Iteration 73, loss = 0.57918802\n",
            "Iteration 74, loss = 0.57892494\n",
            "Iteration 75, loss = 0.57862554\n",
            "Iteration 76, loss = 0.57835511\n",
            "Iteration 77, loss = 0.57808078\n",
            "Iteration 78, loss = 0.57780786\n",
            "Iteration 79, loss = 0.57753624\n",
            "Iteration 80, loss = 0.57722696\n",
            "Iteration 81, loss = 0.57695431\n",
            "Iteration 82, loss = 0.57660842\n",
            "Iteration 83, loss = 0.57639990\n",
            "Iteration 84, loss = 0.57623494\n",
            "Iteration 85, loss = 0.57594787\n",
            "Iteration 86, loss = 0.57557150\n",
            "Iteration 87, loss = 0.57527755\n",
            "Iteration 88, loss = 0.57490136\n",
            "Iteration 89, loss = 0.57463870\n",
            "Iteration 90, loss = 0.57420350\n",
            "Iteration 91, loss = 0.57382071\n",
            "Iteration 92, loss = 0.57349973\n",
            "Iteration 93, loss = 0.57316240\n",
            "Iteration 94, loss = 0.57277331\n",
            "Iteration 95, loss = 0.57224303\n",
            "Iteration 96, loss = 0.57170654\n",
            "Iteration 97, loss = 0.57127444\n",
            "Iteration 98, loss = 0.57075803\n",
            "Iteration 99, loss = 0.57034183\n",
            "Iteration 100, loss = 0.56991355\n",
            "Iteration 101, loss = 0.56954816\n",
            "Iteration 102, loss = 0.56908193\n",
            "Iteration 103, loss = 0.56854709\n",
            "Iteration 104, loss = 0.56805126\n",
            "Iteration 105, loss = 0.56750147\n",
            "Iteration 106, loss = 0.56701195\n",
            "Iteration 107, loss = 0.56647560\n",
            "Iteration 108, loss = 0.56597761\n",
            "Iteration 109, loss = 0.56545971\n",
            "Iteration 110, loss = 0.56495461\n",
            "Iteration 111, loss = 0.56441826\n",
            "Iteration 112, loss = 0.56379794\n",
            "Iteration 113, loss = 0.56314473\n",
            "Iteration 114, loss = 0.56254899\n",
            "Iteration 115, loss = 0.56209955\n",
            "Iteration 116, loss = 0.56160359\n",
            "Iteration 117, loss = 0.56105735\n",
            "Iteration 118, loss = 0.56070239\n",
            "Iteration 119, loss = 0.56032899\n",
            "Iteration 120, loss = 0.55995455\n",
            "Iteration 121, loss = 0.55955228\n",
            "Iteration 122, loss = 0.55918703\n",
            "Iteration 123, loss = 0.55884484\n",
            "Iteration 124, loss = 0.55838076\n",
            "Iteration 125, loss = 0.55808569\n",
            "Iteration 126, loss = 0.55751445\n",
            "Iteration 127, loss = 0.55718773\n",
            "Iteration 128, loss = 0.55680289\n",
            "Iteration 129, loss = 0.55641844\n",
            "Iteration 130, loss = 0.55609968\n",
            "Iteration 131, loss = 0.55576557\n",
            "Iteration 132, loss = 0.55544813\n",
            "Iteration 133, loss = 0.55514959\n",
            "Iteration 134, loss = 0.55485585\n",
            "Iteration 135, loss = 0.55465213\n",
            "Iteration 136, loss = 0.55439104\n",
            "Iteration 137, loss = 0.55423854\n",
            "Iteration 138, loss = 0.55397484\n",
            "Iteration 139, loss = 0.55382484\n",
            "Iteration 140, loss = 0.55362496\n",
            "Iteration 141, loss = 0.55350449\n",
            "Iteration 142, loss = 0.55330935\n",
            "Iteration 143, loss = 0.55312696\n",
            "Iteration 144, loss = 0.55287228\n",
            "Iteration 145, loss = 0.55263719\n",
            "Iteration 146, loss = 0.55255738\n",
            "Iteration 147, loss = 0.55232271\n",
            "Iteration 148, loss = 0.55213450\n",
            "Iteration 149, loss = 0.55196895\n",
            "Iteration 150, loss = 0.55176033\n",
            "Iteration 151, loss = 0.55161366\n",
            "Iteration 152, loss = 0.55140052\n",
            "Iteration 153, loss = 0.55143007\n",
            "Iteration 154, loss = 0.55119174\n",
            "Iteration 155, loss = 0.55115582\n",
            "Iteration 156, loss = 0.55100698\n",
            "Iteration 157, loss = 0.55081218\n",
            "Iteration 158, loss = 0.55060617\n",
            "Iteration 159, loss = 0.55034760\n",
            "Iteration 160, loss = 0.55028817\n",
            "Iteration 161, loss = 0.54998891\n",
            "Iteration 162, loss = 0.54978916\n",
            "Iteration 163, loss = 0.54968667\n",
            "Iteration 164, loss = 0.54957281\n",
            "Iteration 165, loss = 0.54949291\n",
            "Iteration 166, loss = 0.54940404\n",
            "Iteration 167, loss = 0.54932224\n",
            "Iteration 168, loss = 0.54918298\n",
            "Iteration 169, loss = 0.54903869\n",
            "Iteration 170, loss = 0.54893535\n",
            "Iteration 171, loss = 0.54886424\n",
            "Iteration 172, loss = 0.54867618\n",
            "Iteration 173, loss = 0.54861745\n",
            "Iteration 174, loss = 0.54838514\n",
            "Iteration 175, loss = 0.54830627\n",
            "Iteration 176, loss = 0.54819492\n",
            "Iteration 177, loss = 0.54813816\n",
            "Iteration 178, loss = 0.54797866\n",
            "Iteration 179, loss = 0.54781127\n",
            "Iteration 180, loss = 0.54768839\n",
            "Iteration 181, loss = 0.54748322\n",
            "Iteration 182, loss = 0.54751076\n",
            "Iteration 183, loss = 0.54725576\n",
            "Iteration 184, loss = 0.54712612\n",
            "Iteration 185, loss = 0.54701994\n",
            "Iteration 186, loss = 0.54699073\n",
            "Iteration 187, loss = 0.54690768\n",
            "Iteration 188, loss = 0.54685276\n",
            "Iteration 189, loss = 0.54672685\n",
            "Iteration 190, loss = 0.54660527\n",
            "Iteration 191, loss = 0.54656267\n",
            "Iteration 192, loss = 0.54645821\n",
            "Iteration 193, loss = 0.54626282\n",
            "Iteration 194, loss = 0.54609695\n",
            "Iteration 195, loss = 0.54589554\n",
            "Iteration 196, loss = 0.54579439\n",
            "Iteration 197, loss = 0.54571042\n",
            "Iteration 198, loss = 0.54559076\n",
            "Iteration 199, loss = 0.54544129\n",
            "Iteration 200, loss = 0.54534156\n",
            "Iteration 201, loss = 0.54522150\n",
            "Iteration 202, loss = 0.54510986\n",
            "Iteration 203, loss = 0.54500355\n",
            "Iteration 204, loss = 0.54492571\n",
            "Iteration 205, loss = 0.54471696\n",
            "Iteration 206, loss = 0.54451307\n",
            "Iteration 207, loss = 0.54439623\n",
            "Iteration 208, loss = 0.54432227\n",
            "Iteration 209, loss = 0.54431280\n",
            "Iteration 210, loss = 0.54402077\n",
            "Iteration 211, loss = 0.54386480\n",
            "Iteration 212, loss = 0.54375335\n",
            "Iteration 213, loss = 0.54362273\n",
            "Iteration 214, loss = 0.54357744\n",
            "Iteration 215, loss = 0.54338648\n",
            "Iteration 216, loss = 0.54359804\n",
            "Iteration 217, loss = 0.54331480\n",
            "Iteration 218, loss = 0.54314914\n",
            "Iteration 219, loss = 0.54307714\n",
            "Iteration 220, loss = 0.54293793\n",
            "Iteration 221, loss = 0.54283135\n",
            "Iteration 222, loss = 0.54278502\n",
            "Iteration 223, loss = 0.54266023\n",
            "Iteration 224, loss = 0.54262172\n",
            "Iteration 225, loss = 0.54247526\n",
            "Iteration 226, loss = 0.54243382\n",
            "Iteration 227, loss = 0.54237055\n",
            "Iteration 228, loss = 0.54224591\n",
            "Iteration 229, loss = 0.54216170\n",
            "Iteration 230, loss = 0.54219018\n",
            "Iteration 231, loss = 0.54206961\n",
            "Iteration 232, loss = 0.54204521\n",
            "Iteration 233, loss = 0.54194669\n",
            "Iteration 234, loss = 0.54201009\n",
            "Iteration 235, loss = 0.54195008\n",
            "Iteration 236, loss = 0.54191936\n",
            "Iteration 237, loss = 0.54184821\n",
            "Iteration 238, loss = 0.54191197\n",
            "Iteration 239, loss = 0.54180016\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61762838\n",
            "Iteration 2, loss = 0.61685222\n",
            "Iteration 3, loss = 0.61571214\n",
            "Iteration 4, loss = 0.61441758\n",
            "Iteration 5, loss = 0.61318349\n",
            "Iteration 6, loss = 0.61195746\n",
            "Iteration 7, loss = 0.61086455\n",
            "Iteration 8, loss = 0.60959350\n",
            "Iteration 9, loss = 0.60852999\n",
            "Iteration 10, loss = 0.60759281\n",
            "Iteration 11, loss = 0.60667038\n",
            "Iteration 12, loss = 0.60588256\n",
            "Iteration 13, loss = 0.60500642\n",
            "Iteration 14, loss = 0.60429505\n",
            "Iteration 15, loss = 0.60355287\n",
            "Iteration 16, loss = 0.60296533\n",
            "Iteration 17, loss = 0.60238875\n",
            "Iteration 18, loss = 0.60183941\n",
            "Iteration 19, loss = 0.60124485\n",
            "Iteration 20, loss = 0.60063271\n",
            "Iteration 21, loss = 0.60004317\n",
            "Iteration 22, loss = 0.59951396\n",
            "Iteration 23, loss = 0.59887438\n",
            "Iteration 24, loss = 0.59834187\n",
            "Iteration 25, loss = 0.59783710\n",
            "Iteration 26, loss = 0.59731217\n",
            "Iteration 27, loss = 0.59680236\n",
            "Iteration 28, loss = 0.59634036\n",
            "Iteration 29, loss = 0.59585225\n",
            "Iteration 30, loss = 0.59540926\n",
            "Iteration 31, loss = 0.59494784\n",
            "Iteration 32, loss = 0.59442520\n",
            "Iteration 33, loss = 0.59397329\n",
            "Iteration 34, loss = 0.59352410\n",
            "Iteration 35, loss = 0.59310715\n",
            "Iteration 36, loss = 0.59259080\n",
            "Iteration 37, loss = 0.59198806\n",
            "Iteration 38, loss = 0.59141069\n",
            "Iteration 39, loss = 0.59091519\n",
            "Iteration 40, loss = 0.59042012\n",
            "Iteration 41, loss = 0.58996507\n",
            "Iteration 42, loss = 0.58947078\n",
            "Iteration 43, loss = 0.58903801\n",
            "Iteration 44, loss = 0.58869976\n",
            "Iteration 45, loss = 0.58832512\n",
            "Iteration 46, loss = 0.58801003\n",
            "Iteration 47, loss = 0.58772939\n",
            "Iteration 48, loss = 0.58749791\n",
            "Iteration 49, loss = 0.58718487\n",
            "Iteration 50, loss = 0.58696384\n",
            "Iteration 51, loss = 0.58679751\n",
            "Iteration 52, loss = 0.58653756\n",
            "Iteration 53, loss = 0.58627720\n",
            "Iteration 54, loss = 0.58609678\n",
            "Iteration 55, loss = 0.58587111\n",
            "Iteration 56, loss = 0.58563655\n",
            "Iteration 57, loss = 0.58540147\n",
            "Iteration 58, loss = 0.58515985\n",
            "Iteration 59, loss = 0.58495083\n",
            "Iteration 60, loss = 0.58467493\n",
            "Iteration 61, loss = 0.58436402\n",
            "Iteration 62, loss = 0.58419131\n",
            "Iteration 63, loss = 0.58386634\n",
            "Iteration 64, loss = 0.58367166\n",
            "Iteration 65, loss = 0.58340516\n",
            "Iteration 66, loss = 0.58311369\n",
            "Iteration 67, loss = 0.58294822\n",
            "Iteration 68, loss = 0.58272389\n",
            "Iteration 69, loss = 0.58248177\n",
            "Iteration 70, loss = 0.58222376\n",
            "Iteration 71, loss = 0.58199437\n",
            "Iteration 72, loss = 0.58183608\n",
            "Iteration 73, loss = 0.58158739\n",
            "Iteration 74, loss = 0.58128332\n",
            "Iteration 75, loss = 0.58109528\n",
            "Iteration 76, loss = 0.58079797\n",
            "Iteration 77, loss = 0.58055032\n",
            "Iteration 78, loss = 0.58038713\n",
            "Iteration 79, loss = 0.58011096\n",
            "Iteration 80, loss = 0.57987169\n",
            "Iteration 81, loss = 0.57960002\n",
            "Iteration 82, loss = 0.57928697\n",
            "Iteration 83, loss = 0.57902631\n",
            "Iteration 84, loss = 0.57873116\n",
            "Iteration 85, loss = 0.57846126\n",
            "Iteration 86, loss = 0.57816501\n",
            "Iteration 87, loss = 0.57791457\n",
            "Iteration 88, loss = 0.57758301\n",
            "Iteration 89, loss = 0.57729328\n",
            "Iteration 90, loss = 0.57700737\n",
            "Iteration 91, loss = 0.57667723\n",
            "Iteration 92, loss = 0.57637588\n",
            "Iteration 93, loss = 0.57608463\n",
            "Iteration 94, loss = 0.57589851\n",
            "Iteration 95, loss = 0.57547867\n",
            "Iteration 96, loss = 0.57515434\n",
            "Iteration 97, loss = 0.57484887\n",
            "Iteration 98, loss = 0.57450644\n",
            "Iteration 99, loss = 0.57423971\n",
            "Iteration 100, loss = 0.57394845\n",
            "Iteration 101, loss = 0.57367106\n",
            "Iteration 102, loss = 0.57345373\n",
            "Iteration 103, loss = 0.57315003\n",
            "Iteration 104, loss = 0.57286482\n",
            "Iteration 105, loss = 0.57257109\n",
            "Iteration 106, loss = 0.57236005\n",
            "Iteration 107, loss = 0.57210592\n",
            "Iteration 108, loss = 0.57182320\n",
            "Iteration 109, loss = 0.57157851\n",
            "Iteration 110, loss = 0.57130851\n",
            "Iteration 111, loss = 0.57099353\n",
            "Iteration 112, loss = 0.57063653\n",
            "Iteration 113, loss = 0.57026642\n",
            "Iteration 114, loss = 0.56991901\n",
            "Iteration 115, loss = 0.56953357\n",
            "Iteration 116, loss = 0.56930145\n",
            "Iteration 117, loss = 0.56896447\n",
            "Iteration 118, loss = 0.56864356\n",
            "Iteration 119, loss = 0.56836572\n",
            "Iteration 120, loss = 0.56804066\n",
            "Iteration 121, loss = 0.56773562\n",
            "Iteration 122, loss = 0.56755539\n",
            "Iteration 123, loss = 0.56736130\n",
            "Iteration 124, loss = 0.56711146\n",
            "Iteration 125, loss = 0.56683908\n",
            "Iteration 126, loss = 0.56647484\n",
            "Iteration 127, loss = 0.56619872\n",
            "Iteration 128, loss = 0.56586599\n",
            "Iteration 129, loss = 0.56559488\n",
            "Iteration 130, loss = 0.56533314\n",
            "Iteration 131, loss = 0.56508778\n",
            "Iteration 132, loss = 0.56473660\n",
            "Iteration 133, loss = 0.56450473\n",
            "Iteration 134, loss = 0.56427530\n",
            "Iteration 135, loss = 0.56395923\n",
            "Iteration 136, loss = 0.56372237\n",
            "Iteration 137, loss = 0.56337833\n",
            "Iteration 138, loss = 0.56311224\n",
            "Iteration 139, loss = 0.56286408\n",
            "Iteration 140, loss = 0.56260020\n",
            "Iteration 141, loss = 0.56237318\n",
            "Iteration 142, loss = 0.56214346\n",
            "Iteration 143, loss = 0.56184605\n",
            "Iteration 144, loss = 0.56160014\n",
            "Iteration 145, loss = 0.56122180\n",
            "Iteration 146, loss = 0.56102411\n",
            "Iteration 147, loss = 0.56075902\n",
            "Iteration 148, loss = 0.56051512\n",
            "Iteration 149, loss = 0.56014889\n",
            "Iteration 150, loss = 0.55996943\n",
            "Iteration 151, loss = 0.55974368\n",
            "Iteration 152, loss = 0.55959805\n",
            "Iteration 153, loss = 0.55969770\n",
            "Iteration 154, loss = 0.55946636\n",
            "Iteration 155, loss = 0.55930271\n",
            "Iteration 156, loss = 0.55903201\n",
            "Iteration 157, loss = 0.55884274\n",
            "Iteration 158, loss = 0.55846432\n",
            "Iteration 159, loss = 0.55821890\n",
            "Iteration 160, loss = 0.55801632\n",
            "Iteration 161, loss = 0.55771887\n",
            "Iteration 162, loss = 0.55752476\n",
            "Iteration 163, loss = 0.55729095\n",
            "Iteration 164, loss = 0.55711046\n",
            "Iteration 165, loss = 0.55699770\n",
            "Iteration 166, loss = 0.55679932\n",
            "Iteration 167, loss = 0.55662522\n",
            "Iteration 168, loss = 0.55637875\n",
            "Iteration 169, loss = 0.55615027\n",
            "Iteration 170, loss = 0.55596352\n",
            "Iteration 171, loss = 0.55576055\n",
            "Iteration 172, loss = 0.55555154\n",
            "Iteration 173, loss = 0.55535588\n",
            "Iteration 174, loss = 0.55513544\n",
            "Iteration 175, loss = 0.55498198\n",
            "Iteration 176, loss = 0.55483895\n",
            "Iteration 177, loss = 0.55465485\n",
            "Iteration 178, loss = 0.55443179\n",
            "Iteration 179, loss = 0.55429233\n",
            "Iteration 180, loss = 0.55408917\n",
            "Iteration 181, loss = 0.55384782\n",
            "Iteration 182, loss = 0.55370002\n",
            "Iteration 183, loss = 0.55346441\n",
            "Iteration 184, loss = 0.55332935\n",
            "Iteration 185, loss = 0.55310535\n",
            "Iteration 186, loss = 0.55298333\n",
            "Iteration 187, loss = 0.55277597\n",
            "Iteration 188, loss = 0.55259738\n",
            "Iteration 189, loss = 0.55240019\n",
            "Iteration 190, loss = 0.55254449\n",
            "Iteration 191, loss = 0.55251468\n",
            "Iteration 192, loss = 0.55239200\n",
            "Iteration 193, loss = 0.55209030\n",
            "Iteration 194, loss = 0.55187264\n",
            "Iteration 195, loss = 0.55160011\n",
            "Iteration 196, loss = 0.55134965\n",
            "Iteration 197, loss = 0.55123327\n",
            "Iteration 198, loss = 0.55098521\n",
            "Iteration 199, loss = 0.55082595\n",
            "Iteration 200, loss = 0.55064578\n",
            "Iteration 201, loss = 0.55051651\n",
            "Iteration 202, loss = 0.55028788\n",
            "Iteration 203, loss = 0.55006359\n",
            "Iteration 204, loss = 0.54985633\n",
            "Iteration 205, loss = 0.54965974\n",
            "Iteration 206, loss = 0.54941739\n",
            "Iteration 207, loss = 0.54928407\n",
            "Iteration 208, loss = 0.54913204\n",
            "Iteration 209, loss = 0.54912141\n",
            "Iteration 210, loss = 0.54881310\n",
            "Iteration 211, loss = 0.54866333\n",
            "Iteration 212, loss = 0.54847660\n",
            "Iteration 213, loss = 0.54839451\n",
            "Iteration 214, loss = 0.54828436\n",
            "Iteration 215, loss = 0.54816619\n",
            "Iteration 216, loss = 0.54812288\n",
            "Iteration 217, loss = 0.54795356\n",
            "Iteration 218, loss = 0.54778668\n",
            "Iteration 219, loss = 0.54762750\n",
            "Iteration 220, loss = 0.54743091\n",
            "Iteration 221, loss = 0.54734685\n",
            "Iteration 222, loss = 0.54730248\n",
            "Iteration 223, loss = 0.54715850\n",
            "Iteration 224, loss = 0.54710401\n",
            "Iteration 225, loss = 0.54695936\n",
            "Iteration 226, loss = 0.54692655\n",
            "Iteration 227, loss = 0.54681063\n",
            "Iteration 228, loss = 0.54672635\n",
            "Iteration 229, loss = 0.54660803\n",
            "Iteration 230, loss = 0.54652546\n",
            "Iteration 231, loss = 0.54639261\n",
            "Iteration 232, loss = 0.54632396\n",
            "Iteration 233, loss = 0.54614212\n",
            "Iteration 234, loss = 0.54614338\n",
            "Iteration 235, loss = 0.54608512\n",
            "Iteration 236, loss = 0.54594995\n",
            "Iteration 237, loss = 0.54586676\n",
            "Iteration 238, loss = 0.54592507\n",
            "Iteration 239, loss = 0.54581721\n",
            "Iteration 240, loss = 0.54575034\n",
            "Iteration 241, loss = 0.54569760\n",
            "Iteration 242, loss = 0.54556982\n",
            "Iteration 243, loss = 0.54534937\n",
            "Iteration 244, loss = 0.54530920\n",
            "Iteration 245, loss = 0.54506163\n",
            "Iteration 246, loss = 0.54494445\n",
            "Iteration 247, loss = 0.54482088\n",
            "Iteration 248, loss = 0.54464878\n",
            "Iteration 249, loss = 0.54455844\n",
            "Iteration 250, loss = 0.54435102\n",
            "Iteration 1, loss = 0.61756281\n",
            "Iteration 2, loss = 0.61678746\n",
            "Iteration 3, loss = 0.61551433\n",
            "Iteration 4, loss = 0.61406466\n",
            "Iteration 5, loss = 0.61275832\n",
            "Iteration 6, loss = 0.61139014\n",
            "Iteration 7, loss = 0.61018275\n",
            "Iteration 8, loss = 0.60882083\n",
            "Iteration 9, loss = 0.60770150\n",
            "Iteration 10, loss = 0.60664596\n",
            "Iteration 11, loss = 0.60580536\n",
            "Iteration 12, loss = 0.60503210\n",
            "Iteration 13, loss = 0.60414541\n",
            "Iteration 14, loss = 0.60345742\n",
            "Iteration 15, loss = 0.60273866\n",
            "Iteration 16, loss = 0.60214876\n",
            "Iteration 17, loss = 0.60155645\n",
            "Iteration 18, loss = 0.60098285\n",
            "Iteration 19, loss = 0.60041376\n",
            "Iteration 20, loss = 0.59981786\n",
            "Iteration 21, loss = 0.59929265\n",
            "Iteration 22, loss = 0.59865761\n",
            "Iteration 23, loss = 0.59789106\n",
            "Iteration 24, loss = 0.59730522\n",
            "Iteration 25, loss = 0.59667276\n",
            "Iteration 26, loss = 0.59602085\n",
            "Iteration 27, loss = 0.59538918\n",
            "Iteration 28, loss = 0.59476517\n",
            "Iteration 29, loss = 0.59420415\n",
            "Iteration 30, loss = 0.59360792\n",
            "Iteration 31, loss = 0.59298919\n",
            "Iteration 32, loss = 0.59233785\n",
            "Iteration 33, loss = 0.59173787\n",
            "Iteration 34, loss = 0.59105348\n",
            "Iteration 35, loss = 0.59039394\n",
            "Iteration 36, loss = 0.58983637\n",
            "Iteration 37, loss = 0.58907064\n",
            "Iteration 38, loss = 0.58833953\n",
            "Iteration 39, loss = 0.58772116\n",
            "Iteration 40, loss = 0.58701155\n",
            "Iteration 41, loss = 0.58632754\n",
            "Iteration 42, loss = 0.58559078\n",
            "Iteration 43, loss = 0.58496621\n",
            "Iteration 44, loss = 0.58428972\n",
            "Iteration 45, loss = 0.58358511\n",
            "Iteration 46, loss = 0.58293101\n",
            "Iteration 47, loss = 0.58227798\n",
            "Iteration 48, loss = 0.58170222\n",
            "Iteration 49, loss = 0.58101512\n",
            "Iteration 50, loss = 0.58043029\n",
            "Iteration 51, loss = 0.57991960\n",
            "Iteration 52, loss = 0.57933529\n",
            "Iteration 53, loss = 0.57870244\n",
            "Iteration 54, loss = 0.57829782\n",
            "Iteration 55, loss = 0.57767445\n",
            "Iteration 56, loss = 0.57715199\n",
            "Iteration 57, loss = 0.57663478\n",
            "Iteration 58, loss = 0.57604727\n",
            "Iteration 59, loss = 0.57548153\n",
            "Iteration 60, loss = 0.57475842"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 61, loss = 0.57416184\n",
            "Iteration 62, loss = 0.57353961\n",
            "Iteration 63, loss = 0.57287798\n",
            "Iteration 64, loss = 0.57223335\n",
            "Iteration 65, loss = 0.57144876\n",
            "Iteration 66, loss = 0.57074095\n",
            "Iteration 67, loss = 0.57018441\n",
            "Iteration 68, loss = 0.56959535\n",
            "Iteration 69, loss = 0.56896543\n",
            "Iteration 70, loss = 0.56840160\n",
            "Iteration 71, loss = 0.56782256\n",
            "Iteration 72, loss = 0.56726268\n",
            "Iteration 73, loss = 0.56657180\n",
            "Iteration 74, loss = 0.56591685\n",
            "Iteration 75, loss = 0.56546255\n",
            "Iteration 76, loss = 0.56487449\n",
            "Iteration 77, loss = 0.56432787\n",
            "Iteration 78, loss = 0.56379671\n",
            "Iteration 79, loss = 0.56323004\n",
            "Iteration 80, loss = 0.56263426\n",
            "Iteration 81, loss = 0.56212573\n",
            "Iteration 82, loss = 0.56151629\n",
            "Iteration 83, loss = 0.56097539\n",
            "Iteration 84, loss = 0.56051914\n",
            "Iteration 85, loss = 0.56006355\n",
            "Iteration 86, loss = 0.55956176\n",
            "Iteration 87, loss = 0.55907687\n",
            "Iteration 88, loss = 0.55861192\n",
            "Iteration 89, loss = 0.55803563\n",
            "Iteration 90, loss = 0.55760848\n",
            "Iteration 91, loss = 0.55701483\n",
            "Iteration 92, loss = 0.55655069\n",
            "Iteration 93, loss = 0.55609322\n",
            "Iteration 94, loss = 0.55567123\n",
            "Iteration 95, loss = 0.55502534\n",
            "Iteration 96, loss = 0.55460817\n",
            "Iteration 97, loss = 0.55402209\n",
            "Iteration 98, loss = 0.55355900\n",
            "Iteration 99, loss = 0.55318405\n",
            "Iteration 100, loss = 0.55277358\n",
            "Iteration 101, loss = 0.55236759\n",
            "Iteration 102, loss = 0.55205879\n",
            "Iteration 103, loss = 0.55161370\n",
            "Iteration 104, loss = 0.55123778\n",
            "Iteration 105, loss = 0.55086666\n",
            "Iteration 106, loss = 0.55047447\n",
            "Iteration 107, loss = 0.55012781\n",
            "Iteration 108, loss = 0.54966725\n",
            "Iteration 109, loss = 0.54927395\n",
            "Iteration 110, loss = 0.54888330\n",
            "Iteration 111, loss = 0.54853697\n",
            "Iteration 112, loss = 0.54803161\n",
            "Iteration 113, loss = 0.54759915\n",
            "Iteration 114, loss = 0.54717141\n",
            "Iteration 115, loss = 0.54685299\n",
            "Iteration 116, loss = 0.54647567\n",
            "Iteration 117, loss = 0.54607307\n",
            "Iteration 118, loss = 0.54574090\n",
            "Iteration 119, loss = 0.54544404\n",
            "Iteration 120, loss = 0.54504897\n",
            "Iteration 121, loss = 0.54472741\n",
            "Iteration 122, loss = 0.54438772\n",
            "Iteration 123, loss = 0.54404215\n",
            "Iteration 124, loss = 0.54369341\n",
            "Iteration 125, loss = 0.54343200\n",
            "Iteration 126, loss = 0.54302948\n",
            "Iteration 127, loss = 0.54279117\n",
            "Iteration 128, loss = 0.54249796\n",
            "Iteration 129, loss = 0.54233272\n",
            "Iteration 130, loss = 0.54201110\n",
            "Iteration 131, loss = 0.54157225\n",
            "Iteration 132, loss = 0.54113379\n",
            "Iteration 133, loss = 0.54074148\n",
            "Iteration 134, loss = 0.54038149\n",
            "Iteration 135, loss = 0.53995048\n",
            "Iteration 136, loss = 0.53962117\n",
            "Iteration 137, loss = 0.53916907\n",
            "Iteration 138, loss = 0.53877409\n",
            "Iteration 139, loss = 0.53841380\n",
            "Iteration 140, loss = 0.53806495\n",
            "Iteration 141, loss = 0.53771208\n",
            "Iteration 142, loss = 0.53742659\n",
            "Iteration 143, loss = 0.53737448\n",
            "Iteration 144, loss = 0.53686941\n",
            "Iteration 145, loss = 0.53651881\n",
            "Iteration 146, loss = 0.53625293\n",
            "Iteration 147, loss = 0.53595237\n",
            "Iteration 148, loss = 0.53573440\n",
            "Iteration 149, loss = 0.53537873\n",
            "Iteration 150, loss = 0.53518124\n",
            "Iteration 151, loss = 0.53496385\n",
            "Iteration 152, loss = 0.53475622\n",
            "Iteration 153, loss = 0.53472320\n",
            "Iteration 154, loss = 0.53453199\n",
            "Iteration 155, loss = 0.53435198\n",
            "Iteration 156, loss = 0.53418048\n",
            "Iteration 157, loss = 0.53399366\n",
            "Iteration 158, loss = 0.53376552\n",
            "Iteration 159, loss = 0.53339142\n",
            "Iteration 160, loss = 0.53325644\n",
            "Iteration 161, loss = 0.53286228\n",
            "Iteration 162, loss = 0.53275720\n",
            "Iteration 163, loss = 0.53253266\n",
            "Iteration 164, loss = 0.53243114\n",
            "Iteration 165, loss = 0.53231731\n",
            "Iteration 166, loss = 0.53206105\n",
            "Iteration 167, loss = 0.53198550\n",
            "Iteration 168, loss = 0.53170182\n",
            "Iteration 169, loss = 0.53153900\n",
            "Iteration 170, loss = 0.53138415\n",
            "Iteration 171, loss = 0.53125249\n",
            "Iteration 172, loss = 0.53109301\n",
            "Iteration 173, loss = 0.53093246\n",
            "Iteration 174, loss = 0.53079377\n",
            "Iteration 175, loss = 0.53062537\n",
            "Iteration 176, loss = 0.53049244\n",
            "Iteration 177, loss = 0.53036269\n",
            "Iteration 178, loss = 0.53025670\n",
            "Iteration 179, loss = 0.53006383\n",
            "Iteration 180, loss = 0.52990961\n",
            "Iteration 181, loss = 0.52975918\n",
            "Iteration 182, loss = 0.52968845\n",
            "Iteration 183, loss = 0.52949678\n",
            "Iteration 184, loss = 0.52942640\n",
            "Iteration 185, loss = 0.52931952\n",
            "Iteration 186, loss = 0.52921428\n",
            "Iteration 187, loss = 0.52909771\n",
            "Iteration 188, loss = 0.52892582\n",
            "Iteration 189, loss = 0.52886677\n",
            "Iteration 190, loss = 0.52882617\n",
            "Iteration 191, loss = 0.52890744\n",
            "Iteration 192, loss = 0.52882516\n",
            "Iteration 193, loss = 0.52871273\n",
            "Iteration 194, loss = 0.52859940\n",
            "Iteration 195, loss = 0.52848706\n",
            "Iteration 196, loss = 0.52831421\n",
            "Iteration 197, loss = 0.52822673\n",
            "Iteration 198, loss = 0.52806948\n",
            "Iteration 199, loss = 0.52793966\n",
            "Iteration 200, loss = 0.52777690\n",
            "Iteration 201, loss = 0.52755966\n",
            "Iteration 202, loss = 0.52742148\n",
            "Iteration 203, loss = 0.52716511\n",
            "Iteration 204, loss = 0.52699781\n",
            "Iteration 205, loss = 0.52679817\n",
            "Iteration 206, loss = 0.52666409\n",
            "Iteration 207, loss = 0.52648883\n",
            "Iteration 208, loss = 0.52636478\n",
            "Iteration 209, loss = 0.52632180\n",
            "Iteration 210, loss = 0.52595080\n",
            "Iteration 211, loss = 0.52580003\n",
            "Iteration 212, loss = 0.52560865\n",
            "Iteration 213, loss = 0.52553551\n",
            "Iteration 214, loss = 0.52539070\n",
            "Iteration 215, loss = 0.52529094\n",
            "Iteration 216, loss = 0.52521656\n",
            "Iteration 217, loss = 0.52511274\n",
            "Iteration 218, loss = 0.52487813\n",
            "Iteration 219, loss = 0.52470802\n",
            "Iteration 220, loss = 0.52460557\n",
            "Iteration 221, loss = 0.52446432\n",
            "Iteration 222, loss = 0.52441338\n",
            "Iteration 223, loss = 0.52434786\n",
            "Iteration 224, loss = 0.52430039\n",
            "Iteration 225, loss = 0.52411889\n",
            "Iteration 226, loss = 0.52405606\n",
            "Iteration 227, loss = 0.52394814\n",
            "Iteration 228, loss = 0.52377092\n",
            "Iteration 229, loss = 0.52365546\n",
            "Iteration 230, loss = 0.52364928\n",
            "Iteration 231, loss = 0.52355147\n",
            "Iteration 232, loss = 0.52340792\n",
            "Iteration 233, loss = 0.52317724\n",
            "Iteration 234, loss = 0.52310946\n",
            "Iteration 235, loss = 0.52311658\n",
            "Iteration 236, loss = 0.52299133\n",
            "Iteration 237, loss = 0.52278095\n",
            "Iteration 238, loss = 0.52269433\n",
            "Iteration 239, loss = 0.52245040\n",
            "Iteration 240, loss = 0.52232617\n",
            "Iteration 241, loss = 0.52217990\n",
            "Iteration 242, loss = 0.52199075\n",
            "Iteration 243, loss = 0.52168834\n",
            "Iteration 244, loss = 0.52167454\n",
            "Iteration 245, loss = 0.52143235\n",
            "Iteration 246, loss = 0.52122328\n",
            "Iteration 247, loss = 0.52108806\n",
            "Iteration 248, loss = 0.52101486\n",
            "Iteration 249, loss = 0.52100276\n",
            "Iteration 250, loss = 0.52080474\n",
            "Iteration 1, loss = 0.62079721\n",
            "Iteration 2, loss = 0.62005324\n",
            "Iteration 3, loss = 0.61900678\n",
            "Iteration 4, loss = 0.61766780\n",
            "Iteration 5, loss = 0.61643559\n",
            "Iteration 6, loss = 0.61518085\n",
            "Iteration 7, loss = 0.61403740\n",
            "Iteration 8, loss = 0.61288976\n",
            "Iteration 9, loss = 0.61190780\n",
            "Iteration 10, loss = 0.61098604\n",
            "Iteration 11, loss = 0.61018985\n",
            "Iteration 12, loss = 0.60952147\n",
            "Iteration 13, loss = 0.60886466\n",
            "Iteration 14, loss = 0.60818565\n",
            "Iteration 15, loss = 0.60758049\n",
            "Iteration 16, loss = 0.60707209\n",
            "Iteration 17, loss = 0.60648497\n",
            "Iteration 18, loss = 0.60597777\n",
            "Iteration 19, loss = 0.60548418\n",
            "Iteration 20, loss = 0.60501951\n",
            "Iteration 21, loss = 0.60460897\n",
            "Iteration 22, loss = 0.60410824\n",
            "Iteration 23, loss = 0.60353209\n",
            "Iteration 24, loss = 0.60308578\n",
            "Iteration 25, loss = 0.60261600\n",
            "Iteration 26, loss = 0.60213958\n",
            "Iteration 27, loss = 0.60173715\n",
            "Iteration 28, loss = 0.60121898\n",
            "Iteration 29, loss = 0.60074744\n",
            "Iteration 30, loss = 0.60026547\n",
            "Iteration 31, loss = 0.59976581\n",
            "Iteration 32, loss = 0.59927205\n",
            "Iteration 33, loss = 0.59878003\n",
            "Iteration 34, loss = 0.59832639\n",
            "Iteration 35, loss = 0.59786446\n",
            "Iteration 36, loss = 0.59746968\n",
            "Iteration 37, loss = 0.59693135\n",
            "Iteration 38, loss = 0.59639720\n",
            "Iteration 39, loss = 0.59583464\n",
            "Iteration 40, loss = 0.59533087\n",
            "Iteration 41, loss = 0.59475384\n",
            "Iteration 42, loss = 0.59425957\n",
            "Iteration 43, loss = 0.59373095\n",
            "Iteration 44, loss = 0.59323666"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 45, loss = 0.59264518\n",
            "Iteration 46, loss = 0.59213675\n",
            "Iteration 47, loss = 0.59166031\n",
            "Iteration 48, loss = 0.59110663\n",
            "Iteration 49, loss = 0.59053353\n",
            "Iteration 50, loss = 0.58998709\n",
            "Iteration 51, loss = 0.58950475\n",
            "Iteration 52, loss = 0.58904938\n",
            "Iteration 53, loss = 0.58852797\n",
            "Iteration 54, loss = 0.58812035\n",
            "Iteration 55, loss = 0.58762289\n",
            "Iteration 56, loss = 0.58712541\n",
            "Iteration 57, loss = 0.58666104\n",
            "Iteration 58, loss = 0.58615629\n",
            "Iteration 59, loss = 0.58561809\n",
            "Iteration 60, loss = 0.58512577\n",
            "Iteration 61, loss = 0.58466198\n",
            "Iteration 62, loss = 0.58411778\n",
            "Iteration 63, loss = 0.58359615\n",
            "Iteration 64, loss = 0.58300139\n",
            "Iteration 65, loss = 0.58244389\n",
            "Iteration 66, loss = 0.58190597\n",
            "Iteration 67, loss = 0.58141029\n",
            "Iteration 68, loss = 0.58085057\n",
            "Iteration 69, loss = 0.58035273\n",
            "Iteration 70, loss = 0.57970416\n",
            "Iteration 71, loss = 0.57915712\n",
            "Iteration 72, loss = 0.57860550\n",
            "Iteration 73, loss = 0.57797929\n",
            "Iteration 74, loss = 0.57738233\n",
            "Iteration 75, loss = 0.57704390\n",
            "Iteration 76, loss = 0.57639729\n",
            "Iteration 77, loss = 0.57581682\n",
            "Iteration 78, loss = 0.57543752\n",
            "Iteration 79, loss = 0.57485995\n",
            "Iteration 80, loss = 0.57445793\n",
            "Iteration 81, loss = 0.57398971\n",
            "Iteration 82, loss = 0.57361636\n",
            "Iteration 83, loss = 0.57318059\n",
            "Iteration 84, loss = 0.57282423\n",
            "Iteration 85, loss = 0.57252176\n",
            "Iteration 86, loss = 0.57218478\n",
            "Iteration 87, loss = 0.57185258\n",
            "Iteration 88, loss = 0.57147761\n",
            "Iteration 89, loss = 0.57112446\n",
            "Iteration 90, loss = 0.57071716\n",
            "Iteration 91, loss = 0.57028615\n",
            "Iteration 92, loss = 0.56993985\n",
            "Iteration 93, loss = 0.56954471\n",
            "Iteration 94, loss = 0.56924308\n",
            "Iteration 95, loss = 0.56872118\n",
            "Iteration 96, loss = 0.56844243\n",
            "Iteration 97, loss = 0.56803433\n",
            "Iteration 98, loss = 0.56776972\n",
            "Iteration 99, loss = 0.56749363\n",
            "Iteration 100, loss = 0.56718606\n",
            "Iteration 101, loss = 0.56689778\n",
            "Iteration 102, loss = 0.56664384\n",
            "Iteration 103, loss = 0.56635293\n",
            "Iteration 104, loss = 0.56623406\n",
            "Iteration 105, loss = 0.56601177\n",
            "Iteration 106, loss = 0.56568366\n",
            "Iteration 107, loss = 0.56551641\n",
            "Iteration 108, loss = 0.56510360\n",
            "Iteration 109, loss = 0.56480059\n",
            "Iteration 110, loss = 0.56454979\n",
            "Iteration 111, loss = 0.56429531\n",
            "Iteration 112, loss = 0.56395747\n",
            "Iteration 113, loss = 0.56367286\n",
            "Iteration 114, loss = 0.56345796\n",
            "Iteration 115, loss = 0.56326450\n",
            "Iteration 116, loss = 0.56287617\n",
            "Iteration 117, loss = 0.56259053\n",
            "Iteration 118, loss = 0.56238330\n",
            "Iteration 119, loss = 0.56215496\n",
            "Iteration 120, loss = 0.56186900\n",
            "Iteration 121, loss = 0.56167566\n",
            "Iteration 122, loss = 0.56132266\n",
            "Iteration 123, loss = 0.56106314\n",
            "Iteration 124, loss = 0.56088367\n",
            "Iteration 125, loss = 0.56067779\n",
            "Iteration 126, loss = 0.56034009\n",
            "Iteration 127, loss = 0.56018241\n",
            "Iteration 128, loss = 0.55983784\n",
            "Iteration 129, loss = 0.55972969\n",
            "Iteration 130, loss = 0.55940300\n",
            "Iteration 131, loss = 0.55919037\n",
            "Iteration 132, loss = 0.55889555\n",
            "Iteration 133, loss = 0.55867239\n",
            "Iteration 134, loss = 0.55855201\n",
            "Iteration 135, loss = 0.55830447\n",
            "Iteration 136, loss = 0.55815356\n",
            "Iteration 137, loss = 0.55796883\n",
            "Iteration 138, loss = 0.55783932\n",
            "Iteration 139, loss = 0.55769593\n",
            "Iteration 140, loss = 0.55754655\n",
            "Iteration 141, loss = 0.55735505\n",
            "Iteration 142, loss = 0.55723094\n",
            "Iteration 143, loss = 0.55714053\n",
            "Iteration 144, loss = 0.55691044\n",
            "Iteration 145, loss = 0.55677425\n",
            "Iteration 146, loss = 0.55657889\n",
            "Iteration 147, loss = 0.55637529\n",
            "Iteration 148, loss = 0.55623800\n",
            "Iteration 149, loss = 0.55604883\n",
            "Iteration 150, loss = 0.55589847\n",
            "Iteration 151, loss = 0.55574381\n",
            "Iteration 152, loss = 0.55564436\n",
            "Iteration 153, loss = 0.55560322\n",
            "Iteration 154, loss = 0.55536007\n",
            "Iteration 155, loss = 0.55522516\n",
            "Iteration 156, loss = 0.55496692\n",
            "Iteration 157, loss = 0.55488775\n",
            "Iteration 158, loss = 0.55474647\n",
            "Iteration 159, loss = 0.55448185\n",
            "Iteration 160, loss = 0.55435752\n",
            "Iteration 161, loss = 0.55408465\n",
            "Iteration 162, loss = 0.55393144\n",
            "Iteration 163, loss = 0.55377149\n",
            "Iteration 164, loss = 0.55364084\n",
            "Iteration 165, loss = 0.55355824\n",
            "Iteration 166, loss = 0.55328245\n",
            "Iteration 167, loss = 0.55312906\n",
            "Iteration 168, loss = 0.55298317\n",
            "Iteration 169, loss = 0.55280599\n",
            "Iteration 170, loss = 0.55263238\n",
            "Iteration 171, loss = 0.55244581\n",
            "Iteration 172, loss = 0.55227972\n",
            "Iteration 173, loss = 0.55217219\n",
            "Iteration 174, loss = 0.55200676\n",
            "Iteration 175, loss = 0.55173998\n",
            "Iteration 176, loss = 0.55161848\n",
            "Iteration 177, loss = 0.55141735\n",
            "Iteration 178, loss = 0.55122916\n",
            "Iteration 179, loss = 0.55102234\n",
            "Iteration 180, loss = 0.55079965\n",
            "Iteration 181, loss = 0.55056503\n",
            "Iteration 182, loss = 0.55040293\n",
            "Iteration 183, loss = 0.55024137\n",
            "Iteration 184, loss = 0.55018047\n",
            "Iteration 185, loss = 0.55002542\n",
            "Iteration 186, loss = 0.54981871\n",
            "Iteration 187, loss = 0.54969793\n",
            "Iteration 188, loss = 0.54948811\n",
            "Iteration 189, loss = 0.54925702\n",
            "Iteration 190, loss = 0.54909133\n",
            "Iteration 191, loss = 0.54892199\n",
            "Iteration 192, loss = 0.54887892\n",
            "Iteration 193, loss = 0.54873508\n",
            "Iteration 194, loss = 0.54860612\n",
            "Iteration 195, loss = 0.54847942\n",
            "Iteration 196, loss = 0.54837547\n",
            "Iteration 197, loss = 0.54819570\n",
            "Iteration 198, loss = 0.54801947\n",
            "Iteration 199, loss = 0.54782665\n",
            "Iteration 200, loss = 0.54771213\n",
            "Iteration 201, loss = 0.54740468\n",
            "Iteration 202, loss = 0.54717162\n",
            "Iteration 203, loss = 0.54710456\n",
            "Iteration 204, loss = 0.54683997\n",
            "Iteration 205, loss = 0.54661037\n",
            "Iteration 206, loss = 0.54635997\n",
            "Iteration 207, loss = 0.54614683\n",
            "Iteration 208, loss = 0.54594941\n",
            "Iteration 209, loss = 0.54589075\n",
            "Iteration 210, loss = 0.54558196\n",
            "Iteration 211, loss = 0.54540971\n",
            "Iteration 212, loss = 0.54523595\n",
            "Iteration 213, loss = 0.54510112\n",
            "Iteration 214, loss = 0.54496527\n",
            "Iteration 215, loss = 0.54480637\n",
            "Iteration 216, loss = 0.54457005\n",
            "Iteration 217, loss = 0.54443682\n",
            "Iteration 218, loss = 0.54421141\n",
            "Iteration 219, loss = 0.54400060\n",
            "Iteration 220, loss = 0.54387388\n",
            "Iteration 221, loss = 0.54369896\n",
            "Iteration 222, loss = 0.54352793\n",
            "Iteration 223, loss = 0.54343083\n",
            "Iteration 224, loss = 0.54330262\n",
            "Iteration 225, loss = 0.54318851\n",
            "Iteration 226, loss = 0.54303317\n",
            "Iteration 227, loss = 0.54299944\n",
            "Iteration 228, loss = 0.54280229\n",
            "Iteration 229, loss = 0.54271944\n",
            "Iteration 230, loss = 0.54258208\n",
            "Iteration 231, loss = 0.54250355\n",
            "Iteration 232, loss = 0.54242001\n",
            "Iteration 233, loss = 0.54227021\n",
            "Iteration 234, loss = 0.54226304\n",
            "Iteration 235, loss = 0.54223290\n",
            "Iteration 236, loss = 0.54213749\n",
            "Iteration 237, loss = 0.54201937\n",
            "Iteration 238, loss = 0.54199949\n",
            "Iteration 239, loss = 0.54180308\n",
            "Iteration 240, loss = 0.54170994\n",
            "Iteration 241, loss = 0.54166517\n",
            "Iteration 242, loss = 0.54161430\n",
            "Iteration 243, loss = 0.54148204\n",
            "Iteration 244, loss = 0.54146347\n",
            "Iteration 245, loss = 0.54135737\n",
            "Iteration 246, loss = 0.54113861\n",
            "Iteration 247, loss = 0.54105167\n",
            "Iteration 248, loss = 0.54098252\n",
            "Iteration 249, loss = 0.54095948\n",
            "Iteration 250, loss = 0.54080635\n",
            "Iteration 1, loss = 0.62010582\n",
            "Iteration 2, loss = 0.61930514\n",
            "Iteration 3, loss = 0.61826319\n",
            "Iteration 4, loss = 0.61682911\n",
            "Iteration 5, loss = 0.61543360\n",
            "Iteration 6, loss = 0.61413706\n",
            "Iteration 7, loss = 0.61295141\n",
            "Iteration 8, loss = 0.61172441\n",
            "Iteration 9, loss = 0.61081242\n",
            "Iteration 10, loss = 0.60991460\n",
            "Iteration 11, loss = 0.60917637\n",
            "Iteration 12, loss = 0.60858965\n",
            "Iteration 13, loss = 0.60792078\n",
            "Iteration 14, loss = 0.60730029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 15, loss = 0.60678248\n",
            "Iteration 16, loss = 0.60632478\n",
            "Iteration 17, loss = 0.60582791\n",
            "Iteration 18, loss = 0.60531306\n",
            "Iteration 19, loss = 0.60480910\n",
            "Iteration 20, loss = 0.60441098\n",
            "Iteration 21, loss = 0.60409300\n",
            "Iteration 22, loss = 0.60359792\n",
            "Iteration 23, loss = 0.60325513\n",
            "Iteration 24, loss = 0.60288068\n",
            "Iteration 25, loss = 0.60251982\n",
            "Iteration 26, loss = 0.60216641\n",
            "Iteration 27, loss = 0.60186024\n",
            "Iteration 28, loss = 0.60148385\n",
            "Iteration 29, loss = 0.60117667\n",
            "Iteration 30, loss = 0.60087078\n",
            "Iteration 31, loss = 0.60049222\n",
            "Iteration 32, loss = 0.60013302\n",
            "Iteration 33, loss = 0.59970023\n",
            "Iteration 34, loss = 0.59933655\n",
            "Iteration 35, loss = 0.59892240\n",
            "Iteration 36, loss = 0.59855545\n",
            "Iteration 37, loss = 0.59817110\n",
            "Iteration 38, loss = 0.59773733\n",
            "Iteration 39, loss = 0.59731738\n",
            "Iteration 40, loss = 0.59701270\n",
            "Iteration 41, loss = 0.59659154\n",
            "Iteration 42, loss = 0.59622730\n",
            "Iteration 43, loss = 0.59577996\n",
            "Iteration 44, loss = 0.59559981\n",
            "Iteration 45, loss = 0.59513820\n",
            "Iteration 46, loss = 0.59480242\n",
            "Iteration 47, loss = 0.59452497\n",
            "Iteration 48, loss = 0.59421464\n",
            "Iteration 49, loss = 0.59389969\n",
            "Iteration 50, loss = 0.59360376\n",
            "Iteration 51, loss = 0.59338566\n",
            "Iteration 52, loss = 0.59307514\n",
            "Iteration 53, loss = 0.59278326\n",
            "Iteration 54, loss = 0.59252864\n",
            "Iteration 55, loss = 0.59218267\n",
            "Iteration 56, loss = 0.59179838\n",
            "Iteration 57, loss = 0.59144292\n",
            "Iteration 58, loss = 0.59107432\n",
            "Iteration 59, loss = 0.59072772\n",
            "Iteration 60, loss = 0.59033379\n",
            "Iteration 61, loss = 0.58997606\n",
            "Iteration 62, loss = 0.58954634\n",
            "Iteration 63, loss = 0.58924779\n",
            "Iteration 64, loss = 0.58890475\n",
            "Iteration 65, loss = 0.58864831\n",
            "Iteration 66, loss = 0.58836352\n",
            "Iteration 67, loss = 0.58815782\n",
            "Iteration 68, loss = 0.58783305\n",
            "Iteration 69, loss = 0.58756811\n",
            "Iteration 70, loss = 0.58721380\n",
            "Iteration 71, loss = 0.58694958\n",
            "Iteration 72, loss = 0.58664618\n",
            "Iteration 73, loss = 0.58633497\n",
            "Iteration 74, loss = 0.58601464\n",
            "Iteration 75, loss = 0.58576382\n",
            "Iteration 76, loss = 0.58549933\n",
            "Iteration 77, loss = 0.58529067\n",
            "Iteration 78, loss = 0.58500034\n",
            "Iteration 79, loss = 0.58465738\n",
            "Iteration 80, loss = 0.58453297\n",
            "Iteration 81, loss = 0.58413051\n",
            "Iteration 82, loss = 0.58400797\n",
            "Iteration 83, loss = 0.58362854\n",
            "Iteration 84, loss = 0.58342692\n",
            "Iteration 85, loss = 0.58325088\n",
            "Iteration 86, loss = 0.58308466\n",
            "Iteration 87, loss = 0.58297834\n",
            "Iteration 88, loss = 0.58282060\n",
            "Iteration 89, loss = 0.58260746\n",
            "Iteration 90, loss = 0.58236208\n",
            "Iteration 91, loss = 0.58209636\n",
            "Iteration 92, loss = 0.58198327\n",
            "Iteration 93, loss = 0.58176341\n",
            "Iteration 94, loss = 0.58164160\n",
            "Iteration 95, loss = 0.58140872\n",
            "Iteration 96, loss = 0.58129105\n",
            "Iteration 97, loss = 0.58108386\n",
            "Iteration 98, loss = 0.58097283\n",
            "Iteration 99, loss = 0.58070632\n",
            "Iteration 100, loss = 0.58053302\n",
            "Iteration 101, loss = 0.58039990\n",
            "Iteration 102, loss = 0.58017624\n",
            "Iteration 103, loss = 0.57999113\n",
            "Iteration 104, loss = 0.57984540\n",
            "Iteration 105, loss = 0.57970107\n",
            "Iteration 106, loss = 0.57950922\n",
            "Iteration 107, loss = 0.57948545\n",
            "Iteration 108, loss = 0.57919349\n",
            "Iteration 109, loss = 0.57905324\n",
            "Iteration 110, loss = 0.57889101\n",
            "Iteration 111, loss = 0.57872914\n",
            "Iteration 112, loss = 0.57862720\n",
            "Iteration 113, loss = 0.57851013\n",
            "Iteration 114, loss = 0.57835774\n",
            "Iteration 115, loss = 0.57827363\n",
            "Iteration 116, loss = 0.57808403\n",
            "Iteration 117, loss = 0.57797619\n",
            "Iteration 118, loss = 0.57786429\n",
            "Iteration 119, loss = 0.57780876\n",
            "Iteration 120, loss = 0.57769954\n",
            "Iteration 121, loss = 0.57759897\n",
            "Iteration 122, loss = 0.57748308\n",
            "Iteration 123, loss = 0.57731200\n",
            "Iteration 124, loss = 0.57724820\n",
            "Iteration 125, loss = 0.57711188\n",
            "Iteration 126, loss = 0.57700233\n",
            "Iteration 127, loss = 0.57690475\n",
            "Iteration 128, loss = 0.57675941\n",
            "Iteration 129, loss = 0.57675736\n",
            "Iteration 130, loss = 0.57667384\n",
            "Iteration 131, loss = 0.57661817\n",
            "Iteration 132, loss = 0.57641709\n",
            "Iteration 133, loss = 0.57628851\n",
            "Iteration 134, loss = 0.57625054\n",
            "Iteration 135, loss = 0.57607321\n",
            "Iteration 136, loss = 0.57602258\n",
            "Iteration 137, loss = 0.57590272\n",
            "Iteration 138, loss = 0.57586608\n",
            "Iteration 139, loss = 0.57581485\n",
            "Iteration 140, loss = 0.57567184\n",
            "Iteration 141, loss = 0.57558977\n",
            "Iteration 142, loss = 0.57546602\n",
            "Iteration 143, loss = 0.57543567\n",
            "Iteration 144, loss = 0.57535966\n",
            "Iteration 145, loss = 0.57534248\n",
            "Iteration 146, loss = 0.57522330\n",
            "Iteration 147, loss = 0.57510579\n",
            "Iteration 148, loss = 0.57502251\n",
            "Iteration 149, loss = 0.57493491\n",
            "Iteration 150, loss = 0.57488632\n",
            "Iteration 151, loss = 0.57483474\n",
            "Iteration 152, loss = 0.57481661\n",
            "Iteration 153, loss = 0.57477165\n",
            "Iteration 154, loss = 0.57468643\n",
            "Iteration 155, loss = 0.57459866\n",
            "Iteration 156, loss = 0.57443518\n",
            "Iteration 157, loss = 0.57446966\n",
            "Iteration 158, loss = 0.57436031\n",
            "Iteration 159, loss = 0.57432292\n",
            "Iteration 160, loss = 0.57429549\n",
            "Iteration 161, loss = 0.57417923\n",
            "Iteration 162, loss = 0.57408751\n",
            "Iteration 163, loss = 0.57398418\n",
            "Iteration 164, loss = 0.57403109\n",
            "Iteration 165, loss = 0.57398374\n",
            "Iteration 166, loss = 0.57381464\n",
            "Iteration 167, loss = 0.57374633\n",
            "Iteration 168, loss = 0.57370746\n",
            "Iteration 169, loss = 0.57363885\n",
            "Iteration 170, loss = 0.57350241\n",
            "Iteration 171, loss = 0.57338396\n",
            "Iteration 172, loss = 0.57331660\n",
            "Iteration 173, loss = 0.57328637\n",
            "Iteration 174, loss = 0.57322617\n",
            "Iteration 175, loss = 0.57311686\n",
            "Iteration 176, loss = 0.57308206\n",
            "Iteration 177, loss = 0.57297121\n",
            "Iteration 178, loss = 0.57289297\n",
            "Iteration 179, loss = 0.57281547\n",
            "Iteration 180, loss = 0.57273109\n",
            "Iteration 181, loss = 0.57260341\n",
            "Iteration 182, loss = 0.57259523\n",
            "Iteration 183, loss = 0.57252023\n",
            "Iteration 184, loss = 0.57246574\n",
            "Iteration 185, loss = 0.57240744\n",
            "Iteration 186, loss = 0.57231150\n",
            "Iteration 187, loss = 0.57229970\n",
            "Iteration 188, loss = 0.57216612\n",
            "Iteration 189, loss = 0.57206476\n",
            "Iteration 190, loss = 0.57199221\n",
            "Iteration 191, loss = 0.57194441\n",
            "Iteration 192, loss = 0.57183007\n",
            "Iteration 193, loss = 0.57177719\n",
            "Iteration 194, loss = 0.57174417\n",
            "Iteration 195, loss = 0.57168827\n",
            "Iteration 196, loss = 0.57156425\n",
            "Iteration 197, loss = 0.57145151\n",
            "Iteration 198, loss = 0.57137237\n",
            "Iteration 199, loss = 0.57128470\n",
            "Iteration 200, loss = 0.57123302\n",
            "Iteration 201, loss = 0.57107303\n",
            "Iteration 202, loss = 0.57099349\n",
            "Iteration 203, loss = 0.57095467\n",
            "Iteration 204, loss = 0.57084797\n",
            "Iteration 205, loss = 0.57077348\n",
            "Iteration 206, loss = 0.57066281\n",
            "Iteration 207, loss = 0.57053905\n",
            "Iteration 208, loss = 0.57045534\n",
            "Iteration 209, loss = 0.57044437\n",
            "Iteration 210, loss = 0.57025768\n",
            "Iteration 211, loss = 0.57012568\n",
            "Iteration 212, loss = 0.57006220\n",
            "Iteration 213, loss = 0.56994890\n",
            "Iteration 214, loss = 0.56983321\n",
            "Iteration 215, loss = 0.56971635\n",
            "Iteration 216, loss = 0.56959312\n",
            "Iteration 217, loss = 0.56958533\n",
            "Iteration 218, loss = 0.56935339\n",
            "Iteration 219, loss = 0.56923129\n",
            "Iteration 220, loss = 0.56914415\n",
            "Iteration 221, loss = 0.56890451\n",
            "Iteration 222, loss = 0.56875456\n",
            "Iteration 223, loss = 0.56866992\n",
            "Iteration 224, loss = 0.56850064\n",
            "Iteration 225, loss = 0.56836791\n",
            "Iteration 226, loss = 0.56820638\n",
            "Iteration 227, loss = 0.56819363\n",
            "Iteration 228, loss = 0.56806819\n",
            "Iteration 229, loss = 0.56798694\n",
            "Iteration 230, loss = 0.56780567\n",
            "Iteration 231, loss = 0.56759210\n",
            "Iteration 232, loss = 0.56750887\n",
            "Iteration 233, loss = 0.56729058\n",
            "Iteration 234, loss = 0.56714135\n",
            "Iteration 235, loss = 0.56701396\n",
            "Iteration 236, loss = 0.56685312\n",
            "Iteration 237, loss = 0.56671901\n",
            "Iteration 238, loss = 0.56654033\n",
            "Iteration 239, loss = 0.56643211\n",
            "Iteration 240, loss = 0.56624469\n",
            "Iteration 241, loss = 0.56615992\n",
            "Iteration 242, loss = 0.56605347\n",
            "Iteration 243, loss = 0.56584615\n",
            "Iteration 244, loss = 0.56572379\n",
            "Iteration 245, loss = 0.56560976\n",
            "Iteration 246, loss = 0.56544227\n",
            "Iteration 247, loss = 0.56522887\n",
            "Iteration 248, loss = 0.56510806\n",
            "Iteration 249, loss = 0.56500725\n",
            "Iteration 250, loss = 0.56482296\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 250 and for layer number 2 : 0.71125\n",
            "Iteration 1, loss = 0.84424359\n",
            "Iteration 2, loss = 0.81824608\n",
            "Iteration 3, loss = 0.78301818\n",
            "Iteration 4, loss = 0.74369898\n",
            "Iteration 5, loss = 0.70772940\n",
            "Iteration 6, loss = 0.67905372\n",
            "Iteration 7, loss = 0.65459215\n",
            "Iteration 8, loss = 0.63750703\n",
            "Iteration 9, loss = 0.62609078\n",
            "Iteration 10, loss = 0.61732840\n",
            "Iteration 11, loss = 0.61055362\n",
            "Iteration 12, loss = 0.60475642\n",
            "Iteration 13, loss = 0.60059365\n",
            "Iteration 14, loss = 0.59676833\n",
            "Iteration 15, loss = 0.59386373\n",
            "Iteration 16, loss = 0.59105742\n",
            "Iteration 17, loss = 0.58857593\n",
            "Iteration 18, loss = 0.58599616\n",
            "Iteration 19, loss = 0.58413312\n",
            "Iteration 20, loss = 0.58213654\n",
            "Iteration 21, loss = 0.58047540\n",
            "Iteration 22, loss = 0.57899956\n",
            "Iteration 23, loss = 0.57767412\n",
            "Iteration 24, loss = 0.57662470\n",
            "Iteration 25, loss = 0.57549965\n",
            "Iteration 26, loss = 0.57448619\n",
            "Iteration 27, loss = 0.57350458\n",
            "Iteration 28, loss = 0.57272338\n",
            "Iteration 29, loss = 0.57195199\n",
            "Iteration 30, loss = 0.57117292\n",
            "Iteration 31, loss = 0.57052970\n",
            "Iteration 32, loss = 0.56989653\n",
            "Iteration 33, loss = 0.56924336\n",
            "Iteration 34, loss = 0.56855370\n",
            "Iteration 35, loss = 0.56793890\n",
            "Iteration 36, loss = 0.56726158\n",
            "Iteration 37, loss = 0.56681235\n",
            "Iteration 38, loss = 0.56619017\n",
            "Iteration 39, loss = 0.56555914\n",
            "Iteration 40, loss = 0.56496310\n",
            "Iteration 41, loss = 0.56418147\n",
            "Iteration 42, loss = 0.56369303\n",
            "Iteration 43, loss = 0.56314826\n",
            "Iteration 44, loss = 0.56264112\n",
            "Iteration 45, loss = 0.56218156\n",
            "Iteration 46, loss = 0.56170246\n",
            "Iteration 47, loss = 0.56127065\n",
            "Iteration 48, loss = 0.56095972\n",
            "Iteration 49, loss = 0.56051664\n",
            "Iteration 50, loss = 0.56016687\n",
            "Iteration 51, loss = 0.55986462\n",
            "Iteration 52, loss = 0.55963340\n",
            "Iteration 53, loss = 0.55921424\n",
            "Iteration 54, loss = 0.55880487\n",
            "Iteration 55, loss = 0.55855142\n",
            "Iteration 56, loss = 0.55808873\n",
            "Iteration 57, loss = 0.55768277\n",
            "Iteration 58, loss = 0.55738430\n",
            "Iteration 59, loss = 0.55704771\n",
            "Iteration 60, loss = 0.55673806\n",
            "Iteration 61, loss = 0.55628107\n",
            "Iteration 62, loss = 0.55598312\n",
            "Iteration 63, loss = 0.55560140\n",
            "Iteration 64, loss = 0.55527998\n",
            "Iteration 65, loss = 0.55510876\n",
            "Iteration 66, loss = 0.55472229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 67, loss = 0.55441299\n",
            "Iteration 68, loss = 0.55417873\n",
            "Iteration 69, loss = 0.55397098\n",
            "Iteration 70, loss = 0.55377076\n",
            "Iteration 71, loss = 0.55358584\n",
            "Iteration 72, loss = 0.55322471\n",
            "Iteration 73, loss = 0.55307920\n",
            "Iteration 74, loss = 0.55268158\n",
            "Iteration 75, loss = 0.55243838\n",
            "Iteration 76, loss = 0.55227362\n",
            "Iteration 77, loss = 0.55205947\n",
            "Iteration 78, loss = 0.55176691\n",
            "Iteration 79, loss = 0.55148431\n",
            "Iteration 80, loss = 0.55117819\n",
            "Iteration 81, loss = 0.55094568\n",
            "Iteration 82, loss = 0.55068506\n",
            "Iteration 83, loss = 0.55041758\n",
            "Iteration 84, loss = 0.55009782\n",
            "Iteration 85, loss = 0.54980800\n",
            "Iteration 86, loss = 0.54956132\n",
            "Iteration 87, loss = 0.54938696\n",
            "Iteration 88, loss = 0.54914225\n",
            "Iteration 89, loss = 0.54883945\n",
            "Iteration 90, loss = 0.54857565\n",
            "Iteration 91, loss = 0.54837773\n",
            "Iteration 92, loss = 0.54813146\n",
            "Iteration 93, loss = 0.54776457\n",
            "Iteration 94, loss = 0.54734763\n",
            "Iteration 95, loss = 0.54709515\n",
            "Iteration 96, loss = 0.54679073\n",
            "Iteration 97, loss = 0.54658268\n",
            "Iteration 98, loss = 0.54627656\n",
            "Iteration 99, loss = 0.54604382\n",
            "Iteration 100, loss = 0.54579330\n",
            "Iteration 101, loss = 0.54559553\n",
            "Iteration 102, loss = 0.54534505\n",
            "Iteration 103, loss = 0.54529176\n",
            "Iteration 104, loss = 0.54483518\n",
            "Iteration 105, loss = 0.54470627\n",
            "Iteration 106, loss = 0.54448019\n",
            "Iteration 107, loss = 0.54428649\n",
            "Iteration 108, loss = 0.54412449\n",
            "Iteration 109, loss = 0.54402894\n",
            "Iteration 110, loss = 0.54384063\n",
            "Iteration 111, loss = 0.54366888\n",
            "Iteration 112, loss = 0.54333863\n",
            "Iteration 113, loss = 0.54319544\n",
            "Iteration 114, loss = 0.54289790\n",
            "Iteration 115, loss = 0.54277961\n",
            "Iteration 116, loss = 0.54261873\n",
            "Iteration 117, loss = 0.54243487\n",
            "Iteration 118, loss = 0.54229018\n",
            "Iteration 119, loss = 0.54212852\n",
            "Iteration 120, loss = 0.54200008\n",
            "Iteration 121, loss = 0.54194031\n",
            "Iteration 122, loss = 0.54168361\n",
            "Iteration 123, loss = 0.54151179\n",
            "Iteration 124, loss = 0.54123880\n",
            "Iteration 125, loss = 0.54108897\n",
            "Iteration 126, loss = 0.54103856\n",
            "Iteration 127, loss = 0.54088066\n",
            "Iteration 128, loss = 0.54072815\n",
            "Iteration 129, loss = 0.54062706\n",
            "Iteration 130, loss = 0.54044568\n",
            "Iteration 131, loss = 0.54035252\n",
            "Iteration 132, loss = 0.54026052\n",
            "Iteration 133, loss = 0.54013545\n",
            "Iteration 134, loss = 0.54012645\n",
            "Iteration 135, loss = 0.54000457\n",
            "Iteration 136, loss = 0.53986669\n",
            "Iteration 137, loss = 0.53978250\n",
            "Iteration 138, loss = 0.53960434\n",
            "Iteration 139, loss = 0.53949676\n",
            "Iteration 140, loss = 0.53943493\n",
            "Iteration 141, loss = 0.53923818\n",
            "Iteration 142, loss = 0.53912301\n",
            "Iteration 143, loss = 0.53894386\n",
            "Iteration 144, loss = 0.53878587\n",
            "Iteration 145, loss = 0.53861539\n",
            "Iteration 146, loss = 0.53862490\n",
            "Iteration 147, loss = 0.53832874\n",
            "Iteration 148, loss = 0.53820565\n",
            "Iteration 149, loss = 0.53811370\n",
            "Iteration 150, loss = 0.53803219\n",
            "Iteration 151, loss = 0.53794549\n",
            "Iteration 152, loss = 0.53784161\n",
            "Iteration 153, loss = 0.53767880\n",
            "Iteration 154, loss = 0.53757567\n",
            "Iteration 155, loss = 0.53749812\n",
            "Iteration 156, loss = 0.53738471\n",
            "Iteration 157, loss = 0.53737393\n",
            "Iteration 158, loss = 0.53719187\n",
            "Iteration 159, loss = 0.53715987\n",
            "Iteration 160, loss = 0.53705314\n",
            "Iteration 161, loss = 0.53689460\n",
            "Iteration 162, loss = 0.53685175\n",
            "Iteration 163, loss = 0.53661336\n",
            "Iteration 164, loss = 0.53647136\n",
            "Iteration 165, loss = 0.53637743\n",
            "Iteration 166, loss = 0.53615959\n",
            "Iteration 167, loss = 0.53609804\n",
            "Iteration 168, loss = 0.53587348\n",
            "Iteration 169, loss = 0.53579179\n",
            "Iteration 170, loss = 0.53565276\n",
            "Iteration 171, loss = 0.53550374\n",
            "Iteration 172, loss = 0.53537312\n",
            "Iteration 173, loss = 0.53522639\n",
            "Iteration 174, loss = 0.53507235\n",
            "Iteration 175, loss = 0.53501650\n",
            "Iteration 176, loss = 0.53496249\n",
            "Iteration 177, loss = 0.53482123\n",
            "Iteration 178, loss = 0.53462095\n",
            "Iteration 179, loss = 0.53446862\n",
            "Iteration 180, loss = 0.53428219\n",
            "Iteration 181, loss = 0.53417556\n",
            "Iteration 182, loss = 0.53403191\n",
            "Iteration 183, loss = 0.53391473\n",
            "Iteration 184, loss = 0.53384538\n",
            "Iteration 185, loss = 0.53366703\n",
            "Iteration 186, loss = 0.53361035\n",
            "Iteration 187, loss = 0.53352389\n",
            "Iteration 188, loss = 0.53347149\n",
            "Iteration 189, loss = 0.53338269\n",
            "Iteration 190, loss = 0.53327499\n",
            "Iteration 191, loss = 0.53312129\n",
            "Iteration 192, loss = 0.53316938\n",
            "Iteration 193, loss = 0.53293771\n",
            "Iteration 194, loss = 0.53289439\n",
            "Iteration 195, loss = 0.53279676\n",
            "Iteration 196, loss = 0.53272346\n",
            "Iteration 197, loss = 0.53276876\n",
            "Iteration 198, loss = 0.53273791\n",
            "Iteration 199, loss = 0.53269175\n",
            "Iteration 200, loss = 0.53250020\n",
            "Iteration 201, loss = 0.53239149\n",
            "Iteration 202, loss = 0.53237313\n",
            "Iteration 203, loss = 0.53215284\n",
            "Iteration 204, loss = 0.53218682\n",
            "Iteration 205, loss = 0.53206910\n",
            "Iteration 206, loss = 0.53201697\n",
            "Iteration 207, loss = 0.53199085\n",
            "Iteration 208, loss = 0.53193578\n",
            "Iteration 209, loss = 0.53186977\n",
            "Iteration 210, loss = 0.53182646\n",
            "Iteration 211, loss = 0.53182631\n",
            "Iteration 212, loss = 0.53171548\n",
            "Iteration 213, loss = 0.53156399\n",
            "Iteration 214, loss = 0.53144168\n",
            "Iteration 215, loss = 0.53133170\n",
            "Iteration 216, loss = 0.53126012\n",
            "Iteration 217, loss = 0.53122754\n",
            "Iteration 218, loss = 0.53110930\n",
            "Iteration 219, loss = 0.53098060\n",
            "Iteration 220, loss = 0.53088463\n",
            "Iteration 221, loss = 0.53085872\n",
            "Iteration 222, loss = 0.53072104\n",
            "Iteration 223, loss = 0.53075391\n",
            "Iteration 224, loss = 0.53062853\n",
            "Iteration 225, loss = 0.53045769\n",
            "Iteration 226, loss = 0.53036214\n",
            "Iteration 227, loss = 0.53018101\n",
            "Iteration 228, loss = 0.53008658\n",
            "Iteration 229, loss = 0.53004452\n",
            "Iteration 230, loss = 0.52993378\n",
            "Iteration 231, loss = 0.52983556\n",
            "Iteration 232, loss = 0.52982788\n",
            "Iteration 233, loss = 0.52973958\n",
            "Iteration 234, loss = 0.52968347\n",
            "Iteration 235, loss = 0.52960411\n",
            "Iteration 236, loss = 0.52957732\n",
            "Iteration 237, loss = 0.52959907\n",
            "Iteration 238, loss = 0.52969425\n",
            "Iteration 239, loss = 0.52959287\n",
            "Iteration 240, loss = 0.52956324\n",
            "Iteration 241, loss = 0.52948773\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.84514500\n",
            "Iteration 2, loss = 0.81909328\n",
            "Iteration 3, loss = 0.78652477\n",
            "Iteration 4, loss = 0.74811660\n",
            "Iteration 5, loss = 0.71383507\n",
            "Iteration 6, loss = 0.68552273\n",
            "Iteration 7, loss = 0.66168581\n",
            "Iteration 8, loss = 0.64338982\n",
            "Iteration 9, loss = 0.63160645\n",
            "Iteration 10, loss = 0.62136865\n",
            "Iteration 11, loss = 0.61380923\n",
            "Iteration 12, loss = 0.60791210\n",
            "Iteration 13, loss = 0.60355558\n",
            "Iteration 14, loss = 0.59965695\n",
            "Iteration 15, loss = 0.59624464\n",
            "Iteration 16, loss = 0.59328627\n",
            "Iteration 17, loss = 0.59072275\n",
            "Iteration 18, loss = 0.58830866\n",
            "Iteration 19, loss = 0.58624201\n",
            "Iteration 20, loss = 0.58419678\n",
            "Iteration 21, loss = 0.58224472\n",
            "Iteration 22, loss = 0.58048793\n",
            "Iteration 23, loss = 0.57892935\n",
            "Iteration 24, loss = 0.57765823\n",
            "Iteration 25, loss = 0.57634431\n",
            "Iteration 26, loss = 0.57526519\n",
            "Iteration 27, loss = 0.57409528\n",
            "Iteration 28, loss = 0.57316406\n",
            "Iteration 29, loss = 0.57222788\n",
            "Iteration 30, loss = 0.57132622\n",
            "Iteration 31, loss = 0.57066822\n",
            "Iteration 32, loss = 0.56993909\n",
            "Iteration 33, loss = 0.56926119\n",
            "Iteration 34, loss = 0.56861361\n",
            "Iteration 35, loss = 0.56790836\n",
            "Iteration 36, loss = 0.56732057\n",
            "Iteration 37, loss = 0.56682809\n",
            "Iteration 38, loss = 0.56615271\n",
            "Iteration 39, loss = 0.56557409\n",
            "Iteration 40, loss = 0.56501093\n",
            "Iteration 41, loss = 0.56427012\n",
            "Iteration 42, loss = 0.56383162\n",
            "Iteration 43, loss = 0.56324564\n",
            "Iteration 44, loss = 0.56257550\n",
            "Iteration 45, loss = 0.56205907\n",
            "Iteration 46, loss = 0.56161548\n",
            "Iteration 47, loss = 0.56097687\n",
            "Iteration 48, loss = 0.56050221\n",
            "Iteration 49, loss = 0.55997645\n",
            "Iteration 50, loss = 0.55950729\n",
            "Iteration 51, loss = 0.55905100\n",
            "Iteration 52, loss = 0.55864909\n",
            "Iteration 53, loss = 0.55822691\n",
            "Iteration 54, loss = 0.55777537\n",
            "Iteration 55, loss = 0.55742865\n",
            "Iteration 56, loss = 0.55691692\n",
            "Iteration 57, loss = 0.55641760\n",
            "Iteration 58, loss = 0.55598568\n",
            "Iteration 59, loss = 0.55547413\n",
            "Iteration 60, loss = 0.55496972\n",
            "Iteration 61, loss = 0.55443302\n",
            "Iteration 62, loss = 0.55393228\n",
            "Iteration 63, loss = 0.55347585\n",
            "Iteration 64, loss = 0.55302202\n",
            "Iteration 65, loss = 0.55257241\n",
            "Iteration 66, loss = 0.55210691\n",
            "Iteration 67, loss = 0.55157379\n",
            "Iteration 68, loss = 0.55117839\n",
            "Iteration 69, loss = 0.55073343\n",
            "Iteration 70, loss = 0.55030951\n",
            "Iteration 71, loss = 0.54994572\n",
            "Iteration 72, loss = 0.54945093\n",
            "Iteration 73, loss = 0.54910117\n",
            "Iteration 74, loss = 0.54861316\n",
            "Iteration 75, loss = 0.54822427\n",
            "Iteration 76, loss = 0.54796648\n",
            "Iteration 77, loss = 0.54761925\n",
            "Iteration 78, loss = 0.54726992\n",
            "Iteration 79, loss = 0.54702011\n",
            "Iteration 80, loss = 0.54682356\n",
            "Iteration 81, loss = 0.54654362\n",
            "Iteration 82, loss = 0.54640230\n",
            "Iteration 83, loss = 0.54610336\n",
            "Iteration 84, loss = 0.54585375\n",
            "Iteration 85, loss = 0.54554325\n",
            "Iteration 86, loss = 0.54522963\n",
            "Iteration 87, loss = 0.54487368\n",
            "Iteration 88, loss = 0.54453649\n",
            "Iteration 89, loss = 0.54424273\n",
            "Iteration 90, loss = 0.54393287\n",
            "Iteration 91, loss = 0.54370091\n",
            "Iteration 92, loss = 0.54354063\n",
            "Iteration 93, loss = 0.54315551\n",
            "Iteration 94, loss = 0.54284090\n",
            "Iteration 95, loss = 0.54260775\n",
            "Iteration 96, loss = 0.54226539\n",
            "Iteration 97, loss = 0.54218460\n",
            "Iteration 98, loss = 0.54188639\n",
            "Iteration 99, loss = 0.54171213\n",
            "Iteration 100, loss = 0.54137443\n",
            "Iteration 101, loss = 0.54115176\n",
            "Iteration 102, loss = 0.54098741\n",
            "Iteration 103, loss = 0.54078534\n",
            "Iteration 104, loss = 0.54053260\n",
            "Iteration 105, loss = 0.54039428\n",
            "Iteration 106, loss = 0.54021413\n",
            "Iteration 107, loss = 0.54002222\n",
            "Iteration 108, loss = 0.53982397\n",
            "Iteration 109, loss = 0.53977123\n",
            "Iteration 110, loss = 0.53963840\n",
            "Iteration 111, loss = 0.53947559\n",
            "Iteration 112, loss = 0.53924744\n",
            "Iteration 113, loss = 0.53900389\n",
            "Iteration 114, loss = 0.53876616\n",
            "Iteration 115, loss = 0.53869541\n",
            "Iteration 116, loss = 0.53860712\n",
            "Iteration 117, loss = 0.53842544\n",
            "Iteration 118, loss = 0.53824884\n",
            "Iteration 119, loss = 0.53811920\n",
            "Iteration 120, loss = 0.53789437\n",
            "Iteration 121, loss = 0.53774857\n",
            "Iteration 122, loss = 0.53752471\n",
            "Iteration 123, loss = 0.53742103\n",
            "Iteration 124, loss = 0.53716134\n",
            "Iteration 125, loss = 0.53690587\n",
            "Iteration 126, loss = 0.53674444\n",
            "Iteration 127, loss = 0.53646586\n",
            "Iteration 128, loss = 0.53635349\n",
            "Iteration 129, loss = 0.53616121\n",
            "Iteration 130, loss = 0.53596747\n",
            "Iteration 131, loss = 0.53575548\n",
            "Iteration 132, loss = 0.53558699\n",
            "Iteration 133, loss = 0.53546551\n",
            "Iteration 134, loss = 0.53532027\n",
            "Iteration 135, loss = 0.53521455\n",
            "Iteration 136, loss = 0.53504836\n",
            "Iteration 137, loss = 0.53493607\n",
            "Iteration 138, loss = 0.53485314\n",
            "Iteration 139, loss = 0.53471083\n",
            "Iteration 140, loss = 0.53461208\n",
            "Iteration 141, loss = 0.53447452\n",
            "Iteration 142, loss = 0.53424195\n",
            "Iteration 143, loss = 0.53399683\n",
            "Iteration 144, loss = 0.53371405\n",
            "Iteration 145, loss = 0.53362581\n",
            "Iteration 146, loss = 0.53337330\n",
            "Iteration 147, loss = 0.53311798\n",
            "Iteration 148, loss = 0.53298677\n",
            "Iteration 149, loss = 0.53284512\n",
            "Iteration 150, loss = 0.53266775\n",
            "Iteration 151, loss = 0.53245484\n",
            "Iteration 152, loss = 0.53228468\n",
            "Iteration 153, loss = 0.53215305\n",
            "Iteration 154, loss = 0.53199622\n",
            "Iteration 155, loss = 0.53182095\n",
            "Iteration 156, loss = 0.53163534\n",
            "Iteration 157, loss = 0.53159536\n",
            "Iteration 158, loss = 0.53145652\n",
            "Iteration 159, loss = 0.53141267\n",
            "Iteration 160, loss = 0.53127749\n",
            "Iteration 161, loss = 0.53112360\n",
            "Iteration 162, loss = 0.53115639\n",
            "Iteration 163, loss = 0.53093468\n",
            "Iteration 164, loss = 0.53082080\n",
            "Iteration 165, loss = 0.53068017\n",
            "Iteration 166, loss = 0.53048262\n",
            "Iteration 167, loss = 0.53045526\n",
            "Iteration 168, loss = 0.53036890\n",
            "Iteration 169, loss = 0.53021732\n",
            "Iteration 170, loss = 0.53008398\n",
            "Iteration 171, loss = 0.52999110\n",
            "Iteration 172, loss = 0.52978625\n",
            "Iteration 173, loss = 0.52965636\n",
            "Iteration 174, loss = 0.52944824\n",
            "Iteration 175, loss = 0.52936214\n",
            "Iteration 176, loss = 0.52919847\n",
            "Iteration 177, loss = 0.52909706\n",
            "Iteration 178, loss = 0.52888302\n",
            "Iteration 179, loss = 0.52877652\n",
            "Iteration 180, loss = 0.52863237\n",
            "Iteration 181, loss = 0.52850506\n",
            "Iteration 182, loss = 0.52842043\n",
            "Iteration 183, loss = 0.52830067\n",
            "Iteration 184, loss = 0.52824107\n",
            "Iteration 185, loss = 0.52814023\n",
            "Iteration 186, loss = 0.52798196\n",
            "Iteration 187, loss = 0.52799705\n",
            "Iteration 188, loss = 0.52789158\n",
            "Iteration 189, loss = 0.52784611\n",
            "Iteration 190, loss = 0.52782159\n",
            "Iteration 191, loss = 0.52758277\n",
            "Iteration 192, loss = 0.52754709\n",
            "Iteration 193, loss = 0.52731328\n",
            "Iteration 194, loss = 0.52724128\n",
            "Iteration 195, loss = 0.52717716\n",
            "Iteration 196, loss = 0.52702696\n",
            "Iteration 197, loss = 0.52707108\n",
            "Iteration 198, loss = 0.52693569\n",
            "Iteration 199, loss = 0.52678816\n",
            "Iteration 200, loss = 0.52657006\n",
            "Iteration 201, loss = 0.52639727\n",
            "Iteration 202, loss = 0.52627966\n",
            "Iteration 203, loss = 0.52595728\n",
            "Iteration 204, loss = 0.52597549\n",
            "Iteration 205, loss = 0.52580472\n",
            "Iteration 206, loss = 0.52567454\n",
            "Iteration 207, loss = 0.52549653\n",
            "Iteration 208, loss = 0.52532600\n",
            "Iteration 209, loss = 0.52525884\n",
            "Iteration 210, loss = 0.52515735\n",
            "Iteration 211, loss = 0.52505724\n",
            "Iteration 212, loss = 0.52493842\n",
            "Iteration 213, loss = 0.52474856\n",
            "Iteration 214, loss = 0.52472634\n",
            "Iteration 215, loss = 0.52460699\n",
            "Iteration 216, loss = 0.52455151\n",
            "Iteration 217, loss = 0.52444906\n",
            "Iteration 218, loss = 0.52433028\n",
            "Iteration 219, loss = 0.52412497\n",
            "Iteration 220, loss = 0.52398027\n",
            "Iteration 221, loss = 0.52397033\n",
            "Iteration 222, loss = 0.52377206\n",
            "Iteration 223, loss = 0.52364364\n",
            "Iteration 224, loss = 0.52358214\n",
            "Iteration 225, loss = 0.52344023\n",
            "Iteration 226, loss = 0.52327330\n",
            "Iteration 227, loss = 0.52312468\n",
            "Iteration 228, loss = 0.52300296\n",
            "Iteration 229, loss = 0.52293507\n",
            "Iteration 230, loss = 0.52275245\n",
            "Iteration 231, loss = 0.52258272\n",
            "Iteration 232, loss = 0.52248595\n",
            "Iteration 233, loss = 0.52234290\n",
            "Iteration 234, loss = 0.52229126\n",
            "Iteration 235, loss = 0.52213793\n",
            "Iteration 236, loss = 0.52204013\n",
            "Iteration 237, loss = 0.52198491\n",
            "Iteration 238, loss = 0.52189530\n",
            "Iteration 239, loss = 0.52175315\n",
            "Iteration 240, loss = 0.52167122\n",
            "Iteration 241, loss = 0.52141560\n",
            "Iteration 242, loss = 0.52135485\n",
            "Iteration 243, loss = 0.52135042\n",
            "Iteration 244, loss = 0.52114846\n",
            "Iteration 245, loss = 0.52110666\n",
            "Iteration 246, loss = 0.52101428\n",
            "Iteration 247, loss = 0.52099690\n",
            "Iteration 248, loss = 0.52078717\n",
            "Iteration 249, loss = 0.52072199\n",
            "Iteration 250, loss = 0.52068115\n",
            "Iteration 1, loss = 0.86511432\n",
            "Iteration 2, loss = 0.83659966\n",
            "Iteration 3, loss = 0.80055969\n",
            "Iteration 4, loss = 0.75853561\n",
            "Iteration 5, loss = 0.72086132\n",
            "Iteration 6, loss = 0.68969566\n",
            "Iteration 7, loss = 0.66421861\n",
            "Iteration 8, loss = 0.64434004\n",
            "Iteration 9, loss = 0.63160029\n",
            "Iteration 10, loss = 0.62011071\n",
            "Iteration 11, loss = 0.61155999\n",
            "Iteration 12, loss = 0.60497986\n",
            "Iteration 13, loss = 0.59971010\n",
            "Iteration 14, loss = 0.59516710\n",
            "Iteration 15, loss = 0.59089636\n",
            "Iteration 16, loss = 0.58736900\n",
            "Iteration 17, loss = 0.58433935\n",
            "Iteration 18, loss = 0.58137018\n",
            "Iteration 19, loss = 0.57880625\n",
            "Iteration 20, loss = 0.57652473\n",
            "Iteration 21, loss = 0.57419227\n",
            "Iteration 22, loss = 0.57221919\n",
            "Iteration 23, loss = 0.57043440\n",
            "Iteration 24, loss = 0.56876358\n",
            "Iteration 25, loss = 0.56716867\n",
            "Iteration 26, loss = 0.56584442\n",
            "Iteration 27, loss = 0.56433635\n",
            "Iteration 28, loss = 0.56311521\n",
            "Iteration 29, loss = 0.56189909\n",
            "Iteration 30, loss = 0.56081779\n",
            "Iteration 31, loss = 0.55979345\n",
            "Iteration 32, loss = 0.55889623\n",
            "Iteration 33, loss = 0.55803597\n",
            "Iteration 34, loss = 0.55720608\n",
            "Iteration 35, loss = 0.55645261\n",
            "Iteration 36, loss = 0.55572053\n",
            "Iteration 37, loss = 0.55504676\n",
            "Iteration 38, loss = 0.55442651\n",
            "Iteration 39, loss = 0.55373834\n",
            "Iteration 40, loss = 0.55307272\n",
            "Iteration 41, loss = 0.55223474\n",
            "Iteration 42, loss = 0.55169607\n",
            "Iteration 43, loss = 0.55108669\n",
            "Iteration 44, loss = 0.55038771\n",
            "Iteration 45, loss = 0.54992049\n",
            "Iteration 46, loss = 0.54944630\n",
            "Iteration 47, loss = 0.54894736\n",
            "Iteration 48, loss = 0.54847299\n",
            "Iteration 49, loss = 0.54798903\n",
            "Iteration 50, loss = 0.54744079\n",
            "Iteration 51, loss = 0.54699818\n",
            "Iteration 52, loss = 0.54644728\n",
            "Iteration 53, loss = 0.54582215\n",
            "Iteration 54, loss = 0.54523601\n",
            "Iteration 55, loss = 0.54486145\n",
            "Iteration 56, loss = 0.54441470\n",
            "Iteration 57, loss = 0.54396780\n",
            "Iteration 58, loss = 0.54354432\n",
            "Iteration 59, loss = 0.54317617\n",
            "Iteration 60, loss = 0.54271414\n",
            "Iteration 61, loss = 0.54228241\n",
            "Iteration 62, loss = 0.54187862\n",
            "Iteration 63, loss = 0.54145488\n",
            "Iteration 64, loss = 0.54105818\n",
            "Iteration 65, loss = 0.54070902\n",
            "Iteration 66, loss = 0.54033059\n",
            "Iteration 67, loss = 0.53986242\n",
            "Iteration 68, loss = 0.53947009\n",
            "Iteration 69, loss = 0.53907710\n",
            "Iteration 70, loss = 0.53872273\n",
            "Iteration 71, loss = 0.53852677\n",
            "Iteration 72, loss = 0.53825027\n",
            "Iteration 73, loss = 0.53802444\n",
            "Iteration 74, loss = 0.53759156\n",
            "Iteration 75, loss = 0.53719528\n",
            "Iteration 76, loss = 0.53695689\n",
            "Iteration 77, loss = 0.53658347\n",
            "Iteration 78, loss = 0.53635432\n",
            "Iteration 79, loss = 0.53605595\n",
            "Iteration 80, loss = 0.53582474\n",
            "Iteration 81, loss = 0.53554773\n",
            "Iteration 82, loss = 0.53535155\n",
            "Iteration 83, loss = 0.53515822\n",
            "Iteration 84, loss = 0.53487830\n",
            "Iteration 85, loss = 0.53471838\n",
            "Iteration 86, loss = 0.53452936\n",
            "Iteration 87, loss = 0.53428679\n",
            "Iteration 88, loss = 0.53411485\n",
            "Iteration 89, loss = 0.53393016\n",
            "Iteration 90, loss = 0.53375943\n",
            "Iteration 91, loss = 0.53365426\n",
            "Iteration 92, loss = 0.53351596\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 93, loss = 0.53327774\n",
            "Iteration 94, loss = 0.53314514\n",
            "Iteration 95, loss = 0.53281641\n",
            "Iteration 96, loss = 0.53262599\n",
            "Iteration 97, loss = 0.53239384\n",
            "Iteration 98, loss = 0.53232582\n",
            "Iteration 99, loss = 0.53211350\n",
            "Iteration 100, loss = 0.53190029\n",
            "Iteration 101, loss = 0.53173212\n",
            "Iteration 102, loss = 0.53161053\n",
            "Iteration 103, loss = 0.53145592\n",
            "Iteration 104, loss = 0.53128751\n",
            "Iteration 105, loss = 0.53120559\n",
            "Iteration 106, loss = 0.53110726\n",
            "Iteration 107, loss = 0.53099419\n",
            "Iteration 108, loss = 0.53085608\n",
            "Iteration 109, loss = 0.53079898\n",
            "Iteration 110, loss = 0.53075564\n",
            "Iteration 111, loss = 0.53057516\n",
            "Iteration 112, loss = 0.53049676\n",
            "Iteration 113, loss = 0.53038255\n",
            "Iteration 114, loss = 0.53021973\n",
            "Iteration 115, loss = 0.53023859\n",
            "Iteration 116, loss = 0.53017259\n",
            "Iteration 117, loss = 0.53007937\n",
            "Iteration 118, loss = 0.52994985\n",
            "Iteration 119, loss = 0.52982128\n",
            "Iteration 120, loss = 0.52974964\n",
            "Iteration 121, loss = 0.52960121\n",
            "Iteration 122, loss = 0.52940096\n",
            "Iteration 123, loss = 0.52934112\n",
            "Iteration 124, loss = 0.52931373\n",
            "Iteration 125, loss = 0.52915815\n",
            "Iteration 126, loss = 0.52909309\n",
            "Iteration 127, loss = 0.52895152\n",
            "Iteration 128, loss = 0.52882711\n",
            "Iteration 129, loss = 0.52876681\n",
            "Iteration 130, loss = 0.52866889\n",
            "Iteration 131, loss = 0.52865310\n",
            "Iteration 132, loss = 0.52858254\n",
            "Iteration 133, loss = 0.52862777\n",
            "Iteration 134, loss = 0.52852957\n",
            "Iteration 135, loss = 0.52859374\n",
            "Iteration 136, loss = 0.52849947\n",
            "Iteration 137, loss = 0.52844982\n",
            "Iteration 138, loss = 0.52852134\n",
            "Iteration 139, loss = 0.52836411\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.84459665\n",
            "Iteration 2, loss = 0.81870228\n",
            "Iteration 3, loss = 0.78595389\n",
            "Iteration 4, loss = 0.74703750\n",
            "Iteration 5, loss = 0.71258339\n",
            "Iteration 6, loss = 0.68531183\n",
            "Iteration 7, loss = 0.66229352\n",
            "Iteration 8, loss = 0.64435029\n",
            "Iteration 9, loss = 0.63245378\n",
            "Iteration 10, loss = 0.62203620\n",
            "Iteration 11, loss = 0.61422600\n",
            "Iteration 12, loss = 0.60819304\n",
            "Iteration 13, loss = 0.60394945\n",
            "Iteration 14, loss = 0.60001920\n",
            "Iteration 15, loss = 0.59682229\n",
            "Iteration 16, loss = 0.59409794\n",
            "Iteration 17, loss = 0.59187271\n",
            "Iteration 18, loss = 0.58971012\n",
            "Iteration 19, loss = 0.58782321\n",
            "Iteration 20, loss = 0.58622969\n",
            "Iteration 21, loss = 0.58456990\n",
            "Iteration 22, loss = 0.58308345\n",
            "Iteration 23, loss = 0.58175424\n",
            "Iteration 24, loss = 0.58053072\n",
            "Iteration 25, loss = 0.57943520\n",
            "Iteration 26, loss = 0.57847461\n",
            "Iteration 27, loss = 0.57744098\n",
            "Iteration 28, loss = 0.57649424\n",
            "Iteration 29, loss = 0.57561182\n",
            "Iteration 30, loss = 0.57472458\n",
            "Iteration 31, loss = 0.57386337\n",
            "Iteration 32, loss = 0.57320966\n",
            "Iteration 33, loss = 0.57256386\n",
            "Iteration 34, loss = 0.57190437\n",
            "Iteration 35, loss = 0.57129547\n",
            "Iteration 36, loss = 0.57075666\n",
            "Iteration 37, loss = 0.57013798\n",
            "Iteration 38, loss = 0.56964642\n",
            "Iteration 39, loss = 0.56913319\n",
            "Iteration 40, loss = 0.56867031\n",
            "Iteration 41, loss = 0.56805439\n",
            "Iteration 42, loss = 0.56757060\n",
            "Iteration 43, loss = 0.56705874\n",
            "Iteration 44, loss = 0.56652038\n",
            "Iteration 45, loss = 0.56613455\n",
            "Iteration 46, loss = 0.56564884\n",
            "Iteration 47, loss = 0.56532256\n",
            "Iteration 48, loss = 0.56486247\n",
            "Iteration 49, loss = 0.56442581\n",
            "Iteration 50, loss = 0.56400200\n",
            "Iteration 51, loss = 0.56363125\n",
            "Iteration 52, loss = 0.56319439\n",
            "Iteration 53, loss = 0.56270183\n",
            "Iteration 54, loss = 0.56225417\n",
            "Iteration 55, loss = 0.56195227\n",
            "Iteration 56, loss = 0.56154741\n",
            "Iteration 57, loss = 0.56111386\n",
            "Iteration 58, loss = 0.56077495\n",
            "Iteration 59, loss = 0.56040345\n",
            "Iteration 60, loss = 0.56005605\n",
            "Iteration 61, loss = 0.55961365\n",
            "Iteration 62, loss = 0.55924774\n",
            "Iteration 63, loss = 0.55888500\n",
            "Iteration 64, loss = 0.55855013\n",
            "Iteration 65, loss = 0.55820503\n",
            "Iteration 66, loss = 0.55783153\n",
            "Iteration 67, loss = 0.55740062\n",
            "Iteration 68, loss = 0.55713550\n",
            "Iteration 69, loss = 0.55676995\n",
            "Iteration 70, loss = 0.55640435\n",
            "Iteration 71, loss = 0.55623051\n",
            "Iteration 72, loss = 0.55586898\n",
            "Iteration 73, loss = 0.55551588\n",
            "Iteration 74, loss = 0.55517806\n",
            "Iteration 75, loss = 0.55474794\n",
            "Iteration 76, loss = 0.55447635\n",
            "Iteration 77, loss = 0.55405120\n",
            "Iteration 78, loss = 0.55367833\n",
            "Iteration 79, loss = 0.55337910\n",
            "Iteration 80, loss = 0.55296347\n",
            "Iteration 81, loss = 0.55258527\n",
            "Iteration 82, loss = 0.55233845\n",
            "Iteration 83, loss = 0.55204456\n",
            "Iteration 84, loss = 0.55171987\n",
            "Iteration 85, loss = 0.55152066\n",
            "Iteration 86, loss = 0.55125547\n",
            "Iteration 87, loss = 0.55102926\n",
            "Iteration 88, loss = 0.55078327\n",
            "Iteration 89, loss = 0.55049191\n",
            "Iteration 90, loss = 0.55022934\n",
            "Iteration 91, loss = 0.55002248\n",
            "Iteration 92, loss = 0.54970664\n",
            "Iteration 93, loss = 0.54939765\n",
            "Iteration 94, loss = 0.54915329\n",
            "Iteration 95, loss = 0.54878256\n",
            "Iteration 96, loss = 0.54848819\n",
            "Iteration 97, loss = 0.54816994\n",
            "Iteration 98, loss = 0.54797776\n",
            "Iteration 99, loss = 0.54764892\n",
            "Iteration 100, loss = 0.54741962\n",
            "Iteration 101, loss = 0.54716725\n",
            "Iteration 102, loss = 0.54694231\n",
            "Iteration 103, loss = 0.54675451\n",
            "Iteration 104, loss = 0.54660119\n",
            "Iteration 105, loss = 0.54638037\n",
            "Iteration 106, loss = 0.54624164\n",
            "Iteration 107, loss = 0.54596770\n",
            "Iteration 108, loss = 0.54578311\n",
            "Iteration 109, loss = 0.54553795\n",
            "Iteration 110, loss = 0.54538834\n",
            "Iteration 111, loss = 0.54519370\n",
            "Iteration 112, loss = 0.54493799\n",
            "Iteration 113, loss = 0.54472679\n",
            "Iteration 114, loss = 0.54447468\n",
            "Iteration 115, loss = 0.54438885\n",
            "Iteration 116, loss = 0.54411089\n",
            "Iteration 117, loss = 0.54395248\n",
            "Iteration 118, loss = 0.54370300\n",
            "Iteration 119, loss = 0.54354589\n",
            "Iteration 120, loss = 0.54332472\n",
            "Iteration 121, loss = 0.54318818\n",
            "Iteration 122, loss = 0.54290722\n",
            "Iteration 123, loss = 0.54273256\n",
            "Iteration 124, loss = 0.54249093\n",
            "Iteration 125, loss = 0.54230916\n",
            "Iteration 126, loss = 0.54212763\n",
            "Iteration 127, loss = 0.54181847\n",
            "Iteration 128, loss = 0.54165533\n",
            "Iteration 129, loss = 0.54141417\n",
            "Iteration 130, loss = 0.54112551\n",
            "Iteration 131, loss = 0.54102938\n",
            "Iteration 132, loss = 0.54077312\n",
            "Iteration 133, loss = 0.54061478\n",
            "Iteration 134, loss = 0.54037494\n",
            "Iteration 135, loss = 0.54008000\n",
            "Iteration 136, loss = 0.53980382\n",
            "Iteration 137, loss = 0.53957254\n",
            "Iteration 138, loss = 0.53941135\n",
            "Iteration 139, loss = 0.53915533\n",
            "Iteration 140, loss = 0.53879635\n",
            "Iteration 141, loss = 0.53861513\n",
            "Iteration 142, loss = 0.53826862\n",
            "Iteration 143, loss = 0.53797140\n",
            "Iteration 144, loss = 0.53767791\n",
            "Iteration 145, loss = 0.53758785\n",
            "Iteration 146, loss = 0.53735928\n",
            "Iteration 147, loss = 0.53707884\n",
            "Iteration 148, loss = 0.53685146\n",
            "Iteration 149, loss = 0.53667185\n",
            "Iteration 150, loss = 0.53646668\n",
            "Iteration 151, loss = 0.53629684\n",
            "Iteration 152, loss = 0.53600397\n",
            "Iteration 153, loss = 0.53586613\n",
            "Iteration 154, loss = 0.53557390\n",
            "Iteration 155, loss = 0.53541893\n",
            "Iteration 156, loss = 0.53522610\n",
            "Iteration 157, loss = 0.53501203\n",
            "Iteration 158, loss = 0.53479968\n",
            "Iteration 159, loss = 0.53467419\n",
            "Iteration 160, loss = 0.53446330\n",
            "Iteration 161, loss = 0.53430320\n",
            "Iteration 162, loss = 0.53421672\n",
            "Iteration 163, loss = 0.53408747\n",
            "Iteration 164, loss = 0.53395943\n",
            "Iteration 165, loss = 0.53381110\n",
            "Iteration 166, loss = 0.53367461\n",
            "Iteration 167, loss = 0.53350300\n",
            "Iteration 168, loss = 0.53333019\n",
            "Iteration 169, loss = 0.53329169\n",
            "Iteration 170, loss = 0.53306603\n",
            "Iteration 171, loss = 0.53293138\n",
            "Iteration 172, loss = 0.53273355\n",
            "Iteration 173, loss = 0.53262714\n",
            "Iteration 174, loss = 0.53249091\n",
            "Iteration 175, loss = 0.53225176\n",
            "Iteration 176, loss = 0.53202613\n",
            "Iteration 177, loss = 0.53182476\n",
            "Iteration 178, loss = 0.53166752\n",
            "Iteration 179, loss = 0.53141635\n",
            "Iteration 180, loss = 0.53127269\n",
            "Iteration 181, loss = 0.53098876\n",
            "Iteration 182, loss = 0.53085588\n",
            "Iteration 183, loss = 0.53072559\n",
            "Iteration 184, loss = 0.53057094\n",
            "Iteration 185, loss = 0.53053204\n",
            "Iteration 186, loss = 0.53037552\n",
            "Iteration 187, loss = 0.53028680\n",
            "Iteration 188, loss = 0.53008481\n",
            "Iteration 189, loss = 0.52999455\n",
            "Iteration 190, loss = 0.52980150\n",
            "Iteration 191, loss = 0.52961696\n",
            "Iteration 192, loss = 0.52946260\n",
            "Iteration 193, loss = 0.52926877\n",
            "Iteration 194, loss = 0.52909502\n",
            "Iteration 195, loss = 0.52890922\n",
            "Iteration 196, loss = 0.52879534\n",
            "Iteration 197, loss = 0.52872179\n",
            "Iteration 198, loss = 0.52852934\n",
            "Iteration 199, loss = 0.52837975\n",
            "Iteration 200, loss = 0.52821005\n",
            "Iteration 201, loss = 0.52807046\n",
            "Iteration 202, loss = 0.52789121\n",
            "Iteration 203, loss = 0.52768774\n",
            "Iteration 204, loss = 0.52756850\n",
            "Iteration 205, loss = 0.52735540\n",
            "Iteration 206, loss = 0.52726819\n",
            "Iteration 207, loss = 0.52703488\n",
            "Iteration 208, loss = 0.52700996\n",
            "Iteration 209, loss = 0.52691912\n",
            "Iteration 210, loss = 0.52679484\n",
            "Iteration 211, loss = 0.52656275\n",
            "Iteration 212, loss = 0.52667992\n",
            "Iteration 213, loss = 0.52638944\n",
            "Iteration 214, loss = 0.52631161\n",
            "Iteration 215, loss = 0.52602811\n",
            "Iteration 216, loss = 0.52584207\n",
            "Iteration 217, loss = 0.52565258\n",
            "Iteration 218, loss = 0.52557808\n",
            "Iteration 219, loss = 0.52525808\n",
            "Iteration 220, loss = 0.52508991\n",
            "Iteration 221, loss = 0.52493527\n",
            "Iteration 222, loss = 0.52466970\n",
            "Iteration 223, loss = 0.52459799\n",
            "Iteration 224, loss = 0.52436885\n",
            "Iteration 225, loss = 0.52417070\n",
            "Iteration 226, loss = 0.52414174\n",
            "Iteration 227, loss = 0.52388907\n",
            "Iteration 228, loss = 0.52373563\n",
            "Iteration 229, loss = 0.52365206\n",
            "Iteration 230, loss = 0.52347194\n",
            "Iteration 231, loss = 0.52330321\n",
            "Iteration 232, loss = 0.52301855\n",
            "Iteration 233, loss = 0.52268971\n",
            "Iteration 234, loss = 0.52250962\n",
            "Iteration 235, loss = 0.52224486\n",
            "Iteration 236, loss = 0.52206366\n",
            "Iteration 237, loss = 0.52200485\n",
            "Iteration 238, loss = 0.52171578\n",
            "Iteration 239, loss = 0.52150606\n",
            "Iteration 240, loss = 0.52134334\n",
            "Iteration 241, loss = 0.52120370\n",
            "Iteration 242, loss = 0.52109857\n",
            "Iteration 243, loss = 0.52103467\n",
            "Iteration 244, loss = 0.52072611\n",
            "Iteration 245, loss = 0.52063208\n",
            "Iteration 246, loss = 0.52059175\n",
            "Iteration 247, loss = 0.52056481\n",
            "Iteration 248, loss = 0.52034831\n",
            "Iteration 249, loss = 0.52016103\n",
            "Iteration 250, loss = 0.52016031\n",
            "Iteration 1, loss = 0.84006331\n",
            "Iteration 2, loss = 0.81376413\n",
            "Iteration 3, loss = 0.77923652\n",
            "Iteration 4, loss = 0.74192691\n",
            "Iteration 5, loss = 0.71081488\n",
            "Iteration 6, loss = 0.68542628\n",
            "Iteration 7, loss = 0.66332206\n",
            "Iteration 8, loss = 0.64635824\n",
            "Iteration 9, loss = 0.63430855\n",
            "Iteration 10, loss = 0.62333389\n",
            "Iteration 11, loss = 0.61592732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 12, loss = 0.61059120\n",
            "Iteration 13, loss = 0.60572314\n",
            "Iteration 14, loss = 0.60162257\n",
            "Iteration 15, loss = 0.59818823\n",
            "Iteration 16, loss = 0.59510392\n",
            "Iteration 17, loss = 0.59243361\n",
            "Iteration 18, loss = 0.58999220\n",
            "Iteration 19, loss = 0.58775910\n",
            "Iteration 20, loss = 0.58570466\n",
            "Iteration 21, loss = 0.58382153\n",
            "Iteration 22, loss = 0.58221027\n",
            "Iteration 23, loss = 0.58074782\n",
            "Iteration 24, loss = 0.57942583\n",
            "Iteration 25, loss = 0.57825592\n",
            "Iteration 26, loss = 0.57718551\n",
            "Iteration 27, loss = 0.57609237\n",
            "Iteration 28, loss = 0.57502039\n",
            "Iteration 29, loss = 0.57416546\n",
            "Iteration 30, loss = 0.57336706\n",
            "Iteration 31, loss = 0.57254110\n",
            "Iteration 32, loss = 0.57178136\n",
            "Iteration 33, loss = 0.57121735\n",
            "Iteration 34, loss = 0.57065672\n",
            "Iteration 35, loss = 0.57011026\n",
            "Iteration 36, loss = 0.56962549\n",
            "Iteration 37, loss = 0.56905940\n",
            "Iteration 38, loss = 0.56870017\n",
            "Iteration 39, loss = 0.56841004\n",
            "Iteration 40, loss = 0.56813621\n",
            "Iteration 41, loss = 0.56767469\n",
            "Iteration 42, loss = 0.56735452\n",
            "Iteration 43, loss = 0.56692246\n",
            "Iteration 44, loss = 0.56655300\n",
            "Iteration 45, loss = 0.56632262\n",
            "Iteration 46, loss = 0.56592314\n",
            "Iteration 47, loss = 0.56566446\n",
            "Iteration 48, loss = 0.56536787\n",
            "Iteration 49, loss = 0.56507362\n",
            "Iteration 50, loss = 0.56477701\n",
            "Iteration 51, loss = 0.56458465\n",
            "Iteration 52, loss = 0.56430101\n",
            "Iteration 53, loss = 0.56397691\n",
            "Iteration 54, loss = 0.56383676\n",
            "Iteration 55, loss = 0.56355069\n",
            "Iteration 56, loss = 0.56333377\n",
            "Iteration 57, loss = 0.56300657\n",
            "Iteration 58, loss = 0.56280934\n",
            "Iteration 59, loss = 0.56259266\n",
            "Iteration 60, loss = 0.56235040\n",
            "Iteration 61, loss = 0.56206429\n",
            "Iteration 62, loss = 0.56184369\n",
            "Iteration 63, loss = 0.56162229\n",
            "Iteration 64, loss = 0.56138660\n",
            "Iteration 65, loss = 0.56115882\n",
            "Iteration 66, loss = 0.56092963\n",
            "Iteration 67, loss = 0.56062588\n",
            "Iteration 68, loss = 0.56041223\n",
            "Iteration 69, loss = 0.56023079\n",
            "Iteration 70, loss = 0.55999148\n",
            "Iteration 71, loss = 0.55977984\n",
            "Iteration 72, loss = 0.55951684\n",
            "Iteration 73, loss = 0.55937821\n",
            "Iteration 74, loss = 0.55921986\n",
            "Iteration 75, loss = 0.55887053\n",
            "Iteration 76, loss = 0.55866763\n",
            "Iteration 77, loss = 0.55835573\n",
            "Iteration 78, loss = 0.55806711\n",
            "Iteration 79, loss = 0.55774697\n",
            "Iteration 80, loss = 0.55738107\n",
            "Iteration 81, loss = 0.55707906\n",
            "Iteration 82, loss = 0.55689875\n",
            "Iteration 83, loss = 0.55667676\n",
            "Iteration 84, loss = 0.55639916\n",
            "Iteration 85, loss = 0.55622890\n",
            "Iteration 86, loss = 0.55600852\n",
            "Iteration 87, loss = 0.55583013\n",
            "Iteration 88, loss = 0.55565230\n",
            "Iteration 89, loss = 0.55543481\n",
            "Iteration 90, loss = 0.55528561\n",
            "Iteration 91, loss = 0.55506069\n",
            "Iteration 92, loss = 0.55477530\n",
            "Iteration 93, loss = 0.55451576\n",
            "Iteration 94, loss = 0.55430681\n",
            "Iteration 95, loss = 0.55400776\n",
            "Iteration 96, loss = 0.55369259\n",
            "Iteration 97, loss = 0.55338130\n",
            "Iteration 98, loss = 0.55315037\n",
            "Iteration 99, loss = 0.55291589\n",
            "Iteration 100, loss = 0.55264640\n",
            "Iteration 101, loss = 0.55247134\n",
            "Iteration 102, loss = 0.55233386\n",
            "Iteration 103, loss = 0.55216016\n",
            "Iteration 104, loss = 0.55193526\n",
            "Iteration 105, loss = 0.55171716\n",
            "Iteration 106, loss = 0.55161165\n",
            "Iteration 107, loss = 0.55133595\n",
            "Iteration 108, loss = 0.55128412\n",
            "Iteration 109, loss = 0.55106182\n",
            "Iteration 110, loss = 0.55094599\n",
            "Iteration 111, loss = 0.55088210\n",
            "Iteration 112, loss = 0.55070015\n",
            "Iteration 113, loss = 0.55049384\n",
            "Iteration 114, loss = 0.55041983\n",
            "Iteration 115, loss = 0.55029610\n",
            "Iteration 116, loss = 0.55008228\n",
            "Iteration 117, loss = 0.55000776\n",
            "Iteration 118, loss = 0.54975533\n",
            "Iteration 119, loss = 0.54956440\n",
            "Iteration 120, loss = 0.54945570\n",
            "Iteration 121, loss = 0.54927225\n",
            "Iteration 122, loss = 0.54911288\n",
            "Iteration 123, loss = 0.54895665\n",
            "Iteration 124, loss = 0.54875025\n",
            "Iteration 125, loss = 0.54857615\n",
            "Iteration 126, loss = 0.54849266\n",
            "Iteration 127, loss = 0.54822139\n",
            "Iteration 128, loss = 0.54812265\n",
            "Iteration 129, loss = 0.54801617\n",
            "Iteration 130, loss = 0.54771625\n",
            "Iteration 131, loss = 0.54773805\n",
            "Iteration 132, loss = 0.54743297\n",
            "Iteration 133, loss = 0.54737811\n",
            "Iteration 134, loss = 0.54717746\n",
            "Iteration 135, loss = 0.54696117\n",
            "Iteration 136, loss = 0.54676712\n",
            "Iteration 137, loss = 0.54661184\n",
            "Iteration 138, loss = 0.54652299\n",
            "Iteration 139, loss = 0.54633499\n",
            "Iteration 140, loss = 0.54617776\n",
            "Iteration 141, loss = 0.54619609\n",
            "Iteration 142, loss = 0.54596633\n",
            "Iteration 143, loss = 0.54583598\n",
            "Iteration 144, loss = 0.54563675\n",
            "Iteration 145, loss = 0.54564187\n",
            "Iteration 146, loss = 0.54546484\n",
            "Iteration 147, loss = 0.54532720\n",
            "Iteration 148, loss = 0.54518740\n",
            "Iteration 149, loss = 0.54503367\n",
            "Iteration 150, loss = 0.54491398\n",
            "Iteration 151, loss = 0.54481154\n",
            "Iteration 152, loss = 0.54461989\n",
            "Iteration 153, loss = 0.54460808\n",
            "Iteration 154, loss = 0.54444292\n",
            "Iteration 155, loss = 0.54435592\n",
            "Iteration 156, loss = 0.54425112\n",
            "Iteration 157, loss = 0.54412352\n",
            "Iteration 158, loss = 0.54402035\n",
            "Iteration 159, loss = 0.54391572\n",
            "Iteration 160, loss = 0.54390491\n",
            "Iteration 161, loss = 0.54371448\n",
            "Iteration 162, loss = 0.54362302\n",
            "Iteration 163, loss = 0.54364602\n",
            "Iteration 164, loss = 0.54343237\n",
            "Iteration 165, loss = 0.54335945\n",
            "Iteration 166, loss = 0.54338430\n",
            "Iteration 167, loss = 0.54326254\n",
            "Iteration 168, loss = 0.54317258\n",
            "Iteration 169, loss = 0.54322700\n",
            "Iteration 170, loss = 0.54310808\n",
            "Iteration 171, loss = 0.54298395\n",
            "Iteration 172, loss = 0.54295708\n",
            "Iteration 173, loss = 0.54285906\n",
            "Iteration 174, loss = 0.54271957\n",
            "Iteration 175, loss = 0.54253791\n",
            "Iteration 176, loss = 0.54239259\n",
            "Iteration 177, loss = 0.54227054\n",
            "Iteration 178, loss = 0.54215037\n",
            "Iteration 179, loss = 0.54202403\n",
            "Iteration 180, loss = 0.54194543\n",
            "Iteration 181, loss = 0.54185852\n",
            "Iteration 182, loss = 0.54181154\n",
            "Iteration 183, loss = 0.54171741\n",
            "Iteration 184, loss = 0.54164549\n",
            "Iteration 185, loss = 0.54158963\n",
            "Iteration 186, loss = 0.54156451\n",
            "Iteration 187, loss = 0.54150780\n",
            "Iteration 188, loss = 0.54138977\n",
            "Iteration 189, loss = 0.54141423\n",
            "Iteration 190, loss = 0.54125364\n",
            "Iteration 191, loss = 0.54111794\n",
            "Iteration 192, loss = 0.54097003\n",
            "Iteration 193, loss = 0.54089889\n",
            "Iteration 194, loss = 0.54076714\n",
            "Iteration 195, loss = 0.54056800\n",
            "Iteration 196, loss = 0.54056338\n",
            "Iteration 197, loss = 0.54038133\n",
            "Iteration 198, loss = 0.54019930\n",
            "Iteration 199, loss = 0.54007844\n",
            "Iteration 200, loss = 0.53989106\n",
            "Iteration 201, loss = 0.53977066\n",
            "Iteration 202, loss = 0.53962552\n",
            "Iteration 203, loss = 0.53954904\n",
            "Iteration 204, loss = 0.53950365\n",
            "Iteration 205, loss = 0.53940238\n",
            "Iteration 206, loss = 0.53933641\n",
            "Iteration 207, loss = 0.53910746\n",
            "Iteration 208, loss = 0.53894074\n",
            "Iteration 209, loss = 0.53883292\n",
            "Iteration 210, loss = 0.53873241\n",
            "Iteration 211, loss = 0.53865568\n",
            "Iteration 212, loss = 0.53869519\n",
            "Iteration 213, loss = 0.53844708\n",
            "Iteration 214, loss = 0.53833589\n",
            "Iteration 215, loss = 0.53819630\n",
            "Iteration 216, loss = 0.53807712\n",
            "Iteration 217, loss = 0.53794253\n",
            "Iteration 218, loss = 0.53814145\n",
            "Iteration 219, loss = 0.53787100\n",
            "Iteration 220, loss = 0.53772157\n",
            "Iteration 221, loss = 0.53758925\n",
            "Iteration 222, loss = 0.53747169\n",
            "Iteration 223, loss = 0.53729900\n",
            "Iteration 224, loss = 0.53720794\n",
            "Iteration 225, loss = 0.53716762\n",
            "Iteration 226, loss = 0.53706419\n",
            "Iteration 227, loss = 0.53701145\n",
            "Iteration 228, loss = 0.53695801\n",
            "Iteration 229, loss = 0.53699818\n",
            "Iteration 230, loss = 0.53686485\n",
            "Iteration 231, loss = 0.53674118\n",
            "Iteration 232, loss = 0.53658347\n",
            "Iteration 233, loss = 0.53635603\n",
            "Iteration 234, loss = 0.53624671\n",
            "Iteration 235, loss = 0.53615553\n",
            "Iteration 236, loss = 0.53599179\n",
            "Iteration 237, loss = 0.53602798\n",
            "Iteration 238, loss = 0.53581341\n",
            "Iteration 239, loss = 0.53567696\n",
            "Iteration 240, loss = 0.53563899\n",
            "Iteration 241, loss = 0.53542208\n",
            "Iteration 242, loss = 0.53531084\n",
            "Iteration 243, loss = 0.53528500\n",
            "Iteration 244, loss = 0.53501112\n",
            "Iteration 245, loss = 0.53498460\n",
            "Iteration 246, loss = 0.53487070\n",
            "Iteration 247, loss = 0.53478482\n",
            "Iteration 248, loss = 0.53461542\n",
            "Iteration 249, loss = 0.53456172\n",
            "Iteration 250, loss = 0.53458981\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 250 and for layer number 3 : 0.7125\n",
            "Iteration 1, loss = 0.92540177\n",
            "Iteration 2, loss = 0.90606988\n",
            "Iteration 3, loss = 0.87636521\n",
            "Iteration 4, loss = 0.84453499\n",
            "Iteration 5, loss = 0.81069894\n",
            "Iteration 6, loss = 0.78049825\n",
            "Iteration 7, loss = 0.75458834\n",
            "Iteration 8, loss = 0.73221444\n",
            "Iteration 9, loss = 0.71299243\n",
            "Iteration 10, loss = 0.69629293\n",
            "Iteration 11, loss = 0.68190019\n",
            "Iteration 12, loss = 0.66823391\n",
            "Iteration 13, loss = 0.65677101\n",
            "Iteration 14, loss = 0.64656579\n",
            "Iteration 15, loss = 0.63780899\n",
            "Iteration 16, loss = 0.63058347\n",
            "Iteration 17, loss = 0.62420683\n",
            "Iteration 18, loss = 0.61911339\n",
            "Iteration 19, loss = 0.61402184\n",
            "Iteration 20, loss = 0.60966985\n",
            "Iteration 21, loss = 0.60588786\n",
            "Iteration 22, loss = 0.60253512\n",
            "Iteration 23, loss = 0.59952335\n",
            "Iteration 24, loss = 0.59636324\n",
            "Iteration 25, loss = 0.59415604\n",
            "Iteration 26, loss = 0.59185568\n",
            "Iteration 27, loss = 0.58962051\n",
            "Iteration 28, loss = 0.58753568\n",
            "Iteration 29, loss = 0.58565227\n",
            "Iteration 30, loss = 0.58359418\n",
            "Iteration 31, loss = 0.58153387\n",
            "Iteration 32, loss = 0.57977999\n",
            "Iteration 33, loss = 0.57793385\n",
            "Iteration 34, loss = 0.57632165\n",
            "Iteration 35, loss = 0.57508868\n",
            "Iteration 36, loss = 0.57386065\n",
            "Iteration 37, loss = 0.57284075\n",
            "Iteration 38, loss = 0.57151683\n",
            "Iteration 39, loss = 0.57035367\n",
            "Iteration 40, loss = 0.56936141\n",
            "Iteration 41, loss = 0.56829218\n",
            "Iteration 42, loss = 0.56718683\n",
            "Iteration 43, loss = 0.56628285\n",
            "Iteration 44, loss = 0.56540172\n",
            "Iteration 45, loss = 0.56450610\n",
            "Iteration 46, loss = 0.56359359\n",
            "Iteration 47, loss = 0.56274113\n",
            "Iteration 48, loss = 0.56205347\n",
            "Iteration 49, loss = 0.56141325\n",
            "Iteration 50, loss = 0.56076824\n",
            "Iteration 51, loss = 0.56010553\n",
            "Iteration 52, loss = 0.55955061\n",
            "Iteration 53, loss = 0.55909395\n",
            "Iteration 54, loss = 0.55851126\n",
            "Iteration 55, loss = 0.55829418\n",
            "Iteration 56, loss = 0.55769846\n",
            "Iteration 57, loss = 0.55725754\n",
            "Iteration 58, loss = 0.55675838\n",
            "Iteration 59, loss = 0.55638045\n",
            "Iteration 60, loss = 0.55586590\n",
            "Iteration 61, loss = 0.55559102\n",
            "Iteration 62, loss = 0.55526240\n",
            "Iteration 63, loss = 0.55501197\n",
            "Iteration 64, loss = 0.55472593\n",
            "Iteration 65, loss = 0.55441042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 66, loss = 0.55417765\n",
            "Iteration 67, loss = 0.55391604\n",
            "Iteration 68, loss = 0.55367443\n",
            "Iteration 69, loss = 0.55341227\n",
            "Iteration 70, loss = 0.55306736\n",
            "Iteration 71, loss = 0.55291328\n",
            "Iteration 72, loss = 0.55263420\n",
            "Iteration 73, loss = 0.55247432\n",
            "Iteration 74, loss = 0.55217557\n",
            "Iteration 75, loss = 0.55197460\n",
            "Iteration 76, loss = 0.55176851\n",
            "Iteration 77, loss = 0.55160822\n",
            "Iteration 78, loss = 0.55124273\n",
            "Iteration 79, loss = 0.55130751\n",
            "Iteration 80, loss = 0.55101272\n",
            "Iteration 81, loss = 0.55080159\n",
            "Iteration 82, loss = 0.55055801\n",
            "Iteration 83, loss = 0.55028475\n",
            "Iteration 84, loss = 0.55008004\n",
            "Iteration 85, loss = 0.54978130\n",
            "Iteration 86, loss = 0.54946212\n",
            "Iteration 87, loss = 0.54921940\n",
            "Iteration 88, loss = 0.54905529\n",
            "Iteration 89, loss = 0.54883849\n",
            "Iteration 90, loss = 0.54866519\n",
            "Iteration 91, loss = 0.54847662\n",
            "Iteration 92, loss = 0.54822737\n",
            "Iteration 93, loss = 0.54809133\n",
            "Iteration 94, loss = 0.54783013\n",
            "Iteration 95, loss = 0.54764196\n",
            "Iteration 96, loss = 0.54742739\n",
            "Iteration 97, loss = 0.54731608\n",
            "Iteration 98, loss = 0.54716179\n",
            "Iteration 99, loss = 0.54712924\n",
            "Iteration 100, loss = 0.54680985\n",
            "Iteration 101, loss = 0.54673239\n",
            "Iteration 102, loss = 0.54647657\n",
            "Iteration 103, loss = 0.54624866\n",
            "Iteration 104, loss = 0.54612778\n",
            "Iteration 105, loss = 0.54598568\n",
            "Iteration 106, loss = 0.54591547\n",
            "Iteration 107, loss = 0.54585241\n",
            "Iteration 108, loss = 0.54591879\n",
            "Iteration 109, loss = 0.54577268\n",
            "Iteration 110, loss = 0.54575403\n",
            "Iteration 111, loss = 0.54565059\n",
            "Iteration 112, loss = 0.54569602\n",
            "Iteration 113, loss = 0.54556721\n",
            "Iteration 114, loss = 0.54536502\n",
            "Iteration 115, loss = 0.54530154\n",
            "Iteration 116, loss = 0.54523272\n",
            "Iteration 117, loss = 0.54506782\n",
            "Iteration 118, loss = 0.54500117\n",
            "Iteration 119, loss = 0.54493549\n",
            "Iteration 120, loss = 0.54482254\n",
            "Iteration 121, loss = 0.54475022\n",
            "Iteration 122, loss = 0.54464520\n",
            "Iteration 123, loss = 0.54462680\n",
            "Iteration 124, loss = 0.54460052\n",
            "Iteration 125, loss = 0.54447940\n",
            "Iteration 126, loss = 0.54456956\n",
            "Iteration 127, loss = 0.54425805\n",
            "Iteration 128, loss = 0.54413216\n",
            "Iteration 129, loss = 0.54384069\n",
            "Iteration 130, loss = 0.54371370\n",
            "Iteration 131, loss = 0.54352732\n",
            "Iteration 132, loss = 0.54335967\n",
            "Iteration 133, loss = 0.54329342\n",
            "Iteration 134, loss = 0.54322469\n",
            "Iteration 135, loss = 0.54321215\n",
            "Iteration 136, loss = 0.54299755\n",
            "Iteration 137, loss = 0.54285669\n",
            "Iteration 138, loss = 0.54275060\n",
            "Iteration 139, loss = 0.54247230\n",
            "Iteration 140, loss = 0.54230632\n",
            "Iteration 141, loss = 0.54220545\n",
            "Iteration 142, loss = 0.54214524\n",
            "Iteration 143, loss = 0.54205900\n",
            "Iteration 144, loss = 0.54184695\n",
            "Iteration 145, loss = 0.54182552\n",
            "Iteration 146, loss = 0.54157341\n",
            "Iteration 147, loss = 0.54145732\n",
            "Iteration 148, loss = 0.54126089\n",
            "Iteration 149, loss = 0.54101512\n",
            "Iteration 150, loss = 0.54097605\n",
            "Iteration 151, loss = 0.54093882\n",
            "Iteration 152, loss = 0.54102659\n",
            "Iteration 153, loss = 0.54061549\n",
            "Iteration 154, loss = 0.54054110\n",
            "Iteration 155, loss = 0.54014290\n",
            "Iteration 156, loss = 0.54012450\n",
            "Iteration 157, loss = 0.53982654\n",
            "Iteration 158, loss = 0.53964231\n",
            "Iteration 159, loss = 0.53956305\n",
            "Iteration 160, loss = 0.53944233\n",
            "Iteration 161, loss = 0.53931231\n",
            "Iteration 162, loss = 0.53930214\n",
            "Iteration 163, loss = 0.53903052\n",
            "Iteration 164, loss = 0.53877734\n",
            "Iteration 165, loss = 0.53864195\n",
            "Iteration 166, loss = 0.53845220\n",
            "Iteration 167, loss = 0.53851440\n",
            "Iteration 168, loss = 0.53822528\n",
            "Iteration 169, loss = 0.53828039\n",
            "Iteration 170, loss = 0.53811978\n",
            "Iteration 171, loss = 0.53793443\n",
            "Iteration 172, loss = 0.53770276\n",
            "Iteration 173, loss = 0.53757042\n",
            "Iteration 174, loss = 0.53746841\n",
            "Iteration 175, loss = 0.53744624\n",
            "Iteration 176, loss = 0.53732561\n",
            "Iteration 177, loss = 0.53692407\n",
            "Iteration 178, loss = 0.53683696\n",
            "Iteration 179, loss = 0.53647780\n",
            "Iteration 180, loss = 0.53624622\n",
            "Iteration 181, loss = 0.53593573\n",
            "Iteration 182, loss = 0.53577339\n",
            "Iteration 183, loss = 0.53568908\n",
            "Iteration 184, loss = 0.53555303\n",
            "Iteration 185, loss = 0.53556762\n",
            "Iteration 186, loss = 0.53560811\n",
            "Iteration 187, loss = 0.53555030\n",
            "Iteration 188, loss = 0.53519201\n",
            "Iteration 189, loss = 0.53517003\n",
            "Iteration 190, loss = 0.53473487\n",
            "Iteration 191, loss = 0.53489425\n",
            "Iteration 192, loss = 0.53493529\n",
            "Iteration 193, loss = 0.53468970\n",
            "Iteration 194, loss = 0.53444205\n",
            "Iteration 195, loss = 0.53429011\n",
            "Iteration 196, loss = 0.53407521\n",
            "Iteration 197, loss = 0.53398312\n",
            "Iteration 198, loss = 0.53374604\n",
            "Iteration 199, loss = 0.53368447\n",
            "Iteration 200, loss = 0.53341937\n",
            "Iteration 201, loss = 0.53334811\n",
            "Iteration 202, loss = 0.53309054\n",
            "Iteration 203, loss = 0.53301316\n",
            "Iteration 204, loss = 0.53298504\n",
            "Iteration 205, loss = 0.53295263\n",
            "Iteration 206, loss = 0.53287175\n",
            "Iteration 207, loss = 0.53267691\n",
            "Iteration 208, loss = 0.53281640\n",
            "Iteration 209, loss = 0.53261484\n",
            "Iteration 210, loss = 0.53259044\n",
            "Iteration 211, loss = 0.53242977\n",
            "Iteration 212, loss = 0.53227632\n",
            "Iteration 213, loss = 0.53213923\n",
            "Iteration 214, loss = 0.53208291\n",
            "Iteration 215, loss = 0.53171014\n",
            "Iteration 216, loss = 0.53168784\n",
            "Iteration 217, loss = 0.53140439\n",
            "Iteration 218, loss = 0.53126823\n",
            "Iteration 219, loss = 0.53126482\n",
            "Iteration 220, loss = 0.53123705\n",
            "Iteration 221, loss = 0.53099901\n",
            "Iteration 222, loss = 0.53076692\n",
            "Iteration 223, loss = 0.53080080\n",
            "Iteration 224, loss = 0.53062310\n",
            "Iteration 225, loss = 0.53055307\n",
            "Iteration 226, loss = 0.53043600\n",
            "Iteration 227, loss = 0.53038672\n",
            "Iteration 228, loss = 0.53031094\n",
            "Iteration 229, loss = 0.53049112\n",
            "Iteration 230, loss = 0.53018347\n",
            "Iteration 231, loss = 0.53012793\n",
            "Iteration 232, loss = 0.53002628\n",
            "Iteration 233, loss = 0.53001751\n",
            "Iteration 234, loss = 0.52964244\n",
            "Iteration 235, loss = 0.52960693\n",
            "Iteration 236, loss = 0.52940142\n",
            "Iteration 237, loss = 0.52925274\n",
            "Iteration 238, loss = 0.52909800\n",
            "Iteration 239, loss = 0.52925111\n",
            "Iteration 240, loss = 0.52904335\n",
            "Iteration 241, loss = 0.52892960\n",
            "Iteration 242, loss = 0.52897007\n",
            "Iteration 243, loss = 0.52906877\n",
            "Iteration 244, loss = 0.52889105\n",
            "Iteration 245, loss = 0.52883010\n",
            "Iteration 246, loss = 0.52881047\n",
            "Iteration 247, loss = 0.52861500\n",
            "Iteration 248, loss = 0.52828239\n",
            "Iteration 249, loss = 0.52827351\n",
            "Iteration 250, loss = 0.52815040\n",
            "Iteration 1, loss = 0.92236300\n",
            "Iteration 2, loss = 0.90372668\n",
            "Iteration 3, loss = 0.87500833\n",
            "Iteration 4, loss = 0.84285035\n",
            "Iteration 5, loss = 0.80913380\n",
            "Iteration 6, loss = 0.77972252\n",
            "Iteration 7, loss = 0.75423862\n",
            "Iteration 8, loss = 0.73244226\n",
            "Iteration 9, loss = 0.71337957\n",
            "Iteration 10, loss = 0.69694930\n",
            "Iteration 11, loss = 0.68235517\n",
            "Iteration 12, loss = 0.66874001\n",
            "Iteration 13, loss = 0.65701897\n",
            "Iteration 14, loss = 0.64666486\n",
            "Iteration 15, loss = 0.63680422\n",
            "Iteration 16, loss = 0.62903974\n",
            "Iteration 17, loss = 0.62282581\n",
            "Iteration 18, loss = 0.61753655\n",
            "Iteration 19, loss = 0.61255961\n",
            "Iteration 20, loss = 0.60822250\n",
            "Iteration 21, loss = 0.60455223\n",
            "Iteration 22, loss = 0.60093557\n",
            "Iteration 23, loss = 0.59766462\n",
            "Iteration 24, loss = 0.59443533\n",
            "Iteration 25, loss = 0.59186429\n",
            "Iteration 26, loss = 0.58954458\n",
            "Iteration 27, loss = 0.58722957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 28, loss = 0.58504645\n",
            "Iteration 29, loss = 0.58302508\n",
            "Iteration 30, loss = 0.58092002\n",
            "Iteration 31, loss = 0.57890737\n",
            "Iteration 32, loss = 0.57713204\n",
            "Iteration 33, loss = 0.57553550\n",
            "Iteration 34, loss = 0.57399110\n",
            "Iteration 35, loss = 0.57258257\n",
            "Iteration 36, loss = 0.57132345\n",
            "Iteration 37, loss = 0.57036989\n",
            "Iteration 38, loss = 0.56919804\n",
            "Iteration 39, loss = 0.56804072\n",
            "Iteration 40, loss = 0.56720233\n",
            "Iteration 41, loss = 0.56616219\n",
            "Iteration 42, loss = 0.56523123\n",
            "Iteration 43, loss = 0.56418669\n",
            "Iteration 44, loss = 0.56340104\n",
            "Iteration 45, loss = 0.56229242\n",
            "Iteration 46, loss = 0.56136004\n",
            "Iteration 47, loss = 0.56050574\n",
            "Iteration 48, loss = 0.55975555\n",
            "Iteration 49, loss = 0.55893402\n",
            "Iteration 50, loss = 0.55825810\n",
            "Iteration 51, loss = 0.55750539\n",
            "Iteration 52, loss = 0.55689701\n",
            "Iteration 53, loss = 0.55630534\n",
            "Iteration 54, loss = 0.55554960\n",
            "Iteration 55, loss = 0.55530925\n",
            "Iteration 56, loss = 0.55458557\n",
            "Iteration 57, loss = 0.55417055\n",
            "Iteration 58, loss = 0.55375697\n",
            "Iteration 59, loss = 0.55343024\n",
            "Iteration 60, loss = 0.55303828\n",
            "Iteration 61, loss = 0.55275448\n",
            "Iteration 62, loss = 0.55246826\n",
            "Iteration 63, loss = 0.55210971\n",
            "Iteration 64, loss = 0.55175282\n",
            "Iteration 65, loss = 0.55137494\n",
            "Iteration 66, loss = 0.55113308\n",
            "Iteration 67, loss = 0.55081752\n",
            "Iteration 68, loss = 0.55067280\n",
            "Iteration 69, loss = 0.55037239\n",
            "Iteration 70, loss = 0.55015762\n",
            "Iteration 71, loss = 0.54984322\n",
            "Iteration 72, loss = 0.54962258\n",
            "Iteration 73, loss = 0.54941433\n",
            "Iteration 74, loss = 0.54910659\n",
            "Iteration 75, loss = 0.54882617\n",
            "Iteration 76, loss = 0.54858009\n",
            "Iteration 77, loss = 0.54842549\n",
            "Iteration 78, loss = 0.54826356\n",
            "Iteration 79, loss = 0.54817975\n",
            "Iteration 80, loss = 0.54779764\n",
            "Iteration 81, loss = 0.54763140\n",
            "Iteration 82, loss = 0.54736580\n",
            "Iteration 83, loss = 0.54705013\n",
            "Iteration 84, loss = 0.54689619\n",
            "Iteration 85, loss = 0.54652246\n",
            "Iteration 86, loss = 0.54618831\n",
            "Iteration 87, loss = 0.54586111\n",
            "Iteration 88, loss = 0.54563663\n",
            "Iteration 89, loss = 0.54532937\n",
            "Iteration 90, loss = 0.54516871\n",
            "Iteration 91, loss = 0.54484111\n",
            "Iteration 92, loss = 0.54457027\n",
            "Iteration 93, loss = 0.54439667\n",
            "Iteration 94, loss = 0.54405331\n",
            "Iteration 95, loss = 0.54382871\n",
            "Iteration 96, loss = 0.54356907\n",
            "Iteration 97, loss = 0.54331510\n",
            "Iteration 98, loss = 0.54317759\n",
            "Iteration 99, loss = 0.54304768\n",
            "Iteration 100, loss = 0.54279147\n",
            "Iteration 101, loss = 0.54261840\n",
            "Iteration 102, loss = 0.54245119\n",
            "Iteration 103, loss = 0.54230165\n",
            "Iteration 104, loss = 0.54201321\n",
            "Iteration 105, loss = 0.54196650\n",
            "Iteration 106, loss = 0.54187452\n",
            "Iteration 107, loss = 0.54182689\n",
            "Iteration 108, loss = 0.54186328\n",
            "Iteration 109, loss = 0.54161841\n",
            "Iteration 110, loss = 0.54159098\n",
            "Iteration 111, loss = 0.54140119\n",
            "Iteration 112, loss = 0.54132090\n",
            "Iteration 113, loss = 0.54133298\n",
            "Iteration 114, loss = 0.54113774\n",
            "Iteration 115, loss = 0.54102169\n",
            "Iteration 116, loss = 0.54094796\n",
            "Iteration 117, loss = 0.54082351\n",
            "Iteration 118, loss = 0.54077253\n",
            "Iteration 119, loss = 0.54069632\n",
            "Iteration 120, loss = 0.54060634\n",
            "Iteration 121, loss = 0.54040797\n",
            "Iteration 122, loss = 0.54024138\n",
            "Iteration 123, loss = 0.54028824\n",
            "Iteration 124, loss = 0.54029877\n",
            "Iteration 125, loss = 0.54022868\n",
            "Iteration 126, loss = 0.54001368\n",
            "Iteration 127, loss = 0.53985196\n",
            "Iteration 128, loss = 0.53976040\n",
            "Iteration 129, loss = 0.53951986\n",
            "Iteration 130, loss = 0.53942221\n",
            "Iteration 131, loss = 0.53929777\n",
            "Iteration 132, loss = 0.53916102\n",
            "Iteration 133, loss = 0.53903189\n",
            "Iteration 134, loss = 0.53910032\n",
            "Iteration 135, loss = 0.53900365\n",
            "Iteration 136, loss = 0.53908288\n",
            "Iteration 137, loss = 0.53888749\n",
            "Iteration 138, loss = 0.53882052\n",
            "Iteration 139, loss = 0.53874478\n",
            "Iteration 140, loss = 0.53866072\n",
            "Iteration 141, loss = 0.53861333\n",
            "Iteration 142, loss = 0.53871510\n",
            "Iteration 143, loss = 0.53874044\n",
            "Iteration 144, loss = 0.53861559\n",
            "Iteration 145, loss = 0.53866957\n",
            "Iteration 146, loss = 0.53851373\n",
            "Iteration 147, loss = 0.53844718\n",
            "Iteration 148, loss = 0.53822626\n",
            "Iteration 149, loss = 0.53814144\n",
            "Iteration 150, loss = 0.53820563\n",
            "Iteration 151, loss = 0.53810266\n",
            "Iteration 152, loss = 0.53809679\n",
            "Iteration 153, loss = 0.53803421\n",
            "Iteration 154, loss = 0.53800440\n",
            "Iteration 155, loss = 0.53790815\n",
            "Iteration 156, loss = 0.53807663\n",
            "Iteration 157, loss = 0.53775598\n",
            "Iteration 158, loss = 0.53751940\n",
            "Iteration 159, loss = 0.53736868\n",
            "Iteration 160, loss = 0.53736605\n",
            "Iteration 161, loss = 0.53731247\n",
            "Iteration 162, loss = 0.53737783\n",
            "Iteration 163, loss = 0.53721698\n",
            "Iteration 164, loss = 0.53703478\n",
            "Iteration 165, loss = 0.53714732\n",
            "Iteration 166, loss = 0.53695543\n",
            "Iteration 167, loss = 0.53690205\n",
            "Iteration 168, loss = 0.53686330\n",
            "Iteration 169, loss = 0.53704083\n",
            "Iteration 170, loss = 0.53699350\n",
            "Iteration 171, loss = 0.53698387\n",
            "Iteration 172, loss = 0.53692939\n",
            "Iteration 173, loss = 0.53686512\n",
            "Iteration 174, loss = 0.53672144\n",
            "Iteration 175, loss = 0.53683777\n",
            "Iteration 176, loss = 0.53682227\n",
            "Iteration 177, loss = 0.53652605\n",
            "Iteration 178, loss = 0.53659008\n",
            "Iteration 179, loss = 0.53644548\n",
            "Iteration 180, loss = 0.53624399\n",
            "Iteration 181, loss = 0.53602806\n",
            "Iteration 182, loss = 0.53601239\n",
            "Iteration 183, loss = 0.53581716\n",
            "Iteration 184, loss = 0.53575172\n",
            "Iteration 185, loss = 0.53569976\n",
            "Iteration 186, loss = 0.53569924\n",
            "Iteration 187, loss = 0.53579638\n",
            "Iteration 188, loss = 0.53554040\n",
            "Iteration 189, loss = 0.53549239\n",
            "Iteration 190, loss = 0.53542215\n",
            "Iteration 191, loss = 0.53555390\n",
            "Iteration 192, loss = 0.53551448\n",
            "Iteration 193, loss = 0.53545647\n",
            "Iteration 194, loss = 0.53530799\n",
            "Iteration 195, loss = 0.53528437\n",
            "Iteration 196, loss = 0.53529348\n",
            "Iteration 197, loss = 0.53516088\n",
            "Iteration 198, loss = 0.53503815\n",
            "Iteration 199, loss = 0.53505396\n",
            "Iteration 200, loss = 0.53501743\n",
            "Iteration 201, loss = 0.53497168\n",
            "Iteration 202, loss = 0.53479859\n",
            "Iteration 203, loss = 0.53470391\n",
            "Iteration 204, loss = 0.53465550\n",
            "Iteration 205, loss = 0.53480222\n",
            "Iteration 206, loss = 0.53477673\n",
            "Iteration 207, loss = 0.53455231\n",
            "Iteration 208, loss = 0.53472736\n",
            "Iteration 209, loss = 0.53459489\n",
            "Iteration 210, loss = 0.53459161\n",
            "Iteration 211, loss = 0.53454316\n",
            "Iteration 212, loss = 0.53448908\n",
            "Iteration 213, loss = 0.53432117\n",
            "Iteration 214, loss = 0.53425464\n",
            "Iteration 215, loss = 0.53428352\n",
            "Iteration 216, loss = 0.53432400\n",
            "Iteration 217, loss = 0.53419034\n",
            "Iteration 218, loss = 0.53412993\n",
            "Iteration 219, loss = 0.53395249\n",
            "Iteration 220, loss = 0.53408377\n",
            "Iteration 221, loss = 0.53401426\n",
            "Iteration 222, loss = 0.53384494\n",
            "Iteration 223, loss = 0.53397558\n",
            "Iteration 224, loss = 0.53378458\n",
            "Iteration 225, loss = 0.53372374\n",
            "Iteration 226, loss = 0.53387654\n",
            "Iteration 227, loss = 0.53379764\n",
            "Iteration 228, loss = 0.53381196\n",
            "Iteration 229, loss = 0.53381403\n",
            "Iteration 230, loss = 0.53348530\n",
            "Iteration 231, loss = 0.53356283\n",
            "Iteration 232, loss = 0.53344534\n",
            "Iteration 233, loss = 0.53340617\n",
            "Iteration 234, loss = 0.53328546\n",
            "Iteration 235, loss = 0.53323422\n",
            "Iteration 236, loss = 0.53323537\n",
            "Iteration 237, loss = 0.53319344\n",
            "Iteration 238, loss = 0.53316331\n",
            "Iteration 239, loss = 0.53344843\n",
            "Iteration 240, loss = 0.53325527\n",
            "Iteration 241, loss = 0.53321514\n",
            "Iteration 242, loss = 0.53325694\n",
            "Iteration 243, loss = 0.53324379\n",
            "Iteration 244, loss = 0.53314894\n",
            "Iteration 245, loss = 0.53293651\n",
            "Iteration 246, loss = 0.53295328\n",
            "Iteration 247, loss = 0.53291842\n",
            "Iteration 248, loss = 0.53285321\n",
            "Iteration 249, loss = 0.53281305\n",
            "Iteration 250, loss = 0.53266603\n",
            "Iteration 1, loss = 0.93166195\n",
            "Iteration 2, loss = 0.91234492\n",
            "Iteration 3, loss = 0.88372313\n",
            "Iteration 4, loss = 0.84961251\n",
            "Iteration 5, loss = 0.81641210\n",
            "Iteration 6, loss = 0.78527885\n",
            "Iteration 7, loss = 0.75872117\n",
            "Iteration 8, loss = 0.73530918\n",
            "Iteration 9, loss = 0.71535645\n",
            "Iteration 10, loss = 0.69760505\n",
            "Iteration 11, loss = 0.68188671\n",
            "Iteration 12, loss = 0.66752288\n",
            "Iteration 13, loss = 0.65465085\n",
            "Iteration 14, loss = 0.64368593\n",
            "Iteration 15, loss = 0.63335085\n",
            "Iteration 16, loss = 0.62497229\n",
            "Iteration 17, loss = 0.61851546\n",
            "Iteration 18, loss = 0.61250769\n",
            "Iteration 19, loss = 0.60725642\n",
            "Iteration 20, loss = 0.60264352\n",
            "Iteration 21, loss = 0.59848665\n",
            "Iteration 22, loss = 0.59465851\n",
            "Iteration 23, loss = 0.59134538\n",
            "Iteration 24, loss = 0.58812798\n",
            "Iteration 25, loss = 0.58555756\n",
            "Iteration 26, loss = 0.58347267\n",
            "Iteration 27, loss = 0.58105626\n",
            "Iteration 28, loss = 0.57898547\n",
            "Iteration 29, loss = 0.57707857\n",
            "Iteration 30, loss = 0.57510602\n",
            "Iteration 31, loss = 0.57303366\n",
            "Iteration 32, loss = 0.57121707\n",
            "Iteration 33, loss = 0.56956037\n",
            "Iteration 34, loss = 0.56781174\n",
            "Iteration 35, loss = 0.56635167\n",
            "Iteration 36, loss = 0.56496659\n",
            "Iteration 37, loss = 0.56388825\n",
            "Iteration 38, loss = 0.56265033\n",
            "Iteration 39, loss = 0.56150979\n",
            "Iteration 40, loss = 0.56060243\n",
            "Iteration 41, loss = 0.55974129\n",
            "Iteration 42, loss = 0.55864499\n",
            "Iteration 43, loss = 0.55766563\n",
            "Iteration 44, loss = 0.55679573\n",
            "Iteration 45, loss = 0.55585704\n",
            "Iteration 46, loss = 0.55484301\n",
            "Iteration 47, loss = 0.55397414\n",
            "Iteration 48, loss = 0.55323994\n",
            "Iteration 49, loss = 0.55246703\n",
            "Iteration 50, loss = 0.55180371\n",
            "Iteration 51, loss = 0.55112381\n",
            "Iteration 52, loss = 0.55041014\n",
            "Iteration 53, loss = 0.54992329\n",
            "Iteration 54, loss = 0.54928756\n",
            "Iteration 55, loss = 0.54895870\n",
            "Iteration 56, loss = 0.54836925\n",
            "Iteration 57, loss = 0.54789274\n",
            "Iteration 58, loss = 0.54743070\n",
            "Iteration 59, loss = 0.54683477\n",
            "Iteration 60, loss = 0.54645133\n",
            "Iteration 61, loss = 0.54598155\n",
            "Iteration 62, loss = 0.54552238\n",
            "Iteration 63, loss = 0.54515072\n",
            "Iteration 64, loss = 0.54483821\n",
            "Iteration 65, loss = 0.54450111\n",
            "Iteration 66, loss = 0.54421242\n",
            "Iteration 67, loss = 0.54376193\n",
            "Iteration 68, loss = 0.54363452\n",
            "Iteration 69, loss = 0.54321728\n",
            "Iteration 70, loss = 0.54288609\n",
            "Iteration 71, loss = 0.54252895\n",
            "Iteration 72, loss = 0.54228397\n",
            "Iteration 73, loss = 0.54204462\n",
            "Iteration 74, loss = 0.54173786\n",
            "Iteration 75, loss = 0.54144168\n",
            "Iteration 76, loss = 0.54119500\n",
            "Iteration 77, loss = 0.54091047\n",
            "Iteration 78, loss = 0.54069623\n",
            "Iteration 79, loss = 0.54067683\n",
            "Iteration 80, loss = 0.54013276\n",
            "Iteration 81, loss = 0.53989053\n",
            "Iteration 82, loss = 0.53959924\n",
            "Iteration 83, loss = 0.53936832\n",
            "Iteration 84, loss = 0.53909921\n",
            "Iteration 85, loss = 0.53901869\n",
            "Iteration 86, loss = 0.53886630\n",
            "Iteration 87, loss = 0.53863528\n",
            "Iteration 88, loss = 0.53844614\n",
            "Iteration 89, loss = 0.53823882\n",
            "Iteration 90, loss = 0.53804482\n",
            "Iteration 91, loss = 0.53777906\n",
            "Iteration 92, loss = 0.53750544\n",
            "Iteration 93, loss = 0.53723586\n",
            "Iteration 94, loss = 0.53706081\n",
            "Iteration 95, loss = 0.53680307\n",
            "Iteration 96, loss = 0.53657594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 97, loss = 0.53648131\n",
            "Iteration 98, loss = 0.53633934\n",
            "Iteration 99, loss = 0.53616061\n",
            "Iteration 100, loss = 0.53588690\n",
            "Iteration 101, loss = 0.53572652\n",
            "Iteration 102, loss = 0.53554122\n",
            "Iteration 103, loss = 0.53541845\n",
            "Iteration 104, loss = 0.53524751\n",
            "Iteration 105, loss = 0.53511494\n",
            "Iteration 106, loss = 0.53496457\n",
            "Iteration 107, loss = 0.53490532\n",
            "Iteration 108, loss = 0.53475815\n",
            "Iteration 109, loss = 0.53455697\n",
            "Iteration 110, loss = 0.53447970\n",
            "Iteration 111, loss = 0.53441300\n",
            "Iteration 112, loss = 0.53425956\n",
            "Iteration 113, loss = 0.53414518\n",
            "Iteration 114, loss = 0.53402485\n",
            "Iteration 115, loss = 0.53396639\n",
            "Iteration 116, loss = 0.53392574\n",
            "Iteration 117, loss = 0.53373909\n",
            "Iteration 118, loss = 0.53377461\n",
            "Iteration 119, loss = 0.53363294\n",
            "Iteration 120, loss = 0.53362123\n",
            "Iteration 121, loss = 0.53359790\n",
            "Iteration 122, loss = 0.53336937\n",
            "Iteration 123, loss = 0.53322864\n",
            "Iteration 124, loss = 0.53314603\n",
            "Iteration 125, loss = 0.53310922\n",
            "Iteration 126, loss = 0.53300059\n",
            "Iteration 127, loss = 0.53300315\n",
            "Iteration 128, loss = 0.53294813\n",
            "Iteration 129, loss = 0.53288857\n",
            "Iteration 130, loss = 0.53290528\n",
            "Iteration 131, loss = 0.53289451\n",
            "Iteration 132, loss = 0.53291395\n",
            "Iteration 133, loss = 0.53283436\n",
            "Iteration 134, loss = 0.53261960\n",
            "Iteration 135, loss = 0.53265552\n",
            "Iteration 136, loss = 0.53270260\n",
            "Iteration 137, loss = 0.53254937\n",
            "Iteration 138, loss = 0.53253935\n",
            "Iteration 139, loss = 0.53249355\n",
            "Iteration 140, loss = 0.53237285\n",
            "Iteration 141, loss = 0.53230035\n",
            "Iteration 142, loss = 0.53216267\n",
            "Iteration 143, loss = 0.53203890\n",
            "Iteration 144, loss = 0.53195989\n",
            "Iteration 145, loss = 0.53192892\n",
            "Iteration 146, loss = 0.53178198\n",
            "Iteration 147, loss = 0.53173814\n",
            "Iteration 148, loss = 0.53150166\n",
            "Iteration 149, loss = 0.53143147\n",
            "Iteration 150, loss = 0.53135531\n",
            "Iteration 151, loss = 0.53124938\n",
            "Iteration 152, loss = 0.53113380\n",
            "Iteration 153, loss = 0.53095503\n",
            "Iteration 154, loss = 0.53089162\n",
            "Iteration 155, loss = 0.53086375\n",
            "Iteration 156, loss = 0.53085149\n",
            "Iteration 157, loss = 0.53059771\n",
            "Iteration 158, loss = 0.53048593\n",
            "Iteration 159, loss = 0.53072175\n",
            "Iteration 160, loss = 0.53072225\n",
            "Iteration 161, loss = 0.53057661\n",
            "Iteration 162, loss = 0.53070197\n",
            "Iteration 163, loss = 0.53037790\n",
            "Iteration 164, loss = 0.53035044\n",
            "Iteration 165, loss = 0.53034699\n",
            "Iteration 166, loss = 0.53020613\n",
            "Iteration 167, loss = 0.53014485\n",
            "Iteration 168, loss = 0.52999594\n",
            "Iteration 169, loss = 0.53015019\n",
            "Iteration 170, loss = 0.53001537\n",
            "Iteration 171, loss = 0.53002088\n",
            "Iteration 172, loss = 0.53011873\n",
            "Iteration 173, loss = 0.53007490\n",
            "Iteration 174, loss = 0.52987198\n",
            "Iteration 175, loss = 0.52981520\n",
            "Iteration 176, loss = 0.52956081\n",
            "Iteration 177, loss = 0.52937591\n",
            "Iteration 178, loss = 0.52945228\n",
            "Iteration 179, loss = 0.52920721\n",
            "Iteration 180, loss = 0.52921958\n",
            "Iteration 181, loss = 0.52890810\n",
            "Iteration 182, loss = 0.52894055\n",
            "Iteration 183, loss = 0.52884664\n",
            "Iteration 184, loss = 0.52901047\n",
            "Iteration 185, loss = 0.52883121\n",
            "Iteration 186, loss = 0.52877123\n",
            "Iteration 187, loss = 0.52880530\n",
            "Iteration 188, loss = 0.52865693\n",
            "Iteration 189, loss = 0.52864742\n",
            "Iteration 190, loss = 0.52861805\n",
            "Iteration 191, loss = 0.52857983\n",
            "Iteration 192, loss = 0.52866023\n",
            "Iteration 193, loss = 0.52858115\n",
            "Iteration 194, loss = 0.52848561\n",
            "Iteration 195, loss = 0.52844125\n",
            "Iteration 196, loss = 0.52853848\n",
            "Iteration 197, loss = 0.52836810\n",
            "Iteration 198, loss = 0.52836734\n",
            "Iteration 199, loss = 0.52842305\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.93004949\n",
            "Iteration 2, loss = 0.90996806\n",
            "Iteration 3, loss = 0.88055072\n",
            "Iteration 4, loss = 0.84659018\n",
            "Iteration 5, loss = 0.81298067\n",
            "Iteration 6, loss = 0.78206991\n",
            "Iteration 7, loss = 0.75624473\n",
            "Iteration 8, loss = 0.73305263\n",
            "Iteration 9, loss = 0.71302062\n",
            "Iteration 10, loss = 0.69627033\n",
            "Iteration 11, loss = 0.68068281\n",
            "Iteration 12, loss = 0.66784432\n",
            "Iteration 13, loss = 0.65616483\n",
            "Iteration 14, loss = 0.64623851\n",
            "Iteration 15, loss = 0.63710321\n",
            "Iteration 16, loss = 0.62947245\n",
            "Iteration 17, loss = 0.62332210\n",
            "Iteration 18, loss = 0.61772357\n",
            "Iteration 19, loss = 0.61296514\n",
            "Iteration 20, loss = 0.60893825\n",
            "Iteration 21, loss = 0.60537285\n",
            "Iteration 22, loss = 0.60197233\n",
            "Iteration 23, loss = 0.59888836\n",
            "Iteration 24, loss = 0.59592724\n",
            "Iteration 25, loss = 0.59330154\n",
            "Iteration 26, loss = 0.59119156\n",
            "Iteration 27, loss = 0.58870339\n",
            "Iteration 28, loss = 0.58650396\n",
            "Iteration 29, loss = 0.58465794\n",
            "Iteration 30, loss = 0.58280816\n",
            "Iteration 31, loss = 0.58096325\n",
            "Iteration 32, loss = 0.57914326\n",
            "Iteration 33, loss = 0.57759947\n",
            "Iteration 34, loss = 0.57596723\n",
            "Iteration 35, loss = 0.57461254\n",
            "Iteration 36, loss = 0.57309404\n",
            "Iteration 37, loss = 0.57195132\n",
            "Iteration 38, loss = 0.57082853\n",
            "Iteration 39, loss = 0.56968276\n",
            "Iteration 40, loss = 0.56876378\n",
            "Iteration 41, loss = 0.56795050\n",
            "Iteration 42, loss = 0.56670730\n",
            "Iteration 43, loss = 0.56589047\n",
            "Iteration 44, loss = 0.56489212\n",
            "Iteration 45, loss = 0.56408490\n",
            "Iteration 46, loss = 0.56314279\n",
            "Iteration 47, loss = 0.56221700\n",
            "Iteration 48, loss = 0.56123991\n",
            "Iteration 49, loss = 0.56021024\n",
            "Iteration 50, loss = 0.55950861\n",
            "Iteration 51, loss = 0.55881625\n",
            "Iteration 52, loss = 0.55804567\n",
            "Iteration 53, loss = 0.55743272\n",
            "Iteration 54, loss = 0.55675730\n",
            "Iteration 55, loss = 0.55651589\n",
            "Iteration 56, loss = 0.55608563\n",
            "Iteration 57, loss = 0.55564424\n",
            "Iteration 58, loss = 0.55526565\n",
            "Iteration 59, loss = 0.55479143\n",
            "Iteration 60, loss = 0.55430114\n",
            "Iteration 61, loss = 0.55392714\n",
            "Iteration 62, loss = 0.55358673\n",
            "Iteration 63, loss = 0.55323272\n",
            "Iteration 64, loss = 0.55316209\n",
            "Iteration 65, loss = 0.55273953\n",
            "Iteration 66, loss = 0.55263798\n",
            "Iteration 67, loss = 0.55236876\n",
            "Iteration 68, loss = 0.55214964\n",
            "Iteration 69, loss = 0.55181669\n",
            "Iteration 70, loss = 0.55156614\n",
            "Iteration 71, loss = 0.55140596\n",
            "Iteration 72, loss = 0.55129497\n",
            "Iteration 73, loss = 0.55112913\n",
            "Iteration 74, loss = 0.55095720\n",
            "Iteration 75, loss = 0.55084412\n",
            "Iteration 76, loss = 0.55071812\n",
            "Iteration 77, loss = 0.55048781\n",
            "Iteration 78, loss = 0.55036839\n",
            "Iteration 79, loss = 0.55043769\n",
            "Iteration 80, loss = 0.55008919\n",
            "Iteration 81, loss = 0.54996789\n",
            "Iteration 82, loss = 0.54984336\n",
            "Iteration 83, loss = 0.54956273\n",
            "Iteration 84, loss = 0.54928818\n",
            "Iteration 85, loss = 0.54921401\n",
            "Iteration 86, loss = 0.54901825\n",
            "Iteration 87, loss = 0.54890758\n",
            "Iteration 88, loss = 0.54882581\n",
            "Iteration 89, loss = 0.54877306\n",
            "Iteration 90, loss = 0.54856873\n",
            "Iteration 91, loss = 0.54851207\n",
            "Iteration 92, loss = 0.54835563\n",
            "Iteration 93, loss = 0.54816557\n",
            "Iteration 94, loss = 0.54804833\n",
            "Iteration 95, loss = 0.54792993\n",
            "Iteration 96, loss = 0.54784983\n",
            "Iteration 97, loss = 0.54774266\n",
            "Iteration 98, loss = 0.54772164\n",
            "Iteration 99, loss = 0.54760421\n",
            "Iteration 100, loss = 0.54742928\n",
            "Iteration 101, loss = 0.54728446\n",
            "Iteration 102, loss = 0.54712408\n",
            "Iteration 103, loss = 0.54705767\n",
            "Iteration 104, loss = 0.54683412\n",
            "Iteration 105, loss = 0.54670329\n",
            "Iteration 106, loss = 0.54657429\n",
            "Iteration 107, loss = 0.54647062\n",
            "Iteration 108, loss = 0.54652613\n",
            "Iteration 109, loss = 0.54649605\n",
            "Iteration 110, loss = 0.54649798\n",
            "Iteration 111, loss = 0.54647283\n",
            "Iteration 112, loss = 0.54633499\n",
            "Iteration 113, loss = 0.54633940\n",
            "Iteration 114, loss = 0.54621142\n",
            "Iteration 115, loss = 0.54618250\n",
            "Iteration 116, loss = 0.54615024\n",
            "Iteration 117, loss = 0.54607173\n",
            "Iteration 118, loss = 0.54600383\n",
            "Iteration 119, loss = 0.54598453\n",
            "Iteration 120, loss = 0.54603966\n",
            "Iteration 121, loss = 0.54605032\n",
            "Iteration 122, loss = 0.54588161\n",
            "Iteration 123, loss = 0.54578006\n",
            "Iteration 124, loss = 0.54578094\n",
            "Iteration 125, loss = 0.54571232\n",
            "Iteration 126, loss = 0.54566232\n",
            "Iteration 127, loss = 0.54577442\n",
            "Iteration 128, loss = 0.54571461\n",
            "Iteration 129, loss = 0.54566955\n",
            "Iteration 130, loss = 0.54569243\n",
            "Iteration 131, loss = 0.54551785\n",
            "Iteration 132, loss = 0.54545224\n",
            "Iteration 133, loss = 0.54541232\n",
            "Iteration 134, loss = 0.54536686\n",
            "Iteration 135, loss = 0.54543813\n",
            "Iteration 136, loss = 0.54543235\n",
            "Iteration 137, loss = 0.54534983\n",
            "Iteration 138, loss = 0.54535801\n",
            "Iteration 139, loss = 0.54530951\n",
            "Iteration 140, loss = 0.54522699\n",
            "Iteration 141, loss = 0.54519488\n",
            "Iteration 142, loss = 0.54516024\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.93650373\n",
            "Iteration 2, loss = 0.91614372\n",
            "Iteration 3, loss = 0.88631046\n",
            "Iteration 4, loss = 0.85327913\n",
            "Iteration 5, loss = 0.82138194\n",
            "Iteration 6, loss = 0.79229617\n",
            "Iteration 7, loss = 0.76753456\n",
            "Iteration 8, loss = 0.74573124\n",
            "Iteration 9, loss = 0.72648054\n",
            "Iteration 10, loss = 0.71056910\n",
            "Iteration 11, loss = 0.69588855\n",
            "Iteration 12, loss = 0.68427106\n",
            "Iteration 13, loss = 0.67305324\n",
            "Iteration 14, loss = 0.66346976\n",
            "Iteration 15, loss = 0.65484224\n",
            "Iteration 16, loss = 0.64765771\n",
            "Iteration 17, loss = 0.64160092\n",
            "Iteration 18, loss = 0.63596695\n",
            "Iteration 19, loss = 0.63145300\n",
            "Iteration 20, loss = 0.62738962\n",
            "Iteration 21, loss = 0.62388968\n",
            "Iteration 22, loss = 0.62051025\n",
            "Iteration 23, loss = 0.61719150\n",
            "Iteration 24, loss = 0.61415457\n",
            "Iteration 25, loss = 0.61154190\n",
            "Iteration 26, loss = 0.60921490\n",
            "Iteration 27, loss = 0.60680333\n",
            "Iteration 28, loss = 0.60463037\n",
            "Iteration 29, loss = 0.60261868\n",
            "Iteration 30, loss = 0.60053790\n",
            "Iteration 31, loss = 0.59861214\n",
            "Iteration 32, loss = 0.59687878\n",
            "Iteration 33, loss = 0.59521436\n",
            "Iteration 34, loss = 0.59373168\n",
            "Iteration 35, loss = 0.59231031\n",
            "Iteration 36, loss = 0.59089091\n",
            "Iteration 37, loss = 0.58975222\n",
            "Iteration 38, loss = 0.58844761\n",
            "Iteration 39, loss = 0.58743580\n",
            "Iteration 40, loss = 0.58644674\n",
            "Iteration 41, loss = 0.58550410\n",
            "Iteration 42, loss = 0.58440164\n",
            "Iteration 43, loss = 0.58328570\n",
            "Iteration 44, loss = 0.58228471\n",
            "Iteration 45, loss = 0.58128176\n",
            "Iteration 46, loss = 0.58040523\n",
            "Iteration 47, loss = 0.57952866\n",
            "Iteration 48, loss = 0.57856752\n",
            "Iteration 49, loss = 0.57759396\n",
            "Iteration 50, loss = 0.57673645\n",
            "Iteration 51, loss = 0.57577758\n",
            "Iteration 52, loss = 0.57477461\n",
            "Iteration 53, loss = 0.57391312\n",
            "Iteration 54, loss = 0.57295768\n",
            "Iteration 55, loss = 0.57235619\n",
            "Iteration 56, loss = 0.57150601\n",
            "Iteration 57, loss = 0.57068862\n",
            "Iteration 58, loss = 0.56995885\n",
            "Iteration 59, loss = 0.56926414\n",
            "Iteration 60, loss = 0.56848196\n",
            "Iteration 61, loss = 0.56771898\n",
            "Iteration 62, loss = 0.56713732\n",
            "Iteration 63, loss = 0.56642712\n",
            "Iteration 64, loss = 0.56596670\n",
            "Iteration 65, loss = 0.56543547\n",
            "Iteration 66, loss = 0.56509810\n",
            "Iteration 67, loss = 0.56455514\n",
            "Iteration 68, loss = 0.56433789\n",
            "Iteration 69, loss = 0.56375333\n",
            "Iteration 70, loss = 0.56327808\n",
            "Iteration 71, loss = 0.56292281\n",
            "Iteration 72, loss = 0.56257013\n",
            "Iteration 73, loss = 0.56215551\n",
            "Iteration 74, loss = 0.56183300\n",
            "Iteration 75, loss = 0.56170533\n",
            "Iteration 76, loss = 0.56121154\n",
            "Iteration 77, loss = 0.56080663\n",
            "Iteration 78, loss = 0.56057794\n",
            "Iteration 79, loss = 0.56030054\n",
            "Iteration 80, loss = 0.56000938\n",
            "Iteration 81, loss = 0.55969876\n",
            "Iteration 82, loss = 0.55937165\n",
            "Iteration 83, loss = 0.55913175\n",
            "Iteration 84, loss = 0.55876924\n",
            "Iteration 85, loss = 0.55859709\n",
            "Iteration 86, loss = 0.55830636\n",
            "Iteration 87, loss = 0.55803344\n",
            "Iteration 88, loss = 0.55769157\n",
            "Iteration 89, loss = 0.55763339\n",
            "Iteration 90, loss = 0.55740669\n",
            "Iteration 91, loss = 0.55726678\n",
            "Iteration 92, loss = 0.55707046\n",
            "Iteration 93, loss = 0.55675904\n",
            "Iteration 94, loss = 0.55659666\n",
            "Iteration 95, loss = 0.55647031\n",
            "Iteration 96, loss = 0.55623035\n",
            "Iteration 97, loss = 0.55605166\n",
            "Iteration 98, loss = 0.55588317\n",
            "Iteration 99, loss = 0.55568320\n",
            "Iteration 100, loss = 0.55547851\n",
            "Iteration 101, loss = 0.55526354\n",
            "Iteration 102, loss = 0.55509953\n",
            "Iteration 103, loss = 0.55498294\n",
            "Iteration 104, loss = 0.55484102\n",
            "Iteration 105, loss = 0.55457174\n",
            "Iteration 106, loss = 0.55451573\n",
            "Iteration 107, loss = 0.55438375\n",
            "Iteration 108, loss = 0.55428149\n",
            "Iteration 109, loss = 0.55413290\n",
            "Iteration 110, loss = 0.55398899\n",
            "Iteration 111, loss = 0.55399261\n",
            "Iteration 112, loss = 0.55390406\n",
            "Iteration 113, loss = 0.55378590\n",
            "Iteration 114, loss = 0.55384290\n",
            "Iteration 115, loss = 0.55370452\n",
            "Iteration 116, loss = 0.55361862\n",
            "Iteration 117, loss = 0.55354360\n",
            "Iteration 118, loss = 0.55337221\n",
            "Iteration 119, loss = 0.55329853\n",
            "Iteration 120, loss = 0.55319982\n",
            "Iteration 121, loss = 0.55327211\n",
            "Iteration 122, loss = 0.55295736\n",
            "Iteration 123, loss = 0.55293288\n",
            "Iteration 124, loss = 0.55279517\n",
            "Iteration 125, loss = 0.55270083\n",
            "Iteration 126, loss = 0.55261500\n",
            "Iteration 127, loss = 0.55259929\n",
            "Iteration 128, loss = 0.55243765\n",
            "Iteration 129, loss = 0.55236737\n",
            "Iteration 130, loss = 0.55227829\n",
            "Iteration 131, loss = 0.55212700\n",
            "Iteration 132, loss = 0.55216285\n",
            "Iteration 133, loss = 0.55201594\n",
            "Iteration 134, loss = 0.55203048\n",
            "Iteration 135, loss = 0.55192038\n",
            "Iteration 136, loss = 0.55194002\n",
            "Iteration 137, loss = 0.55188866\n",
            "Iteration 138, loss = 0.55194771\n",
            "Iteration 139, loss = 0.55176543\n",
            "Iteration 140, loss = 0.55170046\n",
            "Iteration 141, loss = 0.55163516\n",
            "Iteration 142, loss = 0.55161189\n",
            "Iteration 143, loss = 0.55169968\n",
            "Iteration 144, loss = 0.55153053\n",
            "Iteration 145, loss = 0.55147029\n",
            "Iteration 146, loss = 0.55144082\n",
            "Iteration 147, loss = 0.55136457\n",
            "Iteration 148, loss = 0.55126246\n",
            "Iteration 149, loss = 0.55128184\n",
            "Iteration 150, loss = 0.55115483\n",
            "Iteration 151, loss = 0.55122952\n",
            "Iteration 152, loss = 0.55126025\n",
            "Iteration 153, loss = 0.55117985\n",
            "Iteration 154, loss = 0.55112526\n",
            "Iteration 155, loss = 0.55106347\n",
            "Iteration 156, loss = 0.55121775\n",
            "Iteration 157, loss = 0.55099820\n",
            "Iteration 158, loss = 0.55091012\n",
            "Iteration 159, loss = 0.55097006\n",
            "Iteration 160, loss = 0.55081055\n",
            "Iteration 161, loss = 0.55066081\n",
            "Iteration 162, loss = 0.55084435\n",
            "Iteration 163, loss = 0.55097819\n",
            "Iteration 164, loss = 0.55107552\n",
            "Iteration 165, loss = 0.55096152\n",
            "Iteration 166, loss = 0.55085310\n",
            "Iteration 167, loss = 0.55076802\n",
            "Iteration 168, loss = 0.55078911\n",
            "Iteration 169, loss = 0.55068542\n",
            "Iteration 170, loss = 0.55048435\n",
            "Iteration 171, loss = 0.55039659\n",
            "Iteration 172, loss = 0.55029417\n",
            "Iteration 173, loss = 0.55031129\n",
            "Iteration 174, loss = 0.55039791\n",
            "Iteration 175, loss = 0.55037367\n",
            "Iteration 176, loss = 0.55029732\n",
            "Iteration 177, loss = 0.55030308\n",
            "Iteration 178, loss = 0.55019860\n",
            "Iteration 179, loss = 0.55021757\n",
            "Iteration 180, loss = 0.55028861\n",
            "Iteration 181, loss = 0.55024809\n",
            "Iteration 182, loss = 0.55022688\n",
            "Iteration 183, loss = 0.55017198\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 250 and for layer number 4 : 0.715\n",
            "Iteration 1, loss = 0.88336594\n",
            "Iteration 2, loss = 0.85371585\n",
            "Iteration 3, loss = 0.81114998\n",
            "Iteration 4, loss = 0.76878568\n",
            "Iteration 5, loss = 0.72891304\n",
            "Iteration 6, loss = 0.69812822\n",
            "Iteration 7, loss = 0.67784648\n",
            "Iteration 8, loss = 0.65873975\n",
            "Iteration 9, loss = 0.64585588\n",
            "Iteration 10, loss = 0.63646662\n",
            "Iteration 11, loss = 0.62908658\n",
            "Iteration 12, loss = 0.62375837\n",
            "Iteration 13, loss = 0.61915746\n",
            "Iteration 14, loss = 0.61496245\n",
            "Iteration 15, loss = 0.61163021\n",
            "Iteration 16, loss = 0.60865001\n",
            "Iteration 17, loss = 0.60613154\n",
            "Iteration 18, loss = 0.60378173\n",
            "Iteration 19, loss = 0.60170407\n",
            "Iteration 20, loss = 0.59968740\n",
            "Iteration 21, loss = 0.59787396\n",
            "Iteration 22, loss = 0.59617398\n",
            "Iteration 23, loss = 0.59435058\n",
            "Iteration 24, loss = 0.59263873\n",
            "Iteration 25, loss = 0.59096900\n",
            "Iteration 26, loss = 0.58936046\n",
            "Iteration 27, loss = 0.58778572\n",
            "Iteration 28, loss = 0.58636900\n",
            "Iteration 29, loss = 0.58490469\n",
            "Iteration 30, loss = 0.58355581\n",
            "Iteration 31, loss = 0.58232012\n",
            "Iteration 32, loss = 0.58092064\n",
            "Iteration 33, loss = 0.57974507\n",
            "Iteration 34, loss = 0.57855058\n",
            "Iteration 35, loss = 0.57753596\n",
            "Iteration 36, loss = 0.57642234\n",
            "Iteration 37, loss = 0.57550100\n",
            "Iteration 38, loss = 0.57458003\n",
            "Iteration 39, loss = 0.57363445\n",
            "Iteration 40, loss = 0.57283919\n",
            "Iteration 41, loss = 0.57182274\n",
            "Iteration 42, loss = 0.57091657\n",
            "Iteration 43, loss = 0.57011771\n",
            "Iteration 44, loss = 0.56937148\n",
            "Iteration 45, loss = 0.56858382\n",
            "Iteration 46, loss = 0.56782347\n",
            "Iteration 47, loss = 0.56719403\n",
            "Iteration 48, loss = 0.56660501\n",
            "Iteration 49, loss = 0.56591930\n",
            "Iteration 50, loss = 0.56508853\n",
            "Iteration 51, loss = 0.56449271\n",
            "Iteration 52, loss = 0.56368499\n",
            "Iteration 53, loss = 0.56297676\n",
            "Iteration 54, loss = 0.56236424\n",
            "Iteration 55, loss = 0.56186295\n",
            "Iteration 56, loss = 0.56108268\n",
            "Iteration 57, loss = 0.56055555\n",
            "Iteration 58, loss = 0.55999247\n",
            "Iteration 59, loss = 0.55958193\n",
            "Iteration 60, loss = 0.55901068\n",
            "Iteration 61, loss = 0.55863943\n",
            "Iteration 62, loss = 0.55821083\n",
            "Iteration 63, loss = 0.55788734\n",
            "Iteration 64, loss = 0.55735403\n",
            "Iteration 65, loss = 0.55691118\n",
            "Iteration 66, loss = 0.55652812\n",
            "Iteration 67, loss = 0.55606649\n",
            "Iteration 68, loss = 0.55577844\n",
            "Iteration 69, loss = 0.55533429\n",
            "Iteration 70, loss = 0.55504655\n",
            "Iteration 71, loss = 0.55474930\n",
            "Iteration 72, loss = 0.55424172\n",
            "Iteration 73, loss = 0.55390727\n",
            "Iteration 74, loss = 0.55356165\n",
            "Iteration 75, loss = 0.55313879\n",
            "Iteration 76, loss = 0.55269980\n",
            "Iteration 77, loss = 0.55238251\n",
            "Iteration 78, loss = 0.55204490\n",
            "Iteration 79, loss = 0.55161358\n",
            "Iteration 80, loss = 0.55135349\n",
            "Iteration 81, loss = 0.55094036\n",
            "Iteration 82, loss = 0.55070037\n",
            "Iteration 83, loss = 0.55039199\n",
            "Iteration 84, loss = 0.54990062\n",
            "Iteration 85, loss = 0.54951022\n",
            "Iteration 86, loss = 0.54923000\n",
            "Iteration 87, loss = 0.54888291\n",
            "Iteration 88, loss = 0.54852080\n",
            "Iteration 89, loss = 0.54819029\n",
            "Iteration 90, loss = 0.54788580\n",
            "Iteration 91, loss = 0.54753336\n",
            "Iteration 92, loss = 0.54723242\n",
            "Iteration 93, loss = 0.54696185\n",
            "Iteration 94, loss = 0.54663047\n",
            "Iteration 95, loss = 0.54642423\n",
            "Iteration 96, loss = 0.54612106\n",
            "Iteration 97, loss = 0.54588052\n",
            "Iteration 98, loss = 0.54566276\n",
            "Iteration 99, loss = 0.54547018\n",
            "Iteration 100, loss = 0.54522125\n",
            "Iteration 101, loss = 0.54497855\n",
            "Iteration 102, loss = 0.54484424\n",
            "Iteration 103, loss = 0.54454975\n",
            "Iteration 104, loss = 0.54432694\n",
            "Iteration 105, loss = 0.54405871\n",
            "Iteration 106, loss = 0.54400529\n",
            "Iteration 107, loss = 0.54365645\n",
            "Iteration 108, loss = 0.54342444\n",
            "Iteration 109, loss = 0.54319360\n",
            "Iteration 110, loss = 0.54296398\n",
            "Iteration 111, loss = 0.54259789\n",
            "Iteration 112, loss = 0.54246263\n",
            "Iteration 113, loss = 0.54222654\n",
            "Iteration 114, loss = 0.54204101\n",
            "Iteration 115, loss = 0.54189419\n",
            "Iteration 116, loss = 0.54166685\n",
            "Iteration 117, loss = 0.54157145\n",
            "Iteration 118, loss = 0.54127755\n",
            "Iteration 119, loss = 0.54110824\n",
            "Iteration 120, loss = 0.54091335\n",
            "Iteration 121, loss = 0.54080406\n",
            "Iteration 122, loss = 0.54057483\n",
            "Iteration 123, loss = 0.54047833\n",
            "Iteration 124, loss = 0.54036178\n",
            "Iteration 125, loss = 0.54025044\n",
            "Iteration 126, loss = 0.54020928\n",
            "Iteration 127, loss = 0.53992647\n",
            "Iteration 128, loss = 0.53984166\n",
            "Iteration 129, loss = 0.53959034\n",
            "Iteration 130, loss = 0.53946540\n",
            "Iteration 131, loss = 0.53925245\n",
            "Iteration 132, loss = 0.53923818\n",
            "Iteration 133, loss = 0.53914175\n",
            "Iteration 134, loss = 0.53886620\n",
            "Iteration 135, loss = 0.53879700\n",
            "Iteration 136, loss = 0.53871831\n",
            "Iteration 137, loss = 0.53870791\n",
            "Iteration 138, loss = 0.53862767\n",
            "Iteration 139, loss = 0.53858188\n",
            "Iteration 140, loss = 0.53852717\n",
            "Iteration 141, loss = 0.53850989\n",
            "Iteration 142, loss = 0.53834784\n",
            "Iteration 143, loss = 0.53825307\n",
            "Iteration 144, loss = 0.53806887\n",
            "Iteration 145, loss = 0.53794010\n",
            "Iteration 146, loss = 0.53774559\n",
            "Iteration 147, loss = 0.53758361\n",
            "Iteration 148, loss = 0.53737071\n",
            "Iteration 149, loss = 0.53726470\n",
            "Iteration 150, loss = 0.53734451\n",
            "Iteration 151, loss = 0.53726507\n",
            "Iteration 152, loss = 0.53731548\n",
            "Iteration 153, loss = 0.53736934\n",
            "Iteration 154, loss = 0.53745739\n",
            "Iteration 155, loss = 0.53725930\n",
            "Iteration 156, loss = 0.53704189\n",
            "Iteration 157, loss = 0.53672173\n",
            "Iteration 158, loss = 0.53673070\n",
            "Iteration 159, loss = 0.53640430\n",
            "Iteration 160, loss = 0.53636990\n",
            "Iteration 161, loss = 0.53624303\n",
            "Iteration 162, loss = 0.53638501\n",
            "Iteration 163, loss = 0.53618847\n",
            "Iteration 164, loss = 0.53614041\n",
            "Iteration 165, loss = 0.53609172\n",
            "Iteration 166, loss = 0.53597557\n",
            "Iteration 167, loss = 0.53581952\n",
            "Iteration 168, loss = 0.53576021\n",
            "Iteration 169, loss = 0.53565892\n",
            "Iteration 170, loss = 0.53548493\n",
            "Iteration 171, loss = 0.53525189\n",
            "Iteration 172, loss = 0.53522615\n",
            "Iteration 173, loss = 0.53491178\n",
            "Iteration 174, loss = 0.53476777\n",
            "Iteration 175, loss = 0.53479530\n",
            "Iteration 176, loss = 0.53470860\n",
            "Iteration 177, loss = 0.53470846\n",
            "Iteration 178, loss = 0.53456213\n",
            "Iteration 179, loss = 0.53433062\n",
            "Iteration 180, loss = 0.53415476\n",
            "Iteration 181, loss = 0.53397479\n",
            "Iteration 182, loss = 0.53395149\n",
            "Iteration 183, loss = 0.53400429\n",
            "Iteration 184, loss = 0.53388538\n",
            "Iteration 185, loss = 0.53376509\n",
            "Iteration 186, loss = 0.53369759\n",
            "Iteration 187, loss = 0.53354538\n",
            "Iteration 188, loss = 0.53325988\n",
            "Iteration 189, loss = 0.53317403\n",
            "Iteration 190, loss = 0.53309107\n",
            "Iteration 191, loss = 0.53300766\n",
            "Iteration 192, loss = 0.53301044\n",
            "Iteration 193, loss = 0.53292291\n",
            "Iteration 194, loss = 0.53275166\n",
            "Iteration 195, loss = 0.53278188\n",
            "Iteration 196, loss = 0.53277496\n",
            "Iteration 197, loss = 0.53271782\n",
            "Iteration 198, loss = 0.53257775\n",
            "Iteration 199, loss = 0.53244531\n",
            "Iteration 200, loss = 0.53229277\n",
            "Iteration 201, loss = 0.53213326\n",
            "Iteration 202, loss = 0.53212063\n",
            "Iteration 203, loss = 0.53201250\n",
            "Iteration 204, loss = 0.53185590\n",
            "Iteration 205, loss = 0.53174472\n",
            "Iteration 206, loss = 0.53171398\n",
            "Iteration 207, loss = 0.53204723\n",
            "Iteration 208, loss = 0.53183545\n",
            "Iteration 209, loss = 0.53177669\n",
            "Iteration 210, loss = 0.53164494\n",
            "Iteration 211, loss = 0.53147052\n",
            "Iteration 212, loss = 0.53129335\n",
            "Iteration 213, loss = 0.53115109\n",
            "Iteration 214, loss = 0.53098302\n",
            "Iteration 215, loss = 0.53080974\n",
            "Iteration 216, loss = 0.53068608\n",
            "Iteration 217, loss = 0.53076678\n",
            "Iteration 218, loss = 0.53057831\n",
            "Iteration 219, loss = 0.53054856\n",
            "Iteration 220, loss = 0.53045042\n",
            "Iteration 221, loss = 0.53048278\n",
            "Iteration 222, loss = 0.53033285\n",
            "Iteration 223, loss = 0.53028985\n",
            "Iteration 224, loss = 0.53000746\n",
            "Iteration 225, loss = 0.53012538\n",
            "Iteration 226, loss = 0.53007009\n",
            "Iteration 227, loss = 0.52998224\n",
            "Iteration 228, loss = 0.52989960\n",
            "Iteration 229, loss = 0.52978421\n",
            "Iteration 230, loss = 0.52979270\n",
            "Iteration 231, loss = 0.52970162\n",
            "Iteration 232, loss = 0.52965817\n",
            "Iteration 233, loss = 0.52955779\n",
            "Iteration 234, loss = 0.52954944\n",
            "Iteration 235, loss = 0.52934646\n",
            "Iteration 236, loss = 0.52928118\n",
            "Iteration 237, loss = 0.52925615\n",
            "Iteration 238, loss = 0.52913618\n",
            "Iteration 239, loss = 0.52909061\n",
            "Iteration 240, loss = 0.52898468\n",
            "Iteration 241, loss = 0.52889876\n",
            "Iteration 242, loss = 0.52890523\n",
            "Iteration 243, loss = 0.52873589\n",
            "Iteration 244, loss = 0.52861743\n",
            "Iteration 245, loss = 0.52869180\n",
            "Iteration 246, loss = 0.52870126\n",
            "Iteration 247, loss = 0.52862778\n",
            "Iteration 248, loss = 0.52859273\n",
            "Iteration 249, loss = 0.52846045\n",
            "Iteration 250, loss = 0.52832837\n",
            "Iteration 1, loss = 0.88215379\n",
            "Iteration 2, loss = 0.85222359\n",
            "Iteration 3, loss = 0.81083995\n",
            "Iteration 4, loss = 0.76741428\n",
            "Iteration 5, loss = 0.72754261\n",
            "Iteration 6, loss = 0.69573379\n",
            "Iteration 7, loss = 0.67445780\n",
            "Iteration 8, loss = 0.65527871\n",
            "Iteration 9, loss = 0.64178806\n",
            "Iteration 10, loss = 0.63239091\n",
            "Iteration 11, loss = 0.62427738\n",
            "Iteration 12, loss = 0.61862901\n",
            "Iteration 13, loss = 0.61370662\n",
            "Iteration 14, loss = 0.60941738\n",
            "Iteration 15, loss = 0.60603175\n",
            "Iteration 16, loss = 0.60286680\n",
            "Iteration 17, loss = 0.60003633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 18, loss = 0.59747356\n",
            "Iteration 19, loss = 0.59516755\n",
            "Iteration 20, loss = 0.59302637\n",
            "Iteration 21, loss = 0.59102069\n",
            "Iteration 22, loss = 0.58906081\n",
            "Iteration 23, loss = 0.58713216\n",
            "Iteration 24, loss = 0.58522686\n",
            "Iteration 25, loss = 0.58343551\n",
            "Iteration 26, loss = 0.58140821\n",
            "Iteration 27, loss = 0.57962191\n",
            "Iteration 28, loss = 0.57773768\n",
            "Iteration 29, loss = 0.57599283\n",
            "Iteration 30, loss = 0.57440911\n",
            "Iteration 31, loss = 0.57292679\n",
            "Iteration 32, loss = 0.57134210\n",
            "Iteration 33, loss = 0.57005743\n",
            "Iteration 34, loss = 0.56866518\n",
            "Iteration 35, loss = 0.56746551\n",
            "Iteration 36, loss = 0.56631215\n",
            "Iteration 37, loss = 0.56506011\n",
            "Iteration 38, loss = 0.56388150\n",
            "Iteration 39, loss = 0.56266819\n",
            "Iteration 40, loss = 0.56150018\n",
            "Iteration 41, loss = 0.56027643\n",
            "Iteration 42, loss = 0.55923365\n",
            "Iteration 43, loss = 0.55839916\n",
            "Iteration 44, loss = 0.55764037\n",
            "Iteration 45, loss = 0.55671588\n",
            "Iteration 46, loss = 0.55603532\n",
            "Iteration 47, loss = 0.55539730\n",
            "Iteration 48, loss = 0.55489305\n",
            "Iteration 49, loss = 0.55430102\n",
            "Iteration 50, loss = 0.55366618\n",
            "Iteration 51, loss = 0.55308823\n",
            "Iteration 52, loss = 0.55248338\n",
            "Iteration 53, loss = 0.55180195\n",
            "Iteration 54, loss = 0.55138032\n",
            "Iteration 55, loss = 0.55064507\n",
            "Iteration 56, loss = 0.55004513\n",
            "Iteration 57, loss = 0.54956125\n",
            "Iteration 58, loss = 0.54905056\n",
            "Iteration 59, loss = 0.54869579\n",
            "Iteration 60, loss = 0.54806567\n",
            "Iteration 61, loss = 0.54766645\n",
            "Iteration 62, loss = 0.54719837\n",
            "Iteration 63, loss = 0.54672621\n",
            "Iteration 64, loss = 0.54622130\n",
            "Iteration 65, loss = 0.54567083\n",
            "Iteration 66, loss = 0.54529271\n",
            "Iteration 67, loss = 0.54475088\n",
            "Iteration 68, loss = 0.54431846\n",
            "Iteration 69, loss = 0.54386110\n",
            "Iteration 70, loss = 0.54360986\n",
            "Iteration 71, loss = 0.54328716\n",
            "Iteration 72, loss = 0.54281576\n",
            "Iteration 73, loss = 0.54245138\n",
            "Iteration 74, loss = 0.54203901\n",
            "Iteration 75, loss = 0.54159343\n",
            "Iteration 76, loss = 0.54118515\n",
            "Iteration 77, loss = 0.54086813\n",
            "Iteration 78, loss = 0.54042066\n",
            "Iteration 79, loss = 0.53996870\n",
            "Iteration 80, loss = 0.53966351\n",
            "Iteration 81, loss = 0.53922575\n",
            "Iteration 82, loss = 0.53883012\n",
            "Iteration 83, loss = 0.53845198\n",
            "Iteration 84, loss = 0.53801889\n",
            "Iteration 85, loss = 0.53775136\n",
            "Iteration 86, loss = 0.53734075\n",
            "Iteration 87, loss = 0.53694553\n",
            "Iteration 88, loss = 0.53650020\n",
            "Iteration 89, loss = 0.53610993\n",
            "Iteration 90, loss = 0.53573081\n",
            "Iteration 91, loss = 0.53537613\n",
            "Iteration 92, loss = 0.53504234\n",
            "Iteration 93, loss = 0.53466072\n",
            "Iteration 94, loss = 0.53441641\n",
            "Iteration 95, loss = 0.53430791\n",
            "Iteration 96, loss = 0.53402009\n",
            "Iteration 97, loss = 0.53367372\n",
            "Iteration 98, loss = 0.53342543\n",
            "Iteration 99, loss = 0.53312309\n",
            "Iteration 100, loss = 0.53285196\n",
            "Iteration 101, loss = 0.53257083\n",
            "Iteration 102, loss = 0.53226454\n",
            "Iteration 103, loss = 0.53199796\n",
            "Iteration 104, loss = 0.53182985\n",
            "Iteration 105, loss = 0.53158153\n",
            "Iteration 106, loss = 0.53145871\n",
            "Iteration 107, loss = 0.53126859\n",
            "Iteration 108, loss = 0.53101534\n",
            "Iteration 109, loss = 0.53072079\n",
            "Iteration 110, loss = 0.53037952\n",
            "Iteration 111, loss = 0.53005599\n",
            "Iteration 112, loss = 0.52990832\n",
            "Iteration 113, loss = 0.52973400\n",
            "Iteration 114, loss = 0.52956077\n",
            "Iteration 115, loss = 0.52935312\n",
            "Iteration 116, loss = 0.52936955\n",
            "Iteration 117, loss = 0.52935374\n",
            "Iteration 118, loss = 0.52917633\n",
            "Iteration 119, loss = 0.52910919\n",
            "Iteration 120, loss = 0.52891145\n",
            "Iteration 121, loss = 0.52874484\n",
            "Iteration 122, loss = 0.52855772\n",
            "Iteration 123, loss = 0.52843582\n",
            "Iteration 124, loss = 0.52818377\n",
            "Iteration 125, loss = 0.52808388\n",
            "Iteration 126, loss = 0.52787556\n",
            "Iteration 127, loss = 0.52750644\n",
            "Iteration 128, loss = 0.52727445\n",
            "Iteration 129, loss = 0.52683233\n",
            "Iteration 130, loss = 0.52675250\n",
            "Iteration 131, loss = 0.52643472\n",
            "Iteration 132, loss = 0.52630503\n",
            "Iteration 133, loss = 0.52617970\n",
            "Iteration 134, loss = 0.52593767\n",
            "Iteration 135, loss = 0.52584011\n",
            "Iteration 136, loss = 0.52570856\n",
            "Iteration 137, loss = 0.52566357\n",
            "Iteration 138, loss = 0.52552836\n",
            "Iteration 139, loss = 0.52532887\n",
            "Iteration 140, loss = 0.52528387\n",
            "Iteration 141, loss = 0.52526329\n",
            "Iteration 142, loss = 0.52515066\n",
            "Iteration 143, loss = 0.52518009\n",
            "Iteration 144, loss = 0.52495126\n",
            "Iteration 145, loss = 0.52484589\n",
            "Iteration 146, loss = 0.52458742\n",
            "Iteration 147, loss = 0.52429887\n",
            "Iteration 148, loss = 0.52416796\n",
            "Iteration 149, loss = 0.52398389\n",
            "Iteration 150, loss = 0.52379132\n",
            "Iteration 151, loss = 0.52378260\n",
            "Iteration 152, loss = 0.52382418\n",
            "Iteration 153, loss = 0.52381207\n",
            "Iteration 154, loss = 0.52367169\n",
            "Iteration 155, loss = 0.52341721\n",
            "Iteration 156, loss = 0.52313430\n",
            "Iteration 157, loss = 0.52274374\n",
            "Iteration 158, loss = 0.52263986\n",
            "Iteration 159, loss = 0.52240583\n",
            "Iteration 160, loss = 0.52218727\n",
            "Iteration 161, loss = 0.52206031\n",
            "Iteration 162, loss = 0.52206735\n",
            "Iteration 163, loss = 0.52195280\n",
            "Iteration 164, loss = 0.52169895\n",
            "Iteration 165, loss = 0.52151962\n",
            "Iteration 166, loss = 0.52134267\n",
            "Iteration 167, loss = 0.52118841\n",
            "Iteration 168, loss = 0.52097498\n",
            "Iteration 169, loss = 0.52083776\n",
            "Iteration 170, loss = 0.52083916\n",
            "Iteration 171, loss = 0.52055082\n",
            "Iteration 172, loss = 0.52041256\n",
            "Iteration 173, loss = 0.52028835\n",
            "Iteration 174, loss = 0.52022625\n",
            "Iteration 175, loss = 0.52011479\n",
            "Iteration 176, loss = 0.51996176\n",
            "Iteration 177, loss = 0.51984145\n",
            "Iteration 178, loss = 0.51974168\n",
            "Iteration 179, loss = 0.51959673\n",
            "Iteration 180, loss = 0.51946965\n",
            "Iteration 181, loss = 0.51924536\n",
            "Iteration 182, loss = 0.51917335\n",
            "Iteration 183, loss = 0.51939132\n",
            "Iteration 184, loss = 0.51957364\n",
            "Iteration 185, loss = 0.51943820\n",
            "Iteration 186, loss = 0.51930438\n",
            "Iteration 187, loss = 0.51891515\n",
            "Iteration 188, loss = 0.51846847\n",
            "Iteration 189, loss = 0.51813664\n",
            "Iteration 190, loss = 0.51797320\n",
            "Iteration 191, loss = 0.51799348\n",
            "Iteration 192, loss = 0.51797720\n",
            "Iteration 193, loss = 0.51803809\n",
            "Iteration 194, loss = 0.51791912\n",
            "Iteration 195, loss = 0.51799471\n",
            "Iteration 196, loss = 0.51802324\n",
            "Iteration 197, loss = 0.51777781\n",
            "Iteration 198, loss = 0.51761784\n",
            "Iteration 199, loss = 0.51725642\n",
            "Iteration 200, loss = 0.51724369\n",
            "Iteration 201, loss = 0.51688696\n",
            "Iteration 202, loss = 0.51679669\n",
            "Iteration 203, loss = 0.51675581\n",
            "Iteration 204, loss = 0.51666116\n",
            "Iteration 205, loss = 0.51654872\n",
            "Iteration 206, loss = 0.51643186\n",
            "Iteration 207, loss = 0.51644236\n",
            "Iteration 208, loss = 0.51629244\n",
            "Iteration 209, loss = 0.51618996\n",
            "Iteration 210, loss = 0.51609730\n",
            "Iteration 211, loss = 0.51598946\n",
            "Iteration 212, loss = 0.51578870\n",
            "Iteration 213, loss = 0.51575118\n",
            "Iteration 214, loss = 0.51558837\n",
            "Iteration 215, loss = 0.51544474\n",
            "Iteration 216, loss = 0.51535430\n",
            "Iteration 217, loss = 0.51533390\n",
            "Iteration 218, loss = 0.51528561\n",
            "Iteration 219, loss = 0.51512506\n",
            "Iteration 220, loss = 0.51511275\n",
            "Iteration 221, loss = 0.51507526\n",
            "Iteration 222, loss = 0.51496055\n",
            "Iteration 223, loss = 0.51489354\n",
            "Iteration 224, loss = 0.51473825\n",
            "Iteration 225, loss = 0.51457782\n",
            "Iteration 226, loss = 0.51448005\n",
            "Iteration 227, loss = 0.51433387\n",
            "Iteration 228, loss = 0.51432520\n",
            "Iteration 229, loss = 0.51419252\n",
            "Iteration 230, loss = 0.51419397\n",
            "Iteration 231, loss = 0.51403650\n",
            "Iteration 232, loss = 0.51391581\n",
            "Iteration 233, loss = 0.51376595\n",
            "Iteration 234, loss = 0.51374215\n",
            "Iteration 235, loss = 0.51368587\n",
            "Iteration 236, loss = 0.51349172\n",
            "Iteration 237, loss = 0.51345908\n",
            "Iteration 238, loss = 0.51340787\n",
            "Iteration 239, loss = 0.51324562\n",
            "Iteration 240, loss = 0.51316124\n",
            "Iteration 241, loss = 0.51307134\n",
            "Iteration 242, loss = 0.51321412\n",
            "Iteration 243, loss = 0.51290519\n",
            "Iteration 244, loss = 0.51272906\n",
            "Iteration 245, loss = 0.51275360\n",
            "Iteration 246, loss = 0.51275097\n",
            "Iteration 247, loss = 0.51257302\n",
            "Iteration 248, loss = 0.51253504\n",
            "Iteration 249, loss = 0.51231600\n",
            "Iteration 250, loss = 0.51219491\n",
            "Iteration 1, loss = 0.88505012\n",
            "Iteration 2, loss = 0.85225471\n",
            "Iteration 3, loss = 0.80692705\n",
            "Iteration 4, loss = 0.76187066\n",
            "Iteration 5, loss = 0.72125962\n",
            "Iteration 6, loss = 0.69001636\n",
            "Iteration 7, loss = 0.66839879\n",
            "Iteration 8, loss = 0.65016370\n",
            "Iteration 9, loss = 0.63775462\n",
            "Iteration 10, loss = 0.62892995\n",
            "Iteration 11, loss = 0.62138454\n",
            "Iteration 12, loss = 0.61613777\n",
            "Iteration 13, loss = 0.61152308\n",
            "Iteration 14, loss = 0.60716429\n",
            "Iteration 15, loss = 0.60391754\n",
            "Iteration 16, loss = 0.60054268\n",
            "Iteration 17, loss = 0.59793934\n",
            "Iteration 18, loss = 0.59555825\n",
            "Iteration 19, loss = 0.59346251\n",
            "Iteration 20, loss = 0.59155139\n",
            "Iteration 21, loss = 0.58951223\n",
            "Iteration 22, loss = 0.58759183\n",
            "Iteration 23, loss = 0.58570953\n",
            "Iteration 24, loss = 0.58384280\n",
            "Iteration 25, loss = 0.58210152\n",
            "Iteration 26, loss = 0.58016830\n",
            "Iteration 27, loss = 0.57837726\n",
            "Iteration 28, loss = 0.57656482\n",
            "Iteration 29, loss = 0.57466052\n",
            "Iteration 30, loss = 0.57280573\n",
            "Iteration 31, loss = 0.57095358\n",
            "Iteration 32, loss = 0.56918098\n",
            "Iteration 33, loss = 0.56766041\n",
            "Iteration 34, loss = 0.56606900\n",
            "Iteration 35, loss = 0.56460579\n",
            "Iteration 36, loss = 0.56336991\n",
            "Iteration 37, loss = 0.56205355\n",
            "Iteration 38, loss = 0.56082783\n",
            "Iteration 39, loss = 0.55947813\n",
            "Iteration 40, loss = 0.55812098\n",
            "Iteration 41, loss = 0.55681742\n",
            "Iteration 42, loss = 0.55563897\n",
            "Iteration 43, loss = 0.55440444\n",
            "Iteration 44, loss = 0.55338679\n",
            "Iteration 45, loss = 0.55235047\n",
            "Iteration 46, loss = 0.55162732\n",
            "Iteration 47, loss = 0.55084266\n",
            "Iteration 48, loss = 0.55021080\n",
            "Iteration 49, loss = 0.54960170\n",
            "Iteration 50, loss = 0.54880901\n",
            "Iteration 51, loss = 0.54801736\n",
            "Iteration 52, loss = 0.54727104\n",
            "Iteration 53, loss = 0.54650042\n",
            "Iteration 54, loss = 0.54590923\n",
            "Iteration 55, loss = 0.54515730\n",
            "Iteration 56, loss = 0.54447942\n",
            "Iteration 57, loss = 0.54393684\n",
            "Iteration 58, loss = 0.54339839\n",
            "Iteration 59, loss = 0.54306135\n",
            "Iteration 60, loss = 0.54242856\n",
            "Iteration 61, loss = 0.54192597\n",
            "Iteration 62, loss = 0.54135690\n",
            "Iteration 63, loss = 0.54087972\n",
            "Iteration 64, loss = 0.54047878\n",
            "Iteration 65, loss = 0.53981990\n",
            "Iteration 66, loss = 0.53953724\n",
            "Iteration 67, loss = 0.53893992\n",
            "Iteration 68, loss = 0.53851477\n",
            "Iteration 69, loss = 0.53807490\n",
            "Iteration 70, loss = 0.53759873\n",
            "Iteration 71, loss = 0.53721866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 72, loss = 0.53669204\n",
            "Iteration 73, loss = 0.53617939\n",
            "Iteration 74, loss = 0.53568681\n",
            "Iteration 75, loss = 0.53513738\n",
            "Iteration 76, loss = 0.53456490\n",
            "Iteration 77, loss = 0.53422560\n",
            "Iteration 78, loss = 0.53376142\n",
            "Iteration 79, loss = 0.53334794\n",
            "Iteration 80, loss = 0.53301828\n",
            "Iteration 81, loss = 0.53262479\n",
            "Iteration 82, loss = 0.53209680\n",
            "Iteration 83, loss = 0.53182712\n",
            "Iteration 84, loss = 0.53126292\n",
            "Iteration 85, loss = 0.53094452\n",
            "Iteration 86, loss = 0.53046918\n",
            "Iteration 87, loss = 0.53009524\n",
            "Iteration 88, loss = 0.52966948\n",
            "Iteration 89, loss = 0.52938844\n",
            "Iteration 90, loss = 0.52908594\n",
            "Iteration 91, loss = 0.52882262\n",
            "Iteration 92, loss = 0.52844590\n",
            "Iteration 93, loss = 0.52818897\n",
            "Iteration 94, loss = 0.52793869\n",
            "Iteration 95, loss = 0.52779602\n",
            "Iteration 96, loss = 0.52731289\n",
            "Iteration 97, loss = 0.52709549\n",
            "Iteration 98, loss = 0.52691321\n",
            "Iteration 99, loss = 0.52654699\n",
            "Iteration 100, loss = 0.52629573\n",
            "Iteration 101, loss = 0.52601449\n",
            "Iteration 102, loss = 0.52594261\n",
            "Iteration 103, loss = 0.52562273\n",
            "Iteration 104, loss = 0.52541581\n",
            "Iteration 105, loss = 0.52523782\n",
            "Iteration 106, loss = 0.52512014\n",
            "Iteration 107, loss = 0.52499645\n",
            "Iteration 108, loss = 0.52478098\n",
            "Iteration 109, loss = 0.52445755\n",
            "Iteration 110, loss = 0.52428922\n",
            "Iteration 111, loss = 0.52412815\n",
            "Iteration 112, loss = 0.52393271\n",
            "Iteration 113, loss = 0.52375826\n",
            "Iteration 114, loss = 0.52350129\n",
            "Iteration 115, loss = 0.52330822\n",
            "Iteration 116, loss = 0.52333016\n",
            "Iteration 117, loss = 0.52320371\n",
            "Iteration 118, loss = 0.52315890\n",
            "Iteration 119, loss = 0.52315360\n",
            "Iteration 120, loss = 0.52334969\n",
            "Iteration 121, loss = 0.52307553\n",
            "Iteration 122, loss = 0.52281812\n",
            "Iteration 123, loss = 0.52249253\n",
            "Iteration 124, loss = 0.52219434\n",
            "Iteration 125, loss = 0.52201704\n",
            "Iteration 126, loss = 0.52175775\n",
            "Iteration 127, loss = 0.52149214\n",
            "Iteration 128, loss = 0.52131010\n",
            "Iteration 129, loss = 0.52120647\n",
            "Iteration 130, loss = 0.52107551\n",
            "Iteration 131, loss = 0.52088518\n",
            "Iteration 132, loss = 0.52069858\n",
            "Iteration 133, loss = 0.52066429\n",
            "Iteration 134, loss = 0.52056682\n",
            "Iteration 135, loss = 0.52044684\n",
            "Iteration 136, loss = 0.52035116\n",
            "Iteration 137, loss = 0.52039273\n",
            "Iteration 138, loss = 0.52031628\n",
            "Iteration 139, loss = 0.52016205\n",
            "Iteration 140, loss = 0.52018246\n",
            "Iteration 141, loss = 0.52019436\n",
            "Iteration 142, loss = 0.52010632\n",
            "Iteration 143, loss = 0.52026974\n",
            "Iteration 144, loss = 0.52010303\n",
            "Iteration 145, loss = 0.51996273\n",
            "Iteration 146, loss = 0.51972732\n",
            "Iteration 147, loss = 0.51938107\n",
            "Iteration 148, loss = 0.51915831\n",
            "Iteration 149, loss = 0.51880936\n",
            "Iteration 150, loss = 0.51883335\n",
            "Iteration 151, loss = 0.51890969\n",
            "Iteration 152, loss = 0.51903890\n",
            "Iteration 153, loss = 0.51904551\n",
            "Iteration 154, loss = 0.51885134\n",
            "Iteration 155, loss = 0.51868661\n",
            "Iteration 156, loss = 0.51862434\n",
            "Iteration 157, loss = 0.51832945\n",
            "Iteration 158, loss = 0.51823184\n",
            "Iteration 159, loss = 0.51809408\n",
            "Iteration 160, loss = 0.51797490\n",
            "Iteration 161, loss = 0.51798341\n",
            "Iteration 162, loss = 0.51796662\n",
            "Iteration 163, loss = 0.51788220\n",
            "Iteration 164, loss = 0.51773377\n",
            "Iteration 165, loss = 0.51762954\n",
            "Iteration 166, loss = 0.51757075\n",
            "Iteration 167, loss = 0.51758254\n",
            "Iteration 168, loss = 0.51731918\n",
            "Iteration 169, loss = 0.51733879\n",
            "Iteration 170, loss = 0.51757394\n",
            "Iteration 171, loss = 0.51728112\n",
            "Iteration 172, loss = 0.51717302\n",
            "Iteration 173, loss = 0.51709239\n",
            "Iteration 174, loss = 0.51688080\n",
            "Iteration 175, loss = 0.51693992\n",
            "Iteration 176, loss = 0.51677494\n",
            "Iteration 177, loss = 0.51658956\n",
            "Iteration 178, loss = 0.51642716\n",
            "Iteration 179, loss = 0.51634552\n",
            "Iteration 180, loss = 0.51637389\n",
            "Iteration 181, loss = 0.51619640\n",
            "Iteration 182, loss = 0.51612988\n",
            "Iteration 183, loss = 0.51621726\n",
            "Iteration 184, loss = 0.51622963\n",
            "Iteration 185, loss = 0.51610881\n",
            "Iteration 186, loss = 0.51599573\n",
            "Iteration 187, loss = 0.51587262\n",
            "Iteration 188, loss = 0.51571481\n",
            "Iteration 189, loss = 0.51569069\n",
            "Iteration 190, loss = 0.51563333\n",
            "Iteration 191, loss = 0.51556212\n",
            "Iteration 192, loss = 0.51552936\n",
            "Iteration 193, loss = 0.51558506\n",
            "Iteration 194, loss = 0.51561893\n",
            "Iteration 195, loss = 0.51557442\n",
            "Iteration 196, loss = 0.51545020\n",
            "Iteration 197, loss = 0.51531570\n",
            "Iteration 198, loss = 0.51522442\n",
            "Iteration 199, loss = 0.51512491\n",
            "Iteration 200, loss = 0.51526171\n",
            "Iteration 201, loss = 0.51514591\n",
            "Iteration 202, loss = 0.51494589\n",
            "Iteration 203, loss = 0.51480648\n",
            "Iteration 204, loss = 0.51468102\n",
            "Iteration 205, loss = 0.51462081\n",
            "Iteration 206, loss = 0.51449865\n",
            "Iteration 207, loss = 0.51451011\n",
            "Iteration 208, loss = 0.51435097\n",
            "Iteration 209, loss = 0.51433386\n",
            "Iteration 210, loss = 0.51431180\n",
            "Iteration 211, loss = 0.51416676\n",
            "Iteration 212, loss = 0.51405276\n",
            "Iteration 213, loss = 0.51396315\n",
            "Iteration 214, loss = 0.51391871\n",
            "Iteration 215, loss = 0.51377618\n",
            "Iteration 216, loss = 0.51372040\n",
            "Iteration 217, loss = 0.51381369\n",
            "Iteration 218, loss = 0.51370441\n",
            "Iteration 219, loss = 0.51373413\n",
            "Iteration 220, loss = 0.51381474\n",
            "Iteration 221, loss = 0.51371195\n",
            "Iteration 222, loss = 0.51379448\n",
            "Iteration 223, loss = 0.51377025\n",
            "Iteration 224, loss = 0.51369884\n",
            "Iteration 225, loss = 0.51355113\n",
            "Iteration 226, loss = 0.51343129\n",
            "Iteration 227, loss = 0.51341078\n",
            "Iteration 228, loss = 0.51346999\n",
            "Iteration 229, loss = 0.51319873\n",
            "Iteration 230, loss = 0.51328783\n",
            "Iteration 231, loss = 0.51303335\n",
            "Iteration 232, loss = 0.51279425\n",
            "Iteration 233, loss = 0.51274634\n",
            "Iteration 234, loss = 0.51252801\n",
            "Iteration 235, loss = 0.51256839\n",
            "Iteration 236, loss = 0.51227336\n",
            "Iteration 237, loss = 0.51214446\n",
            "Iteration 238, loss = 0.51212889\n",
            "Iteration 239, loss = 0.51198389\n",
            "Iteration 240, loss = 0.51194642\n",
            "Iteration 241, loss = 0.51187421\n",
            "Iteration 242, loss = 0.51219423\n",
            "Iteration 243, loss = 0.51183866\n",
            "Iteration 244, loss = 0.51170803\n",
            "Iteration 245, loss = 0.51170143\n",
            "Iteration 246, loss = 0.51163586\n",
            "Iteration 247, loss = 0.51172269\n",
            "Iteration 248, loss = 0.51162813\n",
            "Iteration 249, loss = 0.51142180\n",
            "Iteration 250, loss = 0.51129644\n",
            "Iteration 1, loss = 0.87868239\n",
            "Iteration 2, loss = 0.84534527\n",
            "Iteration 3, loss = 0.80264634\n",
            "Iteration 4, loss = 0.75856716\n",
            "Iteration 5, loss = 0.72047810\n",
            "Iteration 6, loss = 0.69020470\n",
            "Iteration 7, loss = 0.66899651\n",
            "Iteration 8, loss = 0.65325101\n",
            "Iteration 9, loss = 0.64084348\n",
            "Iteration 10, loss = 0.63256251\n",
            "Iteration 11, loss = 0.62593344\n",
            "Iteration 12, loss = 0.62085925\n",
            "Iteration 13, loss = 0.61668379\n",
            "Iteration 14, loss = 0.61298618\n",
            "Iteration 15, loss = 0.61027972\n",
            "Iteration 16, loss = 0.60738093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 17, loss = 0.60519015\n",
            "Iteration 18, loss = 0.60314661\n",
            "Iteration 19, loss = 0.60138211\n",
            "Iteration 20, loss = 0.59990388\n",
            "Iteration 21, loss = 0.59816570\n",
            "Iteration 22, loss = 0.59667437\n",
            "Iteration 23, loss = 0.59523266\n",
            "Iteration 24, loss = 0.59383865\n",
            "Iteration 25, loss = 0.59244065\n",
            "Iteration 26, loss = 0.59109045\n",
            "Iteration 27, loss = 0.58969415\n",
            "Iteration 28, loss = 0.58839863\n",
            "Iteration 29, loss = 0.58706673\n",
            "Iteration 30, loss = 0.58587255\n",
            "Iteration 31, loss = 0.58455172\n",
            "Iteration 32, loss = 0.58331959\n",
            "Iteration 33, loss = 0.58212022\n",
            "Iteration 34, loss = 0.58094719\n",
            "Iteration 35, loss = 0.57976950\n",
            "Iteration 36, loss = 0.57873844\n",
            "Iteration 37, loss = 0.57759582\n",
            "Iteration 38, loss = 0.57646096\n",
            "Iteration 39, loss = 0.57532754\n",
            "Iteration 40, loss = 0.57432229\n",
            "Iteration 41, loss = 0.57318304\n",
            "Iteration 42, loss = 0.57217843\n",
            "Iteration 43, loss = 0.57098995\n",
            "Iteration 44, loss = 0.56998664\n",
            "Iteration 45, loss = 0.56894697\n",
            "Iteration 46, loss = 0.56822469\n",
            "Iteration 47, loss = 0.56721476\n",
            "Iteration 48, loss = 0.56646854\n",
            "Iteration 49, loss = 0.56570712\n",
            "Iteration 50, loss = 0.56480305\n",
            "Iteration 51, loss = 0.56388092\n",
            "Iteration 52, loss = 0.56300559\n",
            "Iteration 53, loss = 0.56215389\n",
            "Iteration 54, loss = 0.56146918\n",
            "Iteration 55, loss = 0.56066012\n",
            "Iteration 56, loss = 0.56010518\n",
            "Iteration 57, loss = 0.55936017\n",
            "Iteration 58, loss = 0.55867960\n",
            "Iteration 59, loss = 0.55806008\n",
            "Iteration 60, loss = 0.55745688\n",
            "Iteration 61, loss = 0.55683435\n",
            "Iteration 62, loss = 0.55617745\n",
            "Iteration 63, loss = 0.55567840\n",
            "Iteration 64, loss = 0.55522448\n",
            "Iteration 65, loss = 0.55451282\n",
            "Iteration 66, loss = 0.55408871\n",
            "Iteration 67, loss = 0.55340767\n",
            "Iteration 68, loss = 0.55308825\n",
            "Iteration 69, loss = 0.55248680\n",
            "Iteration 70, loss = 0.55197835\n",
            "Iteration 71, loss = 0.55151704\n",
            "Iteration 72, loss = 0.55094640\n",
            "Iteration 73, loss = 0.55043214\n",
            "Iteration 74, loss = 0.54999353\n",
            "Iteration 75, loss = 0.54958755\n",
            "Iteration 76, loss = 0.54921556\n",
            "Iteration 77, loss = 0.54881768\n",
            "Iteration 78, loss = 0.54841853\n",
            "Iteration 79, loss = 0.54806308\n",
            "Iteration 80, loss = 0.54755768\n",
            "Iteration 81, loss = 0.54715637\n",
            "Iteration 82, loss = 0.54664993\n",
            "Iteration 83, loss = 0.54633178\n",
            "Iteration 84, loss = 0.54586308\n",
            "Iteration 85, loss = 0.54531629\n",
            "Iteration 86, loss = 0.54479784\n",
            "Iteration 87, loss = 0.54431345\n",
            "Iteration 88, loss = 0.54395825\n",
            "Iteration 89, loss = 0.54347640\n",
            "Iteration 90, loss = 0.54309495\n",
            "Iteration 91, loss = 0.54277762\n",
            "Iteration 92, loss = 0.54228906\n",
            "Iteration 93, loss = 0.54182477\n",
            "Iteration 94, loss = 0.54136635\n",
            "Iteration 95, loss = 0.54103392\n",
            "Iteration 96, loss = 0.54055500\n",
            "Iteration 97, loss = 0.54020524\n",
            "Iteration 98, loss = 0.53990612\n",
            "Iteration 99, loss = 0.53946956\n",
            "Iteration 100, loss = 0.53922342\n",
            "Iteration 101, loss = 0.53886199\n",
            "Iteration 102, loss = 0.53863403\n",
            "Iteration 103, loss = 0.53835363\n",
            "Iteration 104, loss = 0.53807245\n",
            "Iteration 105, loss = 0.53787585\n",
            "Iteration 106, loss = 0.53772048\n",
            "Iteration 107, loss = 0.53739586\n",
            "Iteration 108, loss = 0.53703830\n",
            "Iteration 109, loss = 0.53674683\n",
            "Iteration 110, loss = 0.53654103\n",
            "Iteration 111, loss = 0.53635130\n",
            "Iteration 112, loss = 0.53605854\n",
            "Iteration 113, loss = 0.53586204\n",
            "Iteration 114, loss = 0.53548591\n",
            "Iteration 115, loss = 0.53527808\n",
            "Iteration 116, loss = 0.53506597\n",
            "Iteration 117, loss = 0.53496093\n",
            "Iteration 118, loss = 0.53466493\n",
            "Iteration 119, loss = 0.53449262\n",
            "Iteration 120, loss = 0.53439704\n",
            "Iteration 121, loss = 0.53412809\n",
            "Iteration 122, loss = 0.53385286\n",
            "Iteration 123, loss = 0.53358210\n",
            "Iteration 124, loss = 0.53332720\n",
            "Iteration 125, loss = 0.53317083\n",
            "Iteration 126, loss = 0.53286274\n",
            "Iteration 127, loss = 0.53277989\n",
            "Iteration 128, loss = 0.53248450\n",
            "Iteration 129, loss = 0.53231624\n",
            "Iteration 130, loss = 0.53223747\n",
            "Iteration 131, loss = 0.53207475\n",
            "Iteration 132, loss = 0.53183436\n",
            "Iteration 133, loss = 0.53166183\n",
            "Iteration 134, loss = 0.53146902\n",
            "Iteration 135, loss = 0.53131621\n",
            "Iteration 136, loss = 0.53113035\n",
            "Iteration 137, loss = 0.53099015\n",
            "Iteration 138, loss = 0.53085964\n",
            "Iteration 139, loss = 0.53063896\n",
            "Iteration 140, loss = 0.53049519\n",
            "Iteration 141, loss = 0.53028788\n",
            "Iteration 142, loss = 0.53009605\n",
            "Iteration 143, loss = 0.53001063\n",
            "Iteration 144, loss = 0.52968939\n",
            "Iteration 145, loss = 0.52955078\n",
            "Iteration 146, loss = 0.52932441\n",
            "Iteration 147, loss = 0.52903402\n",
            "Iteration 148, loss = 0.52892468\n",
            "Iteration 149, loss = 0.52868529\n",
            "Iteration 150, loss = 0.52853744\n",
            "Iteration 151, loss = 0.52837997\n",
            "Iteration 152, loss = 0.52821335\n",
            "Iteration 153, loss = 0.52816267\n",
            "Iteration 154, loss = 0.52807413\n",
            "Iteration 155, loss = 0.52796213\n",
            "Iteration 156, loss = 0.52782152\n",
            "Iteration 157, loss = 0.52746300\n",
            "Iteration 158, loss = 0.52737167\n",
            "Iteration 159, loss = 0.52717787\n",
            "Iteration 160, loss = 0.52705787\n",
            "Iteration 161, loss = 0.52695369\n",
            "Iteration 162, loss = 0.52680928\n",
            "Iteration 163, loss = 0.52659861\n",
            "Iteration 164, loss = 0.52651859\n",
            "Iteration 165, loss = 0.52635064\n",
            "Iteration 166, loss = 0.52624605\n",
            "Iteration 167, loss = 0.52621231\n",
            "Iteration 168, loss = 0.52607817\n",
            "Iteration 169, loss = 0.52594390\n",
            "Iteration 170, loss = 0.52606914\n",
            "Iteration 171, loss = 0.52601254\n",
            "Iteration 172, loss = 0.52575548\n",
            "Iteration 173, loss = 0.52557508\n",
            "Iteration 174, loss = 0.52543777\n",
            "Iteration 175, loss = 0.52531886\n",
            "Iteration 176, loss = 0.52521512\n",
            "Iteration 177, loss = 0.52508019\n",
            "Iteration 178, loss = 0.52499191\n",
            "Iteration 179, loss = 0.52490653\n",
            "Iteration 180, loss = 0.52478525\n",
            "Iteration 181, loss = 0.52471523\n",
            "Iteration 182, loss = 0.52462331\n",
            "Iteration 183, loss = 0.52458967\n",
            "Iteration 184, loss = 0.52459366\n",
            "Iteration 185, loss = 0.52449418\n",
            "Iteration 186, loss = 0.52445561\n",
            "Iteration 187, loss = 0.52407186\n",
            "Iteration 188, loss = 0.52407868\n",
            "Iteration 189, loss = 0.52398571\n",
            "Iteration 190, loss = 0.52396216\n",
            "Iteration 191, loss = 0.52382399\n",
            "Iteration 192, loss = 0.52370536\n",
            "Iteration 193, loss = 0.52360539\n",
            "Iteration 194, loss = 0.52357143\n",
            "Iteration 195, loss = 0.52360753\n",
            "Iteration 196, loss = 0.52332614\n",
            "Iteration 197, loss = 0.52319019\n",
            "Iteration 198, loss = 0.52307208\n",
            "Iteration 199, loss = 0.52295488\n",
            "Iteration 200, loss = 0.52297903\n",
            "Iteration 201, loss = 0.52297022\n",
            "Iteration 202, loss = 0.52289162\n",
            "Iteration 203, loss = 0.52276278\n",
            "Iteration 204, loss = 0.52261590\n",
            "Iteration 205, loss = 0.52260193\n",
            "Iteration 206, loss = 0.52235518\n",
            "Iteration 207, loss = 0.52226789\n",
            "Iteration 208, loss = 0.52210204\n",
            "Iteration 209, loss = 0.52196443\n",
            "Iteration 210, loss = 0.52195129\n",
            "Iteration 211, loss = 0.52175999\n",
            "Iteration 212, loss = 0.52167043\n",
            "Iteration 213, loss = 0.52148270\n",
            "Iteration 214, loss = 0.52150482\n",
            "Iteration 215, loss = 0.52131993\n",
            "Iteration 216, loss = 0.52119329\n",
            "Iteration 217, loss = 0.52108092\n",
            "Iteration 218, loss = 0.52103191\n",
            "Iteration 219, loss = 0.52076041\n",
            "Iteration 220, loss = 0.52087099\n",
            "Iteration 221, loss = 0.52066007\n",
            "Iteration 222, loss = 0.52056006\n",
            "Iteration 223, loss = 0.52042907\n",
            "Iteration 224, loss = 0.52024630\n",
            "Iteration 225, loss = 0.52012915\n",
            "Iteration 226, loss = 0.52001052\n",
            "Iteration 227, loss = 0.51991243\n",
            "Iteration 228, loss = 0.51984232\n",
            "Iteration 229, loss = 0.51984053\n",
            "Iteration 230, loss = 0.51974766\n",
            "Iteration 231, loss = 0.51970801\n",
            "Iteration 232, loss = 0.51939399\n",
            "Iteration 233, loss = 0.51924295\n",
            "Iteration 234, loss = 0.51921326\n",
            "Iteration 235, loss = 0.51902547\n",
            "Iteration 236, loss = 0.51896353\n",
            "Iteration 237, loss = 0.51884808\n",
            "Iteration 238, loss = 0.51872893\n",
            "Iteration 239, loss = 0.51864319\n",
            "Iteration 240, loss = 0.51865475\n",
            "Iteration 241, loss = 0.51851863\n",
            "Iteration 242, loss = 0.51859590\n",
            "Iteration 243, loss = 0.51823485\n",
            "Iteration 244, loss = 0.51813384\n",
            "Iteration 245, loss = 0.51813165\n",
            "Iteration 246, loss = 0.51809625\n",
            "Iteration 247, loss = 0.51819446\n",
            "Iteration 248, loss = 0.51815268\n",
            "Iteration 249, loss = 0.51794913\n",
            "Iteration 250, loss = 0.51790686\n",
            "Iteration 1, loss = 0.88420876\n",
            "Iteration 2, loss = 0.85029778\n",
            "Iteration 3, loss = 0.80666346\n",
            "Iteration 4, loss = 0.76251740\n",
            "Iteration 5, loss = 0.72239930\n",
            "Iteration 6, loss = 0.69102003\n",
            "Iteration 7, loss = 0.66914065\n",
            "Iteration 8, loss = 0.65366285\n",
            "Iteration 9, loss = 0.64129681\n",
            "Iteration 10, loss = 0.63364634\n",
            "Iteration 11, loss = 0.62684979\n",
            "Iteration 12, loss = 0.62195144\n",
            "Iteration 13, loss = 0.61783002\n",
            "Iteration 14, loss = 0.61387722\n",
            "Iteration 15, loss = 0.61079756\n",
            "Iteration 16, loss = 0.60786781\n",
            "Iteration 17, loss = 0.60543311\n",
            "Iteration 18, loss = 0.60300274\n",
            "Iteration 19, loss = 0.60112791\n",
            "Iteration 20, loss = 0.59925533\n",
            "Iteration 21, loss = 0.59731542\n",
            "Iteration 22, loss = 0.59563197\n",
            "Iteration 23, loss = 0.59403794\n",
            "Iteration 24, loss = 0.59252695\n",
            "Iteration 25, loss = 0.59087718\n",
            "Iteration 26, loss = 0.58932431\n",
            "Iteration 27, loss = 0.58783735\n",
            "Iteration 28, loss = 0.58652909\n",
            "Iteration 29, loss = 0.58526168\n",
            "Iteration 30, loss = 0.58384581\n",
            "Iteration 31, loss = 0.58249022\n",
            "Iteration 32, loss = 0.58107302\n",
            "Iteration 33, loss = 0.57968523\n",
            "Iteration 34, loss = 0.57826974\n",
            "Iteration 35, loss = 0.57685409\n",
            "Iteration 36, loss = 0.57570653\n",
            "Iteration 37, loss = 0.57454359\n",
            "Iteration 38, loss = 0.57337072\n",
            "Iteration 39, loss = 0.57236750\n",
            "Iteration 40, loss = 0.57151315\n",
            "Iteration 41, loss = 0.57056749\n",
            "Iteration 42, loss = 0.56961456\n",
            "Iteration 43, loss = 0.56866093\n",
            "Iteration 44, loss = 0.56779404\n",
            "Iteration 45, loss = 0.56702039\n",
            "Iteration 46, loss = 0.56636969\n",
            "Iteration 47, loss = 0.56567263\n",
            "Iteration 48, loss = 0.56506709\n",
            "Iteration 49, loss = 0.56456720\n",
            "Iteration 50, loss = 0.56395569\n",
            "Iteration 51, loss = 0.56323524\n",
            "Iteration 52, loss = 0.56254141\n",
            "Iteration 53, loss = 0.56184053\n",
            "Iteration 54, loss = 0.56130169\n",
            "Iteration 55, loss = 0.56068746\n",
            "Iteration 56, loss = 0.56027446\n",
            "Iteration 57, loss = 0.55964821\n",
            "Iteration 58, loss = 0.55911947\n",
            "Iteration 59, loss = 0.55863265\n",
            "Iteration 60, loss = 0.55817650\n",
            "Iteration 61, loss = 0.55778245\n",
            "Iteration 62, loss = 0.55727190\n",
            "Iteration 63, loss = 0.55689895\n",
            "Iteration 64, loss = 0.55664518\n",
            "Iteration 65, loss = 0.55617918\n",
            "Iteration 66, loss = 0.55581549\n",
            "Iteration 67, loss = 0.55536771\n",
            "Iteration 68, loss = 0.55518132\n",
            "Iteration 69, loss = 0.55482123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 70, loss = 0.55467827\n",
            "Iteration 71, loss = 0.55436780\n",
            "Iteration 72, loss = 0.55404453\n",
            "Iteration 73, loss = 0.55373464\n",
            "Iteration 74, loss = 0.55344119\n",
            "Iteration 75, loss = 0.55320977\n",
            "Iteration 76, loss = 0.55299683\n",
            "Iteration 77, loss = 0.55278411\n",
            "Iteration 78, loss = 0.55251193\n",
            "Iteration 79, loss = 0.55228212\n",
            "Iteration 80, loss = 0.55204654\n",
            "Iteration 81, loss = 0.55182542\n",
            "Iteration 82, loss = 0.55150055\n",
            "Iteration 83, loss = 0.55129268\n",
            "Iteration 84, loss = 0.55103820\n",
            "Iteration 85, loss = 0.55075821\n",
            "Iteration 86, loss = 0.55045633\n",
            "Iteration 87, loss = 0.55023268\n",
            "Iteration 88, loss = 0.55010605\n",
            "Iteration 89, loss = 0.54982742\n",
            "Iteration 90, loss = 0.54971483\n",
            "Iteration 91, loss = 0.54972757\n",
            "Iteration 92, loss = 0.54939568\n",
            "Iteration 93, loss = 0.54898982\n",
            "Iteration 94, loss = 0.54881902\n",
            "Iteration 95, loss = 0.54858255\n",
            "Iteration 96, loss = 0.54831362\n",
            "Iteration 97, loss = 0.54811557\n",
            "Iteration 98, loss = 0.54799809\n",
            "Iteration 99, loss = 0.54778299\n",
            "Iteration 100, loss = 0.54765785\n",
            "Iteration 101, loss = 0.54740884\n",
            "Iteration 102, loss = 0.54725663\n",
            "Iteration 103, loss = 0.54706772\n",
            "Iteration 104, loss = 0.54692220\n",
            "Iteration 105, loss = 0.54679752\n",
            "Iteration 106, loss = 0.54662427\n",
            "Iteration 107, loss = 0.54639225\n",
            "Iteration 108, loss = 0.54621099\n",
            "Iteration 109, loss = 0.54610797\n",
            "Iteration 110, loss = 0.54593550\n",
            "Iteration 111, loss = 0.54586041\n",
            "Iteration 112, loss = 0.54564492\n",
            "Iteration 113, loss = 0.54554842\n",
            "Iteration 114, loss = 0.54546008\n",
            "Iteration 115, loss = 0.54524899\n",
            "Iteration 116, loss = 0.54515192\n",
            "Iteration 117, loss = 0.54505288\n",
            "Iteration 118, loss = 0.54479587\n",
            "Iteration 119, loss = 0.54463250\n",
            "Iteration 120, loss = 0.54452734\n",
            "Iteration 121, loss = 0.54425207\n",
            "Iteration 122, loss = 0.54408486\n",
            "Iteration 123, loss = 0.54394875\n",
            "Iteration 124, loss = 0.54371186\n",
            "Iteration 125, loss = 0.54363433\n",
            "Iteration 126, loss = 0.54331513\n",
            "Iteration 127, loss = 0.54320696\n",
            "Iteration 128, loss = 0.54298310\n",
            "Iteration 129, loss = 0.54284629\n",
            "Iteration 130, loss = 0.54268682\n",
            "Iteration 131, loss = 0.54248868\n",
            "Iteration 132, loss = 0.54230650\n",
            "Iteration 133, loss = 0.54214827\n",
            "Iteration 134, loss = 0.54197261\n",
            "Iteration 135, loss = 0.54181534\n",
            "Iteration 136, loss = 0.54167607\n",
            "Iteration 137, loss = 0.54167254\n",
            "Iteration 138, loss = 0.54154016\n",
            "Iteration 139, loss = 0.54135835\n",
            "Iteration 140, loss = 0.54119593\n",
            "Iteration 141, loss = 0.54097055\n",
            "Iteration 142, loss = 0.54079159\n",
            "Iteration 143, loss = 0.54062195\n",
            "Iteration 144, loss = 0.54035561\n",
            "Iteration 145, loss = 0.54022586\n",
            "Iteration 146, loss = 0.54003420\n",
            "Iteration 147, loss = 0.53985661\n",
            "Iteration 148, loss = 0.53970262\n",
            "Iteration 149, loss = 0.53956974\n",
            "Iteration 150, loss = 0.53951964\n",
            "Iteration 151, loss = 0.53938195\n",
            "Iteration 152, loss = 0.53918085\n",
            "Iteration 153, loss = 0.53913227\n",
            "Iteration 154, loss = 0.53897941\n",
            "Iteration 155, loss = 0.53883506\n",
            "Iteration 156, loss = 0.53863939\n",
            "Iteration 157, loss = 0.53846476\n",
            "Iteration 158, loss = 0.53820396\n",
            "Iteration 159, loss = 0.53802165\n",
            "Iteration 160, loss = 0.53787899\n",
            "Iteration 161, loss = 0.53772227\n",
            "Iteration 162, loss = 0.53753486\n",
            "Iteration 163, loss = 0.53725080\n",
            "Iteration 164, loss = 0.53711853\n",
            "Iteration 165, loss = 0.53691431\n",
            "Iteration 166, loss = 0.53675786\n",
            "Iteration 167, loss = 0.53657283\n",
            "Iteration 168, loss = 0.53640559\n",
            "Iteration 169, loss = 0.53626383\n",
            "Iteration 170, loss = 0.53634432\n",
            "Iteration 171, loss = 0.53637854\n",
            "Iteration 172, loss = 0.53625644\n",
            "Iteration 173, loss = 0.53608987\n",
            "Iteration 174, loss = 0.53568733\n",
            "Iteration 175, loss = 0.53551559\n",
            "Iteration 176, loss = 0.53531452\n",
            "Iteration 177, loss = 0.53505349\n",
            "Iteration 178, loss = 0.53490349\n",
            "Iteration 179, loss = 0.53485469\n",
            "Iteration 180, loss = 0.53470325\n",
            "Iteration 181, loss = 0.53455529\n",
            "Iteration 182, loss = 0.53447901\n",
            "Iteration 183, loss = 0.53461682\n",
            "Iteration 184, loss = 0.53429817\n",
            "Iteration 185, loss = 0.53407708\n",
            "Iteration 186, loss = 0.53409191\n",
            "Iteration 187, loss = 0.53385466\n",
            "Iteration 188, loss = 0.53385963\n",
            "Iteration 189, loss = 0.53368813\n",
            "Iteration 190, loss = 0.53366462\n",
            "Iteration 191, loss = 0.53347692\n",
            "Iteration 192, loss = 0.53331859\n",
            "Iteration 193, loss = 0.53321726\n",
            "Iteration 194, loss = 0.53314220\n",
            "Iteration 195, loss = 0.53319821\n",
            "Iteration 196, loss = 0.53292876\n",
            "Iteration 197, loss = 0.53280856\n",
            "Iteration 198, loss = 0.53262628\n",
            "Iteration 199, loss = 0.53247102\n",
            "Iteration 200, loss = 0.53240201\n",
            "Iteration 201, loss = 0.53238039\n",
            "Iteration 202, loss = 0.53219271\n",
            "Iteration 203, loss = 0.53208315\n",
            "Iteration 204, loss = 0.53183332\n",
            "Iteration 205, loss = 0.53187664\n",
            "Iteration 206, loss = 0.53168833\n",
            "Iteration 207, loss = 0.53161972\n",
            "Iteration 208, loss = 0.53155096\n",
            "Iteration 209, loss = 0.53138165\n",
            "Iteration 210, loss = 0.53129869\n",
            "Iteration 211, loss = 0.53113071\n",
            "Iteration 212, loss = 0.53099854\n",
            "Iteration 213, loss = 0.53083248\n",
            "Iteration 214, loss = 0.53079774\n",
            "Iteration 215, loss = 0.53059313\n",
            "Iteration 216, loss = 0.53045465\n",
            "Iteration 217, loss = 0.53030120\n",
            "Iteration 218, loss = 0.53020122\n",
            "Iteration 219, loss = 0.53005106\n",
            "Iteration 220, loss = 0.53010932\n",
            "Iteration 221, loss = 0.52994113\n",
            "Iteration 222, loss = 0.52982408\n",
            "Iteration 223, loss = 0.52963812\n",
            "Iteration 224, loss = 0.52940410\n",
            "Iteration 225, loss = 0.52944430\n",
            "Iteration 226, loss = 0.52925055\n",
            "Iteration 227, loss = 0.52899594\n",
            "Iteration 228, loss = 0.52889031\n",
            "Iteration 229, loss = 0.52880185\n",
            "Iteration 230, loss = 0.52862304\n",
            "Iteration 231, loss = 0.52844945\n",
            "Iteration 232, loss = 0.52821052\n",
            "Iteration 233, loss = 0.52801636\n",
            "Iteration 234, loss = 0.52791865\n",
            "Iteration 235, loss = 0.52787721\n",
            "Iteration 236, loss = 0.52774973\n",
            "Iteration 237, loss = 0.52759462\n",
            "Iteration 238, loss = 0.52738051\n",
            "Iteration 239, loss = 0.52722010\n",
            "Iteration 240, loss = 0.52714430\n",
            "Iteration 241, loss = 0.52698203\n",
            "Iteration 242, loss = 0.52689261\n",
            "Iteration 243, loss = 0.52664368\n",
            "Iteration 244, loss = 0.52647917\n",
            "Iteration 245, loss = 0.52633966\n",
            "Iteration 246, loss = 0.52624680\n",
            "Iteration 247, loss = 0.52630392\n",
            "Iteration 248, loss = 0.52615341\n",
            "Iteration 249, loss = 0.52591167\n",
            "Iteration 250, loss = 0.52580523\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 250 and for layer number 5 : 0.7162499999999999\n",
            "Iteration 1, loss = 1.36577264\n",
            "Iteration 2, loss = 1.21631564\n",
            "Iteration 3, loss = 1.02778964\n",
            "Iteration 4, loss = 0.86161348\n",
            "Iteration 5, loss = 0.75517608\n",
            "Iteration 6, loss = 0.69677016\n",
            "Iteration 7, loss = 0.67054166\n",
            "Iteration 8, loss = 0.65696349\n",
            "Iteration 9, loss = 0.64924227\n",
            "Iteration 10, loss = 0.64200040\n",
            "Iteration 11, loss = 0.63502000\n",
            "Iteration 12, loss = 0.62804496\n",
            "Iteration 13, loss = 0.62121423\n",
            "Iteration 14, loss = 0.61544081\n",
            "Iteration 15, loss = 0.60966913\n",
            "Iteration 16, loss = 0.60458321\n",
            "Iteration 17, loss = 0.60053459\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 18, loss = 0.59669563\n",
            "Iteration 19, loss = 0.59365093\n",
            "Iteration 20, loss = 0.59065223\n",
            "Iteration 21, loss = 0.58804416\n",
            "Iteration 22, loss = 0.58582895\n",
            "Iteration 23, loss = 0.58349854\n",
            "Iteration 24, loss = 0.58165816\n",
            "Iteration 25, loss = 0.57966754\n",
            "Iteration 26, loss = 0.57764872\n",
            "Iteration 27, loss = 0.57592592\n",
            "Iteration 28, loss = 0.57428512\n",
            "Iteration 29, loss = 0.57254934\n",
            "Iteration 30, loss = 0.57118603\n",
            "Iteration 31, loss = 0.56960122\n",
            "Iteration 32, loss = 0.56838182\n",
            "Iteration 33, loss = 0.56685777\n",
            "Iteration 34, loss = 0.56551142\n",
            "Iteration 35, loss = 0.56440679\n",
            "Iteration 36, loss = 0.56326099\n",
            "Iteration 37, loss = 0.56204087\n",
            "Iteration 38, loss = 0.56109031\n",
            "Iteration 39, loss = 0.55986212\n",
            "Iteration 40, loss = 0.55891318\n",
            "Iteration 41, loss = 0.55804948\n",
            "Iteration 42, loss = 0.55708778\n",
            "Iteration 43, loss = 0.55612363\n",
            "Iteration 44, loss = 0.55520370\n",
            "Iteration 45, loss = 0.55433500\n",
            "Iteration 46, loss = 0.55324730\n",
            "Iteration 47, loss = 0.55239553\n",
            "Iteration 48, loss = 0.55173779\n",
            "Iteration 49, loss = 0.55101393\n",
            "Iteration 50, loss = 0.55038937\n",
            "Iteration 51, loss = 0.54971687\n",
            "Iteration 52, loss = 0.54916416\n",
            "Iteration 53, loss = 0.54852380\n",
            "Iteration 54, loss = 0.54794764\n",
            "Iteration 55, loss = 0.54748227\n",
            "Iteration 56, loss = 0.54684467\n",
            "Iteration 57, loss = 0.54634707\n",
            "Iteration 58, loss = 0.54580420\n",
            "Iteration 59, loss = 0.54519260\n",
            "Iteration 60, loss = 0.54468122\n",
            "Iteration 61, loss = 0.54419114\n",
            "Iteration 62, loss = 0.54359535\n",
            "Iteration 63, loss = 0.54298356\n",
            "Iteration 64, loss = 0.54246837\n",
            "Iteration 65, loss = 0.54181965\n",
            "Iteration 66, loss = 0.54124425\n",
            "Iteration 67, loss = 0.54066048\n",
            "Iteration 68, loss = 0.54002205\n",
            "Iteration 69, loss = 0.53948822\n",
            "Iteration 70, loss = 0.53892974\n",
            "Iteration 71, loss = 0.53834934\n",
            "Iteration 72, loss = 0.53786761\n",
            "Iteration 73, loss = 0.53739308\n",
            "Iteration 74, loss = 0.53716736\n",
            "Iteration 75, loss = 0.53641757\n",
            "Iteration 76, loss = 0.53606271\n",
            "Iteration 77, loss = 0.53557915\n",
            "Iteration 78, loss = 0.53511829\n",
            "Iteration 79, loss = 0.53464439\n",
            "Iteration 80, loss = 0.53433061\n",
            "Iteration 81, loss = 0.53404424\n",
            "Iteration 82, loss = 0.53369755\n",
            "Iteration 83, loss = 0.53324430\n",
            "Iteration 84, loss = 0.53294977\n",
            "Iteration 85, loss = 0.53250768\n",
            "Iteration 86, loss = 0.53216655\n",
            "Iteration 87, loss = 0.53175103\n",
            "Iteration 88, loss = 0.53142669\n",
            "Iteration 89, loss = 0.53126093\n",
            "Iteration 90, loss = 0.53081181\n",
            "Iteration 91, loss = 0.53051382\n",
            "Iteration 92, loss = 0.53014209\n",
            "Iteration 93, loss = 0.52983351\n",
            "Iteration 94, loss = 0.52955779\n",
            "Iteration 95, loss = 0.52924251\n",
            "Iteration 96, loss = 0.52894063\n",
            "Iteration 97, loss = 0.52859632\n",
            "Iteration 98, loss = 0.52836618\n",
            "Iteration 99, loss = 0.52816789\n",
            "Iteration 100, loss = 0.52777562\n",
            "Iteration 101, loss = 0.52736082\n",
            "Iteration 102, loss = 0.52689572\n",
            "Iteration 103, loss = 0.52655346\n",
            "Iteration 104, loss = 0.52632362\n",
            "Iteration 105, loss = 0.52602093\n",
            "Iteration 106, loss = 0.52565738\n",
            "Iteration 107, loss = 0.52529612\n",
            "Iteration 108, loss = 0.52513463\n",
            "Iteration 109, loss = 0.52454681\n",
            "Iteration 110, loss = 0.52419489\n",
            "Iteration 111, loss = 0.52393741\n",
            "Iteration 112, loss = 0.52364714\n",
            "Iteration 113, loss = 0.52336835\n",
            "Iteration 114, loss = 0.52292347\n",
            "Iteration 115, loss = 0.52261625\n",
            "Iteration 116, loss = 0.52216289\n",
            "Iteration 117, loss = 0.52191115\n",
            "Iteration 118, loss = 0.52167411\n",
            "Iteration 119, loss = 0.52129701\n",
            "Iteration 120, loss = 0.52086030\n",
            "Iteration 121, loss = 0.52065410\n",
            "Iteration 122, loss = 0.52036663\n",
            "Iteration 123, loss = 0.52007206\n",
            "Iteration 124, loss = 0.51971518\n",
            "Iteration 125, loss = 0.51938290\n",
            "Iteration 126, loss = 0.51906406\n",
            "Iteration 127, loss = 0.51878832\n",
            "Iteration 128, loss = 0.51843197\n",
            "Iteration 129, loss = 0.51804059\n",
            "Iteration 130, loss = 0.51767480\n",
            "Iteration 131, loss = 0.51750205\n",
            "Iteration 132, loss = 0.51704063\n",
            "Iteration 133, loss = 0.51670742\n",
            "Iteration 134, loss = 0.51652978\n",
            "Iteration 135, loss = 0.51621894\n",
            "Iteration 136, loss = 0.51597440\n",
            "Iteration 137, loss = 0.51563672\n",
            "Iteration 138, loss = 0.51525275\n",
            "Iteration 139, loss = 0.51503894\n",
            "Iteration 140, loss = 0.51466413\n",
            "Iteration 141, loss = 0.51433617\n",
            "Iteration 142, loss = 0.51414569\n",
            "Iteration 143, loss = 0.51380601\n",
            "Iteration 144, loss = 0.51343096\n",
            "Iteration 145, loss = 0.51312277\n",
            "Iteration 146, loss = 0.51284898\n",
            "Iteration 147, loss = 0.51248492\n",
            "Iteration 148, loss = 0.51225234\n",
            "Iteration 149, loss = 0.51185209\n",
            "Iteration 150, loss = 0.51170754\n",
            "Iteration 151, loss = 0.51134963\n",
            "Iteration 152, loss = 0.51111595\n",
            "Iteration 153, loss = 0.51070866\n",
            "Iteration 154, loss = 0.51042514\n",
            "Iteration 155, loss = 0.50993130\n",
            "Iteration 156, loss = 0.50975291\n",
            "Iteration 157, loss = 0.50953039\n",
            "Iteration 158, loss = 0.50905971\n",
            "Iteration 159, loss = 0.50877156\n",
            "Iteration 160, loss = 0.50845924\n",
            "Iteration 161, loss = 0.50826456\n",
            "Iteration 162, loss = 0.50795596\n",
            "Iteration 163, loss = 0.50773307\n",
            "Iteration 164, loss = 0.50741636\n",
            "Iteration 165, loss = 0.50715164\n",
            "Iteration 166, loss = 0.50680300\n",
            "Iteration 167, loss = 0.50650680\n",
            "Iteration 168, loss = 0.50637832\n",
            "Iteration 169, loss = 0.50608983\n",
            "Iteration 170, loss = 0.50568664\n",
            "Iteration 171, loss = 0.50540610\n",
            "Iteration 172, loss = 0.50507140\n",
            "Iteration 173, loss = 0.50475273\n",
            "Iteration 174, loss = 0.50461285\n",
            "Iteration 175, loss = 0.50421829\n",
            "Iteration 176, loss = 0.50387057\n",
            "Iteration 177, loss = 0.50357414\n",
            "Iteration 178, loss = 0.50340073\n",
            "Iteration 179, loss = 0.50306695\n",
            "Iteration 180, loss = 0.50295475\n",
            "Iteration 181, loss = 0.50270728\n",
            "Iteration 182, loss = 0.50260136\n",
            "Iteration 183, loss = 0.50226492\n",
            "Iteration 184, loss = 0.50195030\n",
            "Iteration 185, loss = 0.50177161\n",
            "Iteration 186, loss = 0.50149373\n",
            "Iteration 187, loss = 0.50122689\n",
            "Iteration 188, loss = 0.50086275\n",
            "Iteration 189, loss = 0.50069897\n",
            "Iteration 190, loss = 0.50061594\n",
            "Iteration 191, loss = 0.50059219\n",
            "Iteration 192, loss = 0.50036108\n",
            "Iteration 193, loss = 0.49995113\n",
            "Iteration 194, loss = 0.49973413\n",
            "Iteration 195, loss = 0.49936805\n",
            "Iteration 196, loss = 0.49916302\n",
            "Iteration 197, loss = 0.49881444\n",
            "Iteration 198, loss = 0.49857613\n",
            "Iteration 199, loss = 0.49853651\n",
            "Iteration 200, loss = 0.49807248\n",
            "Iteration 201, loss = 0.49779421\n",
            "Iteration 202, loss = 0.49736330\n",
            "Iteration 203, loss = 0.49713300\n",
            "Iteration 204, loss = 0.49710463\n",
            "Iteration 205, loss = 0.49696405\n",
            "Iteration 206, loss = 0.49668330\n",
            "Iteration 207, loss = 0.49649793\n",
            "Iteration 208, loss = 0.49629252\n",
            "Iteration 209, loss = 0.49597063\n",
            "Iteration 210, loss = 0.49577283\n",
            "Iteration 211, loss = 0.49546721\n",
            "Iteration 212, loss = 0.49528882\n",
            "Iteration 213, loss = 0.49507299\n",
            "Iteration 214, loss = 0.49491583\n",
            "Iteration 215, loss = 0.49481540\n",
            "Iteration 216, loss = 0.49446953\n",
            "Iteration 217, loss = 0.49419296\n",
            "Iteration 218, loss = 0.49374357\n",
            "Iteration 219, loss = 0.49347607\n",
            "Iteration 220, loss = 0.49346481\n",
            "Iteration 221, loss = 0.49307310\n",
            "Iteration 222, loss = 0.49289382\n",
            "Iteration 223, loss = 0.49292448\n",
            "Iteration 224, loss = 0.49223281\n",
            "Iteration 225, loss = 0.49220041\n",
            "Iteration 226, loss = 0.49209075\n",
            "Iteration 227, loss = 0.49182001\n",
            "Iteration 228, loss = 0.49186503\n",
            "Iteration 229, loss = 0.49166416\n",
            "Iteration 230, loss = 0.49137258\n",
            "Iteration 231, loss = 0.49121841\n",
            "Iteration 232, loss = 0.49080095\n",
            "Iteration 233, loss = 0.49050506\n",
            "Iteration 234, loss = 0.49010516\n",
            "Iteration 235, loss = 0.48998829\n",
            "Iteration 236, loss = 0.48972227\n",
            "Iteration 237, loss = 0.48939852\n",
            "Iteration 238, loss = 0.48950248\n",
            "Iteration 239, loss = 0.48890828\n",
            "Iteration 240, loss = 0.48868125\n",
            "Iteration 241, loss = 0.48838732\n",
            "Iteration 242, loss = 0.48824369\n",
            "Iteration 243, loss = 0.48803435\n",
            "Iteration 244, loss = 0.48768301\n",
            "Iteration 245, loss = 0.48785038\n",
            "Iteration 246, loss = 0.48761026\n",
            "Iteration 247, loss = 0.48724606\n",
            "Iteration 248, loss = 0.48703753\n",
            "Iteration 249, loss = 0.48696423\n",
            "Iteration 250, loss = 0.48665787\n",
            "Iteration 1, loss = 1.35514429\n",
            "Iteration 2, loss = 1.20785553\n",
            "Iteration 3, loss = 1.02088892\n",
            "Iteration 4, loss = 0.85738136\n",
            "Iteration 5, loss = 0.75279440\n",
            "Iteration 6, loss = 0.69511781\n",
            "Iteration 7, loss = 0.66830518\n",
            "Iteration 8, loss = 0.65594188\n",
            "Iteration 9, loss = 0.64756070\n",
            "Iteration 10, loss = 0.63958029\n",
            "Iteration 11, loss = 0.63226591\n",
            "Iteration 12, loss = 0.62537086\n",
            "Iteration 13, loss = 0.61859528\n",
            "Iteration 14, loss = 0.61262202\n",
            "Iteration 15, loss = 0.60683866\n",
            "Iteration 16, loss = 0.60192057\n",
            "Iteration 17, loss = 0.59769403\n",
            "Iteration 18, loss = 0.59407468\n",
            "Iteration 19, loss = 0.59087969\n",
            "Iteration 20, loss = 0.58787421\n",
            "Iteration 21, loss = 0.58490220\n",
            "Iteration 22, loss = 0.58260316\n",
            "Iteration 23, loss = 0.57994422\n",
            "Iteration 24, loss = 0.57770562\n",
            "Iteration 25, loss = 0.57568745\n",
            "Iteration 26, loss = 0.57360864\n",
            "Iteration 27, loss = 0.57185682\n",
            "Iteration 28, loss = 0.57009674\n",
            "Iteration 29, loss = 0.56836102\n",
            "Iteration 30, loss = 0.56690076\n",
            "Iteration 31, loss = 0.56525335\n",
            "Iteration 32, loss = 0.56398245\n",
            "Iteration 33, loss = 0.56232638\n",
            "Iteration 34, loss = 0.56101582\n",
            "Iteration 35, loss = 0.55982524\n",
            "Iteration 36, loss = 0.55856738\n",
            "Iteration 37, loss = 0.55735555\n",
            "Iteration 38, loss = 0.55627713\n",
            "Iteration 39, loss = 0.55510543\n",
            "Iteration 40, loss = 0.55407973\n",
            "Iteration 41, loss = 0.55330870\n",
            "Iteration 42, loss = 0.55233209\n",
            "Iteration 43, loss = 0.55132070\n",
            "Iteration 44, loss = 0.55036474\n",
            "Iteration 45, loss = 0.54944197\n",
            "Iteration 46, loss = 0.54837056\n",
            "Iteration 47, loss = 0.54738358\n",
            "Iteration 48, loss = 0.54652592\n",
            "Iteration 49, loss = 0.54571200\n",
            "Iteration 50, loss = 0.54485943\n",
            "Iteration 51, loss = 0.54400365\n",
            "Iteration 52, loss = 0.54341597\n",
            "Iteration 53, loss = 0.54259256\n",
            "Iteration 54, loss = 0.54195325\n",
            "Iteration 55, loss = 0.54135470\n",
            "Iteration 56, loss = 0.54070979\n",
            "Iteration 57, loss = 0.54016796\n",
            "Iteration 58, loss = 0.53969689\n",
            "Iteration 59, loss = 0.53917235\n",
            "Iteration 60, loss = 0.53869064\n",
            "Iteration 61, loss = 0.53838360\n",
            "Iteration 62, loss = 0.53768739\n",
            "Iteration 63, loss = 0.53703995\n",
            "Iteration 64, loss = 0.53649471\n",
            "Iteration 65, loss = 0.53589237\n",
            "Iteration 66, loss = 0.53544138\n",
            "Iteration 67, loss = 0.53490436\n",
            "Iteration 68, loss = 0.53439025\n",
            "Iteration 69, loss = 0.53396900\n",
            "Iteration 70, loss = 0.53351046\n",
            "Iteration 71, loss = 0.53296450\n",
            "Iteration 72, loss = 0.53245616\n",
            "Iteration 73, loss = 0.53198883\n",
            "Iteration 74, loss = 0.53171321\n",
            "Iteration 75, loss = 0.53101324\n",
            "Iteration 76, loss = 0.53067590\n",
            "Iteration 77, loss = 0.53026639\n",
            "Iteration 78, loss = 0.52987296\n",
            "Iteration 79, loss = 0.52957369\n",
            "Iteration 80, loss = 0.52923171\n",
            "Iteration 81, loss = 0.52877918\n",
            "Iteration 82, loss = 0.52845922\n",
            "Iteration 83, loss = 0.52806888\n",
            "Iteration 84, loss = 0.52770694\n",
            "Iteration 85, loss = 0.52724198\n",
            "Iteration 86, loss = 0.52678667\n",
            "Iteration 87, loss = 0.52638671\n",
            "Iteration 88, loss = 0.52591062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 89, loss = 0.52570284\n",
            "Iteration 90, loss = 0.52524291\n",
            "Iteration 91, loss = 0.52484179\n",
            "Iteration 92, loss = 0.52438492\n",
            "Iteration 93, loss = 0.52405214\n",
            "Iteration 94, loss = 0.52381102\n",
            "Iteration 95, loss = 0.52339081\n",
            "Iteration 96, loss = 0.52297938\n",
            "Iteration 97, loss = 0.52261205\n",
            "Iteration 98, loss = 0.52217822\n",
            "Iteration 99, loss = 0.52179697\n",
            "Iteration 100, loss = 0.52129746\n",
            "Iteration 101, loss = 0.52101927\n",
            "Iteration 102, loss = 0.52061005\n",
            "Iteration 103, loss = 0.52034454\n",
            "Iteration 104, loss = 0.52015692\n",
            "Iteration 105, loss = 0.51979616\n",
            "Iteration 106, loss = 0.51940073\n",
            "Iteration 107, loss = 0.51912936\n",
            "Iteration 108, loss = 0.51893490\n",
            "Iteration 109, loss = 0.51845004\n",
            "Iteration 110, loss = 0.51820597\n",
            "Iteration 111, loss = 0.51794268\n",
            "Iteration 112, loss = 0.51767564\n",
            "Iteration 113, loss = 0.51748003\n",
            "Iteration 114, loss = 0.51712466\n",
            "Iteration 115, loss = 0.51692308\n",
            "Iteration 116, loss = 0.51660287\n",
            "Iteration 117, loss = 0.51631498\n",
            "Iteration 118, loss = 0.51610752\n",
            "Iteration 119, loss = 0.51573181\n",
            "Iteration 120, loss = 0.51538704\n",
            "Iteration 121, loss = 0.51495356\n",
            "Iteration 122, loss = 0.51486355\n",
            "Iteration 123, loss = 0.51441849\n",
            "Iteration 124, loss = 0.51415901\n",
            "Iteration 125, loss = 0.51390740\n",
            "Iteration 126, loss = 0.51359845\n",
            "Iteration 127, loss = 0.51340911\n",
            "Iteration 128, loss = 0.51297929\n",
            "Iteration 129, loss = 0.51269676\n",
            "Iteration 130, loss = 0.51232668\n",
            "Iteration 131, loss = 0.51219657\n",
            "Iteration 132, loss = 0.51169586\n",
            "Iteration 133, loss = 0.51136207\n",
            "Iteration 134, loss = 0.51109099\n",
            "Iteration 135, loss = 0.51087480\n",
            "Iteration 136, loss = 0.51056979\n",
            "Iteration 137, loss = 0.51032456\n",
            "Iteration 138, loss = 0.51005568\n",
            "Iteration 139, loss = 0.50985310\n",
            "Iteration 140, loss = 0.50955879\n",
            "Iteration 141, loss = 0.50929265\n",
            "Iteration 142, loss = 0.50926849\n",
            "Iteration 143, loss = 0.50876538\n",
            "Iteration 144, loss = 0.50839873\n",
            "Iteration 145, loss = 0.50811519\n",
            "Iteration 146, loss = 0.50771953\n",
            "Iteration 147, loss = 0.50735215\n",
            "Iteration 148, loss = 0.50708792\n",
            "Iteration 149, loss = 0.50679498\n",
            "Iteration 150, loss = 0.50649485\n",
            "Iteration 151, loss = 0.50609960\n",
            "Iteration 152, loss = 0.50582013\n",
            "Iteration 153, loss = 0.50551546\n",
            "Iteration 154, loss = 0.50512894\n",
            "Iteration 155, loss = 0.50488042\n",
            "Iteration 156, loss = 0.50460068\n",
            "Iteration 157, loss = 0.50429551\n",
            "Iteration 158, loss = 0.50397380\n",
            "Iteration 159, loss = 0.50382683\n",
            "Iteration 160, loss = 0.50357294\n",
            "Iteration 161, loss = 0.50339565\n",
            "Iteration 162, loss = 0.50329490\n",
            "Iteration 163, loss = 0.50309448\n",
            "Iteration 164, loss = 0.50284658\n",
            "Iteration 165, loss = 0.50264631\n",
            "Iteration 166, loss = 0.50236569\n",
            "Iteration 167, loss = 0.50202179\n",
            "Iteration 168, loss = 0.50191047\n",
            "Iteration 169, loss = 0.50163282\n",
            "Iteration 170, loss = 0.50136437\n",
            "Iteration 171, loss = 0.50111850\n",
            "Iteration 172, loss = 0.50075805\n",
            "Iteration 173, loss = 0.50041021\n",
            "Iteration 174, loss = 0.50011290\n",
            "Iteration 175, loss = 0.49998879\n",
            "Iteration 176, loss = 0.49973993\n",
            "Iteration 177, loss = 0.49940391\n",
            "Iteration 178, loss = 0.49927179\n",
            "Iteration 179, loss = 0.49890769\n",
            "Iteration 180, loss = 0.49867277\n",
            "Iteration 181, loss = 0.49827444\n",
            "Iteration 182, loss = 0.49837573\n",
            "Iteration 183, loss = 0.49797115\n",
            "Iteration 184, loss = 0.49782260\n",
            "Iteration 185, loss = 0.49764770\n",
            "Iteration 186, loss = 0.49730963\n",
            "Iteration 187, loss = 0.49719445\n",
            "Iteration 188, loss = 0.49683310\n",
            "Iteration 189, loss = 0.49657023\n",
            "Iteration 190, loss = 0.49651669\n",
            "Iteration 191, loss = 0.49646990\n",
            "Iteration 192, loss = 0.49613116\n",
            "Iteration 193, loss = 0.49595627\n",
            "Iteration 194, loss = 0.49580219\n",
            "Iteration 195, loss = 0.49540949\n",
            "Iteration 196, loss = 0.49529174\n",
            "Iteration 197, loss = 0.49509679\n",
            "Iteration 198, loss = 0.49510256\n",
            "Iteration 199, loss = 0.49494341\n",
            "Iteration 200, loss = 0.49472225\n",
            "Iteration 201, loss = 0.49438534\n",
            "Iteration 202, loss = 0.49404440\n",
            "Iteration 203, loss = 0.49417178\n",
            "Iteration 204, loss = 0.49408455\n",
            "Iteration 205, loss = 0.49409727\n",
            "Iteration 206, loss = 0.49388061\n",
            "Iteration 207, loss = 0.49373669\n",
            "Iteration 208, loss = 0.49345617\n",
            "Iteration 209, loss = 0.49321830\n",
            "Iteration 210, loss = 0.49305389\n",
            "Iteration 211, loss = 0.49291605\n",
            "Iteration 212, loss = 0.49269789\n",
            "Iteration 213, loss = 0.49249754\n",
            "Iteration 214, loss = 0.49227560\n",
            "Iteration 215, loss = 0.49261391\n",
            "Iteration 216, loss = 0.49219521\n",
            "Iteration 217, loss = 0.49195227\n",
            "Iteration 218, loss = 0.49159014\n",
            "Iteration 219, loss = 0.49134425\n",
            "Iteration 220, loss = 0.49157963\n",
            "Iteration 221, loss = 0.49107636\n",
            "Iteration 222, loss = 0.49110126\n",
            "Iteration 223, loss = 0.49129684\n",
            "Iteration 224, loss = 0.49097142\n",
            "Iteration 225, loss = 0.49097382\n",
            "Iteration 226, loss = 0.49087583\n",
            "Iteration 227, loss = 0.49060608\n",
            "Iteration 228, loss = 0.49016557\n",
            "Iteration 229, loss = 0.48976353\n",
            "Iteration 230, loss = 0.48949142\n",
            "Iteration 231, loss = 0.48933734\n",
            "Iteration 232, loss = 0.48917234\n",
            "Iteration 233, loss = 0.48893708\n",
            "Iteration 234, loss = 0.48861020\n",
            "Iteration 235, loss = 0.48863641\n",
            "Iteration 236, loss = 0.48860956\n",
            "Iteration 237, loss = 0.48810072\n",
            "Iteration 238, loss = 0.48839949\n",
            "Iteration 239, loss = 0.48802878\n",
            "Iteration 240, loss = 0.48790320\n",
            "Iteration 241, loss = 0.48776562\n",
            "Iteration 242, loss = 0.48755975\n",
            "Iteration 243, loss = 0.48751318\n",
            "Iteration 244, loss = 0.48703430\n",
            "Iteration 245, loss = 0.48697857\n",
            "Iteration 246, loss = 0.48700781\n",
            "Iteration 247, loss = 0.48689940\n",
            "Iteration 248, loss = 0.48681828\n",
            "Iteration 249, loss = 0.48677051\n",
            "Iteration 250, loss = 0.48659596\n",
            "Iteration 1, loss = 1.35569250\n",
            "Iteration 2, loss = 1.20977855\n",
            "Iteration 3, loss = 1.02421468\n",
            "Iteration 4, loss = 0.86114766\n",
            "Iteration 5, loss = 0.75352175\n",
            "Iteration 6, loss = 0.69589482\n",
            "Iteration 7, loss = 0.66854088\n",
            "Iteration 8, loss = 0.65502259\n",
            "Iteration 9, loss = 0.64663631\n",
            "Iteration 10, loss = 0.63885391\n",
            "Iteration 11, loss = 0.63126598\n",
            "Iteration 12, loss = 0.62391326\n",
            "Iteration 13, loss = 0.61710692\n",
            "Iteration 14, loss = 0.61051424\n",
            "Iteration 15, loss = 0.60440264\n",
            "Iteration 16, loss = 0.59904249\n",
            "Iteration 17, loss = 0.59421694\n",
            "Iteration 18, loss = 0.58983593\n",
            "Iteration 19, loss = 0.58606930\n",
            "Iteration 20, loss = 0.58266201\n",
            "Iteration 21, loss = 0.57939851\n",
            "Iteration 22, loss = 0.57651751\n",
            "Iteration 23, loss = 0.57331381\n",
            "Iteration 24, loss = 0.57061743\n",
            "Iteration 25, loss = 0.56816832\n",
            "Iteration 26, loss = 0.56558906\n",
            "Iteration 27, loss = 0.56354880\n",
            "Iteration 28, loss = 0.56127377\n",
            "Iteration 29, loss = 0.55916195\n",
            "Iteration 30, loss = 0.55726156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 31, loss = 0.55534880\n",
            "Iteration 32, loss = 0.55363341\n",
            "Iteration 33, loss = 0.55176312\n",
            "Iteration 34, loss = 0.55028963\n",
            "Iteration 35, loss = 0.54884212\n",
            "Iteration 36, loss = 0.54733604\n",
            "Iteration 37, loss = 0.54583440\n",
            "Iteration 38, loss = 0.54439355\n",
            "Iteration 39, loss = 0.54300394\n",
            "Iteration 40, loss = 0.54174970\n",
            "Iteration 41, loss = 0.54070347\n",
            "Iteration 42, loss = 0.53965873\n",
            "Iteration 43, loss = 0.53854609\n",
            "Iteration 44, loss = 0.53749954\n",
            "Iteration 45, loss = 0.53666066\n",
            "Iteration 46, loss = 0.53548315\n",
            "Iteration 47, loss = 0.53456246\n",
            "Iteration 48, loss = 0.53368647\n",
            "Iteration 49, loss = 0.53288963\n",
            "Iteration 50, loss = 0.53210325\n",
            "Iteration 51, loss = 0.53127099\n",
            "Iteration 52, loss = 0.53064426\n",
            "Iteration 53, loss = 0.52966743\n",
            "Iteration 54, loss = 0.52903463\n",
            "Iteration 55, loss = 0.52829084\n",
            "Iteration 56, loss = 0.52752085\n",
            "Iteration 57, loss = 0.52692793\n",
            "Iteration 58, loss = 0.52620528\n",
            "Iteration 59, loss = 0.52564618\n",
            "Iteration 60, loss = 0.52501302\n",
            "Iteration 61, loss = 0.52461704\n",
            "Iteration 62, loss = 0.52397652\n",
            "Iteration 63, loss = 0.52328006\n",
            "Iteration 64, loss = 0.52274315\n",
            "Iteration 65, loss = 0.52209173\n",
            "Iteration 66, loss = 0.52161141\n",
            "Iteration 67, loss = 0.52102971\n",
            "Iteration 68, loss = 0.52051457\n",
            "Iteration 69, loss = 0.51994342\n",
            "Iteration 70, loss = 0.51949139\n",
            "Iteration 71, loss = 0.51894718\n",
            "Iteration 72, loss = 0.51834461\n",
            "Iteration 73, loss = 0.51783013\n",
            "Iteration 74, loss = 0.51731223\n",
            "Iteration 75, loss = 0.51688740\n",
            "Iteration 76, loss = 0.51646968\n",
            "Iteration 77, loss = 0.51623903\n",
            "Iteration 78, loss = 0.51580940\n",
            "Iteration 79, loss = 0.51535095\n",
            "Iteration 80, loss = 0.51513440\n",
            "Iteration 81, loss = 0.51460954\n",
            "Iteration 82, loss = 0.51437374\n",
            "Iteration 83, loss = 0.51399374\n",
            "Iteration 84, loss = 0.51366061\n",
            "Iteration 85, loss = 0.51329859\n",
            "Iteration 86, loss = 0.51284274\n",
            "Iteration 87, loss = 0.51240931\n",
            "Iteration 88, loss = 0.51200998\n",
            "Iteration 89, loss = 0.51163393\n",
            "Iteration 90, loss = 0.51123178\n",
            "Iteration 91, loss = 0.51078401\n",
            "Iteration 92, loss = 0.51037211\n",
            "Iteration 93, loss = 0.51000532\n",
            "Iteration 94, loss = 0.50948955\n",
            "Iteration 95, loss = 0.50923813\n",
            "Iteration 96, loss = 0.50879173\n",
            "Iteration 97, loss = 0.50827459\n",
            "Iteration 98, loss = 0.50788042\n",
            "Iteration 99, loss = 0.50765154\n",
            "Iteration 100, loss = 0.50721205\n",
            "Iteration 101, loss = 0.50691786\n",
            "Iteration 102, loss = 0.50651989\n",
            "Iteration 103, loss = 0.50615824\n",
            "Iteration 104, loss = 0.50586519\n",
            "Iteration 105, loss = 0.50556665\n",
            "Iteration 106, loss = 0.50509417\n",
            "Iteration 107, loss = 0.50485705\n",
            "Iteration 108, loss = 0.50463847\n",
            "Iteration 109, loss = 0.50429215\n",
            "Iteration 110, loss = 0.50390829\n",
            "Iteration 111, loss = 0.50366847\n",
            "Iteration 112, loss = 0.50339423\n",
            "Iteration 113, loss = 0.50308668\n",
            "Iteration 114, loss = 0.50277058\n",
            "Iteration 115, loss = 0.50258803\n",
            "Iteration 116, loss = 0.50232659\n",
            "Iteration 117, loss = 0.50196972\n",
            "Iteration 118, loss = 0.50175203\n",
            "Iteration 119, loss = 0.50146362\n",
            "Iteration 120, loss = 0.50133419\n",
            "Iteration 121, loss = 0.50107893\n",
            "Iteration 122, loss = 0.50094707\n",
            "Iteration 123, loss = 0.50063183\n",
            "Iteration 124, loss = 0.50039005\n",
            "Iteration 125, loss = 0.50015109\n",
            "Iteration 126, loss = 0.50000054\n",
            "Iteration 127, loss = 0.49987877\n",
            "Iteration 128, loss = 0.49961840\n",
            "Iteration 129, loss = 0.49932302\n",
            "Iteration 130, loss = 0.49909672\n",
            "Iteration 131, loss = 0.49906614\n",
            "Iteration 132, loss = 0.49867396\n",
            "Iteration 133, loss = 0.49839738\n",
            "Iteration 134, loss = 0.49828698\n",
            "Iteration 135, loss = 0.49812284\n",
            "Iteration 136, loss = 0.49781359\n",
            "Iteration 137, loss = 0.49766731\n",
            "Iteration 138, loss = 0.49742966\n",
            "Iteration 139, loss = 0.49729896\n",
            "Iteration 140, loss = 0.49693897\n",
            "Iteration 141, loss = 0.49679213\n",
            "Iteration 142, loss = 0.49665055\n",
            "Iteration 143, loss = 0.49641770\n",
            "Iteration 144, loss = 0.49617866\n",
            "Iteration 145, loss = 0.49592519\n",
            "Iteration 146, loss = 0.49574596\n",
            "Iteration 147, loss = 0.49556817\n",
            "Iteration 148, loss = 0.49543211\n",
            "Iteration 149, loss = 0.49532935\n",
            "Iteration 150, loss = 0.49496914\n",
            "Iteration 151, loss = 0.49473017\n",
            "Iteration 152, loss = 0.49460200\n",
            "Iteration 153, loss = 0.49440604\n",
            "Iteration 154, loss = 0.49417629\n",
            "Iteration 155, loss = 0.49392635\n",
            "Iteration 156, loss = 0.49369083\n",
            "Iteration 157, loss = 0.49356758\n",
            "Iteration 158, loss = 0.49324968\n",
            "Iteration 159, loss = 0.49304637\n",
            "Iteration 160, loss = 0.49288787\n",
            "Iteration 161, loss = 0.49260723\n",
            "Iteration 162, loss = 0.49238571\n",
            "Iteration 163, loss = 0.49234288\n",
            "Iteration 164, loss = 0.49187799\n",
            "Iteration 165, loss = 0.49160123\n",
            "Iteration 166, loss = 0.49144122\n",
            "Iteration 167, loss = 0.49109575\n",
            "Iteration 168, loss = 0.49072733\n",
            "Iteration 169, loss = 0.49058246\n",
            "Iteration 170, loss = 0.49025595\n",
            "Iteration 171, loss = 0.49008105\n",
            "Iteration 172, loss = 0.48979297\n",
            "Iteration 173, loss = 0.48959766\n",
            "Iteration 174, loss = 0.48930297\n",
            "Iteration 175, loss = 0.48903951\n",
            "Iteration 176, loss = 0.48874531\n",
            "Iteration 177, loss = 0.48837297\n",
            "Iteration 178, loss = 0.48821016\n",
            "Iteration 179, loss = 0.48794466\n",
            "Iteration 180, loss = 0.48757867\n",
            "Iteration 181, loss = 0.48739663\n",
            "Iteration 182, loss = 0.48737866\n",
            "Iteration 183, loss = 0.48719382\n",
            "Iteration 184, loss = 0.48689394\n",
            "Iteration 185, loss = 0.48664788\n",
            "Iteration 186, loss = 0.48637122\n",
            "Iteration 187, loss = 0.48621657\n",
            "Iteration 188, loss = 0.48600749\n",
            "Iteration 189, loss = 0.48552255\n",
            "Iteration 190, loss = 0.48556991\n",
            "Iteration 191, loss = 0.48535921\n",
            "Iteration 192, loss = 0.48497296\n",
            "Iteration 193, loss = 0.48480282\n",
            "Iteration 194, loss = 0.48450497\n",
            "Iteration 195, loss = 0.48440554\n",
            "Iteration 196, loss = 0.48412461\n",
            "Iteration 197, loss = 0.48400514\n",
            "Iteration 198, loss = 0.48384378\n",
            "Iteration 199, loss = 0.48370706\n",
            "Iteration 200, loss = 0.48348180\n",
            "Iteration 201, loss = 0.48332394\n",
            "Iteration 202, loss = 0.48304940\n",
            "Iteration 203, loss = 0.48316556\n",
            "Iteration 204, loss = 0.48312847\n",
            "Iteration 205, loss = 0.48287101\n",
            "Iteration 206, loss = 0.48275481\n",
            "Iteration 207, loss = 0.48247326\n",
            "Iteration 208, loss = 0.48214877\n",
            "Iteration 209, loss = 0.48204603\n",
            "Iteration 210, loss = 0.48186539\n",
            "Iteration 211, loss = 0.48177815\n",
            "Iteration 212, loss = 0.48161905\n",
            "Iteration 213, loss = 0.48141703\n",
            "Iteration 214, loss = 0.48134820\n",
            "Iteration 215, loss = 0.48157824\n",
            "Iteration 216, loss = 0.48120955\n",
            "Iteration 217, loss = 0.48106640\n",
            "Iteration 218, loss = 0.48106582\n",
            "Iteration 219, loss = 0.48073360\n",
            "Iteration 220, loss = 0.48105630\n",
            "Iteration 221, loss = 0.48049548\n",
            "Iteration 222, loss = 0.48037124\n",
            "Iteration 223, loss = 0.48035894\n",
            "Iteration 224, loss = 0.47988312\n",
            "Iteration 225, loss = 0.47970712\n",
            "Iteration 226, loss = 0.47948611\n",
            "Iteration 227, loss = 0.47914038\n",
            "Iteration 228, loss = 0.47894598\n",
            "Iteration 229, loss = 0.47861101\n",
            "Iteration 230, loss = 0.47835873\n",
            "Iteration 231, loss = 0.47822289\n",
            "Iteration 232, loss = 0.47791974\n",
            "Iteration 233, loss = 0.47763945\n",
            "Iteration 234, loss = 0.47740320\n",
            "Iteration 235, loss = 0.47713493\n",
            "Iteration 236, loss = 0.47717697\n",
            "Iteration 237, loss = 0.47667765\n",
            "Iteration 238, loss = 0.47652837\n",
            "Iteration 239, loss = 0.47624770\n",
            "Iteration 240, loss = 0.47610213\n",
            "Iteration 241, loss = 0.47577589\n",
            "Iteration 242, loss = 0.47566299\n",
            "Iteration 243, loss = 0.47559308\n",
            "Iteration 244, loss = 0.47520701\n",
            "Iteration 245, loss = 0.47505874\n",
            "Iteration 246, loss = 0.47515207\n",
            "Iteration 247, loss = 0.47488703\n",
            "Iteration 248, loss = 0.47473109\n",
            "Iteration 249, loss = 0.47473051\n",
            "Iteration 250, loss = 0.47452314\n",
            "Iteration 1, loss = 1.36723929\n",
            "Iteration 2, loss = 1.21905982\n",
            "Iteration 3, loss = 1.03107889\n",
            "Iteration 4, loss = 0.86195746\n",
            "Iteration 5, loss = 0.75246019\n",
            "Iteration 6, loss = 0.69773006\n",
            "Iteration 7, loss = 0.67069081\n",
            "Iteration 8, loss = 0.65769343\n",
            "Iteration 9, loss = 0.64888341\n",
            "Iteration 10, loss = 0.64099600\n",
            "Iteration 11, loss = 0.63323861\n",
            "Iteration 12, loss = 0.62561845\n",
            "Iteration 13, loss = 0.61893797\n",
            "Iteration 14, loss = 0.61269725\n",
            "Iteration 15, loss = 0.60721156\n",
            "Iteration 16, loss = 0.60205105\n",
            "Iteration 17, loss = 0.59786319\n",
            "Iteration 18, loss = 0.59406231\n",
            "Iteration 19, loss = 0.59071031\n",
            "Iteration 20, loss = 0.58803417\n",
            "Iteration 21, loss = 0.58526381\n",
            "Iteration 22, loss = 0.58253915\n",
            "Iteration 23, loss = 0.57958451\n",
            "Iteration 24, loss = 0.57694386\n",
            "Iteration 25, loss = 0.57480505\n",
            "Iteration 26, loss = 0.57235466\n",
            "Iteration 27, loss = 0.57040575\n",
            "Iteration 28, loss = 0.56840383\n",
            "Iteration 29, loss = 0.56663716\n",
            "Iteration 30, loss = 0.56492198\n",
            "Iteration 31, loss = 0.56329133\n",
            "Iteration 32, loss = 0.56191467\n",
            "Iteration 33, loss = 0.56045268\n",
            "Iteration 34, loss = 0.55912756\n",
            "Iteration 35, loss = 0.55794052\n",
            "Iteration 36, loss = 0.55654732\n",
            "Iteration 37, loss = 0.55516411\n",
            "Iteration 38, loss = 0.55392465\n",
            "Iteration 39, loss = 0.55276870\n",
            "Iteration 40, loss = 0.55161758\n",
            "Iteration 41, loss = 0.55055639\n",
            "Iteration 42, loss = 0.54954353\n",
            "Iteration 43, loss = 0.54854303\n",
            "Iteration 44, loss = 0.54750768\n",
            "Iteration 45, loss = 0.54666345\n",
            "Iteration 46, loss = 0.54566846\n",
            "Iteration 47, loss = 0.54496082\n",
            "Iteration 48, loss = 0.54419184\n",
            "Iteration 49, loss = 0.54348414\n",
            "Iteration 50, loss = 0.54278759\n",
            "Iteration 51, loss = 0.54212916\n",
            "Iteration 52, loss = 0.54153784\n",
            "Iteration 53, loss = 0.54082423\n",
            "Iteration 54, loss = 0.54022182\n",
            "Iteration 55, loss = 0.53954595\n",
            "Iteration 56, loss = 0.53892056\n",
            "Iteration 57, loss = 0.53836775\n",
            "Iteration 58, loss = 0.53769234\n",
            "Iteration 59, loss = 0.53724987\n",
            "Iteration 60, loss = 0.53663885\n",
            "Iteration 61, loss = 0.53612754\n",
            "Iteration 62, loss = 0.53547468\n",
            "Iteration 63, loss = 0.53504210\n",
            "Iteration 64, loss = 0.53427768\n",
            "Iteration 65, loss = 0.53366504\n",
            "Iteration 66, loss = 0.53303150\n",
            "Iteration 67, loss = 0.53238321\n",
            "Iteration 68, loss = 0.53194125\n",
            "Iteration 69, loss = 0.53133059\n",
            "Iteration 70, loss = 0.53074222\n",
            "Iteration 71, loss = 0.53014142\n",
            "Iteration 72, loss = 0.52957523\n",
            "Iteration 73, loss = 0.52908565\n",
            "Iteration 74, loss = 0.52854630\n",
            "Iteration 75, loss = 0.52804022\n",
            "Iteration 76, loss = 0.52757641\n",
            "Iteration 77, loss = 0.52717378\n",
            "Iteration 78, loss = 0.52668493\n",
            "Iteration 79, loss = 0.52635280\n",
            "Iteration 80, loss = 0.52604054\n",
            "Iteration 81, loss = 0.52543617\n",
            "Iteration 82, loss = 0.52503550\n",
            "Iteration 83, loss = 0.52449942\n",
            "Iteration 84, loss = 0.52396905\n",
            "Iteration 85, loss = 0.52358350\n",
            "Iteration 86, loss = 0.52302113\n",
            "Iteration 87, loss = 0.52252843\n",
            "Iteration 88, loss = 0.52205340\n",
            "Iteration 89, loss = 0.52165514\n",
            "Iteration 90, loss = 0.52115976\n",
            "Iteration 91, loss = 0.52074615\n",
            "Iteration 92, loss = 0.52025791\n",
            "Iteration 93, loss = 0.51985299\n",
            "Iteration 94, loss = 0.51946563\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 95, loss = 0.51903519\n",
            "Iteration 96, loss = 0.51853386\n",
            "Iteration 97, loss = 0.51813098\n",
            "Iteration 98, loss = 0.51771788\n",
            "Iteration 99, loss = 0.51733601\n",
            "Iteration 100, loss = 0.51689669\n",
            "Iteration 101, loss = 0.51656435\n",
            "Iteration 102, loss = 0.51608763\n",
            "Iteration 103, loss = 0.51582360\n",
            "Iteration 104, loss = 0.51553747\n",
            "Iteration 105, loss = 0.51525748\n",
            "Iteration 106, loss = 0.51499401\n",
            "Iteration 107, loss = 0.51478558\n",
            "Iteration 108, loss = 0.51448378\n",
            "Iteration 109, loss = 0.51413283\n",
            "Iteration 110, loss = 0.51371076\n",
            "Iteration 111, loss = 0.51337152\n",
            "Iteration 112, loss = 0.51291200\n",
            "Iteration 113, loss = 0.51249863\n",
            "Iteration 114, loss = 0.51205963\n",
            "Iteration 115, loss = 0.51173540\n",
            "Iteration 116, loss = 0.51146686\n",
            "Iteration 117, loss = 0.51119443\n",
            "Iteration 118, loss = 0.51078437\n",
            "Iteration 119, loss = 0.51062081\n",
            "Iteration 120, loss = 0.51031826\n",
            "Iteration 121, loss = 0.50998651\n",
            "Iteration 122, loss = 0.50978558\n",
            "Iteration 123, loss = 0.50941361\n",
            "Iteration 124, loss = 0.50913029\n",
            "Iteration 125, loss = 0.50883698\n",
            "Iteration 126, loss = 0.50869105\n",
            "Iteration 127, loss = 0.50858617\n",
            "Iteration 128, loss = 0.50816938\n",
            "Iteration 129, loss = 0.50793804\n",
            "Iteration 130, loss = 0.50764031\n",
            "Iteration 131, loss = 0.50742087\n",
            "Iteration 132, loss = 0.50712230\n",
            "Iteration 133, loss = 0.50692221\n",
            "Iteration 134, loss = 0.50678377\n",
            "Iteration 135, loss = 0.50639626\n",
            "Iteration 136, loss = 0.50603770\n",
            "Iteration 137, loss = 0.50586967\n",
            "Iteration 138, loss = 0.50564069\n",
            "Iteration 139, loss = 0.50535606\n",
            "Iteration 140, loss = 0.50516058\n",
            "Iteration 141, loss = 0.50497520\n",
            "Iteration 142, loss = 0.50468096\n",
            "Iteration 143, loss = 0.50442426\n",
            "Iteration 144, loss = 0.50413265\n",
            "Iteration 145, loss = 0.50386107\n",
            "Iteration 146, loss = 0.50353061\n",
            "Iteration 147, loss = 0.50334634\n",
            "Iteration 148, loss = 0.50311783\n",
            "Iteration 149, loss = 0.50288664\n",
            "Iteration 150, loss = 0.50236505\n",
            "Iteration 151, loss = 0.50208089\n",
            "Iteration 152, loss = 0.50173600\n",
            "Iteration 153, loss = 0.50143136\n",
            "Iteration 154, loss = 0.50119702\n",
            "Iteration 155, loss = 0.50081917\n",
            "Iteration 156, loss = 0.50057737\n",
            "Iteration 157, loss = 0.50038199\n",
            "Iteration 158, loss = 0.50010093\n",
            "Iteration 159, loss = 0.49982835\n",
            "Iteration 160, loss = 0.49967910\n",
            "Iteration 161, loss = 0.49938863\n",
            "Iteration 162, loss = 0.49913924\n",
            "Iteration 163, loss = 0.49901202\n",
            "Iteration 164, loss = 0.49852383\n",
            "Iteration 165, loss = 0.49828099\n",
            "Iteration 166, loss = 0.49812412\n",
            "Iteration 167, loss = 0.49781615\n",
            "Iteration 168, loss = 0.49758333\n",
            "Iteration 169, loss = 0.49753652\n",
            "Iteration 170, loss = 0.49729156\n",
            "Iteration 171, loss = 0.49696199\n",
            "Iteration 172, loss = 0.49669771\n",
            "Iteration 173, loss = 0.49667868\n",
            "Iteration 174, loss = 0.49652562\n",
            "Iteration 175, loss = 0.49617646\n",
            "Iteration 176, loss = 0.49608681\n",
            "Iteration 177, loss = 0.49566038\n",
            "Iteration 178, loss = 0.49554962\n",
            "Iteration 179, loss = 0.49531796\n",
            "Iteration 180, loss = 0.49493488\n",
            "Iteration 181, loss = 0.49472215\n",
            "Iteration 182, loss = 0.49475523\n",
            "Iteration 183, loss = 0.49468497\n",
            "Iteration 184, loss = 0.49442485\n",
            "Iteration 185, loss = 0.49419867\n",
            "Iteration 186, loss = 0.49386104\n",
            "Iteration 187, loss = 0.49372019\n",
            "Iteration 188, loss = 0.49365549\n",
            "Iteration 189, loss = 0.49331288\n",
            "Iteration 190, loss = 0.49342631\n",
            "Iteration 191, loss = 0.49313070\n",
            "Iteration 192, loss = 0.49267346\n",
            "Iteration 193, loss = 0.49240412\n",
            "Iteration 194, loss = 0.49207607\n",
            "Iteration 195, loss = 0.49183511\n",
            "Iteration 196, loss = 0.49169131\n",
            "Iteration 197, loss = 0.49153235\n",
            "Iteration 198, loss = 0.49134857\n",
            "Iteration 199, loss = 0.49116338\n",
            "Iteration 200, loss = 0.49094298\n",
            "Iteration 201, loss = 0.49079517\n",
            "Iteration 202, loss = 0.49065132\n",
            "Iteration 203, loss = 0.49049360\n",
            "Iteration 204, loss = 0.49038778\n",
            "Iteration 205, loss = 0.49017970\n",
            "Iteration 206, loss = 0.49001372\n",
            "Iteration 207, loss = 0.48991734\n",
            "Iteration 208, loss = 0.48972907\n",
            "Iteration 209, loss = 0.48969759\n",
            "Iteration 210, loss = 0.48946656\n",
            "Iteration 211, loss = 0.48941303\n",
            "Iteration 212, loss = 0.48935257\n",
            "Iteration 213, loss = 0.48889466\n",
            "Iteration 214, loss = 0.48861328\n",
            "Iteration 215, loss = 0.48862721\n",
            "Iteration 216, loss = 0.48838778\n",
            "Iteration 217, loss = 0.48839503\n",
            "Iteration 218, loss = 0.48866441\n",
            "Iteration 219, loss = 0.48812909\n",
            "Iteration 220, loss = 0.48826819\n",
            "Iteration 221, loss = 0.48757166\n",
            "Iteration 222, loss = 0.48742240\n",
            "Iteration 223, loss = 0.48709119\n",
            "Iteration 224, loss = 0.48681778\n",
            "Iteration 225, loss = 0.48657323\n",
            "Iteration 226, loss = 0.48647362\n",
            "Iteration 227, loss = 0.48622623\n",
            "Iteration 228, loss = 0.48616008\n",
            "Iteration 229, loss = 0.48593940\n",
            "Iteration 230, loss = 0.48564519\n",
            "Iteration 231, loss = 0.48546517\n",
            "Iteration 232, loss = 0.48534577\n",
            "Iteration 233, loss = 0.48524575\n",
            "Iteration 234, loss = 0.48513565\n",
            "Iteration 235, loss = 0.48485328\n",
            "Iteration 236, loss = 0.48480374\n",
            "Iteration 237, loss = 0.48472408\n",
            "Iteration 238, loss = 0.48438629\n",
            "Iteration 239, loss = 0.48436289\n",
            "Iteration 240, loss = 0.48435321\n",
            "Iteration 241, loss = 0.48403946\n",
            "Iteration 242, loss = 0.48390486\n",
            "Iteration 243, loss = 0.48391153\n",
            "Iteration 244, loss = 0.48356569\n",
            "Iteration 245, loss = 0.48356369\n",
            "Iteration 246, loss = 0.48372630\n",
            "Iteration 247, loss = 0.48329504\n",
            "Iteration 248, loss = 0.48308955\n",
            "Iteration 249, loss = 0.48287742\n",
            "Iteration 250, loss = 0.48278167\n",
            "Iteration 1, loss = 1.34182386\n",
            "Iteration 2, loss = 1.20486792\n",
            "Iteration 3, loss = 1.03052691\n",
            "Iteration 4, loss = 0.87573110\n",
            "Iteration 5, loss = 0.76767102\n",
            "Iteration 6, loss = 0.71005122\n",
            "Iteration 7, loss = 0.68025523\n",
            "Iteration 8, loss = 0.66452169\n",
            "Iteration 9, loss = 0.65582803\n",
            "Iteration 10, loss = 0.64798558\n",
            "Iteration 11, loss = 0.64058304\n",
            "Iteration 12, loss = 0.63342237\n",
            "Iteration 13, loss = 0.62701482\n",
            "Iteration 14, loss = 0.62068021\n",
            "Iteration 15, loss = 0.61547319\n",
            "Iteration 16, loss = 0.61076412\n",
            "Iteration 17, loss = 0.60674878\n",
            "Iteration 18, loss = 0.60287881\n",
            "Iteration 19, loss = 0.59941555\n",
            "Iteration 20, loss = 0.59658795\n",
            "Iteration 21, loss = 0.59379273\n",
            "Iteration 22, loss = 0.59137819\n",
            "Iteration 23, loss = 0.58868711\n",
            "Iteration 24, loss = 0.58623099\n",
            "Iteration 25, loss = 0.58407023\n",
            "Iteration 26, loss = 0.58184564\n",
            "Iteration 27, loss = 0.57995834\n",
            "Iteration 28, loss = 0.57811124\n",
            "Iteration 29, loss = 0.57634992\n",
            "Iteration 30, loss = 0.57458462\n",
            "Iteration 31, loss = 0.57298054\n",
            "Iteration 32, loss = 0.57144934\n",
            "Iteration 33, loss = 0.56999777\n",
            "Iteration 34, loss = 0.56847783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 35, loss = 0.56719457\n",
            "Iteration 36, loss = 0.56571401\n",
            "Iteration 37, loss = 0.56429184\n",
            "Iteration 38, loss = 0.56282301\n",
            "Iteration 39, loss = 0.56135988\n",
            "Iteration 40, loss = 0.55987407\n",
            "Iteration 41, loss = 0.55831365\n",
            "Iteration 42, loss = 0.55708077\n",
            "Iteration 43, loss = 0.55575808\n",
            "Iteration 44, loss = 0.55445939\n",
            "Iteration 45, loss = 0.55327371\n",
            "Iteration 46, loss = 0.55209696\n",
            "Iteration 47, loss = 0.55130947\n",
            "Iteration 48, loss = 0.55024949\n",
            "Iteration 49, loss = 0.54932537\n",
            "Iteration 50, loss = 0.54833315\n",
            "Iteration 51, loss = 0.54736602\n",
            "Iteration 52, loss = 0.54661875\n",
            "Iteration 53, loss = 0.54587476\n",
            "Iteration 54, loss = 0.54513703\n",
            "Iteration 55, loss = 0.54429737\n",
            "Iteration 56, loss = 0.54357680\n",
            "Iteration 57, loss = 0.54285780\n",
            "Iteration 58, loss = 0.54213330\n",
            "Iteration 59, loss = 0.54160968\n",
            "Iteration 60, loss = 0.54088100\n",
            "Iteration 61, loss = 0.54034655\n",
            "Iteration 62, loss = 0.53973508\n",
            "Iteration 63, loss = 0.53941835\n",
            "Iteration 64, loss = 0.53872499\n",
            "Iteration 65, loss = 0.53810126\n",
            "Iteration 66, loss = 0.53764695\n",
            "Iteration 67, loss = 0.53703482\n",
            "Iteration 68, loss = 0.53660654\n",
            "Iteration 69, loss = 0.53600849\n",
            "Iteration 70, loss = 0.53553453\n",
            "Iteration 71, loss = 0.53495367\n",
            "Iteration 72, loss = 0.53445071\n",
            "Iteration 73, loss = 0.53399473\n",
            "Iteration 74, loss = 0.53354493\n",
            "Iteration 75, loss = 0.53323114\n",
            "Iteration 76, loss = 0.53281414\n",
            "Iteration 77, loss = 0.53234541\n",
            "Iteration 78, loss = 0.53195831\n",
            "Iteration 79, loss = 0.53161136\n",
            "Iteration 80, loss = 0.53126047\n",
            "Iteration 81, loss = 0.53085038\n",
            "Iteration 82, loss = 0.53049602\n",
            "Iteration 83, loss = 0.53010471\n",
            "Iteration 84, loss = 0.52972940\n",
            "Iteration 85, loss = 0.52942890\n",
            "Iteration 86, loss = 0.52903268\n",
            "Iteration 87, loss = 0.52860258\n",
            "Iteration 88, loss = 0.52823206\n",
            "Iteration 89, loss = 0.52794637\n",
            "Iteration 90, loss = 0.52760289\n",
            "Iteration 91, loss = 0.52728967\n",
            "Iteration 92, loss = 0.52693172\n",
            "Iteration 93, loss = 0.52664655\n",
            "Iteration 94, loss = 0.52628103\n",
            "Iteration 95, loss = 0.52585124\n",
            "Iteration 96, loss = 0.52557953\n",
            "Iteration 97, loss = 0.52530700\n",
            "Iteration 98, loss = 0.52495444\n",
            "Iteration 99, loss = 0.52463324\n",
            "Iteration 100, loss = 0.52435696\n",
            "Iteration 101, loss = 0.52406326\n",
            "Iteration 102, loss = 0.52373880\n",
            "Iteration 103, loss = 0.52357136\n",
            "Iteration 104, loss = 0.52330951\n",
            "Iteration 105, loss = 0.52302206\n",
            "Iteration 106, loss = 0.52279563\n",
            "Iteration 107, loss = 0.52267320\n",
            "Iteration 108, loss = 0.52232405\n",
            "Iteration 109, loss = 0.52213232\n",
            "Iteration 110, loss = 0.52180247\n",
            "Iteration 111, loss = 0.52145225\n",
            "Iteration 112, loss = 0.52126538\n",
            "Iteration 113, loss = 0.52094972\n",
            "Iteration 114, loss = 0.52063710\n",
            "Iteration 115, loss = 0.52041963\n",
            "Iteration 116, loss = 0.52016501\n",
            "Iteration 117, loss = 0.51994950\n",
            "Iteration 118, loss = 0.51968411\n",
            "Iteration 119, loss = 0.51963826\n",
            "Iteration 120, loss = 0.51927120\n",
            "Iteration 121, loss = 0.51907988\n",
            "Iteration 122, loss = 0.51882328\n",
            "Iteration 123, loss = 0.51856471\n",
            "Iteration 124, loss = 0.51839948\n",
            "Iteration 125, loss = 0.51809568\n",
            "Iteration 126, loss = 0.51792599\n",
            "Iteration 127, loss = 0.51780522\n",
            "Iteration 128, loss = 0.51755738\n",
            "Iteration 129, loss = 0.51728052\n",
            "Iteration 130, loss = 0.51698920\n",
            "Iteration 131, loss = 0.51668455\n",
            "Iteration 132, loss = 0.51661627\n",
            "Iteration 133, loss = 0.51626315\n",
            "Iteration 134, loss = 0.51614263\n",
            "Iteration 135, loss = 0.51589745\n",
            "Iteration 136, loss = 0.51566872\n",
            "Iteration 137, loss = 0.51559520\n",
            "Iteration 138, loss = 0.51540490\n",
            "Iteration 139, loss = 0.51520248\n",
            "Iteration 140, loss = 0.51494708\n",
            "Iteration 141, loss = 0.51480726\n",
            "Iteration 142, loss = 0.51462756\n",
            "Iteration 143, loss = 0.51451956\n",
            "Iteration 144, loss = 0.51423886\n",
            "Iteration 145, loss = 0.51421063\n",
            "Iteration 146, loss = 0.51394809\n",
            "Iteration 147, loss = 0.51387299\n",
            "Iteration 148, loss = 0.51367021\n",
            "Iteration 149, loss = 0.51357374\n",
            "Iteration 150, loss = 0.51334572\n",
            "Iteration 151, loss = 0.51322501\n",
            "Iteration 152, loss = 0.51302900\n",
            "Iteration 153, loss = 0.51287177\n",
            "Iteration 154, loss = 0.51277825\n",
            "Iteration 155, loss = 0.51278512\n",
            "Iteration 156, loss = 0.51276972\n",
            "Iteration 157, loss = 0.51260030\n",
            "Iteration 158, loss = 0.51236373\n",
            "Iteration 159, loss = 0.51222677\n",
            "Iteration 160, loss = 0.51197345\n",
            "Iteration 161, loss = 0.51174980\n",
            "Iteration 162, loss = 0.51166179\n",
            "Iteration 163, loss = 0.51158052\n",
            "Iteration 164, loss = 0.51125207\n",
            "Iteration 165, loss = 0.51108281\n",
            "Iteration 166, loss = 0.51085771\n",
            "Iteration 167, loss = 0.51076873\n",
            "Iteration 168, loss = 0.51063824\n",
            "Iteration 169, loss = 0.51051966\n",
            "Iteration 170, loss = 0.51048734\n",
            "Iteration 171, loss = 0.51024973\n",
            "Iteration 172, loss = 0.50997772\n",
            "Iteration 173, loss = 0.50993956\n",
            "Iteration 174, loss = 0.50999497\n",
            "Iteration 175, loss = 0.50983296\n",
            "Iteration 176, loss = 0.50957546\n",
            "Iteration 177, loss = 0.50948717\n",
            "Iteration 178, loss = 0.50928462\n",
            "Iteration 179, loss = 0.50924133\n",
            "Iteration 180, loss = 0.50895414\n",
            "Iteration 181, loss = 0.50887183\n",
            "Iteration 182, loss = 0.50872777\n",
            "Iteration 183, loss = 0.50877802\n",
            "Iteration 184, loss = 0.50865577\n",
            "Iteration 185, loss = 0.50845977\n",
            "Iteration 186, loss = 0.50836608\n",
            "Iteration 187, loss = 0.50818923\n",
            "Iteration 188, loss = 0.50803508\n",
            "Iteration 189, loss = 0.50782959\n",
            "Iteration 190, loss = 0.50772157\n",
            "Iteration 191, loss = 0.50755464\n",
            "Iteration 192, loss = 0.50738673\n",
            "Iteration 193, loss = 0.50722245\n",
            "Iteration 194, loss = 0.50689474\n",
            "Iteration 195, loss = 0.50671747\n",
            "Iteration 196, loss = 0.50655550\n",
            "Iteration 197, loss = 0.50644272\n",
            "Iteration 198, loss = 0.50630171\n",
            "Iteration 199, loss = 0.50614826\n",
            "Iteration 200, loss = 0.50601847\n",
            "Iteration 201, loss = 0.50587447\n",
            "Iteration 202, loss = 0.50583123\n",
            "Iteration 203, loss = 0.50562651\n",
            "Iteration 204, loss = 0.50573364\n",
            "Iteration 205, loss = 0.50549381\n",
            "Iteration 206, loss = 0.50539144\n",
            "Iteration 207, loss = 0.50525440\n",
            "Iteration 208, loss = 0.50512619\n",
            "Iteration 209, loss = 0.50500666\n",
            "Iteration 210, loss = 0.50480214\n",
            "Iteration 211, loss = 0.50466678\n",
            "Iteration 212, loss = 0.50468148\n",
            "Iteration 213, loss = 0.50455417\n",
            "Iteration 214, loss = 0.50426692\n",
            "Iteration 215, loss = 0.50420195\n",
            "Iteration 216, loss = 0.50404079\n",
            "Iteration 217, loss = 0.50385737\n",
            "Iteration 218, loss = 0.50439982\n",
            "Iteration 219, loss = 0.50388609\n",
            "Iteration 220, loss = 0.50406117\n",
            "Iteration 221, loss = 0.50373184\n",
            "Iteration 222, loss = 0.50355048\n",
            "Iteration 223, loss = 0.50343650\n",
            "Iteration 224, loss = 0.50331904\n",
            "Iteration 225, loss = 0.50325448\n",
            "Iteration 226, loss = 0.50305582\n",
            "Iteration 227, loss = 0.50287456\n",
            "Iteration 228, loss = 0.50281799\n",
            "Iteration 229, loss = 0.50268303\n",
            "Iteration 230, loss = 0.50263885\n",
            "Iteration 231, loss = 0.50247427\n",
            "Iteration 232, loss = 0.50249424\n",
            "Iteration 233, loss = 0.50242037\n",
            "Iteration 234, loss = 0.50232437\n",
            "Iteration 235, loss = 0.50229807\n",
            "Iteration 236, loss = 0.50218550\n",
            "Iteration 237, loss = 0.50209411\n",
            "Iteration 238, loss = 0.50194667\n",
            "Iteration 239, loss = 0.50186940\n",
            "Iteration 240, loss = 0.50183464\n",
            "Iteration 241, loss = 0.50159298\n",
            "Iteration 242, loss = 0.50166575\n",
            "Iteration 243, loss = 0.50179752\n",
            "Iteration 244, loss = 0.50150307\n",
            "Iteration 245, loss = 0.50152092\n",
            "Iteration 246, loss = 0.50156039\n",
            "Iteration 247, loss = 0.50139240\n",
            "Iteration 248, loss = 0.50123216\n",
            "Iteration 249, loss = 0.50123985\n",
            "Iteration 250, loss = 0.50120864\n",
            "Validation accuracy for learning rate= 0.01  and epoaches= 250 and for layer number 6 : 0.7\n",
            "Iteration 1, loss = 0.61693719\n",
            "Iteration 2, loss = 0.61686447\n",
            "Iteration 3, loss = 0.61675151\n",
            "Iteration 4, loss = 0.61661842\n",
            "Iteration 5, loss = 0.61646426\n",
            "Iteration 6, loss = 0.61631571\n",
            "Iteration 7, loss = 0.61614581\n",
            "Iteration 8, loss = 0.61596065\n",
            "Iteration 9, loss = 0.61578398\n",
            "Iteration 10, loss = 0.61560733\n",
            "Iteration 11, loss = 0.61543375\n",
            "Iteration 12, loss = 0.61527975\n",
            "Iteration 13, loss = 0.61511094\n",
            "Iteration 14, loss = 0.61495375\n",
            "Iteration 15, loss = 0.61478984\n",
            "Iteration 16, loss = 0.61463306\n",
            "Iteration 17, loss = 0.61447873\n",
            "Iteration 18, loss = 0.61432621\n",
            "Iteration 19, loss = 0.61417436\n",
            "Iteration 20, loss = 0.61400728\n",
            "Iteration 21, loss = 0.61383297\n",
            "Iteration 22, loss = 0.61367462\n",
            "Iteration 23, loss = 0.61350652\n",
            "Iteration 24, loss = 0.61335359\n",
            "Iteration 25, loss = 0.61319555\n",
            "Iteration 26, loss = 0.61306514\n",
            "Iteration 27, loss = 0.61290532\n",
            "Iteration 28, loss = 0.61277722\n",
            "Iteration 29, loss = 0.61265346\n",
            "Iteration 30, loss = 0.61251235\n",
            "Iteration 31, loss = 0.61238930\n",
            "Iteration 32, loss = 0.61226155\n",
            "Iteration 33, loss = 0.61213591\n",
            "Iteration 34, loss = 0.61202127\n",
            "Iteration 35, loss = 0.61189676\n",
            "Iteration 36, loss = 0.61176314\n",
            "Iteration 37, loss = 0.61162506\n",
            "Iteration 38, loss = 0.61149544\n",
            "Iteration 39, loss = 0.61137006\n",
            "Iteration 40, loss = 0.61124508\n",
            "Iteration 41, loss = 0.61110902\n",
            "Iteration 42, loss = 0.61099032\n",
            "Iteration 43, loss = 0.61085784\n",
            "Iteration 44, loss = 0.61073928\n",
            "Iteration 45, loss = 0.61062182\n",
            "Iteration 46, loss = 0.61048916\n",
            "Iteration 47, loss = 0.61036991\n",
            "Iteration 48, loss = 0.61024277\n",
            "Iteration 49, loss = 0.61013546\n",
            "Iteration 50, loss = 0.61002089\n",
            "Iteration 51, loss = 0.60990687\n",
            "Iteration 52, loss = 0.60979970\n",
            "Iteration 53, loss = 0.60969865\n",
            "Iteration 54, loss = 0.60959045\n",
            "Iteration 55, loss = 0.60948292\n",
            "Iteration 56, loss = 0.60937738\n",
            "Iteration 57, loss = 0.60926927\n",
            "Iteration 58, loss = 0.60917448\n",
            "Iteration 59, loss = 0.60906914\n",
            "Iteration 60, loss = 0.60896820\n",
            "Iteration 61, loss = 0.60886422\n",
            "Iteration 62, loss = 0.60877775\n",
            "Iteration 63, loss = 0.60867478\n",
            "Iteration 64, loss = 0.60857955\n",
            "Iteration 65, loss = 0.60848439\n",
            "Iteration 66, loss = 0.60838570\n",
            "Iteration 67, loss = 0.60829814\n",
            "Iteration 68, loss = 0.60820577\n",
            "Iteration 69, loss = 0.60812774\n",
            "Iteration 70, loss = 0.60803664\n",
            "Iteration 71, loss = 0.60794173\n",
            "Iteration 72, loss = 0.60785282\n",
            "Iteration 73, loss = 0.60776810\n",
            "Iteration 74, loss = 0.60766500\n",
            "Iteration 75, loss = 0.60758161\n",
            "Iteration 76, loss = 0.60748356\n",
            "Iteration 77, loss = 0.60739894\n",
            "Iteration 78, loss = 0.60731203\n",
            "Iteration 79, loss = 0.60722005\n",
            "Iteration 80, loss = 0.60712968\n",
            "Iteration 81, loss = 0.60704893\n",
            "Iteration 82, loss = 0.60696141\n",
            "Iteration 83, loss = 0.60688701\n",
            "Iteration 84, loss = 0.60679662\n",
            "Iteration 85, loss = 0.60670958\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61771206\n",
            "Iteration 2, loss = 0.61763256\n",
            "Iteration 3, loss = 0.61750896\n",
            "Iteration 4, loss = 0.61736013\n",
            "Iteration 5, loss = 0.61720601\n",
            "Iteration 6, loss = 0.61703925\n",
            "Iteration 7, loss = 0.61687983\n",
            "Iteration 8, loss = 0.61668458\n",
            "Iteration 9, loss = 0.61650897\n",
            "Iteration 10, loss = 0.61634191\n",
            "Iteration 11, loss = 0.61616438\n",
            "Iteration 12, loss = 0.61600889\n",
            "Iteration 13, loss = 0.61582381\n",
            "Iteration 14, loss = 0.61566827\n",
            "Iteration 15, loss = 0.61549674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 16, loss = 0.61534090\n",
            "Iteration 17, loss = 0.61517681\n",
            "Iteration 18, loss = 0.61502298\n",
            "Iteration 19, loss = 0.61486598\n",
            "Iteration 20, loss = 0.61470307\n",
            "Iteration 21, loss = 0.61454151\n",
            "Iteration 22, loss = 0.61438853\n",
            "Iteration 23, loss = 0.61422513\n",
            "Iteration 24, loss = 0.61407653\n",
            "Iteration 25, loss = 0.61391791\n",
            "Iteration 26, loss = 0.61377394\n",
            "Iteration 27, loss = 0.61362218\n",
            "Iteration 28, loss = 0.61348831\n",
            "Iteration 29, loss = 0.61334584\n",
            "Iteration 30, loss = 0.61321506\n",
            "Iteration 31, loss = 0.61309244\n",
            "Iteration 32, loss = 0.61295282\n",
            "Iteration 33, loss = 0.61282899\n",
            "Iteration 34, loss = 0.61269835\n",
            "Iteration 35, loss = 0.61258404\n",
            "Iteration 36, loss = 0.61244344\n",
            "Iteration 37, loss = 0.61230626\n",
            "Iteration 38, loss = 0.61216372\n",
            "Iteration 39, loss = 0.61203699\n",
            "Iteration 40, loss = 0.61189655\n",
            "Iteration 41, loss = 0.61175462\n",
            "Iteration 42, loss = 0.61161427\n",
            "Iteration 43, loss = 0.61148385\n",
            "Iteration 44, loss = 0.61135062\n",
            "Iteration 45, loss = 0.61122322\n",
            "Iteration 46, loss = 0.61108342\n",
            "Iteration 47, loss = 0.61094533\n",
            "Iteration 48, loss = 0.61081445\n",
            "Iteration 49, loss = 0.61068681\n",
            "Iteration 50, loss = 0.61056174\n",
            "Iteration 51, loss = 0.61044813\n",
            "Iteration 52, loss = 0.61033493\n",
            "Iteration 53, loss = 0.61021829\n",
            "Iteration 54, loss = 0.61010919\n",
            "Iteration 55, loss = 0.61000586\n",
            "Iteration 56, loss = 0.60988739\n",
            "Iteration 57, loss = 0.60976884\n",
            "Iteration 58, loss = 0.60966597\n",
            "Iteration 59, loss = 0.60955587\n",
            "Iteration 60, loss = 0.60944274\n",
            "Iteration 61, loss = 0.60932539\n",
            "Iteration 62, loss = 0.60923146\n",
            "Iteration 63, loss = 0.60909925\n",
            "Iteration 64, loss = 0.60898774\n",
            "Iteration 65, loss = 0.60887739\n",
            "Iteration 66, loss = 0.60875534\n",
            "Iteration 67, loss = 0.60864213\n",
            "Iteration 68, loss = 0.60852801\n",
            "Iteration 69, loss = 0.60842906\n",
            "Iteration 70, loss = 0.60832290\n",
            "Iteration 71, loss = 0.60821626\n",
            "Iteration 72, loss = 0.60811197\n",
            "Iteration 73, loss = 0.60800907\n",
            "Iteration 74, loss = 0.60789491\n",
            "Iteration 75, loss = 0.60780669\n",
            "Iteration 76, loss = 0.60769204\n",
            "Iteration 77, loss = 0.60759716\n",
            "Iteration 78, loss = 0.60749301\n",
            "Iteration 79, loss = 0.60738373\n",
            "Iteration 80, loss = 0.60728635\n",
            "Iteration 81, loss = 0.60719595\n",
            "Iteration 82, loss = 0.60709942\n",
            "Iteration 83, loss = 0.60701638\n",
            "Iteration 84, loss = 0.60692635\n",
            "Iteration 85, loss = 0.60683945\n",
            "Iteration 86, loss = 0.60675609\n",
            "Iteration 87, loss = 0.60667504\n",
            "Iteration 88, loss = 0.60659137\n",
            "Iteration 89, loss = 0.60651963\n",
            "Iteration 90, loss = 0.60642717\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61763916\n",
            "Iteration 2, loss = 0.61755997\n",
            "Iteration 3, loss = 0.61742232\n",
            "Iteration 4, loss = 0.61725393\n",
            "Iteration 5, loss = 0.61708619\n",
            "Iteration 6, loss = 0.61689799\n",
            "Iteration 7, loss = 0.61671618\n",
            "Iteration 8, loss = 0.61650164\n",
            "Iteration 9, loss = 0.61630973\n",
            "Iteration 10, loss = 0.61611724\n",
            "Iteration 11, loss = 0.61593663\n",
            "Iteration 12, loss = 0.61576363\n",
            "Iteration 13, loss = 0.61556226\n",
            "Iteration 14, loss = 0.61538666\n",
            "Iteration 15, loss = 0.61520889\n",
            "Iteration 16, loss = 0.61503323\n",
            "Iteration 17, loss = 0.61486521\n",
            "Iteration 18, loss = 0.61470006\n",
            "Iteration 19, loss = 0.61453828\n",
            "Iteration 20, loss = 0.61437299\n",
            "Iteration 21, loss = 0.61421653\n",
            "Iteration 22, loss = 0.61405569\n",
            "Iteration 23, loss = 0.61388640\n",
            "Iteration 24, loss = 0.61373448\n",
            "Iteration 25, loss = 0.61357646\n",
            "Iteration 26, loss = 0.61342231\n",
            "Iteration 27, loss = 0.61327118\n",
            "Iteration 28, loss = 0.61311335\n",
            "Iteration 29, loss = 0.61297079\n",
            "Iteration 30, loss = 0.61282693\n",
            "Iteration 31, loss = 0.61269890\n",
            "Iteration 32, loss = 0.61255474\n",
            "Iteration 33, loss = 0.61241223\n",
            "Iteration 34, loss = 0.61226940\n",
            "Iteration 35, loss = 0.61213160\n",
            "Iteration 36, loss = 0.61198811\n",
            "Iteration 37, loss = 0.61184581\n",
            "Iteration 38, loss = 0.61170049\n",
            "Iteration 39, loss = 0.61157611\n",
            "Iteration 40, loss = 0.61142730\n",
            "Iteration 41, loss = 0.61127667\n",
            "Iteration 42, loss = 0.61113732\n",
            "Iteration 43, loss = 0.61100411\n",
            "Iteration 44, loss = 0.61086913\n",
            "Iteration 45, loss = 0.61073484\n",
            "Iteration 46, loss = 0.61060145\n",
            "Iteration 47, loss = 0.61046314\n",
            "Iteration 48, loss = 0.61033730\n",
            "Iteration 49, loss = 0.61020822\n",
            "Iteration 50, loss = 0.61007740\n",
            "Iteration 51, loss = 0.60996338\n",
            "Iteration 52, loss = 0.60985178\n",
            "Iteration 53, loss = 0.60973070\n",
            "Iteration 54, loss = 0.60961527\n",
            "Iteration 55, loss = 0.60949856\n",
            "Iteration 56, loss = 0.60938041\n",
            "Iteration 57, loss = 0.60926013\n",
            "Iteration 58, loss = 0.60914580\n",
            "Iteration 59, loss = 0.60902755\n",
            "Iteration 60, loss = 0.60890665\n",
            "Iteration 61, loss = 0.60878651\n",
            "Iteration 62, loss = 0.60867591\n",
            "Iteration 63, loss = 0.60854401\n",
            "Iteration 64, loss = 0.60842057\n",
            "Iteration 65, loss = 0.60829508\n",
            "Iteration 66, loss = 0.60817432\n",
            "Iteration 67, loss = 0.60805562\n",
            "Iteration 68, loss = 0.60794420\n",
            "Iteration 69, loss = 0.60783815\n",
            "Iteration 70, loss = 0.60773163\n",
            "Iteration 71, loss = 0.60763356\n",
            "Iteration 72, loss = 0.60751712\n",
            "Iteration 73, loss = 0.60740617\n",
            "Iteration 74, loss = 0.60728371\n",
            "Iteration 75, loss = 0.60719874\n",
            "Iteration 76, loss = 0.60707233\n",
            "Iteration 77, loss = 0.60697847\n",
            "Iteration 78, loss = 0.60686966\n",
            "Iteration 79, loss = 0.60676477\n",
            "Iteration 80, loss = 0.60666713\n",
            "Iteration 81, loss = 0.60657029\n",
            "Iteration 82, loss = 0.60647206\n",
            "Iteration 83, loss = 0.60638523\n",
            "Iteration 84, loss = 0.60629166\n",
            "Iteration 85, loss = 0.60620042\n",
            "Iteration 86, loss = 0.60612214\n",
            "Iteration 87, loss = 0.60604223\n",
            "Iteration 88, loss = 0.60596838\n",
            "Iteration 89, loss = 0.60589523\n",
            "Iteration 90, loss = 0.60580521\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62088357\n",
            "Iteration 2, loss = 0.62080758\n",
            "Iteration 3, loss = 0.62069495\n",
            "Iteration 4, loss = 0.62054238\n",
            "Iteration 5, loss = 0.62038891\n",
            "Iteration 6, loss = 0.62021289\n",
            "Iteration 7, loss = 0.62003518\n",
            "Iteration 8, loss = 0.61983724\n",
            "Iteration 9, loss = 0.61965125\n",
            "Iteration 10, loss = 0.61946574\n",
            "Iteration 11, loss = 0.61928678\n",
            "Iteration 12, loss = 0.61912370\n",
            "Iteration 13, loss = 0.61894901\n",
            "Iteration 14, loss = 0.61877305\n",
            "Iteration 15, loss = 0.61860692\n",
            "Iteration 16, loss = 0.61844169\n",
            "Iteration 17, loss = 0.61826895\n",
            "Iteration 18, loss = 0.61810540\n",
            "Iteration 19, loss = 0.61794667\n",
            "Iteration 20, loss = 0.61779409\n",
            "Iteration 21, loss = 0.61765462\n",
            "Iteration 22, loss = 0.61749969\n",
            "Iteration 23, loss = 0.61734374\n",
            "Iteration 24, loss = 0.61718970\n",
            "Iteration 25, loss = 0.61702731\n",
            "Iteration 26, loss = 0.61686454\n",
            "Iteration 27, loss = 0.61672331\n",
            "Iteration 28, loss = 0.61655046\n",
            "Iteration 29, loss = 0.61640417\n",
            "Iteration 30, loss = 0.61625657\n",
            "Iteration 31, loss = 0.61612342\n",
            "Iteration 32, loss = 0.61597996\n",
            "Iteration 33, loss = 0.61583342\n",
            "Iteration 34, loss = 0.61569987\n",
            "Iteration 35, loss = 0.61556063\n",
            "Iteration 36, loss = 0.61542958\n",
            "Iteration 37, loss = 0.61530817\n",
            "Iteration 38, loss = 0.61517959\n",
            "Iteration 39, loss = 0.61506558\n",
            "Iteration 40, loss = 0.61494687\n",
            "Iteration 41, loss = 0.61483037\n",
            "Iteration 42, loss = 0.61470883\n",
            "Iteration 43, loss = 0.61460197\n",
            "Iteration 44, loss = 0.61447871\n",
            "Iteration 45, loss = 0.61436047\n",
            "Iteration 46, loss = 0.61425494\n",
            "Iteration 47, loss = 0.61414539\n",
            "Iteration 48, loss = 0.61403406\n",
            "Iteration 49, loss = 0.61392290\n",
            "Iteration 50, loss = 0.61380453\n",
            "Iteration 51, loss = 0.61371073\n",
            "Iteration 52, loss = 0.61360963\n",
            "Iteration 53, loss = 0.61350809\n",
            "Iteration 54, loss = 0.61341021\n",
            "Iteration 55, loss = 0.61331563\n",
            "Iteration 56, loss = 0.61322032\n",
            "Iteration 57, loss = 0.61312362\n",
            "Iteration 58, loss = 0.61303187\n",
            "Iteration 59, loss = 0.61293793\n",
            "Iteration 60, loss = 0.61284660\n",
            "Iteration 61, loss = 0.61275442\n",
            "Iteration 62, loss = 0.61266764\n",
            "Iteration 63, loss = 0.61256719\n",
            "Iteration 64, loss = 0.61246187\n",
            "Iteration 65, loss = 0.61235746\n",
            "Iteration 66, loss = 0.61225596\n",
            "Iteration 67, loss = 0.61215694\n",
            "Iteration 68, loss = 0.61206416\n",
            "Iteration 69, loss = 0.61197032\n",
            "Iteration 70, loss = 0.61187100\n",
            "Iteration 71, loss = 0.61178403\n",
            "Iteration 72, loss = 0.61167796\n",
            "Iteration 73, loss = 0.61157994\n",
            "Iteration 74, loss = 0.61147541\n",
            "Iteration 75, loss = 0.61140259\n",
            "Iteration 76, loss = 0.61129541\n",
            "Iteration 77, loss = 0.61120835\n",
            "Iteration 78, loss = 0.61111527\n",
            "Iteration 79, loss = 0.61103216\n",
            "Iteration 80, loss = 0.61095465\n",
            "Iteration 81, loss = 0.61087089\n",
            "Iteration 82, loss = 0.61079131\n",
            "Iteration 83, loss = 0.61071503\n",
            "Iteration 84, loss = 0.61063483\n",
            "Iteration 85, loss = 0.61055422\n",
            "Iteration 86, loss = 0.61048168\n",
            "Iteration 87, loss = 0.61040607\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62018025\n",
            "Iteration 2, loss = 0.62009819\n",
            "Iteration 3, loss = 0.61998317\n",
            "Iteration 4, loss = 0.61981666\n",
            "Iteration 5, loss = 0.61963710\n",
            "Iteration 6, loss = 0.61944478\n",
            "Iteration 7, loss = 0.61925290\n",
            "Iteration 8, loss = 0.61903845\n",
            "Iteration 9, loss = 0.61885557\n",
            "Iteration 10, loss = 0.61866268\n",
            "Iteration 11, loss = 0.61848604\n",
            "Iteration 12, loss = 0.61833134\n",
            "Iteration 13, loss = 0.61815251\n",
            "Iteration 14, loss = 0.61796856\n",
            "Iteration 15, loss = 0.61780050\n",
            "Iteration 16, loss = 0.61763388\n",
            "Iteration 17, loss = 0.61745805\n",
            "Iteration 18, loss = 0.61727872\n",
            "Iteration 19, loss = 0.61709604\n",
            "Iteration 20, loss = 0.61693584\n",
            "Iteration 21, loss = 0.61678432\n",
            "Iteration 22, loss = 0.61660435\n",
            "Iteration 23, loss = 0.61645117\n",
            "Iteration 24, loss = 0.61629760\n",
            "Iteration 25, loss = 0.61613359\n",
            "Iteration 26, loss = 0.61598174\n",
            "Iteration 27, loss = 0.61583842\n",
            "Iteration 28, loss = 0.61567659\n",
            "Iteration 29, loss = 0.61552571\n",
            "Iteration 30, loss = 0.61538139\n",
            "Iteration 31, loss = 0.61524275\n",
            "Iteration 32, loss = 0.61509965\n",
            "Iteration 33, loss = 0.61494620\n",
            "Iteration 34, loss = 0.61481205\n",
            "Iteration 35, loss = 0.61466606\n",
            "Iteration 36, loss = 0.61454011\n",
            "Iteration 37, loss = 0.61442324\n",
            "Iteration 38, loss = 0.61429471\n",
            "Iteration 39, loss = 0.61417885\n",
            "Iteration 40, loss = 0.61406637\n",
            "Iteration 41, loss = 0.61394071\n",
            "Iteration 42, loss = 0.61382750\n",
            "Iteration 43, loss = 0.61370365\n",
            "Iteration 44, loss = 0.61359746\n",
            "Iteration 45, loss = 0.61346493\n",
            "Iteration 46, loss = 0.61335820\n",
            "Iteration 47, loss = 0.61324493\n",
            "Iteration 48, loss = 0.61312768\n",
            "Iteration 49, loss = 0.61300162\n",
            "Iteration 50, loss = 0.61287863\n",
            "Iteration 51, loss = 0.61278578\n",
            "Iteration 52, loss = 0.61266496\n",
            "Iteration 53, loss = 0.61256068\n",
            "Iteration 54, loss = 0.61244833\n",
            "Iteration 55, loss = 0.61235408\n",
            "Iteration 56, loss = 0.61225595\n",
            "Iteration 57, loss = 0.61215760\n",
            "Iteration 58, loss = 0.61206425\n",
            "Iteration 59, loss = 0.61197433\n",
            "Iteration 60, loss = 0.61188279\n",
            "Iteration 61, loss = 0.61179557\n",
            "Iteration 62, loss = 0.61170404\n",
            "Iteration 63, loss = 0.61161334\n",
            "Iteration 64, loss = 0.61151561\n",
            "Iteration 65, loss = 0.61141306\n",
            "Iteration 66, loss = 0.61130998\n",
            "Iteration 67, loss = 0.61122290\n",
            "Iteration 68, loss = 0.61112557\n",
            "Iteration 69, loss = 0.61103638\n",
            "Iteration 70, loss = 0.61095068\n",
            "Iteration 71, loss = 0.61087186\n",
            "Iteration 72, loss = 0.61077796\n",
            "Iteration 73, loss = 0.61069137\n",
            "Iteration 74, loss = 0.61060678\n",
            "Iteration 75, loss = 0.61052912\n",
            "Iteration 76, loss = 0.61045012\n",
            "Iteration 77, loss = 0.61037871\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 100 and for layer number 2 : 0.7\n",
            "Iteration 1, loss = 0.84801653\n",
            "Iteration 2, loss = 0.84528764\n",
            "Iteration 3, loss = 0.84117560\n",
            "Iteration 4, loss = 0.83582942\n",
            "Iteration 5, loss = 0.82975974\n",
            "Iteration 6, loss = 0.82353252\n",
            "Iteration 7, loss = 0.81664243\n",
            "Iteration 8, loss = 0.81002340\n",
            "Iteration 9, loss = 0.80368440\n",
            "Iteration 10, loss = 0.79741786\n",
            "Iteration 11, loss = 0.79116889\n",
            "Iteration 12, loss = 0.78479499\n",
            "Iteration 13, loss = 0.77897234\n",
            "Iteration 14, loss = 0.77276067\n",
            "Iteration 15, loss = 0.76728918\n",
            "Iteration 16, loss = 0.76206401\n",
            "Iteration 17, loss = 0.75702104\n",
            "Iteration 18, loss = 0.75199301\n",
            "Iteration 19, loss = 0.74737027\n",
            "Iteration 20, loss = 0.74276500\n",
            "Iteration 21, loss = 0.73837289\n",
            "Iteration 22, loss = 0.73407699\n",
            "Iteration 23, loss = 0.73007924\n",
            "Iteration 24, loss = 0.72625222\n",
            "Iteration 25, loss = 0.72244250\n",
            "Iteration 26, loss = 0.71862878\n",
            "Iteration 27, loss = 0.71502063\n",
            "Iteration 28, loss = 0.71165062\n",
            "Iteration 29, loss = 0.70825508\n",
            "Iteration 30, loss = 0.70488572\n",
            "Iteration 31, loss = 0.70184174\n",
            "Iteration 32, loss = 0.69891904\n",
            "Iteration 33, loss = 0.69611030\n",
            "Iteration 34, loss = 0.69321969\n",
            "Iteration 35, loss = 0.69047473\n",
            "Iteration 36, loss = 0.68810891\n",
            "Iteration 37, loss = 0.68540721\n",
            "Iteration 38, loss = 0.68303292\n",
            "Iteration 39, loss = 0.68048045\n",
            "Iteration 40, loss = 0.67779155\n",
            "Iteration 41, loss = 0.67527131\n",
            "Iteration 42, loss = 0.67280099\n",
            "Iteration 43, loss = 0.67046862\n",
            "Iteration 44, loss = 0.66827526\n",
            "Iteration 45, loss = 0.66614495\n",
            "Iteration 46, loss = 0.66422713\n",
            "Iteration 47, loss = 0.66229631\n",
            "Iteration 48, loss = 0.66059117\n",
            "Iteration 49, loss = 0.65875777\n",
            "Iteration 50, loss = 0.65706317\n",
            "Iteration 51, loss = 0.65538023\n",
            "Iteration 52, loss = 0.65371124\n",
            "Iteration 53, loss = 0.65216698\n",
            "Iteration 54, loss = 0.65065006\n",
            "Iteration 55, loss = 0.64920995\n",
            "Iteration 56, loss = 0.64790729\n",
            "Iteration 57, loss = 0.64652529\n",
            "Iteration 58, loss = 0.64516241\n",
            "Iteration 59, loss = 0.64389573\n",
            "Iteration 60, loss = 0.64264924\n",
            "Iteration 61, loss = 0.64145436\n",
            "Iteration 62, loss = 0.64022518\n",
            "Iteration 63, loss = 0.63902072\n",
            "Iteration 64, loss = 0.63780270\n",
            "Iteration 65, loss = 0.63673321\n",
            "Iteration 66, loss = 0.63556199\n",
            "Iteration 67, loss = 0.63443060\n",
            "Iteration 68, loss = 0.63346065\n",
            "Iteration 69, loss = 0.63238086\n",
            "Iteration 70, loss = 0.63142079\n",
            "Iteration 71, loss = 0.63054257\n",
            "Iteration 72, loss = 0.62958069\n",
            "Iteration 73, loss = 0.62866353\n",
            "Iteration 74, loss = 0.62786278\n",
            "Iteration 75, loss = 0.62707678\n",
            "Iteration 76, loss = 0.62631715\n",
            "Iteration 77, loss = 0.62554014\n",
            "Iteration 78, loss = 0.62477088\n",
            "Iteration 79, loss = 0.62400197\n",
            "Iteration 80, loss = 0.62327855\n",
            "Iteration 81, loss = 0.62255956\n",
            "Iteration 82, loss = 0.62183722\n",
            "Iteration 83, loss = 0.62108874\n",
            "Iteration 84, loss = 0.62039759\n",
            "Iteration 85, loss = 0.61967253\n",
            "Iteration 86, loss = 0.61896942\n",
            "Iteration 87, loss = 0.61829945\n",
            "Iteration 88, loss = 0.61761280\n",
            "Iteration 89, loss = 0.61697162\n",
            "Iteration 90, loss = 0.61630797\n",
            "Iteration 91, loss = 0.61566924\n",
            "Iteration 92, loss = 0.61500381\n",
            "Iteration 93, loss = 0.61436935\n",
            "Iteration 94, loss = 0.61373605\n",
            "Iteration 95, loss = 0.61307396\n",
            "Iteration 96, loss = 0.61248474\n",
            "Iteration 97, loss = 0.61183704\n",
            "Iteration 98, loss = 0.61128115\n",
            "Iteration 99, loss = 0.61063105\n",
            "Iteration 100, loss = 0.61004000\n",
            "Iteration 1, loss = 0.84884434\n",
            "Iteration 2, loss = 0.84610604\n",
            "Iteration 3, loss = 0.84230708\n",
            "Iteration 4, loss = 0.83712877\n",
            "Iteration 5, loss = 0.83146006\n",
            "Iteration 6, loss = 0.82550296\n",
            "Iteration 7, loss = 0.81909492\n",
            "Iteration 8, loss = 0.81265443\n",
            "Iteration 9, loss = 0.80669577\n",
            "Iteration 10, loss = 0.80039728\n",
            "Iteration 11, loss = 0.79425360\n",
            "Iteration 12, loss = 0.78824072\n",
            "Iteration 13, loss = 0.78261576\n",
            "Iteration 14, loss = 0.77671008\n",
            "Iteration 15, loss = 0.77114439\n",
            "Iteration 16, loss = 0.76606181\n",
            "Iteration 17, loss = 0.76108403\n",
            "Iteration 18, loss = 0.75613587\n",
            "Iteration 19, loss = 0.75170254\n",
            "Iteration 20, loss = 0.74713289\n",
            "Iteration 21, loss = 0.74293770\n",
            "Iteration 22, loss = 0.73884508\n",
            "Iteration 23, loss = 0.73495863\n",
            "Iteration 24, loss = 0.73114961\n",
            "Iteration 25, loss = 0.72759086\n",
            "Iteration 26, loss = 0.72382245\n",
            "Iteration 27, loss = 0.72028484\n",
            "Iteration 28, loss = 0.71690495\n",
            "Iteration 29, loss = 0.71349934\n",
            "Iteration 30, loss = 0.71009845\n",
            "Iteration 31, loss = 0.70704925\n",
            "Iteration 32, loss = 0.70404036\n",
            "Iteration 33, loss = 0.70113012\n",
            "Iteration 34, loss = 0.69813861\n",
            "Iteration 35, loss = 0.69541121\n",
            "Iteration 36, loss = 0.69283424\n",
            "Iteration 37, loss = 0.69007100\n",
            "Iteration 38, loss = 0.68761821\n",
            "Iteration 39, loss = 0.68511950\n",
            "Iteration 40, loss = 0.68252852\n",
            "Iteration 41, loss = 0.68002110\n",
            "Iteration 42, loss = 0.67761807\n",
            "Iteration 43, loss = 0.67543891\n",
            "Iteration 44, loss = 0.67320696\n",
            "Iteration 45, loss = 0.67119636\n",
            "Iteration 46, loss = 0.66930743\n",
            "Iteration 47, loss = 0.66722632\n",
            "Iteration 48, loss = 0.66538411\n",
            "Iteration 49, loss = 0.66343125\n",
            "Iteration 50, loss = 0.66151966\n",
            "Iteration 51, loss = 0.65973000\n",
            "Iteration 52, loss = 0.65793582\n",
            "Iteration 53, loss = 0.65622869\n",
            "Iteration 54, loss = 0.65468002\n",
            "Iteration 55, loss = 0.65320535\n",
            "Iteration 56, loss = 0.65181447\n",
            "Iteration 57, loss = 0.65035231\n",
            "Iteration 58, loss = 0.64891425\n",
            "Iteration 59, loss = 0.64755073\n",
            "Iteration 60, loss = 0.64626251\n",
            "Iteration 61, loss = 0.64505130\n",
            "Iteration 62, loss = 0.64379440\n",
            "Iteration 63, loss = 0.64255345\n",
            "Iteration 64, loss = 0.64133698\n",
            "Iteration 65, loss = 0.64017137\n",
            "Iteration 66, loss = 0.63893323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 67, loss = 0.63778882\n",
            "Iteration 68, loss = 0.63673076\n",
            "Iteration 69, loss = 0.63560115\n",
            "Iteration 70, loss = 0.63455761\n",
            "Iteration 71, loss = 0.63353220\n",
            "Iteration 72, loss = 0.63246273\n",
            "Iteration 73, loss = 0.63147427\n",
            "Iteration 74, loss = 0.63055777\n",
            "Iteration 75, loss = 0.62967430\n",
            "Iteration 76, loss = 0.62883840\n",
            "Iteration 77, loss = 0.62794709\n",
            "Iteration 78, loss = 0.62718417\n",
            "Iteration 79, loss = 0.62641941\n",
            "Iteration 80, loss = 0.62567713\n",
            "Iteration 81, loss = 0.62496967\n",
            "Iteration 82, loss = 0.62424083\n",
            "Iteration 83, loss = 0.62351271\n",
            "Iteration 84, loss = 0.62281916\n",
            "Iteration 85, loss = 0.62211463\n",
            "Iteration 86, loss = 0.62139155\n",
            "Iteration 87, loss = 0.62068848\n",
            "Iteration 88, loss = 0.61997756\n",
            "Iteration 89, loss = 0.61929985\n",
            "Iteration 90, loss = 0.61858862\n",
            "Iteration 91, loss = 0.61794400\n",
            "Iteration 92, loss = 0.61723905\n",
            "Iteration 93, loss = 0.61656102\n",
            "Iteration 94, loss = 0.61591205\n",
            "Iteration 95, loss = 0.61519728\n",
            "Iteration 96, loss = 0.61459602\n",
            "Iteration 97, loss = 0.61397021\n",
            "Iteration 98, loss = 0.61336479\n",
            "Iteration 99, loss = 0.61277025\n",
            "Iteration 100, loss = 0.61218560\n",
            "Iteration 1, loss = 0.86923367\n",
            "Iteration 2, loss = 0.86623006\n",
            "Iteration 3, loss = 0.86201844\n",
            "Iteration 4, loss = 0.85633892\n",
            "Iteration 5, loss = 0.85008599\n",
            "Iteration 6, loss = 0.84356157\n",
            "Iteration 7, loss = 0.83652964\n",
            "Iteration 8, loss = 0.82935828\n",
            "Iteration 9, loss = 0.82278608\n",
            "Iteration 10, loss = 0.81571552\n",
            "Iteration 11, loss = 0.80886293\n",
            "Iteration 12, loss = 0.80218902\n",
            "Iteration 13, loss = 0.79599872\n",
            "Iteration 14, loss = 0.78964699\n",
            "Iteration 15, loss = 0.78337376\n",
            "Iteration 16, loss = 0.77753155\n",
            "Iteration 17, loss = 0.77180928\n",
            "Iteration 18, loss = 0.76609001\n",
            "Iteration 19, loss = 0.76110090\n",
            "Iteration 20, loss = 0.75627648\n",
            "Iteration 21, loss = 0.75155283\n",
            "Iteration 22, loss = 0.74704216\n",
            "Iteration 23, loss = 0.74279740\n",
            "Iteration 24, loss = 0.73856804\n",
            "Iteration 25, loss = 0.73462565\n",
            "Iteration 26, loss = 0.73055780\n",
            "Iteration 27, loss = 0.72667968\n",
            "Iteration 28, loss = 0.72311381\n",
            "Iteration 29, loss = 0.71931481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 30, loss = 0.71559199\n",
            "Iteration 31, loss = 0.71214502\n",
            "Iteration 32, loss = 0.70883599\n",
            "Iteration 33, loss = 0.70545153\n",
            "Iteration 34, loss = 0.70226445\n",
            "Iteration 35, loss = 0.69916085\n",
            "Iteration 36, loss = 0.69616862\n",
            "Iteration 37, loss = 0.69307073\n",
            "Iteration 38, loss = 0.69021815\n",
            "Iteration 39, loss = 0.68742445\n",
            "Iteration 40, loss = 0.68467159\n",
            "Iteration 41, loss = 0.68190363\n",
            "Iteration 42, loss = 0.67946455\n",
            "Iteration 43, loss = 0.67717399\n",
            "Iteration 44, loss = 0.67499244\n",
            "Iteration 45, loss = 0.67284265\n",
            "Iteration 46, loss = 0.67090200\n",
            "Iteration 47, loss = 0.66876285\n",
            "Iteration 48, loss = 0.66681518\n",
            "Iteration 49, loss = 0.66482789\n",
            "Iteration 50, loss = 0.66288284\n",
            "Iteration 51, loss = 0.66109203\n",
            "Iteration 52, loss = 0.65916577\n",
            "Iteration 53, loss = 0.65740321\n",
            "Iteration 54, loss = 0.65573998\n",
            "Iteration 55, loss = 0.65410347\n",
            "Iteration 56, loss = 0.65259831\n",
            "Iteration 57, loss = 0.65109783\n",
            "Iteration 58, loss = 0.64958658\n",
            "Iteration 59, loss = 0.64816078\n",
            "Iteration 60, loss = 0.64678991\n",
            "Iteration 61, loss = 0.64549181\n",
            "Iteration 62, loss = 0.64412795\n",
            "Iteration 63, loss = 0.64282614\n",
            "Iteration 64, loss = 0.64150110\n",
            "Iteration 65, loss = 0.64025473\n",
            "Iteration 66, loss = 0.63892570\n",
            "Iteration 67, loss = 0.63768365\n",
            "Iteration 68, loss = 0.63652642\n",
            "Iteration 69, loss = 0.63536168\n",
            "Iteration 70, loss = 0.63421997\n",
            "Iteration 71, loss = 0.63317325\n",
            "Iteration 72, loss = 0.63205658\n",
            "Iteration 73, loss = 0.63100731\n",
            "Iteration 74, loss = 0.63001822\n",
            "Iteration 75, loss = 0.62901823\n",
            "Iteration 76, loss = 0.62809679\n",
            "Iteration 77, loss = 0.62707914\n",
            "Iteration 78, loss = 0.62623111\n",
            "Iteration 79, loss = 0.62533088\n",
            "Iteration 80, loss = 0.62450127\n",
            "Iteration 81, loss = 0.62368795\n",
            "Iteration 82, loss = 0.62285271\n",
            "Iteration 83, loss = 0.62204441\n",
            "Iteration 84, loss = 0.62127094\n",
            "Iteration 85, loss = 0.62052542\n",
            "Iteration 86, loss = 0.61970997\n",
            "Iteration 87, loss = 0.61893977\n",
            "Iteration 88, loss = 0.61815595\n",
            "Iteration 89, loss = 0.61740030\n",
            "Iteration 90, loss = 0.61662252\n",
            "Iteration 91, loss = 0.61594190\n",
            "Iteration 92, loss = 0.61517044\n",
            "Iteration 93, loss = 0.61448395\n",
            "Iteration 94, loss = 0.61380520\n",
            "Iteration 95, loss = 0.61298957\n",
            "Iteration 96, loss = 0.61235810\n",
            "Iteration 97, loss = 0.61163696\n",
            "Iteration 98, loss = 0.61092207\n",
            "Iteration 99, loss = 0.61023909\n",
            "Iteration 100, loss = 0.60959741\n",
            "Iteration 1, loss = 0.84858612\n",
            "Iteration 2, loss = 0.84586103"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 3, loss = 0.84202808\n",
            "Iteration 4, loss = 0.83677269\n",
            "Iteration 5, loss = 0.83102491\n",
            "Iteration 6, loss = 0.82523503\n",
            "Iteration 7, loss = 0.81887373\n",
            "Iteration 8, loss = 0.81242895\n",
            "Iteration 9, loss = 0.80642299\n",
            "Iteration 10, loss = 0.80007903\n",
            "Iteration 11, loss = 0.79384885\n",
            "Iteration 12, loss = 0.78766457\n",
            "Iteration 13, loss = 0.78212698\n",
            "Iteration 14, loss = 0.77616718\n",
            "Iteration 15, loss = 0.77057092\n",
            "Iteration 16, loss = 0.76520465\n",
            "Iteration 17, loss = 0.76001179\n",
            "Iteration 18, loss = 0.75481795\n",
            "Iteration 19, loss = 0.75019268\n",
            "Iteration 20, loss = 0.74570973\n",
            "Iteration 21, loss = 0.74143312\n",
            "Iteration 22, loss = 0.73714002\n",
            "Iteration 23, loss = 0.73317639\n",
            "Iteration 24, loss = 0.72916594\n",
            "Iteration 25, loss = 0.72564340\n",
            "Iteration 26, loss = 0.72187504\n",
            "Iteration 27, loss = 0.71826418\n",
            "Iteration 28, loss = 0.71475157\n",
            "Iteration 29, loss = 0.71126395\n",
            "Iteration 30, loss = 0.70780464\n",
            "Iteration 31, loss = 0.70433933\n",
            "Iteration 32, loss = 0.70129476\n",
            "Iteration 33, loss = 0.69812788\n",
            "Iteration 34, loss = 0.69512355\n",
            "Iteration 35, loss = 0.69245263\n",
            "Iteration 36, loss = 0.68965835\n",
            "Iteration 37, loss = 0.68690679\n",
            "Iteration 38, loss = 0.68418766\n",
            "Iteration 39, loss = 0.68157247\n",
            "Iteration 40, loss = 0.67890392\n",
            "Iteration 41, loss = 0.67638975\n",
            "Iteration 42, loss = 0.67403004\n",
            "Iteration 43, loss = 0.67200248\n",
            "Iteration 44, loss = 0.66999841\n",
            "Iteration 45, loss = 0.66803721\n",
            "Iteration 46, loss = 0.66615995\n",
            "Iteration 47, loss = 0.66413871\n",
            "Iteration 48, loss = 0.66225463\n",
            "Iteration 49, loss = 0.66048563\n",
            "Iteration 50, loss = 0.65875979\n",
            "Iteration 51, loss = 0.65707940\n",
            "Iteration 52, loss = 0.65546792\n",
            "Iteration 53, loss = 0.65384613\n",
            "Iteration 54, loss = 0.65236260\n",
            "Iteration 55, loss = 0.65091725\n",
            "Iteration 56, loss = 0.64959092\n",
            "Iteration 57, loss = 0.64820250\n",
            "Iteration 58, loss = 0.64691513\n",
            "Iteration 59, loss = 0.64568032\n",
            "Iteration 60, loss = 0.64440258\n",
            "Iteration 61, loss = 0.64327986\n",
            "Iteration 62, loss = 0.64202316\n",
            "Iteration 63, loss = 0.64089945\n",
            "Iteration 64, loss = 0.63978166\n",
            "Iteration 65, loss = 0.63867096\n",
            "Iteration 66, loss = 0.63754716\n",
            "Iteration 67, loss = 0.63647957\n",
            "Iteration 68, loss = 0.63545464\n",
            "Iteration 69, loss = 0.63444067\n",
            "Iteration 70, loss = 0.63343899\n",
            "Iteration 71, loss = 0.63246926\n",
            "Iteration 72, loss = 0.63142338\n",
            "Iteration 73, loss = 0.63047341\n",
            "Iteration 74, loss = 0.62955579\n",
            "Iteration 75, loss = 0.62870095\n",
            "Iteration 76, loss = 0.62785827\n",
            "Iteration 77, loss = 0.62697778\n",
            "Iteration 78, loss = 0.62622000\n",
            "Iteration 79, loss = 0.62543734\n",
            "Iteration 80, loss = 0.62467271\n",
            "Iteration 81, loss = 0.62399221\n",
            "Iteration 82, loss = 0.62324994\n",
            "Iteration 83, loss = 0.62256615\n",
            "Iteration 84, loss = 0.62187724\n",
            "Iteration 85, loss = 0.62128307\n",
            "Iteration 86, loss = 0.62055994\n",
            "Iteration 87, loss = 0.61987967\n",
            "Iteration 88, loss = 0.61925896\n",
            "Iteration 89, loss = 0.61861177\n",
            "Iteration 90, loss = 0.61796749\n",
            "Iteration 91, loss = 0.61735920\n",
            "Iteration 92, loss = 0.61671408\n",
            "Iteration 93, loss = 0.61610607\n",
            "Iteration 94, loss = 0.61551042\n",
            "Iteration 95, loss = 0.61475085\n",
            "Iteration 96, loss = 0.61417133\n",
            "Iteration 97, loss = 0.61357502\n",
            "Iteration 98, loss = 0.61292309\n",
            "Iteration 99, loss = 0.61240326\n",
            "Iteration 100, loss = 0.61185817\n",
            "Iteration 1, loss = 0.84407341\n",
            "Iteration 2, loss = 0.84130009\n",
            "Iteration 3, loss = 0.83725245\n",
            "Iteration 4, loss = 0.83218094\n",
            "Iteration 5, loss = 0.82697800\n",
            "Iteration 6, loss = 0.82168607\n",
            "Iteration 7, loss = 0.81584661\n",
            "Iteration 8, loss = 0.81000185\n",
            "Iteration 9, loss = 0.80435363\n",
            "Iteration 10, loss = 0.79816107\n",
            "Iteration 11, loss = 0.79237071\n",
            "Iteration 12, loss = 0.78666337\n",
            "Iteration 13, loss = 0.78102998\n",
            "Iteration 14, loss = 0.77550694\n",
            "Iteration 15, loss = 0.77026749\n",
            "Iteration 16, loss = 0.76517323\n",
            "Iteration 17, loss = 0.76032485\n",
            "Iteration 18, loss = 0.75561036\n",
            "Iteration 19, loss = 0.75098454\n",
            "Iteration 20, loss = 0.74658742\n",
            "Iteration 21, loss = 0.74243839\n",
            "Iteration 22, loss = 0.73833552\n",
            "Iteration 23, loss = 0.73436580\n",
            "Iteration 24, loss = 0.73058470\n",
            "Iteration 25, loss = 0.72698704\n",
            "Iteration 26, loss = 0.72315689\n",
            "Iteration 27, loss = 0.71954935\n",
            "Iteration 28, loss = 0.71591667\n",
            "Iteration 29, loss = 0.71260479\n",
            "Iteration 30, loss = 0.70914756\n",
            "Iteration 31, loss = 0.70589731\n",
            "Iteration 32, loss = 0.70265938\n",
            "Iteration 33, loss = 0.69947938\n",
            "Iteration 34, loss = 0.69654095\n",
            "Iteration 35, loss = 0.69348703\n",
            "Iteration 36, loss = 0.69054236\n",
            "Iteration 37, loss = 0.68762425\n",
            "Iteration 38, loss = 0.68489428\n",
            "Iteration 39, loss = 0.68225194\n",
            "Iteration 40, loss = 0.67961680\n",
            "Iteration 41, loss = 0.67710505\n",
            "Iteration 42, loss = 0.67477275\n",
            "Iteration 43, loss = 0.67274744\n",
            "Iteration 44, loss = 0.67056462\n",
            "Iteration 45, loss = 0.66859460\n",
            "Iteration 46, loss = 0.66655832\n",
            "Iteration 47, loss = 0.66454959\n",
            "Iteration 48, loss = 0.66265032\n",
            "Iteration 49, loss = 0.66091053\n",
            "Iteration 50, loss = 0.65917129\n",
            "Iteration 51, loss = 0.65752328\n",
            "Iteration 52, loss = 0.65594358\n",
            "Iteration 53, loss = 0.65432003\n",
            "Iteration 54, loss = 0.65283905\n",
            "Iteration 55, loss = 0.65128565\n",
            "Iteration 56, loss = 0.64996104\n",
            "Iteration 57, loss = 0.64857742\n",
            "Iteration 58, loss = 0.64723652\n",
            "Iteration 59, loss = 0.64599833\n",
            "Iteration 60, loss = 0.64471383\n",
            "Iteration 61, loss = 0.64352032\n",
            "Iteration 62, loss = 0.64224376\n",
            "Iteration 63, loss = 0.64109397\n",
            "Iteration 64, loss = 0.63993373\n",
            "Iteration 65, loss = 0.63886747\n",
            "Iteration 66, loss = 0.63775884\n",
            "Iteration 67, loss = 0.63666581\n",
            "Iteration 68, loss = 0.63561012\n",
            "Iteration 69, loss = 0.63454685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 70, loss = 0.63350548\n",
            "Iteration 71, loss = 0.63253331\n",
            "Iteration 72, loss = 0.63148168\n",
            "Iteration 73, loss = 0.63054464\n",
            "Iteration 74, loss = 0.62962690\n",
            "Iteration 75, loss = 0.62875371\n",
            "Iteration 76, loss = 0.62785837\n",
            "Iteration 77, loss = 0.62703086\n",
            "Iteration 78, loss = 0.62624236\n",
            "Iteration 79, loss = 0.62546140\n",
            "Iteration 80, loss = 0.62474273\n",
            "Iteration 81, loss = 0.62408242\n",
            "Iteration 82, loss = 0.62337742\n",
            "Iteration 83, loss = 0.62269981\n",
            "Iteration 84, loss = 0.62201595\n",
            "Iteration 85, loss = 0.62140697\n",
            "Iteration 86, loss = 0.62071370\n",
            "Iteration 87, loss = 0.62004325\n",
            "Iteration 88, loss = 0.61943810\n",
            "Iteration 89, loss = 0.61880019\n",
            "Iteration 90, loss = 0.61817375\n",
            "Iteration 91, loss = 0.61758277\n",
            "Iteration 92, loss = 0.61693716\n",
            "Iteration 93, loss = 0.61634558\n",
            "Iteration 94, loss = 0.61577297\n",
            "Iteration 95, loss = 0.61512111\n",
            "Iteration 96, loss = 0.61457329\n",
            "Iteration 97, loss = 0.61400662\n",
            "Iteration 98, loss = 0.61334432\n",
            "Iteration 99, loss = 0.61284428\n",
            "Iteration 100, loss = 0.61230045\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 100 and for layer number 3 : 0.6987500000000001\n",
            "Iteration 1, loss = 0.92831019\n",
            "Iteration 2, loss = 0.92628737\n",
            "Iteration 3, loss = 0.92292720\n",
            "Iteration 4, loss = 0.91890896\n",
            "Iteration 5, loss = 0.91406816\n",
            "Iteration 6, loss = 0.90896363\n",
            "Iteration 7, loss = 0.90386015\n",
            "Iteration 8, loss = 0.89884936\n",
            "Iteration 9, loss = 0.89376039\n",
            "Iteration 10, loss = 0.88868258\n",
            "Iteration 11, loss = 0.88381471\n",
            "Iteration 12, loss = 0.87878426\n",
            "Iteration 13, loss = 0.87398219\n",
            "Iteration 14, loss = 0.86928989\n",
            "Iteration 15, loss = 0.86471610\n",
            "Iteration 16, loss = 0.86053140\n",
            "Iteration 17, loss = 0.85612820\n",
            "Iteration 18, loss = 0.85197041\n",
            "Iteration 19, loss = 0.84782638\n",
            "Iteration 20, loss = 0.84387295\n",
            "Iteration 21, loss = 0.83992078\n",
            "Iteration 22, loss = 0.83601275\n",
            "Iteration 23, loss = 0.83220154\n",
            "Iteration 24, loss = 0.82811202\n",
            "Iteration 25, loss = 0.82449396\n",
            "Iteration 26, loss = 0.82082671\n",
            "Iteration 27, loss = 0.81721320\n",
            "Iteration 28, loss = 0.81380905\n",
            "Iteration 29, loss = 0.81040528\n",
            "Iteration 30, loss = 0.80705060\n",
            "Iteration 31, loss = 0.80392526\n",
            "Iteration 32, loss = 0.80063213\n",
            "Iteration 33, loss = 0.79760810\n",
            "Iteration 34, loss = 0.79459235\n",
            "Iteration 35, loss = 0.79159397\n",
            "Iteration 36, loss = 0.78886164\n",
            "Iteration 37, loss = 0.78586148\n",
            "Iteration 38, loss = 0.78317255\n",
            "Iteration 39, loss = 0.78022418\n",
            "Iteration 40, loss = 0.77762076\n",
            "Iteration 41, loss = 0.77495681\n",
            "Iteration 42, loss = 0.77229239\n",
            "Iteration 43, loss = 0.76974491\n",
            "Iteration 44, loss = 0.76720757\n",
            "Iteration 45, loss = 0.76456895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 46, loss = 0.76198026\n",
            "Iteration 47, loss = 0.75960833\n",
            "Iteration 48, loss = 0.75721478\n",
            "Iteration 49, loss = 0.75488200\n",
            "Iteration 50, loss = 0.75248586\n",
            "Iteration 51, loss = 0.75018259\n",
            "Iteration 52, loss = 0.74800238\n",
            "Iteration 53, loss = 0.74586725\n",
            "Iteration 54, loss = 0.74362634\n",
            "Iteration 55, loss = 0.74151143\n",
            "Iteration 56, loss = 0.73950295\n",
            "Iteration 57, loss = 0.73741831\n",
            "Iteration 58, loss = 0.73543089\n",
            "Iteration 59, loss = 0.73347890\n",
            "Iteration 60, loss = 0.73143214\n",
            "Iteration 61, loss = 0.72947052\n",
            "Iteration 62, loss = 0.72761557\n",
            "Iteration 63, loss = 0.72575659\n",
            "Iteration 64, loss = 0.72393461\n",
            "Iteration 65, loss = 0.72217066\n",
            "Iteration 66, loss = 0.72037809\n",
            "Iteration 67, loss = 0.71872244\n",
            "Iteration 68, loss = 0.71699545\n",
            "Iteration 69, loss = 0.71528424\n",
            "Iteration 70, loss = 0.71358358\n",
            "Iteration 71, loss = 0.71201015\n",
            "Iteration 72, loss = 0.71043121\n",
            "Iteration 73, loss = 0.70886096\n",
            "Iteration 74, loss = 0.70734836\n",
            "Iteration 75, loss = 0.70577004\n",
            "Iteration 76, loss = 0.70432856\n",
            "Iteration 77, loss = 0.70282172\n",
            "Iteration 78, loss = 0.70132546\n",
            "Iteration 79, loss = 0.69988901\n",
            "Iteration 80, loss = 0.69838408\n",
            "Iteration 81, loss = 0.69691519\n",
            "Iteration 82, loss = 0.69553924\n",
            "Iteration 83, loss = 0.69407292\n",
            "Iteration 84, loss = 0.69278138\n",
            "Iteration 85, loss = 0.69142090\n",
            "Iteration 86, loss = 0.69015005\n",
            "Iteration 87, loss = 0.68883737\n",
            "Iteration 88, loss = 0.68750556\n",
            "Iteration 89, loss = 0.68619651\n",
            "Iteration 90, loss = 0.68485066\n",
            "Iteration 91, loss = 0.68350643\n",
            "Iteration 92, loss = 0.68209430\n",
            "Iteration 93, loss = 0.68077022\n",
            "Iteration 94, loss = 0.67944943\n",
            "Iteration 95, loss = 0.67819217\n",
            "Iteration 96, loss = 0.67696762\n",
            "Iteration 97, loss = 0.67577008\n",
            "Iteration 98, loss = 0.67458090\n",
            "Iteration 99, loss = 0.67341100\n",
            "Iteration 100, loss = 0.67223428\n",
            "Iteration 1, loss = 0.92515262\n",
            "Iteration 2, loss = 0.92320793\n",
            "Iteration 3, loss = 0.91997489\n",
            "Iteration 4, loss = 0.91591886\n",
            "Iteration 5, loss = 0.91108617\n",
            "Iteration 6, loss = 0.90611197\n",
            "Iteration 7, loss = 0.90111358\n",
            "Iteration 8, loss = 0.89611580\n",
            "Iteration 9, loss = 0.89103097\n",
            "Iteration 10, loss = 0.88614356\n",
            "Iteration 11, loss = 0.88144205\n",
            "Iteration 12, loss = 0.87657599\n",
            "Iteration 13, loss = 0.87196161\n",
            "Iteration 14, loss = 0.86739565\n",
            "Iteration 15, loss = 0.86293846\n",
            "Iteration 16, loss = 0.85890079\n",
            "Iteration 17, loss = 0.85483203\n",
            "Iteration 18, loss = 0.85098669\n",
            "Iteration 19, loss = 0.84699066\n",
            "Iteration 20, loss = 0.84324236\n",
            "Iteration 21, loss = 0.83945760\n",
            "Iteration 22, loss = 0.83574260\n",
            "Iteration 23, loss = 0.83208773\n",
            "Iteration 24, loss = 0.82831715\n",
            "Iteration 25, loss = 0.82490285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 26, loss = 0.82145077\n",
            "Iteration 27, loss = 0.81803640\n",
            "Iteration 28, loss = 0.81467422\n",
            "Iteration 29, loss = 0.81143074\n",
            "Iteration 30, loss = 0.80809690\n",
            "Iteration 31, loss = 0.80507070\n",
            "Iteration 32, loss = 0.80183450\n",
            "Iteration 33, loss = 0.79879721\n",
            "Iteration 34, loss = 0.79582125\n",
            "Iteration 35, loss = 0.79278224\n",
            "Iteration 36, loss = 0.78996639\n",
            "Iteration 37, loss = 0.78695614\n",
            "Iteration 38, loss = 0.78424656\n",
            "Iteration 39, loss = 0.78127393\n",
            "Iteration 40, loss = 0.77872840\n",
            "Iteration 41, loss = 0.77606929\n",
            "Iteration 42, loss = 0.77339733\n",
            "Iteration 43, loss = 0.77081091\n",
            "Iteration 44, loss = 0.76835674\n",
            "Iteration 45, loss = 0.76574743\n",
            "Iteration 46, loss = 0.76315724\n",
            "Iteration 47, loss = 0.76082949\n",
            "Iteration 48, loss = 0.75845520\n",
            "Iteration 49, loss = 0.75612422\n",
            "Iteration 50, loss = 0.75378236\n",
            "Iteration 51, loss = 0.75155630\n",
            "Iteration 52, loss = 0.74945850\n",
            "Iteration 53, loss = 0.74738251\n",
            "Iteration 54, loss = 0.74516311\n",
            "Iteration 55, loss = 0.74313444\n",
            "Iteration 56, loss = 0.74120140\n",
            "Iteration 57, loss = 0.73915571\n",
            "Iteration 58, loss = 0.73719056\n",
            "Iteration 59, loss = 0.73528087\n",
            "Iteration 60, loss = 0.73334510\n",
            "Iteration 61, loss = 0.73141029\n",
            "Iteration 62, loss = 0.72959204\n",
            "Iteration 63, loss = 0.72775738\n",
            "Iteration 64, loss = 0.72591830\n",
            "Iteration 65, loss = 0.72420142\n",
            "Iteration 66, loss = 0.72236917\n",
            "Iteration 67, loss = 0.72072695\n",
            "Iteration 68, loss = 0.71904202\n",
            "Iteration 69, loss = 0.71728188\n",
            "Iteration 70, loss = 0.71565896\n",
            "Iteration 71, loss = 0.71408007\n",
            "Iteration 72, loss = 0.71245632\n",
            "Iteration 73, loss = 0.71088491\n",
            "Iteration 74, loss = 0.70937593\n",
            "Iteration 75, loss = 0.70783125\n",
            "Iteration 76, loss = 0.70637332\n",
            "Iteration 77, loss = 0.70488077\n",
            "Iteration 78, loss = 0.70340466\n",
            "Iteration 79, loss = 0.70197001\n",
            "Iteration 80, loss = 0.70049560\n",
            "Iteration 81, loss = 0.69904607\n",
            "Iteration 82, loss = 0.69762145\n",
            "Iteration 83, loss = 0.69614817\n",
            "Iteration 84, loss = 0.69479063\n",
            "Iteration 85, loss = 0.69346291\n",
            "Iteration 86, loss = 0.69214622\n",
            "Iteration 87, loss = 0.69083577\n",
            "Iteration 88, loss = 0.68948877\n",
            "Iteration 89, loss = 0.68815146\n",
            "Iteration 90, loss = 0.68677016\n",
            "Iteration 91, loss = 0.68540869\n",
            "Iteration 92, loss = 0.68391370\n",
            "Iteration 93, loss = 0.68257332\n",
            "Iteration 94, loss = 0.68117831\n",
            "Iteration 95, loss = 0.67987551\n",
            "Iteration 96, loss = 0.67862677\n",
            "Iteration 97, loss = 0.67738780\n",
            "Iteration 98, loss = 0.67615795\n",
            "Iteration 99, loss = 0.67497811\n",
            "Iteration 100, loss = 0.67374746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.93462281\n",
            "Iteration 2, loss = 0.93261224\n",
            "Iteration 3, loss = 0.92941834\n",
            "Iteration 4, loss = 0.92525033\n",
            "Iteration 5, loss = 0.92065853\n",
            "Iteration 6, loss = 0.91555772\n",
            "Iteration 7, loss = 0.91059613\n",
            "Iteration 8, loss = 0.90541505\n",
            "Iteration 9, loss = 0.90029749\n",
            "Iteration 10, loss = 0.89529951\n",
            "Iteration 11, loss = 0.89038858\n",
            "Iteration 12, loss = 0.88547267\n",
            "Iteration 13, loss = 0.88066014\n",
            "Iteration 14, loss = 0.87599931\n",
            "Iteration 15, loss = 0.87133466\n",
            "Iteration 16, loss = 0.86701607\n",
            "Iteration 17, loss = 0.86274708\n",
            "Iteration 18, loss = 0.85864838\n",
            "Iteration 19, loss = 0.85439727\n",
            "Iteration 20, loss = 0.85037709\n",
            "Iteration 21, loss = 0.84634247\n",
            "Iteration 22, loss = 0.84239937\n",
            "Iteration 23, loss = 0.83855448\n",
            "Iteration 24, loss = 0.83465813\n",
            "Iteration 25, loss = 0.83096674\n",
            "Iteration 26, loss = 0.82741978\n",
            "Iteration 27, loss = 0.82371122\n",
            "Iteration 28, loss = 0.82025754\n",
            "Iteration 29, loss = 0.81690640\n",
            "Iteration 30, loss = 0.81348565\n",
            "Iteration 31, loss = 0.81033127\n",
            "Iteration 32, loss = 0.80706458\n",
            "Iteration 33, loss = 0.80388785\n",
            "Iteration 34, loss = 0.80070719\n",
            "Iteration 35, loss = 0.79753268\n",
            "Iteration 36, loss = 0.79445505\n",
            "Iteration 37, loss = 0.79131373\n",
            "Iteration 38, loss = 0.78835925\n",
            "Iteration 39, loss = 0.78525734\n",
            "Iteration 40, loss = 0.78258735\n",
            "Iteration 41, loss = 0.77976841\n",
            "Iteration 42, loss = 0.77689181\n",
            "Iteration 43, loss = 0.77404346\n",
            "Iteration 44, loss = 0.77144905\n",
            "Iteration 45, loss = 0.76875241\n",
            "Iteration 46, loss = 0.76603951\n",
            "Iteration 47, loss = 0.76363139\n",
            "Iteration 48, loss = 0.76122007\n",
            "Iteration 49, loss = 0.75874285\n",
            "Iteration 50, loss = 0.75632330\n",
            "Iteration 51, loss = 0.75401154\n",
            "Iteration 52, loss = 0.75172788\n",
            "Iteration 53, loss = 0.74946183\n",
            "Iteration 54, loss = 0.74711655\n",
            "Iteration 55, loss = 0.74496158\n",
            "Iteration 56, loss = 0.74289829\n",
            "Iteration 57, loss = 0.74075807\n",
            "Iteration 58, loss = 0.73864727\n",
            "Iteration 59, loss = 0.73658594\n",
            "Iteration 60, loss = 0.73447008\n",
            "Iteration 61, loss = 0.73240466\n",
            "Iteration 62, loss = 0.73039789\n",
            "Iteration 63, loss = 0.72845788\n",
            "Iteration 64, loss = 0.72653095\n",
            "Iteration 65, loss = 0.72471988\n",
            "Iteration 66, loss = 0.72273809\n",
            "Iteration 67, loss = 0.72088747\n",
            "Iteration 68, loss = 0.71908629\n",
            "Iteration 69, loss = 0.71713136\n",
            "Iteration 70, loss = 0.71527881\n",
            "Iteration 71, loss = 0.71355173\n",
            "Iteration 72, loss = 0.71174876\n",
            "Iteration 73, loss = 0.70999877\n",
            "Iteration 74, loss = 0.70831828\n",
            "Iteration 75, loss = 0.70659337\n",
            "Iteration 76, loss = 0.70499195\n",
            "Iteration 77, loss = 0.70337414\n",
            "Iteration 78, loss = 0.70177357\n",
            "Iteration 79, loss = 0.70025428\n",
            "Iteration 80, loss = 0.69861256\n",
            "Iteration 81, loss = 0.69707178\n",
            "Iteration 82, loss = 0.69552728\n",
            "Iteration 83, loss = 0.69393161\n",
            "Iteration 84, loss = 0.69241273\n",
            "Iteration 85, loss = 0.69095717\n",
            "Iteration 86, loss = 0.68948161\n",
            "Iteration 87, loss = 0.68804856\n",
            "Iteration 88, loss = 0.68658037\n",
            "Iteration 89, loss = 0.68510199\n",
            "Iteration 90, loss = 0.68363864\n",
            "Iteration 91, loss = 0.68220180\n",
            "Iteration 92, loss = 0.68063484\n",
            "Iteration 93, loss = 0.67918267\n",
            "Iteration 94, loss = 0.67773565\n",
            "Iteration 95, loss = 0.67630324\n",
            "Iteration 96, loss = 0.67493092\n",
            "Iteration 97, loss = 0.67360341\n",
            "Iteration 98, loss = 0.67227445\n",
            "Iteration 99, loss = 0.67098636\n",
            "Iteration 100, loss = 0.66966581\n",
            "Iteration 1, loss = 0.93310249\n",
            "Iteration 2, loss = 0.93101092\n",
            "Iteration 3, loss = 0.92770703\n",
            "Iteration 4, loss = 0.92348094\n",
            "Iteration 5, loss = 0.91870634\n",
            "Iteration 6, loss = 0.91350854\n",
            "Iteration 7, loss = 0.90855626\n",
            "Iteration 8, loss = 0.90333343\n",
            "Iteration 9, loss = 0.89816244\n",
            "Iteration 10, loss = 0.89306903\n",
            "Iteration 11, loss = 0.88797547\n",
            "Iteration 12, loss = 0.88302867\n",
            "Iteration 13, loss = 0.87830591\n",
            "Iteration 14, loss = 0.87363722\n",
            "Iteration 15, loss = 0.86901748\n",
            "Iteration 16, loss = 0.86479539\n",
            "Iteration 17, loss = 0.86038833\n",
            "Iteration 18, loss = 0.85639915\n",
            "Iteration 19, loss = 0.85227054\n",
            "Iteration 20, loss = 0.84834947\n",
            "Iteration 21, loss = 0.84451866\n",
            "Iteration 22, loss = 0.84076112\n",
            "Iteration 23, loss = 0.83717459\n",
            "Iteration 24, loss = 0.83341012\n",
            "Iteration 25, loss = 0.82979470\n",
            "Iteration 26, loss = 0.82628064\n",
            "Iteration 27, loss = 0.82271049\n",
            "Iteration 28, loss = 0.81929932\n",
            "Iteration 29, loss = 0.81605612\n",
            "Iteration 30, loss = 0.81276286\n",
            "Iteration 31, loss = 0.80960758\n",
            "Iteration 32, loss = 0.80651049\n",
            "Iteration 33, loss = 0.80331972\n",
            "Iteration 34, loss = 0.80023102\n",
            "Iteration 35, loss = 0.79699869\n",
            "Iteration 36, loss = 0.79400137\n",
            "Iteration 37, loss = 0.79097394\n",
            "Iteration 38, loss = 0.78805094\n",
            "Iteration 39, loss = 0.78511982\n",
            "Iteration 40, loss = 0.78247517\n",
            "Iteration 41, loss = 0.77979332\n",
            "Iteration 42, loss = 0.77699418\n",
            "Iteration 43, loss = 0.77429562\n",
            "Iteration 44, loss = 0.77172579\n",
            "Iteration 45, loss = 0.76916050\n",
            "Iteration 46, loss = 0.76647297\n",
            "Iteration 47, loss = 0.76409478\n",
            "Iteration 48, loss = 0.76166267\n",
            "Iteration 49, loss = 0.75916081\n",
            "Iteration 50, loss = 0.75678157\n",
            "Iteration 51, loss = 0.75448138\n",
            "Iteration 52, loss = 0.75215647\n",
            "Iteration 53, loss = 0.74984933\n",
            "Iteration 54, loss = 0.74751943\n",
            "Iteration 55, loss = 0.74539031\n",
            "Iteration 56, loss = 0.74335546\n",
            "Iteration 57, loss = 0.74121902\n",
            "Iteration 58, loss = 0.73912598\n",
            "Iteration 59, loss = 0.73715404\n",
            "Iteration 60, loss = 0.73507830\n",
            "Iteration 61, loss = 0.73318207\n",
            "Iteration 62, loss = 0.73126676\n",
            "Iteration 63, loss = 0.72939938\n",
            "Iteration 64, loss = 0.72756473\n",
            "Iteration 65, loss = 0.72579634\n",
            "Iteration 66, loss = 0.72394008\n",
            "Iteration 67, loss = 0.72217471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 68, loss = 0.72043589\n",
            "Iteration 69, loss = 0.71850239\n",
            "Iteration 70, loss = 0.71671073\n",
            "Iteration 71, loss = 0.71501354\n",
            "Iteration 72, loss = 0.71325594\n",
            "Iteration 73, loss = 0.71157480\n",
            "Iteration 74, loss = 0.70997376\n",
            "Iteration 75, loss = 0.70834349\n",
            "Iteration 76, loss = 0.70678517\n",
            "Iteration 77, loss = 0.70527284\n",
            "Iteration 78, loss = 0.70373065\n",
            "Iteration 79, loss = 0.70226150\n",
            "Iteration 80, loss = 0.70070864\n",
            "Iteration 81, loss = 0.69912475\n",
            "Iteration 82, loss = 0.69757835\n",
            "Iteration 83, loss = 0.69597960\n",
            "Iteration 84, loss = 0.69442451\n",
            "Iteration 85, loss = 0.69299803\n",
            "Iteration 86, loss = 0.69155555\n",
            "Iteration 87, loss = 0.69017218\n",
            "Iteration 88, loss = 0.68882177\n",
            "Iteration 89, loss = 0.68743527\n",
            "Iteration 90, loss = 0.68603636\n",
            "Iteration 91, loss = 0.68469807\n",
            "Iteration 92, loss = 0.68322054\n",
            "Iteration 93, loss = 0.68188267\n",
            "Iteration 94, loss = 0.68056752\n",
            "Iteration 95, loss = 0.67922529\n",
            "Iteration 96, loss = 0.67794626\n",
            "Iteration 97, loss = 0.67673420\n",
            "Iteration 98, loss = 0.67547574\n",
            "Iteration 99, loss = 0.67423622\n",
            "Iteration 100, loss = 0.67305365\n",
            "Iteration 1, loss = 0.93946865\n",
            "Iteration 2, loss = 0.93734548\n",
            "Iteration 3, loss = 0.93396022\n",
            "Iteration 4, loss = 0.92974495\n",
            "Iteration 5, loss = 0.92508441\n",
            "Iteration 6, loss = 0.92006070\n",
            "Iteration 7, loss = 0.91518496\n",
            "Iteration 8, loss = 0.91020915\n",
            "Iteration 9, loss = 0.90529195\n",
            "Iteration 10, loss = 0.90050938\n",
            "Iteration 11, loss = 0.89552786\n",
            "Iteration 12, loss = 0.89092624\n",
            "Iteration 13, loss = 0.88627671\n",
            "Iteration 14, loss = 0.88176332\n",
            "Iteration 15, loss = 0.87748747\n",
            "Iteration 16, loss = 0.87338356\n",
            "Iteration 17, loss = 0.86932619\n",
            "Iteration 18, loss = 0.86557100\n",
            "Iteration 19, loss = 0.86175953\n",
            "Iteration 20, loss = 0.85810708\n",
            "Iteration 21, loss = 0.85457487\n",
            "Iteration 22, loss = 0.85096897\n",
            "Iteration 23, loss = 0.84761148\n",
            "Iteration 24, loss = 0.84404867\n",
            "Iteration 25, loss = 0.84066128\n",
            "Iteration 26, loss = 0.83730958\n",
            "Iteration 27, loss = 0.83388757\n",
            "Iteration 28, loss = 0.83057501\n",
            "Iteration 29, loss = 0.82738517\n",
            "Iteration 30, loss = 0.82415322\n",
            "Iteration 31, loss = 0.82106702\n",
            "Iteration 32, loss = 0.81815273\n",
            "Iteration 33, loss = 0.81509467\n",
            "Iteration 34, loss = 0.81215013\n",
            "Iteration 35, loss = 0.80906566\n",
            "Iteration 36, loss = 0.80619125\n",
            "Iteration 37, loss = 0.80336983\n",
            "Iteration 38, loss = 0.80049635\n",
            "Iteration 39, loss = 0.79772338\n",
            "Iteration 40, loss = 0.79507950\n",
            "Iteration 41, loss = 0.79258288"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 42, loss = 0.78995614\n",
            "Iteration 43, loss = 0.78747175\n",
            "Iteration 44, loss = 0.78509049\n",
            "Iteration 45, loss = 0.78279095\n",
            "Iteration 46, loss = 0.78030914\n",
            "Iteration 47, loss = 0.77804290\n",
            "Iteration 48, loss = 0.77571093\n",
            "Iteration 49, loss = 0.77337670\n",
            "Iteration 50, loss = 0.77119120\n",
            "Iteration 51, loss = 0.76906050\n",
            "Iteration 52, loss = 0.76683251\n",
            "Iteration 53, loss = 0.76474393\n",
            "Iteration 54, loss = 0.76252897\n",
            "Iteration 55, loss = 0.76055234\n",
            "Iteration 56, loss = 0.75863057\n",
            "Iteration 57, loss = 0.75661412\n",
            "Iteration 58, loss = 0.75460319\n",
            "Iteration 59, loss = 0.75272004\n",
            "Iteration 60, loss = 0.75074771\n",
            "Iteration 61, loss = 0.74888193\n",
            "Iteration 62, loss = 0.74706429\n",
            "Iteration 63, loss = 0.74522519\n",
            "Iteration 64, loss = 0.74342654\n",
            "Iteration 65, loss = 0.74166896\n",
            "Iteration 66, loss = 0.73980177\n",
            "Iteration 67, loss = 0.73797811\n",
            "Iteration 68, loss = 0.73626208\n",
            "Iteration 69, loss = 0.73436834\n",
            "Iteration 70, loss = 0.73252826\n",
            "Iteration 71, loss = 0.73089009\n",
            "Iteration 72, loss = 0.72915676\n",
            "Iteration 73, loss = 0.72752511\n",
            "Iteration 74, loss = 0.72591320\n",
            "Iteration 75, loss = 0.72434095\n",
            "Iteration 76, loss = 0.72284166\n",
            "Iteration 77, loss = 0.72136264\n",
            "Iteration 78, loss = 0.71987881\n",
            "Iteration 79, loss = 0.71844868\n",
            "Iteration 80, loss = 0.71699466\n",
            "Iteration 81, loss = 0.71551285\n",
            "Iteration 82, loss = 0.71406031\n",
            "Iteration 83, loss = 0.71258023\n",
            "Iteration 84, loss = 0.71109177\n",
            "Iteration 85, loss = 0.70973876\n",
            "Iteration 86, loss = 0.70839930\n",
            "Iteration 87, loss = 0.70708242\n",
            "Iteration 88, loss = 0.70577438\n",
            "Iteration 89, loss = 0.70451113\n",
            "Iteration 90, loss = 0.70322118\n",
            "Iteration 91, loss = 0.70202655\n",
            "Iteration 92, loss = 0.70078451\n",
            "Iteration 93, loss = 0.69961631\n",
            "Iteration 94, loss = 0.69844678\n",
            "Iteration 95, loss = 0.69724830\n",
            "Iteration 96, loss = 0.69613427\n",
            "Iteration 97, loss = 0.69501374\n",
            "Iteration 98, loss = 0.69384144\n",
            "Iteration 99, loss = 0.69264942\n",
            "Iteration 100, loss = 0.69152324\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 100 and for layer number 4 : 0.6025\n",
            "Iteration 1, loss = 0.88791930\n",
            "Iteration 2, loss = 0.88478701\n",
            "Iteration 3, loss = 0.87977954\n",
            "Iteration 4, loss = 0.87382746\n",
            "Iteration 5, loss = 0.86693408\n",
            "Iteration 6, loss = 0.85991963\n",
            "Iteration 7, loss = 0.85349317\n",
            "Iteration 8, loss = 0.84609686\n",
            "Iteration 9, loss = 0.83903239\n",
            "Iteration 10, loss = 0.83199676\n",
            "Iteration 11, loss = 0.82476252\n",
            "Iteration 12, loss = 0.81799272\n",
            "Iteration 13, loss = 0.81153730\n",
            "Iteration 14, loss = 0.80501491\n",
            "Iteration 15, loss = 0.79881692\n",
            "Iteration 16, loss = 0.79283969\n",
            "Iteration 17, loss = 0.78704952\n",
            "Iteration 18, loss = 0.78142027\n",
            "Iteration 19, loss = 0.77612815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 20, loss = 0.77058668\n",
            "Iteration 21, loss = 0.76563879\n",
            "Iteration 22, loss = 0.76077411\n",
            "Iteration 23, loss = 0.75622954\n",
            "Iteration 24, loss = 0.75181798\n",
            "Iteration 25, loss = 0.74780850\n",
            "Iteration 26, loss = 0.74367194\n",
            "Iteration 27, loss = 0.73976022\n",
            "Iteration 28, loss = 0.73608417\n",
            "Iteration 29, loss = 0.73214581\n",
            "Iteration 30, loss = 0.72863038\n",
            "Iteration 31, loss = 0.72522531\n",
            "Iteration 32, loss = 0.72208870\n",
            "Iteration 33, loss = 0.71905862\n",
            "Iteration 34, loss = 0.71605055\n",
            "Iteration 35, loss = 0.71338575\n",
            "Iteration 36, loss = 0.71054771\n",
            "Iteration 37, loss = 0.70780651\n",
            "Iteration 38, loss = 0.70507846\n",
            "Iteration 39, loss = 0.70241483\n",
            "Iteration 40, loss = 0.69974696\n",
            "Iteration 41, loss = 0.69719897\n",
            "Iteration 42, loss = 0.69474538\n",
            "Iteration 43, loss = 0.69229605\n",
            "Iteration 44, loss = 0.68998519\n",
            "Iteration 45, loss = 0.68769249\n",
            "Iteration 46, loss = 0.68541216\n",
            "Iteration 47, loss = 0.68334998\n",
            "Iteration 48, loss = 0.68137544\n",
            "Iteration 49, loss = 0.67942804\n",
            "Iteration 50, loss = 0.67769897\n",
            "Iteration 51, loss = 0.67587683\n",
            "Iteration 52, loss = 0.67441020\n",
            "Iteration 53, loss = 0.67273943\n",
            "Iteration 54, loss = 0.67136315\n",
            "Iteration 55, loss = 0.66969470\n",
            "Iteration 56, loss = 0.66819849\n",
            "Iteration 57, loss = 0.66678144\n",
            "Iteration 58, loss = 0.66532375\n",
            "Iteration 59, loss = 0.66394656\n",
            "Iteration 60, loss = 0.66254487\n",
            "Iteration 61, loss = 0.66119003\n",
            "Iteration 62, loss = 0.65990917\n",
            "Iteration 63, loss = 0.65860006\n",
            "Iteration 64, loss = 0.65745658\n",
            "Iteration 65, loss = 0.65639810\n",
            "Iteration 66, loss = 0.65515649\n",
            "Iteration 67, loss = 0.65406199\n",
            "Iteration 68, loss = 0.65300071\n",
            "Iteration 69, loss = 0.65201781\n",
            "Iteration 70, loss = 0.65097950\n",
            "Iteration 71, loss = 0.65009911\n",
            "Iteration 72, loss = 0.64908267\n",
            "Iteration 73, loss = 0.64825233\n",
            "Iteration 74, loss = 0.64739105\n",
            "Iteration 75, loss = 0.64651078\n",
            "Iteration 76, loss = 0.64567306\n",
            "Iteration 77, loss = 0.64489576\n",
            "Iteration 78, loss = 0.64421106\n",
            "Iteration 79, loss = 0.64343015\n",
            "Iteration 80, loss = 0.64269074\n",
            "Iteration 81, loss = 0.64200003\n",
            "Iteration 82, loss = 0.64132052\n",
            "Iteration 83, loss = 0.64064109\n",
            "Iteration 84, loss = 0.63989578\n",
            "Iteration 85, loss = 0.63924996\n",
            "Iteration 86, loss = 0.63849229\n",
            "Iteration 87, loss = 0.63783327\n",
            "Iteration 88, loss = 0.63715252\n",
            "Iteration 89, loss = 0.63649693\n",
            "Iteration 90, loss = 0.63589299\n",
            "Iteration 91, loss = 0.63518930\n",
            "Iteration 92, loss = 0.63457609\n",
            "Iteration 93, loss = 0.63395665\n",
            "Iteration 94, loss = 0.63331641\n",
            "Iteration 95, loss = 0.63264966\n",
            "Iteration 96, loss = 0.63204170\n",
            "Iteration 97, loss = 0.63145729\n",
            "Iteration 98, loss = 0.63083574\n",
            "Iteration 99, loss = 0.63029201\n",
            "Iteration 100, loss = 0.62972358\n",
            "Iteration 1, loss = 0.88667506\n",
            "Iteration 2, loss = 0.88351311\n",
            "Iteration 3, loss = 0.87864988\n",
            "Iteration 4, loss = 0.87260271\n",
            "Iteration 5, loss = 0.86575091\n",
            "Iteration 6, loss = 0.85865495\n",
            "Iteration 7, loss = 0.85211290\n",
            "Iteration 8, loss = 0.84485323\n",
            "Iteration 9, loss = 0.83785117\n",
            "Iteration 10, loss = 0.83103528\n",
            "Iteration 11, loss = 0.82391039\n",
            "Iteration 12, loss = 0.81728381\n",
            "Iteration 13, loss = 0.81056237\n",
            "Iteration 14, loss = 0.80417780\n",
            "Iteration 15, loss = 0.79788403\n",
            "Iteration 16, loss = 0.79172048\n",
            "Iteration 17, loss = 0.78589084\n",
            "Iteration 18, loss = 0.78024639\n",
            "Iteration 19, loss = 0.77527683\n",
            "Iteration 20, loss = 0.76983525\n",
            "Iteration 21, loss = 0.76512748\n",
            "Iteration 22, loss = 0.76031041\n",
            "Iteration 23, loss = 0.75566756\n",
            "Iteration 24, loss = 0.75123287\n",
            "Iteration 25, loss = 0.74717290\n",
            "Iteration 26, loss = 0.74309808\n",
            "Iteration 27, loss = 0.73909650\n",
            "Iteration 28, loss = 0.73534184\n",
            "Iteration 29, loss = 0.73146632\n",
            "Iteration 30, loss = 0.72783360\n",
            "Iteration 31, loss = 0.72415824\n",
            "Iteration 32, loss = 0.72081620\n",
            "Iteration 33, loss = 0.71757346\n",
            "Iteration 34, loss = 0.71430564\n",
            "Iteration 35, loss = 0.71143757\n",
            "Iteration 36, loss = 0.70844375\n",
            "Iteration 37, loss = 0.70547686\n",
            "Iteration 38, loss = 0.70270393\n",
            "Iteration 39, loss = 0.70016842\n",
            "Iteration 40, loss = 0.69738412\n",
            "Iteration 41, loss = 0.69481862\n",
            "Iteration 42, loss = 0.69239524\n",
            "Iteration 43, loss = 0.68976645\n",
            "Iteration 44, loss = 0.68741275\n",
            "Iteration 45, loss = 0.68502015\n",
            "Iteration 46, loss = 0.68269702\n",
            "Iteration 47, loss = 0.68062921\n",
            "Iteration 48, loss = 0.67855526\n",
            "Iteration 49, loss = 0.67663369\n",
            "Iteration 50, loss = 0.67475577\n",
            "Iteration 51, loss = 0.67285993\n",
            "Iteration 52, loss = 0.67128721\n",
            "Iteration 53, loss = 0.66942488\n",
            "Iteration 54, loss = 0.66796022\n",
            "Iteration 55, loss = 0.66608528\n",
            "Iteration 56, loss = 0.66438449\n",
            "Iteration 57, loss = 0.66280374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 58, loss = 0.66126445\n",
            "Iteration 59, loss = 0.65981469\n",
            "Iteration 60, loss = 0.65833838\n",
            "Iteration 61, loss = 0.65696295\n",
            "Iteration 62, loss = 0.65566478\n",
            "Iteration 63, loss = 0.65425246\n",
            "Iteration 64, loss = 0.65304333\n",
            "Iteration 65, loss = 0.65190526\n",
            "Iteration 66, loss = 0.65073936\n",
            "Iteration 67, loss = 0.64955138\n",
            "Iteration 68, loss = 0.64845330\n",
            "Iteration 69, loss = 0.64739906\n",
            "Iteration 70, loss = 0.64639450\n",
            "Iteration 71, loss = 0.64546332\n",
            "Iteration 72, loss = 0.64444218\n",
            "Iteration 73, loss = 0.64356265\n",
            "Iteration 74, loss = 0.64269640\n",
            "Iteration 75, loss = 0.64173035\n",
            "Iteration 76, loss = 0.64084432\n",
            "Iteration 77, loss = 0.63998132\n",
            "Iteration 78, loss = 0.63918837\n",
            "Iteration 79, loss = 0.63833688\n",
            "Iteration 80, loss = 0.63748454\n",
            "Iteration 81, loss = 0.63668468\n",
            "Iteration 82, loss = 0.63590312\n",
            "Iteration 83, loss = 0.63515432\n",
            "Iteration 84, loss = 0.63435972\n",
            "Iteration 85, loss = 0.63367797\n",
            "Iteration 86, loss = 0.63287418\n",
            "Iteration 87, loss = 0.63216616\n",
            "Iteration 88, loss = 0.63143260\n",
            "Iteration 89, loss = 0.63074916\n",
            "Iteration 90, loss = 0.63012356\n",
            "Iteration 91, loss = 0.62939877\n",
            "Iteration 92, loss = 0.62878563\n",
            "Iteration 93, loss = 0.62818137\n",
            "Iteration 94, loss = 0.62753163\n",
            "Iteration 95, loss = 0.62687453\n",
            "Iteration 96, loss = 0.62626042\n",
            "Iteration 97, loss = 0.62562589\n",
            "Iteration 98, loss = 0.62499300\n",
            "Iteration 99, loss = 0.62440384\n",
            "Iteration 100, loss = 0.62381887\n",
            "Iteration 1, loss = 0.88994796\n",
            "Iteration 2, loss = 0.88647879\n",
            "Iteration 3, loss = 0.88112416\n",
            "Iteration 4, loss = 0.87477056\n",
            "Iteration 5, loss = 0.86759332\n",
            "Iteration 6, loss = 0.86026211\n",
            "Iteration 7, loss = 0.85335631\n",
            "Iteration 8, loss = 0.84586796\n",
            "Iteration 9, loss = 0.83874205\n",
            "Iteration 10, loss = 0.83175349\n",
            "Iteration 11, loss = 0.82448515\n",
            "Iteration 12, loss = 0.81778325\n",
            "Iteration 13, loss = 0.81084241\n",
            "Iteration 14, loss = 0.80421718\n",
            "Iteration 15, loss = 0.79782291\n",
            "Iteration 16, loss = 0.79153897\n",
            "Iteration 17, loss = 0.78578672\n",
            "Iteration 18, loss = 0.78028990\n",
            "Iteration 19, loss = 0.77514713\n",
            "Iteration 20, loss = 0.76965946\n",
            "Iteration 21, loss = 0.76458983\n",
            "Iteration 22, loss = 0.75975845\n",
            "Iteration 23, loss = 0.75512558\n",
            "Iteration 24, loss = 0.75073017\n",
            "Iteration 25, loss = 0.74659412\n",
            "Iteration 26, loss = 0.74241300\n",
            "Iteration 27, loss = 0.73858643\n",
            "Iteration 28, loss = 0.73478476\n",
            "Iteration 29, loss = 0.73086329\n",
            "Iteration 30, loss = 0.72730311\n",
            "Iteration 31, loss = 0.72350860\n",
            "Iteration 32, loss = 0.72016727\n",
            "Iteration 33, loss = 0.71697302\n",
            "Iteration 34, loss = 0.71360923\n",
            "Iteration 35, loss = 0.71068663\n",
            "Iteration 36, loss = 0.70783843"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 37, loss = 0.70491116\n",
            "Iteration 38, loss = 0.70215099\n",
            "Iteration 39, loss = 0.69968857\n",
            "Iteration 40, loss = 0.69682383\n",
            "Iteration 41, loss = 0.69427119\n",
            "Iteration 42, loss = 0.69198212\n",
            "Iteration 43, loss = 0.68919723\n",
            "Iteration 44, loss = 0.68691056\n",
            "Iteration 45, loss = 0.68454722\n",
            "Iteration 46, loss = 0.68224448\n",
            "Iteration 47, loss = 0.68018345\n",
            "Iteration 48, loss = 0.67814086\n",
            "Iteration 49, loss = 0.67618343\n",
            "Iteration 50, loss = 0.67429781\n",
            "Iteration 51, loss = 0.67240548\n",
            "Iteration 52, loss = 0.67073257\n",
            "Iteration 53, loss = 0.66887623\n",
            "Iteration 54, loss = 0.66727539\n",
            "Iteration 55, loss = 0.66548070\n",
            "Iteration 56, loss = 0.66375165\n",
            "Iteration 57, loss = 0.66206313\n",
            "Iteration 58, loss = 0.66052993\n",
            "Iteration 59, loss = 0.65902693\n",
            "Iteration 60, loss = 0.65749027\n",
            "Iteration 61, loss = 0.65599758\n",
            "Iteration 62, loss = 0.65470773\n",
            "Iteration 63, loss = 0.65327373\n",
            "Iteration 64, loss = 0.65200876\n",
            "Iteration 65, loss = 0.65082649\n",
            "Iteration 66, loss = 0.64977568\n",
            "Iteration 67, loss = 0.64850480\n",
            "Iteration 68, loss = 0.64743751\n",
            "Iteration 69, loss = 0.64638809\n",
            "Iteration 70, loss = 0.64542021\n",
            "Iteration 71, loss = 0.64443926\n",
            "Iteration 72, loss = 0.64338729\n",
            "Iteration 73, loss = 0.64241557\n",
            "Iteration 74, loss = 0.64150518\n",
            "Iteration 75, loss = 0.64053993\n",
            "Iteration 76, loss = 0.63963000\n",
            "Iteration 77, loss = 0.63876754\n",
            "Iteration 78, loss = 0.63794665\n",
            "Iteration 79, loss = 0.63710295\n",
            "Iteration 80, loss = 0.63618590\n",
            "Iteration 81, loss = 0.63534354\n",
            "Iteration 82, loss = 0.63453306\n",
            "Iteration 83, loss = 0.63371928\n",
            "Iteration 84, loss = 0.63288966\n",
            "Iteration 85, loss = 0.63216956\n",
            "Iteration 86, loss = 0.63129485\n",
            "Iteration 87, loss = 0.63059605\n",
            "Iteration 88, loss = 0.62984039\n",
            "Iteration 89, loss = 0.62914261\n",
            "Iteration 90, loss = 0.62847791\n",
            "Iteration 91, loss = 0.62776941\n",
            "Iteration 92, loss = 0.62715342\n",
            "Iteration 93, loss = 0.62653126\n",
            "Iteration 94, loss = 0.62586031\n",
            "Iteration 95, loss = 0.62520916\n",
            "Iteration 96, loss = 0.62453176\n",
            "Iteration 97, loss = 0.62392353\n",
            "Iteration 98, loss = 0.62329005\n",
            "Iteration 99, loss = 0.62268263\n",
            "Iteration 100, loss = 0.62210977\n",
            "Iteration 1, loss = 0.88349736\n",
            "Iteration 2, loss = 0.87995489\n",
            "Iteration 3, loss = 0.87484423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss = 0.86857561\n",
            "Iteration 5, loss = 0.86173197\n",
            "Iteration 6, loss = 0.85454050\n",
            "Iteration 7, loss = 0.84774058\n",
            "Iteration 8, loss = 0.84088394\n",
            "Iteration 9, loss = 0.83393267\n",
            "Iteration 10, loss = 0.82742553\n",
            "Iteration 11, loss = 0.82086785\n",
            "Iteration 12, loss = 0.81452708\n",
            "Iteration 13, loss = 0.80823130\n",
            "Iteration 14, loss = 0.80203476\n",
            "Iteration 15, loss = 0.79614496\n",
            "Iteration 16, loss = 0.79049779\n",
            "Iteration 17, loss = 0.78504210\n",
            "Iteration 18, loss = 0.77952845\n",
            "Iteration 19, loss = 0.77469909\n",
            "Iteration 20, loss = 0.76936348\n",
            "Iteration 21, loss = 0.76442730\n",
            "Iteration 22, loss = 0.75965800\n",
            "Iteration 23, loss = 0.75500764\n",
            "Iteration 24, loss = 0.75071470\n",
            "Iteration 25, loss = 0.74647049\n",
            "Iteration 26, loss = 0.74261509\n",
            "Iteration 27, loss = 0.73863169\n",
            "Iteration 28, loss = 0.73493685\n",
            "Iteration 29, loss = 0.73128501\n",
            "Iteration 30, loss = 0.72789941\n",
            "Iteration 31, loss = 0.72426266\n",
            "Iteration 32, loss = 0.72101993\n",
            "Iteration 33, loss = 0.71785043\n",
            "Iteration 34, loss = 0.71458345\n",
            "Iteration 35, loss = 0.71165677\n",
            "Iteration 36, loss = 0.70878583\n",
            "Iteration 37, loss = 0.70588880\n",
            "Iteration 38, loss = 0.70314079\n",
            "Iteration 39, loss = 0.70061319\n",
            "Iteration 40, loss = 0.69798247\n",
            "Iteration 41, loss = 0.69549327\n",
            "Iteration 42, loss = 0.69327842\n",
            "Iteration 43, loss = 0.69073254\n",
            "Iteration 44, loss = 0.68867848\n",
            "Iteration 45, loss = 0.68642668\n",
            "Iteration 46, loss = 0.68434546\n",
            "Iteration 47, loss = 0.68236535\n",
            "Iteration 48, loss = 0.68048017\n",
            "Iteration 49, loss = 0.67864669\n",
            "Iteration 50, loss = 0.67680541\n",
            "Iteration 51, loss = 0.67496606\n",
            "Iteration 52, loss = 0.67331816\n",
            "Iteration 53, loss = 0.67162095\n",
            "Iteration 54, loss = 0.67012659\n",
            "Iteration 55, loss = 0.66851262\n",
            "Iteration 56, loss = 0.66698837\n",
            "Iteration 57, loss = 0.66532003\n",
            "Iteration 58, loss = 0.66385469\n",
            "Iteration 59, loss = 0.66236615\n",
            "Iteration 60, loss = 0.66095100\n",
            "Iteration 61, loss = 0.65952010\n",
            "Iteration 62, loss = 0.65829264\n",
            "Iteration 63, loss = 0.65702747\n",
            "Iteration 64, loss = 0.65574388\n",
            "Iteration 65, loss = 0.65461995\n",
            "Iteration 66, loss = 0.65353183\n",
            "Iteration 67, loss = 0.65225074\n",
            "Iteration 68, loss = 0.65117719\n",
            "Iteration 69, loss = 0.64998474\n",
            "Iteration 70, loss = 0.64900805\n",
            "Iteration 71, loss = 0.64805924\n",
            "Iteration 72, loss = 0.64707263\n",
            "Iteration 73, loss = 0.64619598\n",
            "Iteration 74, loss = 0.64539438\n",
            "Iteration 75, loss = 0.64456929\n",
            "Iteration 76, loss = 0.64368399\n",
            "Iteration 77, loss = 0.64289466\n",
            "Iteration 78, loss = 0.64211176\n",
            "Iteration 79, loss = 0.64132455\n",
            "Iteration 80, loss = 0.64048025\n",
            "Iteration 81, loss = 0.63968838\n",
            "Iteration 82, loss = 0.63893461\n",
            "Iteration 83, loss = 0.63821109\n",
            "Iteration 84, loss = 0.63747211\n",
            "Iteration 85, loss = 0.63685252\n",
            "Iteration 86, loss = 0.63609556\n",
            "Iteration 87, loss = 0.63543753\n",
            "Iteration 88, loss = 0.63471118\n",
            "Iteration 89, loss = 0.63404085\n",
            "Iteration 90, loss = 0.63340349\n",
            "Iteration 91, loss = 0.63278193\n",
            "Iteration 92, loss = 0.63214743\n",
            "Iteration 93, loss = 0.63157648\n",
            "Iteration 94, loss = 0.63095984\n",
            "Iteration 95, loss = 0.63041787\n",
            "Iteration 96, loss = 0.62977208\n",
            "Iteration 97, loss = 0.62918399\n",
            "Iteration 98, loss = 0.62857863\n",
            "Iteration 99, loss = 0.62795666\n",
            "Iteration 100, loss = 0.62743278\n",
            "Iteration 1, loss = 0.88921381\n",
            "Iteration 2, loss = 0.88561120\n",
            "Iteration 3, loss = 0.88039745\n",
            "Iteration 4, loss = 0.87411510\n",
            "Iteration 5, loss = 0.86695385\n",
            "Iteration 6, loss = 0.85950718\n",
            "Iteration 7, loss = 0.85233922\n",
            "Iteration 8, loss = 0.84527473\n",
            "Iteration 9, loss = 0.83809709\n",
            "Iteration 10, loss = 0.83164349\n",
            "Iteration 11, loss = 0.82493838\n",
            "Iteration 12, loss = 0.81870921\n",
            "Iteration 13, loss = 0.81251318\n",
            "Iteration 14, loss = 0.80607251\n",
            "Iteration 15, loss = 0.79981985\n",
            "Iteration 16, loss = 0.79393281\n",
            "Iteration 17, loss = 0.78828581\n",
            "Iteration 18, loss = 0.78245479\n",
            "Iteration 19, loss = 0.77739177\n",
            "Iteration 20, loss = 0.77198009\n",
            "Iteration 21, loss = 0.76686429\n",
            "Iteration 22, loss = 0.76204430\n",
            "Iteration 23, loss = 0.75723984\n",
            "Iteration 24, loss = 0.75306036\n",
            "Iteration 25, loss = 0.74865567\n",
            "Iteration 26, loss = 0.74459266\n",
            "Iteration 27, loss = 0.74053604\n",
            "Iteration 28, loss = 0.73681378\n",
            "Iteration 29, loss = 0.73320126\n",
            "Iteration 30, loss = 0.72947076\n",
            "Iteration 31, loss = 0.72576794\n",
            "Iteration 32, loss = 0.72239030\n",
            "Iteration 33, loss = 0.71920770\n",
            "Iteration 34, loss = 0.71580616\n",
            "Iteration 35, loss = 0.71274085\n",
            "Iteration 36, loss = 0.70975790\n",
            "Iteration 37, loss = 0.70664366\n",
            "Iteration 38, loss = 0.70366441\n",
            "Iteration 39, loss = 0.70088208\n",
            "Iteration 40, loss = 0.69810738\n",
            "Iteration 41, loss = 0.69560192\n",
            "Iteration 42, loss = 0.69316336\n",
            "Iteration 43, loss = 0.69061309\n",
            "Iteration 44, loss = 0.68855524\n",
            "Iteration 45, loss = 0.68625829\n",
            "Iteration 46, loss = 0.68415410\n",
            "Iteration 47, loss = 0.68216332\n",
            "Iteration 48, loss = 0.68023366\n",
            "Iteration 49, loss = 0.67837345\n",
            "Iteration 50, loss = 0.67652492"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 51, loss = 0.67465587\n",
            "Iteration 52, loss = 0.67291367\n",
            "Iteration 53, loss = 0.67120569\n",
            "Iteration 54, loss = 0.66960302\n",
            "Iteration 55, loss = 0.66800315\n",
            "Iteration 56, loss = 0.66652548\n",
            "Iteration 57, loss = 0.66485587\n",
            "Iteration 58, loss = 0.66349741\n",
            "Iteration 59, loss = 0.66208478\n",
            "Iteration 60, loss = 0.66070181\n",
            "Iteration 61, loss = 0.65933908\n",
            "Iteration 62, loss = 0.65815998\n",
            "Iteration 63, loss = 0.65691715\n",
            "Iteration 64, loss = 0.65568373\n",
            "Iteration 65, loss = 0.65459061\n",
            "Iteration 66, loss = 0.65352896\n",
            "Iteration 67, loss = 0.65233664\n",
            "Iteration 68, loss = 0.65132622\n",
            "Iteration 69, loss = 0.65019996\n",
            "Iteration 70, loss = 0.64924598\n",
            "Iteration 71, loss = 0.64821908\n",
            "Iteration 72, loss = 0.64725881\n",
            "Iteration 73, loss = 0.64638887\n",
            "Iteration 74, loss = 0.64557963\n",
            "Iteration 75, loss = 0.64472783\n",
            "Iteration 76, loss = 0.64381309\n",
            "Iteration 77, loss = 0.64297595\n",
            "Iteration 78, loss = 0.64215750\n",
            "Iteration 79, loss = 0.64136949\n",
            "Iteration 80, loss = 0.64060358\n",
            "Iteration 81, loss = 0.63981219\n",
            "Iteration 82, loss = 0.63909431\n",
            "Iteration 83, loss = 0.63840286\n",
            "Iteration 84, loss = 0.63766622\n",
            "Iteration 85, loss = 0.63702776\n",
            "Iteration 86, loss = 0.63626099\n",
            "Iteration 87, loss = 0.63554534\n",
            "Iteration 88, loss = 0.63482509\n",
            "Iteration 89, loss = 0.63410681\n",
            "Iteration 90, loss = 0.63339896\n",
            "Iteration 91, loss = 0.63272694\n",
            "Iteration 92, loss = 0.63204511\n",
            "Iteration 93, loss = 0.63144044\n",
            "Iteration 94, loss = 0.63084003\n",
            "Iteration 95, loss = 0.63028250\n",
            "Iteration 96, loss = 0.62965499\n",
            "Iteration 97, loss = 0.62905022\n",
            "Iteration 98, loss = 0.62844429\n",
            "Iteration 99, loss = 0.62780816\n",
            "Iteration 100, loss = 0.62726404\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 100 and for layer number 5 : 0.6987500000000001\n",
            "Iteration 1, loss = 1.38926569\n",
            "Iteration 2, loss = 1.37280501\n",
            "Iteration 3, loss = 1.34746797\n",
            "Iteration 4, loss = 1.31619589\n",
            "Iteration 5, loss = 1.28284600\n",
            "Iteration 6, loss = 1.24835476\n",
            "Iteration 7, loss = 1.21472357\n",
            "Iteration 8, loss = 1.18128774\n",
            "Iteration 9, loss = 1.14886509\n",
            "Iteration 10, loss = 1.11879494\n",
            "Iteration 11, loss = 1.08953649\n",
            "Iteration 12, loss = 1.06187751\n",
            "Iteration 13, loss = 1.03601333\n",
            "Iteration 14, loss = 1.01129749\n",
            "Iteration 15, loss = 0.98904579\n",
            "Iteration 16, loss = 0.96813749\n",
            "Iteration 17, loss = 0.94777197\n",
            "Iteration 18, loss = 0.92847460\n",
            "Iteration 19, loss = 0.91099975\n",
            "Iteration 20, loss = 0.89440660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 21, loss = 0.87873317\n",
            "Iteration 22, loss = 0.86430695\n",
            "Iteration 23, loss = 0.85086938\n",
            "Iteration 24, loss = 0.83820984\n",
            "Iteration 25, loss = 0.82677373\n",
            "Iteration 26, loss = 0.81533009\n",
            "Iteration 27, loss = 0.80526332\n",
            "Iteration 28, loss = 0.79582014\n",
            "Iteration 29, loss = 0.78704031\n",
            "Iteration 30, loss = 0.77874972\n",
            "Iteration 31, loss = 0.77095374\n",
            "Iteration 32, loss = 0.76369540\n",
            "Iteration 33, loss = 0.75713497\n",
            "Iteration 34, loss = 0.75066766\n",
            "Iteration 35, loss = 0.74477483\n",
            "Iteration 36, loss = 0.73962234\n",
            "Iteration 37, loss = 0.73447076\n",
            "Iteration 38, loss = 0.72976653\n",
            "Iteration 39, loss = 0.72535374\n",
            "Iteration 40, loss = 0.72111863\n",
            "Iteration 41, loss = 0.71707931\n",
            "Iteration 42, loss = 0.71336464\n",
            "Iteration 43, loss = 0.70972019\n",
            "Iteration 44, loss = 0.70623239\n",
            "Iteration 45, loss = 0.70314381\n",
            "Iteration 46, loss = 0.69989642\n",
            "Iteration 47, loss = 0.69679974\n",
            "Iteration 48, loss = 0.69401116\n",
            "Iteration 49, loss = 0.69139935\n",
            "Iteration 50, loss = 0.68879462\n",
            "Iteration 51, loss = 0.68651470\n",
            "Iteration 52, loss = 0.68421757\n",
            "Iteration 53, loss = 0.68200484\n",
            "Iteration 54, loss = 0.68007585\n",
            "Iteration 55, loss = 0.67808367\n",
            "Iteration 56, loss = 0.67618600\n",
            "Iteration 57, loss = 0.67429057\n",
            "Iteration 58, loss = 0.67248101\n",
            "Iteration 59, loss = 0.67080894\n",
            "Iteration 60, loss = 0.66927500\n",
            "Iteration 61, loss = 0.66761747\n",
            "Iteration 62, loss = 0.66614964\n",
            "Iteration 63, loss = 0.66454833\n",
            "Iteration 64, loss = 0.66305573\n",
            "Iteration 65, loss = 0.66161184\n",
            "Iteration 66, loss = 0.66013871\n",
            "Iteration 67, loss = 0.65874027\n",
            "Iteration 68, loss = 0.65728994\n",
            "Iteration 69, loss = 0.65605861\n",
            "Iteration 70, loss = 0.65477739\n",
            "Iteration 71, loss = 0.65354960\n",
            "Iteration 72, loss = 0.65235350\n",
            "Iteration 73, loss = 0.65124673\n",
            "Iteration 74, loss = 0.65008456\n",
            "Iteration 75, loss = 0.64907371\n",
            "Iteration 76, loss = 0.64797402\n",
            "Iteration 77, loss = 0.64701224\n",
            "Iteration 78, loss = 0.64597245\n",
            "Iteration 79, loss = 0.64505579\n",
            "Iteration 80, loss = 0.64412907\n",
            "Iteration 81, loss = 0.64320081\n",
            "Iteration 82, loss = 0.64230711\n",
            "Iteration 83, loss = 0.64139953\n",
            "Iteration 84, loss = 0.64054124\n",
            "Iteration 85, loss = 0.63968788\n",
            "Iteration 86, loss = 0.63885672\n",
            "Iteration 87, loss = 0.63805459\n",
            "Iteration 88, loss = 0.63726151\n",
            "Iteration 89, loss = 0.63648889\n",
            "Iteration 90, loss = 0.63578801\n",
            "Iteration 91, loss = 0.63499720\n",
            "Iteration 92, loss = 0.63423864\n",
            "Iteration 93, loss = 0.63348850\n",
            "Iteration 94, loss = 0.63277767\n",
            "Iteration 95, loss = 0.63198463\n",
            "Iteration 96, loss = 0.63126116\n",
            "Iteration 97, loss = 0.63054150\n",
            "Iteration 98, loss = 0.62982044\n",
            "Iteration 99, loss = 0.62910262\n",
            "Iteration 100, loss = 0.62846924\n",
            "Iteration 1, loss = 1.37877399\n",
            "Iteration 2, loss = 1.36256087\n",
            "Iteration 3, loss = 1.33739238\n",
            "Iteration 4, loss = 1.30659378\n",
            "Iteration 5, loss = 1.27335856\n",
            "Iteration 6, loss = 1.23889435\n",
            "Iteration 7, loss = 1.20499233\n",
            "Iteration 8, loss = 1.17181650\n",
            "Iteration 9, loss = 1.13971284\n",
            "Iteration 10, loss = 1.10956324\n",
            "Iteration 11, loss = 1.08045416\n",
            "Iteration 12, loss = 1.05294176\n",
            "Iteration 13, loss = 1.02710830\n",
            "Iteration 14, loss = 1.00261165\n",
            "Iteration 15, loss = 0.98070591\n",
            "Iteration 16, loss = 0.95987920\n",
            "Iteration 17, loss = 0.94020854\n",
            "Iteration 18, loss = 0.92122763\n",
            "Iteration 19, loss = 0.90383330\n",
            "Iteration 20, loss = 0.88748408\n",
            "Iteration 21, loss = 0.87191253\n",
            "Iteration 22, loss = 0.85761649\n",
            "Iteration 23, loss = 0.84438059\n",
            "Iteration 24, loss = 0.83166341\n",
            "Iteration 25, loss = 0.82034630\n",
            "Iteration 26, loss = 0.80900028\n",
            "Iteration 27, loss = 0.79924699\n",
            "Iteration 28, loss = 0.78975345\n",
            "Iteration 29, loss = 0.78110653\n",
            "Iteration 30, loss = 0.77320177\n",
            "Iteration 31, loss = 0.76575073\n",
            "Iteration 32, loss = 0.75849142\n",
            "Iteration 33, loss = 0.75189914\n",
            "Iteration 34, loss = 0.74559673\n",
            "Iteration 35, loss = 0.73983119\n",
            "Iteration 36, loss = 0.73485730\n",
            "Iteration 37, loss = 0.72990421\n",
            "Iteration 38, loss = 0.72544570\n",
            "Iteration 39, loss = 0.72117603\n",
            "Iteration 40, loss = 0.71703850\n",
            "Iteration 41, loss = 0.71322451\n",
            "Iteration 42, loss = 0.70966686\n",
            "Iteration 43, loss = 0.70626584\n",
            "Iteration 44, loss = 0.70282098\n",
            "Iteration 45, loss = 0.69979339\n",
            "Iteration 46, loss = 0.69664159\n",
            "Iteration 47, loss = 0.69359879\n",
            "Iteration 48, loss = 0.69084866\n",
            "Iteration 49, loss = 0.68833276\n",
            "Iteration 50, loss = 0.68587860\n",
            "Iteration 51, loss = 0.68370393\n",
            "Iteration 52, loss = 0.68146700\n",
            "Iteration 53, loss = 0.67934255\n",
            "Iteration 54, loss = 0.67741969\n",
            "Iteration 55, loss = 0.67540396\n",
            "Iteration 56, loss = 0.67352010\n",
            "Iteration 57, loss = 0.67177485\n",
            "Iteration 58, loss = 0.67002014\n",
            "Iteration 59, loss = 0.66846780\n",
            "Iteration 60, loss = 0.66695573\n",
            "Iteration 61, loss = 0.66542439\n",
            "Iteration 62, loss = 0.66394678\n",
            "Iteration 63, loss = 0.66235577\n",
            "Iteration 64, loss = 0.66088106\n",
            "Iteration 65, loss = 0.65935602\n",
            "Iteration 66, loss = 0.65793334\n",
            "Iteration 67, loss = 0.65652492\n",
            "Iteration 68, loss = 0.65508253\n",
            "Iteration 69, loss = 0.65383545\n",
            "Iteration 70, loss = 0.65253073\n",
            "Iteration 71, loss = 0.65132221\n",
            "Iteration 72, loss = 0.65005109\n",
            "Iteration 73, loss = 0.64889076\n",
            "Iteration 74, loss = 0.64770514"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 75, loss = 0.64660515\n",
            "Iteration 76, loss = 0.64544655\n",
            "Iteration 77, loss = 0.64443220\n",
            "Iteration 78, loss = 0.64331724\n",
            "Iteration 79, loss = 0.64235456\n",
            "Iteration 80, loss = 0.64132897\n",
            "Iteration 81, loss = 0.64038882\n",
            "Iteration 82, loss = 0.63947120\n",
            "Iteration 83, loss = 0.63852183\n",
            "Iteration 84, loss = 0.63767949\n",
            "Iteration 85, loss = 0.63682970\n",
            "Iteration 86, loss = 0.63597486\n",
            "Iteration 87, loss = 0.63514300\n",
            "Iteration 88, loss = 0.63435968\n",
            "Iteration 89, loss = 0.63354415\n",
            "Iteration 90, loss = 0.63282646\n",
            "Iteration 91, loss = 0.63199980\n",
            "Iteration 92, loss = 0.63122688\n",
            "Iteration 93, loss = 0.63045721\n",
            "Iteration 94, loss = 0.62970582\n",
            "Iteration 95, loss = 0.62887494\n",
            "Iteration 96, loss = 0.62811898\n",
            "Iteration 97, loss = 0.62738428\n",
            "Iteration 98, loss = 0.62665124\n",
            "Iteration 99, loss = 0.62594012\n",
            "Iteration 100, loss = 0.62527980\n",
            "Iteration 1, loss = 1.37951553\n",
            "Iteration 2, loss = 1.36345276\n",
            "Iteration 3, loss = 1.33844177\n",
            "Iteration 4, loss = 1.30796298\n",
            "Iteration 5, loss = 1.27459126\n",
            "Iteration 6, loss = 1.24040492\n",
            "Iteration 7, loss = 1.20671426\n",
            "Iteration 8, loss = 1.17282486\n",
            "Iteration 9, loss = 1.14067304\n",
            "Iteration 10, loss = 1.10992688\n",
            "Iteration 11, loss = 1.08024299\n",
            "Iteration 12, loss = 1.05249805\n",
            "Iteration 13, loss = 1.02632044\n",
            "Iteration 14, loss = 1.00219240\n",
            "Iteration 15, loss = 0.97949832\n",
            "Iteration 16, loss = 0.95866806\n",
            "Iteration 17, loss = 0.93831233\n",
            "Iteration 18, loss = 0.91955176\n",
            "Iteration 19, loss = 0.90227991\n",
            "Iteration 20, loss = 0.88594294\n",
            "Iteration 21, loss = 0.87069089\n",
            "Iteration 22, loss = 0.85667586\n",
            "Iteration 23, loss = 0.84359414\n",
            "Iteration 24, loss = 0.83093219\n",
            "Iteration 25, loss = 0.81954024\n",
            "Iteration 26, loss = 0.80797193\n",
            "Iteration 27, loss = 0.79813972\n",
            "Iteration 28, loss = 0.78827489\n",
            "Iteration 29, loss = 0.77945534\n",
            "Iteration 30, loss = 0.77167073\n",
            "Iteration 31, loss = 0.76379821\n",
            "Iteration 32, loss = 0.75668072\n",
            "Iteration 33, loss = 0.74977028\n",
            "Iteration 34, loss = 0.74333202\n",
            "Iteration 35, loss = 0.73744851\n",
            "Iteration 36, loss = 0.73224584\n",
            "Iteration 37, loss = 0.72713655\n",
            "Iteration 38, loss = 0.72251036\n",
            "Iteration 39, loss = 0.71809401\n",
            "Iteration 40, loss = 0.71381178\n",
            "Iteration 41, loss = 0.71006817\n",
            "Iteration 42, loss = 0.70642663\n",
            "Iteration 43, loss = 0.70297709\n",
            "Iteration 44, loss = 0.69956808\n",
            "Iteration 45, loss = 0.69665075\n",
            "Iteration 46, loss = 0.69345907\n",
            "Iteration 47, loss = 0.69050376\n",
            "Iteration 48, loss = 0.68787513\n",
            "Iteration 49, loss = 0.68529710\n",
            "Iteration 50, loss = 0.68283550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 51, loss = 0.68057320\n",
            "Iteration 52, loss = 0.67831132\n",
            "Iteration 53, loss = 0.67622728\n",
            "Iteration 54, loss = 0.67421572\n",
            "Iteration 55, loss = 0.67206718\n",
            "Iteration 56, loss = 0.67019947\n",
            "Iteration 57, loss = 0.66839904\n",
            "Iteration 58, loss = 0.66666072\n",
            "Iteration 59, loss = 0.66504789\n",
            "Iteration 60, loss = 0.66353676\n",
            "Iteration 61, loss = 0.66204168\n",
            "Iteration 62, loss = 0.66055577\n",
            "Iteration 63, loss = 0.65901870\n",
            "Iteration 64, loss = 0.65756080\n",
            "Iteration 65, loss = 0.65607636\n",
            "Iteration 66, loss = 0.65469643\n",
            "Iteration 67, loss = 0.65336830\n",
            "Iteration 68, loss = 0.65205637\n",
            "Iteration 69, loss = 0.65084949\n",
            "Iteration 70, loss = 0.64960277\n",
            "Iteration 71, loss = 0.64837394\n",
            "Iteration 72, loss = 0.64713407\n",
            "Iteration 73, loss = 0.64596524\n",
            "Iteration 74, loss = 0.64480252\n",
            "Iteration 75, loss = 0.64368401\n",
            "Iteration 76, loss = 0.64254765\n",
            "Iteration 77, loss = 0.64150617\n",
            "Iteration 78, loss = 0.64039261\n",
            "Iteration 79, loss = 0.63941413\n",
            "Iteration 80, loss = 0.63838386\n",
            "Iteration 81, loss = 0.63743753\n",
            "Iteration 82, loss = 0.63650474\n",
            "Iteration 83, loss = 0.63557633\n",
            "Iteration 84, loss = 0.63469972\n",
            "Iteration 85, loss = 0.63385219\n",
            "Iteration 86, loss = 0.63298910\n",
            "Iteration 87, loss = 0.63211509\n",
            "Iteration 88, loss = 0.63130517\n",
            "Iteration 89, loss = 0.63045727\n",
            "Iteration 90, loss = 0.62971541\n",
            "Iteration 91, loss = 0.62884744\n",
            "Iteration 92, loss = 0.62806282\n",
            "Iteration 93, loss = 0.62730010\n",
            "Iteration 94, loss = 0.62658916\n",
            "Iteration 95, loss = 0.62581543\n",
            "Iteration 96, loss = 0.62512103\n",
            "Iteration 97, loss = 0.62443816\n",
            "Iteration 98, loss = 0.62373486\n",
            "Iteration 99, loss = 0.62304011\n",
            "Iteration 100, loss = 0.62238348\n",
            "Iteration 1, loss = 1.39138454\n",
            "Iteration 2, loss = 1.37512562\n",
            "Iteration 3, loss = 1.34984865\n",
            "Iteration 4, loss = 1.31828804\n",
            "Iteration 5, loss = 1.28379813\n",
            "Iteration 6, loss = 1.24919730\n",
            "Iteration 7, loss = 1.21383002\n",
            "Iteration 8, loss = 1.17922096\n",
            "Iteration 9, loss = 1.14576061\n",
            "Iteration 10, loss = 1.11442987\n",
            "Iteration 11, loss = 1.08433757\n",
            "Iteration 12, loss = 1.05600586\n",
            "Iteration 13, loss = 1.02936580\n",
            "Iteration 14, loss = 1.00449374\n",
            "Iteration 15, loss = 0.98126814\n",
            "Iteration 16, loss = 0.96045322\n",
            "Iteration 17, loss = 0.93985076\n",
            "Iteration 18, loss = 0.92131534\n",
            "Iteration 19, loss = 0.90427757\n",
            "Iteration 20, loss = 0.88794149\n",
            "Iteration 21, loss = 0.87257105\n",
            "Iteration 22, loss = 0.85865619"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 23, loss = 0.84553420\n",
            "Iteration 24, loss = 0.83288102\n",
            "Iteration 25, loss = 0.82164075\n",
            "Iteration 26, loss = 0.81031884\n",
            "Iteration 27, loss = 0.80044622\n",
            "Iteration 28, loss = 0.79078083\n",
            "Iteration 29, loss = 0.78199497\n",
            "Iteration 30, loss = 0.77411266\n",
            "Iteration 31, loss = 0.76624354\n",
            "Iteration 32, loss = 0.75938182\n",
            "Iteration 33, loss = 0.75241517\n",
            "Iteration 34, loss = 0.74614686\n",
            "Iteration 35, loss = 0.74020298\n",
            "Iteration 36, loss = 0.73504213\n",
            "Iteration 37, loss = 0.72986011\n",
            "Iteration 38, loss = 0.72514082\n",
            "Iteration 39, loss = 0.72075279\n",
            "Iteration 40, loss = 0.71636099\n",
            "Iteration 41, loss = 0.71262350\n",
            "Iteration 42, loss = 0.70892549\n",
            "Iteration 43, loss = 0.70543635\n",
            "Iteration 44, loss = 0.70195493\n",
            "Iteration 45, loss = 0.69894456\n",
            "Iteration 46, loss = 0.69579898\n",
            "Iteration 47, loss = 0.69295315\n",
            "Iteration 48, loss = 0.69035026\n",
            "Iteration 49, loss = 0.68781951\n",
            "Iteration 50, loss = 0.68550089\n",
            "Iteration 51, loss = 0.68324006\n",
            "Iteration 52, loss = 0.68114943\n",
            "Iteration 53, loss = 0.67907425\n",
            "Iteration 54, loss = 0.67705865\n",
            "Iteration 55, loss = 0.67497031\n",
            "Iteration 56, loss = 0.67311956\n",
            "Iteration 57, loss = 0.67123379\n",
            "Iteration 58, loss = 0.66945703\n",
            "Iteration 59, loss = 0.66776204\n",
            "Iteration 60, loss = 0.66623540\n",
            "Iteration 61, loss = 0.66472561\n",
            "Iteration 62, loss = 0.66332128\n",
            "Iteration 63, loss = 0.66181759\n",
            "Iteration 64, loss = 0.66047596\n",
            "Iteration 65, loss = 0.65908815\n",
            "Iteration 66, loss = 0.65776076\n",
            "Iteration 67, loss = 0.65648902\n",
            "Iteration 68, loss = 0.65521748\n",
            "Iteration 69, loss = 0.65400813\n",
            "Iteration 70, loss = 0.65278035\n",
            "Iteration 71, loss = 0.65149094\n",
            "Iteration 72, loss = 0.65028077\n",
            "Iteration 73, loss = 0.64905842\n",
            "Iteration 74, loss = 0.64790424\n",
            "Iteration 75, loss = 0.64677162\n",
            "Iteration 76, loss = 0.64562035\n",
            "Iteration 77, loss = 0.64452933\n",
            "Iteration 78, loss = 0.64345644\n",
            "Iteration 79, loss = 0.64241040\n",
            "Iteration 80, loss = 0.64140986\n",
            "Iteration 81, loss = 0.64043956\n",
            "Iteration 82, loss = 0.63949342\n",
            "Iteration 83, loss = 0.63859004\n",
            "Iteration 84, loss = 0.63771080\n",
            "Iteration 85, loss = 0.63683352\n",
            "Iteration 86, loss = 0.63594807\n",
            "Iteration 87, loss = 0.63506056\n",
            "Iteration 88, loss = 0.63421892\n",
            "Iteration 89, loss = 0.63337974\n",
            "Iteration 90, loss = 0.63261508\n",
            "Iteration 91, loss = 0.63177660\n",
            "Iteration 92, loss = 0.63099911\n",
            "Iteration 93, loss = 0.63024576\n",
            "Iteration 94, loss = 0.62950227\n",
            "Iteration 95, loss = 0.62874205\n",
            "Iteration 96, loss = 0.62802933\n",
            "Iteration 97, loss = 0.62731275\n",
            "Iteration 98, loss = 0.62657765\n",
            "Iteration 99, loss = 0.62582972\n",
            "Iteration 100, loss = 0.62512388\n",
            "Iteration 1, loss = 1.36378915\n",
            "Iteration 2, loss = 1.34879364\n",
            "Iteration 3, loss = 1.32549217\n",
            "Iteration 4, loss = 1.29708034\n",
            "Iteration 5, loss = 1.26554033\n",
            "Iteration 6, loss = 1.23383872\n",
            "Iteration 7, loss = 1.20200469\n",
            "Iteration 8, loss = 1.17053414\n",
            "Iteration 9, loss = 1.14064775\n",
            "Iteration 10, loss = 1.11151975\n",
            "Iteration 11, loss = 1.08406253\n",
            "Iteration 12, loss = 1.05814511\n",
            "Iteration 13, loss = 1.03354346\n",
            "Iteration 14, loss = 1.01070814\n",
            "Iteration 15, loss = 0.98890098\n",
            "Iteration 16, loss = 0.96886449\n",
            "Iteration 17, loss = 0.94923104\n",
            "Iteration 18, loss = 0.93151074\n",
            "Iteration 19, loss = 0.91502500\n",
            "Iteration 20, loss = 0.89945880\n",
            "Iteration 21, loss = 0.88472298\n",
            "Iteration 22, loss = 0.87121456\n",
            "Iteration 23, loss = 0.85846639\n",
            "Iteration 24, loss = 0.84606854\n",
            "Iteration 25, loss = 0.83470170\n",
            "Iteration 26, loss = 0.82388112\n",
            "Iteration 27, loss = 0.81424012\n",
            "Iteration 28, loss = 0.80487305\n",
            "Iteration 29, loss = 0.79605592\n",
            "Iteration 30, loss = 0.78816723\n",
            "Iteration 31, loss = 0.78027235\n",
            "Iteration 32, loss = 0.77329705\n",
            "Iteration 33, loss = 0.76630727\n",
            "Iteration 34, loss = 0.75998863\n",
            "Iteration 35, loss = 0.75396852\n",
            "Iteration 36, loss = 0.74833678\n",
            "Iteration 37, loss = 0.74285837\n",
            "Iteration 38, loss = 0.73803442\n",
            "Iteration 39, loss = 0.73341573\n",
            "Iteration 40, loss = 0.72891424\n",
            "Iteration 41, loss = 0.72504769\n",
            "Iteration 42, loss = 0.72109686\n",
            "Iteration 43, loss = 0.71757315\n",
            "Iteration 44, loss = 0.71400332\n",
            "Iteration 45, loss = 0.71081284\n",
            "Iteration 46, loss = 0.70768563\n",
            "Iteration 47, loss = 0.70471192\n",
            "Iteration 48, loss = 0.70194281\n",
            "Iteration 49, loss = 0.69925315\n",
            "Iteration 50, loss = 0.69669744\n",
            "Iteration 51, loss = 0.69424636\n",
            "Iteration 52, loss = 0.69189830\n",
            "Iteration 53, loss = 0.68973931\n",
            "Iteration 54, loss = 0.68760345\n",
            "Iteration 55, loss = 0.68543024\n",
            "Iteration 56, loss = 0.68342348\n",
            "Iteration 57, loss = 0.68147262\n",
            "Iteration 58, loss = 0.67964124\n",
            "Iteration 59, loss = 0.67787307\n",
            "Iteration 60, loss = 0.67621673\n",
            "Iteration 61, loss = 0.67462365\n",
            "Iteration 62, loss = 0.67309221\n",
            "Iteration 63, loss = 0.67150391\n",
            "Iteration 64, loss = 0.67007641\n",
            "Iteration 65, loss = 0.66858230\n",
            "Iteration 66, loss = 0.66714332\n",
            "Iteration 67, loss = 0.66581116\n",
            "Iteration 68, loss = 0.66446415\n",
            "Iteration 69, loss = 0.66313131\n",
            "Iteration 70, loss = 0.66182390\n",
            "Iteration 71, loss = 0.66046432\n",
            "Iteration 72, loss = 0.65919492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 73, loss = 0.65790677\n",
            "Iteration 74, loss = 0.65671474\n",
            "Iteration 75, loss = 0.65551452\n",
            "Iteration 76, loss = 0.65432494\n",
            "Iteration 77, loss = 0.65319837\n",
            "Iteration 78, loss = 0.65212463\n",
            "Iteration 79, loss = 0.65105388\n",
            "Iteration 80, loss = 0.65003300\n",
            "Iteration 81, loss = 0.64903759\n",
            "Iteration 82, loss = 0.64809585\n",
            "Iteration 83, loss = 0.64717154\n",
            "Iteration 84, loss = 0.64626768\n",
            "Iteration 85, loss = 0.64537314\n",
            "Iteration 86, loss = 0.64449899\n",
            "Iteration 87, loss = 0.64359621\n",
            "Iteration 88, loss = 0.64275708\n",
            "Iteration 89, loss = 0.64191812\n",
            "Iteration 90, loss = 0.64113195\n",
            "Iteration 91, loss = 0.64030779\n",
            "Iteration 92, loss = 0.63949773\n",
            "Iteration 93, loss = 0.63875682\n",
            "Iteration 94, loss = 0.63795612\n",
            "Iteration 95, loss = 0.63717242\n",
            "Iteration 96, loss = 0.63645109\n",
            "Iteration 97, loss = 0.63570073\n",
            "Iteration 98, loss = 0.63498115\n",
            "Iteration 99, loss = 0.63421861\n",
            "Iteration 100, loss = 0.63348999\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 100 and for layer number 6 : 0.6799999999999999\n",
            "Iteration 1, loss = 0.61693719\n",
            "Iteration 2, loss = 0.61686447\n",
            "Iteration 3, loss = 0.61675151\n",
            "Iteration 4, loss = 0.61661842\n",
            "Iteration 5, loss = 0.61646426\n",
            "Iteration 6, loss = 0.61631571\n",
            "Iteration 7, loss = 0.61614581\n",
            "Iteration 8, loss = 0.61596065\n",
            "Iteration 9, loss = 0.61578398\n",
            "Iteration 10, loss = 0.61560733\n",
            "Iteration 11, loss = 0.61543375\n",
            "Iteration 12, loss = 0.61527975\n",
            "Iteration 13, loss = 0.61511094\n",
            "Iteration 14, loss = 0.61495375\n",
            "Iteration 15, loss = 0.61478984\n",
            "Iteration 16, loss = 0.61463306\n",
            "Iteration 17, loss = 0.61447873\n",
            "Iteration 18, loss = 0.61432621\n",
            "Iteration 19, loss = 0.61417436\n",
            "Iteration 20, loss = 0.61400728\n",
            "Iteration 21, loss = 0.61383297\n",
            "Iteration 22, loss = 0.61367462\n",
            "Iteration 23, loss = 0.61350652\n",
            "Iteration 24, loss = 0.61335359\n",
            "Iteration 25, loss = 0.61319555\n",
            "Iteration 26, loss = 0.61306514\n",
            "Iteration 27, loss = 0.61290532\n",
            "Iteration 28, loss = 0.61277722\n",
            "Iteration 29, loss = 0.61265346\n",
            "Iteration 30, loss = 0.61251235\n",
            "Iteration 31, loss = 0.61238930\n",
            "Iteration 32, loss = 0.61226155\n",
            "Iteration 33, loss = 0.61213591\n",
            "Iteration 34, loss = 0.61202127\n",
            "Iteration 35, loss = 0.61189676\n",
            "Iteration 36, loss = 0.61176314\n",
            "Iteration 37, loss = 0.61162506\n",
            "Iteration 38, loss = 0.61149544\n",
            "Iteration 39, loss = 0.61137006\n",
            "Iteration 40, loss = 0.61124508\n",
            "Iteration 41, loss = 0.61110902"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 42, loss = 0.61099032\n",
            "Iteration 43, loss = 0.61085784\n",
            "Iteration 44, loss = 0.61073928\n",
            "Iteration 45, loss = 0.61062182\n",
            "Iteration 46, loss = 0.61048916\n",
            "Iteration 47, loss = 0.61036991\n",
            "Iteration 48, loss = 0.61024277\n",
            "Iteration 49, loss = 0.61013546\n",
            "Iteration 50, loss = 0.61002089\n",
            "Iteration 51, loss = 0.60990687\n",
            "Iteration 52, loss = 0.60979970\n",
            "Iteration 53, loss = 0.60969865\n",
            "Iteration 54, loss = 0.60959045\n",
            "Iteration 55, loss = 0.60948292\n",
            "Iteration 56, loss = 0.60937738\n",
            "Iteration 57, loss = 0.60926927\n",
            "Iteration 58, loss = 0.60917448\n",
            "Iteration 59, loss = 0.60906914\n",
            "Iteration 60, loss = 0.60896820\n",
            "Iteration 61, loss = 0.60886422\n",
            "Iteration 62, loss = 0.60877775\n",
            "Iteration 63, loss = 0.60867478\n",
            "Iteration 64, loss = 0.60857955\n",
            "Iteration 65, loss = 0.60848439\n",
            "Iteration 66, loss = 0.60838570\n",
            "Iteration 67, loss = 0.60829814\n",
            "Iteration 68, loss = 0.60820577\n",
            "Iteration 69, loss = 0.60812774\n",
            "Iteration 70, loss = 0.60803664\n",
            "Iteration 71, loss = 0.60794173\n",
            "Iteration 72, loss = 0.60785282\n",
            "Iteration 73, loss = 0.60776810\n",
            "Iteration 74, loss = 0.60766500\n",
            "Iteration 75, loss = 0.60758161\n",
            "Iteration 76, loss = 0.60748356\n",
            "Iteration 77, loss = 0.60739894\n",
            "Iteration 78, loss = 0.60731203\n",
            "Iteration 79, loss = 0.60722005\n",
            "Iteration 80, loss = 0.60712968\n",
            "Iteration 81, loss = 0.60704893\n",
            "Iteration 82, loss = 0.60696141\n",
            "Iteration 83, loss = 0.60688701\n",
            "Iteration 84, loss = 0.60679662\n",
            "Iteration 85, loss = 0.60670958\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61771206\n",
            "Iteration 2, loss = 0.61763256\n",
            "Iteration 3, loss = 0.61750896\n",
            "Iteration 4, loss = 0.61736013\n",
            "Iteration 5, loss = 0.61720601\n",
            "Iteration 6, loss = 0.61703925\n",
            "Iteration 7, loss = 0.61687983\n",
            "Iteration 8, loss = 0.61668458\n",
            "Iteration 9, loss = 0.61650897\n",
            "Iteration 10, loss = 0.61634191\n",
            "Iteration 11, loss = 0.61616438\n",
            "Iteration 12, loss = 0.61600889\n",
            "Iteration 13, loss = 0.61582381\n",
            "Iteration 14, loss = 0.61566827\n",
            "Iteration 15, loss = 0.61549674\n",
            "Iteration 16, loss = 0.61534090\n",
            "Iteration 17, loss = 0.61517681\n",
            "Iteration 18, loss = 0.61502298\n",
            "Iteration 19, loss = 0.61486598\n",
            "Iteration 20, loss = 0.61470307\n",
            "Iteration 21, loss = 0.61454151\n",
            "Iteration 22, loss = 0.61438853\n",
            "Iteration 23, loss = 0.61422513\n",
            "Iteration 24, loss = 0.61407653\n",
            "Iteration 25, loss = 0.61391791\n",
            "Iteration 26, loss = 0.61377394\n",
            "Iteration 27, loss = 0.61362218\n",
            "Iteration 28, loss = 0.61348831\n",
            "Iteration 29, loss = 0.61334584\n",
            "Iteration 30, loss = 0.61321506\n",
            "Iteration 31, loss = 0.61309244\n",
            "Iteration 32, loss = 0.61295282\n",
            "Iteration 33, loss = 0.61282899\n",
            "Iteration 34, loss = 0.61269835\n",
            "Iteration 35, loss = 0.61258404\n",
            "Iteration 36, loss = 0.61244344\n",
            "Iteration 37, loss = 0.61230626\n",
            "Iteration 38, loss = 0.61216372\n",
            "Iteration 39, loss = 0.61203699\n",
            "Iteration 40, loss = 0.61189655\n",
            "Iteration 41, loss = 0.61175462\n",
            "Iteration 42, loss = 0.61161427\n",
            "Iteration 43, loss = 0.61148385\n",
            "Iteration 44, loss = 0.61135062\n",
            "Iteration 45, loss = 0.61122322\n",
            "Iteration 46, loss = 0.61108342\n",
            "Iteration 47, loss = 0.61094533\n",
            "Iteration 48, loss = 0.61081445\n",
            "Iteration 49, loss = 0.61068681\n",
            "Iteration 50, loss = 0.61056174\n",
            "Iteration 51, loss = 0.61044813\n",
            "Iteration 52, loss = 0.61033493\n",
            "Iteration 53, loss = 0.61021829\n",
            "Iteration 54, loss = 0.61010919\n",
            "Iteration 55, loss = 0.61000586\n",
            "Iteration 56, loss = 0.60988739\n",
            "Iteration 57, loss = 0.60976884\n",
            "Iteration 58, loss = 0.60966597\n",
            "Iteration 59, loss = 0.60955587\n",
            "Iteration 60, loss = 0.60944274\n",
            "Iteration 61, loss = 0.60932539\n",
            "Iteration 62, loss = 0.60923146\n",
            "Iteration 63, loss = 0.60909925\n",
            "Iteration 64, loss = 0.60898774\n",
            "Iteration 65, loss = 0.60887739\n",
            "Iteration 66, loss = 0.60875534\n",
            "Iteration 67, loss = 0.60864213\n",
            "Iteration 68, loss = 0.60852801\n",
            "Iteration 69, loss = 0.60842906\n",
            "Iteration 70, loss = 0.60832290\n",
            "Iteration 71, loss = 0.60821626\n",
            "Iteration 72, loss = 0.60811197\n",
            "Iteration 73, loss = 0.60800907\n",
            "Iteration 74, loss = 0.60789491\n",
            "Iteration 75, loss = 0.60780669\n",
            "Iteration 76, loss = 0.60769204\n",
            "Iteration 77, loss = 0.60759716\n",
            "Iteration 78, loss = 0.60749301\n",
            "Iteration 79, loss = 0.60738373\n",
            "Iteration 80, loss = 0.60728635\n",
            "Iteration 81, loss = 0.60719595\n",
            "Iteration 82, loss = 0.60709942\n",
            "Iteration 83, loss = 0.60701638\n",
            "Iteration 84, loss = 0.60692635\n",
            "Iteration 85, loss = 0.60683945\n",
            "Iteration 86, loss = 0.60675609\n",
            "Iteration 87, loss = 0.60667504\n",
            "Iteration 88, loss = 0.60659137\n",
            "Iteration 89, loss = 0.60651963\n",
            "Iteration 90, loss = 0.60642717\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61763916\n",
            "Iteration 2, loss = 0.61755997\n",
            "Iteration 3, loss = 0.61742232\n",
            "Iteration 4, loss = 0.61725393\n",
            "Iteration 5, loss = 0.61708619\n",
            "Iteration 6, loss = 0.61689799\n",
            "Iteration 7, loss = 0.61671618\n",
            "Iteration 8, loss = 0.61650164\n",
            "Iteration 9, loss = 0.61630973\n",
            "Iteration 10, loss = 0.61611724\n",
            "Iteration 11, loss = 0.61593663\n",
            "Iteration 12, loss = 0.61576363\n",
            "Iteration 13, loss = 0.61556226\n",
            "Iteration 14, loss = 0.61538666\n",
            "Iteration 15, loss = 0.61520889\n",
            "Iteration 16, loss = 0.61503323\n",
            "Iteration 17, loss = 0.61486521\n",
            "Iteration 18, loss = 0.61470006\n",
            "Iteration 19, loss = 0.61453828\n",
            "Iteration 20, loss = 0.61437299\n",
            "Iteration 21, loss = 0.61421653\n",
            "Iteration 22, loss = 0.61405569\n",
            "Iteration 23, loss = 0.61388640\n",
            "Iteration 24, loss = 0.61373448\n",
            "Iteration 25, loss = 0.61357646\n",
            "Iteration 26, loss = 0.61342231\n",
            "Iteration 27, loss = 0.61327118\n",
            "Iteration 28, loss = 0.61311335\n",
            "Iteration 29, loss = 0.61297079\n",
            "Iteration 30, loss = 0.61282693\n",
            "Iteration 31, loss = 0.61269890\n",
            "Iteration 32, loss = 0.61255474\n",
            "Iteration 33, loss = 0.61241223\n",
            "Iteration 34, loss = 0.61226940\n",
            "Iteration 35, loss = 0.61213160\n",
            "Iteration 36, loss = 0.61198811\n",
            "Iteration 37, loss = 0.61184581\n",
            "Iteration 38, loss = 0.61170049\n",
            "Iteration 39, loss = 0.61157611\n",
            "Iteration 40, loss = 0.61142730\n",
            "Iteration 41, loss = 0.61127667\n",
            "Iteration 42, loss = 0.61113732\n",
            "Iteration 43, loss = 0.61100411\n",
            "Iteration 44, loss = 0.61086913\n",
            "Iteration 45, loss = 0.61073484\n",
            "Iteration 46, loss = 0.61060145\n",
            "Iteration 47, loss = 0.61046314\n",
            "Iteration 48, loss = 0.61033730\n",
            "Iteration 49, loss = 0.61020822\n",
            "Iteration 50, loss = 0.61007740\n",
            "Iteration 51, loss = 0.60996338\n",
            "Iteration 52, loss = 0.60985178\n",
            "Iteration 53, loss = 0.60973070\n",
            "Iteration 54, loss = 0.60961527\n",
            "Iteration 55, loss = 0.60949856\n",
            "Iteration 56, loss = 0.60938041\n",
            "Iteration 57, loss = 0.60926013\n",
            "Iteration 58, loss = 0.60914580\n",
            "Iteration 59, loss = 0.60902755\n",
            "Iteration 60, loss = 0.60890665\n",
            "Iteration 61, loss = 0.60878651\n",
            "Iteration 62, loss = 0.60867591\n",
            "Iteration 63, loss = 0.60854401\n",
            "Iteration 64, loss = 0.60842057\n",
            "Iteration 65, loss = 0.60829508\n",
            "Iteration 66, loss = 0.60817432\n",
            "Iteration 67, loss = 0.60805562\n",
            "Iteration 68, loss = 0.60794420\n",
            "Iteration 69, loss = 0.60783815\n",
            "Iteration 70, loss = 0.60773163\n",
            "Iteration 71, loss = 0.60763356\n",
            "Iteration 72, loss = 0.60751712\n",
            "Iteration 73, loss = 0.60740617\n",
            "Iteration 74, loss = 0.60728371\n",
            "Iteration 75, loss = 0.60719874\n",
            "Iteration 76, loss = 0.60707233\n",
            "Iteration 77, loss = 0.60697847\n",
            "Iteration 78, loss = 0.60686966\n",
            "Iteration 79, loss = 0.60676477\n",
            "Iteration 80, loss = 0.60666713\n",
            "Iteration 81, loss = 0.60657029\n",
            "Iteration 82, loss = 0.60647206\n",
            "Iteration 83, loss = 0.60638523\n",
            "Iteration 84, loss = 0.60629166\n",
            "Iteration 85, loss = 0.60620042\n",
            "Iteration 86, loss = 0.60612214\n",
            "Iteration 87, loss = 0.60604223\n",
            "Iteration 88, loss = 0.60596838\n",
            "Iteration 89, loss = 0.60589523\n",
            "Iteration 90, loss = 0.60580521\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62088357\n",
            "Iteration 2, loss = 0.62080758\n",
            "Iteration 3, loss = 0.62069495\n",
            "Iteration 4, loss = 0.62054238\n",
            "Iteration 5, loss = 0.62038891\n",
            "Iteration 6, loss = 0.62021289\n",
            "Iteration 7, loss = 0.62003518\n",
            "Iteration 8, loss = 0.61983724\n",
            "Iteration 9, loss = 0.61965125\n",
            "Iteration 10, loss = 0.61946574\n",
            "Iteration 11, loss = 0.61928678\n",
            "Iteration 12, loss = 0.61912370\n",
            "Iteration 13, loss = 0.61894901\n",
            "Iteration 14, loss = 0.61877305\n",
            "Iteration 15, loss = 0.61860692\n",
            "Iteration 16, loss = 0.61844169\n",
            "Iteration 17, loss = 0.61826895\n",
            "Iteration 18, loss = 0.61810540\n",
            "Iteration 19, loss = 0.61794667\n",
            "Iteration 20, loss = 0.61779409\n",
            "Iteration 21, loss = 0.61765462\n",
            "Iteration 22, loss = 0.61749969\n",
            "Iteration 23, loss = 0.61734374\n",
            "Iteration 24, loss = 0.61718970\n",
            "Iteration 25, loss = 0.61702731\n",
            "Iteration 26, loss = 0.61686454\n",
            "Iteration 27, loss = 0.61672331\n",
            "Iteration 28, loss = 0.61655046\n",
            "Iteration 29, loss = 0.61640417\n",
            "Iteration 30, loss = 0.61625657\n",
            "Iteration 31, loss = 0.61612342\n",
            "Iteration 32, loss = 0.61597996\n",
            "Iteration 33, loss = 0.61583342\n",
            "Iteration 34, loss = 0.61569987\n",
            "Iteration 35, loss = 0.61556063\n",
            "Iteration 36, loss = 0.61542958\n",
            "Iteration 37, loss = 0.61530817\n",
            "Iteration 38, loss = 0.61517959\n",
            "Iteration 39, loss = 0.61506558\n",
            "Iteration 40, loss = 0.61494687\n",
            "Iteration 41, loss = 0.61483037\n",
            "Iteration 42, loss = 0.61470883\n",
            "Iteration 43, loss = 0.61460197\n",
            "Iteration 44, loss = 0.61447871\n",
            "Iteration 45, loss = 0.61436047\n",
            "Iteration 46, loss = 0.61425494\n",
            "Iteration 47, loss = 0.61414539\n",
            "Iteration 48, loss = 0.61403406\n",
            "Iteration 49, loss = 0.61392290\n",
            "Iteration 50, loss = 0.61380453\n",
            "Iteration 51, loss = 0.61371073\n",
            "Iteration 52, loss = 0.61360963\n",
            "Iteration 53, loss = 0.61350809\n",
            "Iteration 54, loss = 0.61341021\n",
            "Iteration 55, loss = 0.61331563\n",
            "Iteration 56, loss = 0.61322032\n",
            "Iteration 57, loss = 0.61312362\n",
            "Iteration 58, loss = 0.61303187\n",
            "Iteration 59, loss = 0.61293793\n",
            "Iteration 60, loss = 0.61284660\n",
            "Iteration 61, loss = 0.61275442\n",
            "Iteration 62, loss = 0.61266764\n",
            "Iteration 63, loss = 0.61256719\n",
            "Iteration 64, loss = 0.61246187\n",
            "Iteration 65, loss = 0.61235746\n",
            "Iteration 66, loss = 0.61225596\n",
            "Iteration 67, loss = 0.61215694\n",
            "Iteration 68, loss = 0.61206416\n",
            "Iteration 69, loss = 0.61197032\n",
            "Iteration 70, loss = 0.61187100\n",
            "Iteration 71, loss = 0.61178403\n",
            "Iteration 72, loss = 0.61167796\n",
            "Iteration 73, loss = 0.61157994\n",
            "Iteration 74, loss = 0.61147541\n",
            "Iteration 75, loss = 0.61140259\n",
            "Iteration 76, loss = 0.61129541\n",
            "Iteration 77, loss = 0.61120835\n",
            "Iteration 78, loss = 0.61111527\n",
            "Iteration 79, loss = 0.61103216\n",
            "Iteration 80, loss = 0.61095465\n",
            "Iteration 81, loss = 0.61087089\n",
            "Iteration 82, loss = 0.61079131\n",
            "Iteration 83, loss = 0.61071503\n",
            "Iteration 84, loss = 0.61063483\n",
            "Iteration 85, loss = 0.61055422\n",
            "Iteration 86, loss = 0.61048168\n",
            "Iteration 87, loss = 0.61040607\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62018025\n",
            "Iteration 2, loss = 0.62009819\n",
            "Iteration 3, loss = 0.61998317\n",
            "Iteration 4, loss = 0.61981666\n",
            "Iteration 5, loss = 0.61963710\n",
            "Iteration 6, loss = 0.61944478\n",
            "Iteration 7, loss = 0.61925290\n",
            "Iteration 8, loss = 0.61903845\n",
            "Iteration 9, loss = 0.61885557\n",
            "Iteration 10, loss = 0.61866268\n",
            "Iteration 11, loss = 0.61848604\n",
            "Iteration 12, loss = 0.61833134\n",
            "Iteration 13, loss = 0.61815251\n",
            "Iteration 14, loss = 0.61796856\n",
            "Iteration 15, loss = 0.61780050\n",
            "Iteration 16, loss = 0.61763388\n",
            "Iteration 17, loss = 0.61745805\n",
            "Iteration 18, loss = 0.61727872\n",
            "Iteration 19, loss = 0.61709604\n",
            "Iteration 20, loss = 0.61693584\n",
            "Iteration 21, loss = 0.61678432\n",
            "Iteration 22, loss = 0.61660435\n",
            "Iteration 23, loss = 0.61645117\n",
            "Iteration 24, loss = 0.61629760\n",
            "Iteration 25, loss = 0.61613359\n",
            "Iteration 26, loss = 0.61598174\n",
            "Iteration 27, loss = 0.61583842\n",
            "Iteration 28, loss = 0.61567659\n",
            "Iteration 29, loss = 0.61552571\n",
            "Iteration 30, loss = 0.61538139\n",
            "Iteration 31, loss = 0.61524275\n",
            "Iteration 32, loss = 0.61509965\n",
            "Iteration 33, loss = 0.61494620\n",
            "Iteration 34, loss = 0.61481205\n",
            "Iteration 35, loss = 0.61466606\n",
            "Iteration 36, loss = 0.61454011\n",
            "Iteration 37, loss = 0.61442324\n",
            "Iteration 38, loss = 0.61429471\n",
            "Iteration 39, loss = 0.61417885\n",
            "Iteration 40, loss = 0.61406637\n",
            "Iteration 41, loss = 0.61394071\n",
            "Iteration 42, loss = 0.61382750\n",
            "Iteration 43, loss = 0.61370365\n",
            "Iteration 44, loss = 0.61359746\n",
            "Iteration 45, loss = 0.61346493\n",
            "Iteration 46, loss = 0.61335820\n",
            "Iteration 47, loss = 0.61324493\n",
            "Iteration 48, loss = 0.61312768\n",
            "Iteration 49, loss = 0.61300162\n",
            "Iteration 50, loss = 0.61287863\n",
            "Iteration 51, loss = 0.61278578\n",
            "Iteration 52, loss = 0.61266496\n",
            "Iteration 53, loss = 0.61256068\n",
            "Iteration 54, loss = 0.61244833\n",
            "Iteration 55, loss = 0.61235408\n",
            "Iteration 56, loss = 0.61225595\n",
            "Iteration 57, loss = 0.61215760\n",
            "Iteration 58, loss = 0.61206425\n",
            "Iteration 59, loss = 0.61197433\n",
            "Iteration 60, loss = 0.61188279\n",
            "Iteration 61, loss = 0.61179557\n",
            "Iteration 62, loss = 0.61170404\n",
            "Iteration 63, loss = 0.61161334\n",
            "Iteration 64, loss = 0.61151561\n",
            "Iteration 65, loss = 0.61141306\n",
            "Iteration 66, loss = 0.61130998\n",
            "Iteration 67, loss = 0.61122290\n",
            "Iteration 68, loss = 0.61112557\n",
            "Iteration 69, loss = 0.61103638\n",
            "Iteration 70, loss = 0.61095068\n",
            "Iteration 71, loss = 0.61087186\n",
            "Iteration 72, loss = 0.61077796\n",
            "Iteration 73, loss = 0.61069137\n",
            "Iteration 74, loss = 0.61060678\n",
            "Iteration 75, loss = 0.61052912\n",
            "Iteration 76, loss = 0.61045012\n",
            "Iteration 77, loss = 0.61037871\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 150 and for layer number 2 : 0.7\n",
            "Iteration 1, loss = 0.84801653\n",
            "Iteration 2, loss = 0.84528764\n",
            "Iteration 3, loss = 0.84117560\n",
            "Iteration 4, loss = 0.83582942\n",
            "Iteration 5, loss = 0.82975974\n",
            "Iteration 6, loss = 0.82353252\n",
            "Iteration 7, loss = 0.81664243\n",
            "Iteration 8, loss = 0.81002340\n",
            "Iteration 9, loss = 0.80368440\n",
            "Iteration 10, loss = 0.79741786\n",
            "Iteration 11, loss = 0.79116889\n",
            "Iteration 12, loss = 0.78479499\n",
            "Iteration 13, loss = 0.77897234\n",
            "Iteration 14, loss = 0.77276067\n",
            "Iteration 15, loss = 0.76728918\n",
            "Iteration 16, loss = 0.76206401\n",
            "Iteration 17, loss = 0.75702104\n",
            "Iteration 18, loss = 0.75199301\n",
            "Iteration 19, loss = 0.74737027\n",
            "Iteration 20, loss = 0.74276500\n",
            "Iteration 21, loss = 0.73837289\n",
            "Iteration 22, loss = 0.73407699\n",
            "Iteration 23, loss = 0.73007924\n",
            "Iteration 24, loss = 0.72625222\n",
            "Iteration 25, loss = 0.72244250\n",
            "Iteration 26, loss = 0.71862878\n",
            "Iteration 27, loss = 0.71502063\n",
            "Iteration 28, loss = 0.71165062\n",
            "Iteration 29, loss = 0.70825508\n",
            "Iteration 30, loss = 0.70488572\n",
            "Iteration 31, loss = 0.70184174\n",
            "Iteration 32, loss = 0.69891904\n",
            "Iteration 33, loss = 0.69611030\n",
            "Iteration 34, loss = 0.69321969\n",
            "Iteration 35, loss = 0.69047473\n",
            "Iteration 36, loss = 0.68810891\n",
            "Iteration 37, loss = 0.68540721\n",
            "Iteration 38, loss = 0.68303292\n",
            "Iteration 39, loss = 0.68048045\n",
            "Iteration 40, loss = 0.67779155\n",
            "Iteration 41, loss = 0.67527131\n",
            "Iteration 42, loss = 0.67280099\n",
            "Iteration 43, loss = 0.67046862\n",
            "Iteration 44, loss = 0.66827526\n",
            "Iteration 45, loss = 0.66614495\n",
            "Iteration 46, loss = 0.66422713\n",
            "Iteration 47, loss = 0.66229631\n",
            "Iteration 48, loss = 0.66059117\n",
            "Iteration 49, loss = 0.65875777\n",
            "Iteration 50, loss = 0.65706317\n",
            "Iteration 51, loss = 0.65538023\n",
            "Iteration 52, loss = 0.65371124\n",
            "Iteration 53, loss = 0.65216698\n",
            "Iteration 54, loss = 0.65065006\n",
            "Iteration 55, loss = 0.64920995\n",
            "Iteration 56, loss = 0.64790729\n",
            "Iteration 57, loss = 0.64652529\n",
            "Iteration 58, loss = 0.64516241\n",
            "Iteration 59, loss = 0.64389573\n",
            "Iteration 60, loss = 0.64264924\n",
            "Iteration 61, loss = 0.64145436\n",
            "Iteration 62, loss = 0.64022518\n",
            "Iteration 63, loss = 0.63902072\n",
            "Iteration 64, loss = 0.63780270\n",
            "Iteration 65, loss = 0.63673321\n",
            "Iteration 66, loss = 0.63556199\n",
            "Iteration 67, loss = 0.63443060\n",
            "Iteration 68, loss = 0.63346065\n",
            "Iteration 69, loss = 0.63238086\n",
            "Iteration 70, loss = 0.63142079\n",
            "Iteration 71, loss = 0.63054257\n",
            "Iteration 72, loss = 0.62958069\n",
            "Iteration 73, loss = 0.62866353\n",
            "Iteration 74, loss = 0.62786278\n",
            "Iteration 75, loss = 0.62707678\n",
            "Iteration 76, loss = 0.62631715\n",
            "Iteration 77, loss = 0.62554014\n",
            "Iteration 78, loss = 0.62477088\n",
            "Iteration 79, loss = 0.62400197\n",
            "Iteration 80, loss = 0.62327855\n",
            "Iteration 81, loss = 0.62255956\n",
            "Iteration 82, loss = 0.62183722\n",
            "Iteration 83, loss = 0.62108874\n",
            "Iteration 84, loss = 0.62039759\n",
            "Iteration 85, loss = 0.61967253\n",
            "Iteration 86, loss = 0.61896942\n",
            "Iteration 87, loss = 0.61829945\n",
            "Iteration 88, loss = 0.61761280\n",
            "Iteration 89, loss = 0.61697162\n",
            "Iteration 90, loss = 0.61630797\n",
            "Iteration 91, loss = 0.61566924\n",
            "Iteration 92, loss = 0.61500381\n",
            "Iteration 93, loss = 0.61436935\n",
            "Iteration 94, loss = 0.61373605\n",
            "Iteration 95, loss = 0.61307396\n",
            "Iteration 96, loss = 0.61248474\n",
            "Iteration 97, loss = 0.61183704\n",
            "Iteration 98, loss = 0.61128115\n",
            "Iteration 99, loss = 0.61063105\n",
            "Iteration 100, loss = 0.61004000\n",
            "Iteration 101, loss = 0.60947662\n",
            "Iteration 102, loss = 0.60889804\n",
            "Iteration 103, loss = 0.60843826\n",
            "Iteration 104, loss = 0.60783015\n",
            "Iteration 105, loss = 0.60731472\n",
            "Iteration 106, loss = 0.60679356\n",
            "Iteration 107, loss = 0.60628627\n",
            "Iteration 108, loss = 0.60578700\n",
            "Iteration 109, loss = 0.60530442\n",
            "Iteration 110, loss = 0.60483648\n",
            "Iteration 111, loss = 0.60436338\n",
            "Iteration 112, loss = 0.60393270\n",
            "Iteration 113, loss = 0.60352094\n",
            "Iteration 114, loss = 0.60307740\n",
            "Iteration 115, loss = 0.60268764\n",
            "Iteration 116, loss = 0.60225422\n",
            "Iteration 117, loss = 0.60183950\n",
            "Iteration 118, loss = 0.60144200\n",
            "Iteration 119, loss = 0.60106225\n",
            "Iteration 120, loss = 0.60063420\n",
            "Iteration 121, loss = 0.60023915\n",
            "Iteration 122, loss = 0.59983608\n",
            "Iteration 123, loss = 0.59942553\n",
            "Iteration 124, loss = 0.59901972\n",
            "Iteration 125, loss = 0.59862884\n",
            "Iteration 126, loss = 0.59824233\n",
            "Iteration 127, loss = 0.59785970\n",
            "Iteration 128, loss = 0.59750168\n",
            "Iteration 129, loss = 0.59713842\n",
            "Iteration 130, loss = 0.59680408\n",
            "Iteration 131, loss = 0.59648184\n",
            "Iteration 132, loss = 0.59612641\n",
            "Iteration 133, loss = 0.59579540\n",
            "Iteration 134, loss = 0.59545269\n",
            "Iteration 135, loss = 0.59513843\n",
            "Iteration 136, loss = 0.59481077\n",
            "Iteration 137, loss = 0.59450553\n",
            "Iteration 138, loss = 0.59417295\n",
            "Iteration 139, loss = 0.59386987\n",
            "Iteration 140, loss = 0.59352151\n",
            "Iteration 141, loss = 0.59320636\n",
            "Iteration 142, loss = 0.59288570\n",
            "Iteration 143, loss = 0.59254332\n",
            "Iteration 144, loss = 0.59224173\n",
            "Iteration 145, loss = 0.59192463\n",
            "Iteration 146, loss = 0.59160331\n",
            "Iteration 147, loss = 0.59128449\n",
            "Iteration 148, loss = 0.59096323\n",
            "Iteration 149, loss = 0.59063084\n",
            "Iteration 150, loss = 0.59038802\n",
            "Iteration 1, loss = 0.84884434\n",
            "Iteration 2, loss = 0.84610604\n",
            "Iteration 3, loss = 0.84230708\n",
            "Iteration 4, loss = 0.83712877\n",
            "Iteration 5, loss = 0.83146006\n",
            "Iteration 6, loss = 0.82550296\n",
            "Iteration 7, loss = 0.81909492\n",
            "Iteration 8, loss = 0.81265443\n",
            "Iteration 9, loss = 0.80669577\n",
            "Iteration 10, loss = 0.80039728\n",
            "Iteration 11, loss = 0.79425360\n",
            "Iteration 12, loss = 0.78824072\n",
            "Iteration 13, loss = 0.78261576\n",
            "Iteration 14, loss = 0.77671008\n",
            "Iteration 15, loss = 0.77114439\n",
            "Iteration 16, loss = 0.76606181\n",
            "Iteration 17, loss = 0.76108403\n",
            "Iteration 18, loss = 0.75613587\n",
            "Iteration 19, loss = 0.75170254\n",
            "Iteration 20, loss = 0.74713289\n",
            "Iteration 21, loss = 0.74293770\n",
            "Iteration 22, loss = 0.73884508\n",
            "Iteration 23, loss = 0.73495863\n",
            "Iteration 24, loss = 0.73114961\n",
            "Iteration 25, loss = 0.72759086\n",
            "Iteration 26, loss = 0.72382245\n",
            "Iteration 27, loss = 0.72028484\n",
            "Iteration 28, loss = 0.71690495\n",
            "Iteration 29, loss = 0.71349934\n",
            "Iteration 30, loss = 0.71009845\n",
            "Iteration 31, loss = 0.70704925\n",
            "Iteration 32, loss = 0.70404036\n",
            "Iteration 33, loss = 0.70113012\n",
            "Iteration 34, loss = 0.69813861\n",
            "Iteration 35, loss = 0.69541121\n",
            "Iteration 36, loss = 0.69283424\n",
            "Iteration 37, loss = 0.69007100\n",
            "Iteration 38, loss = 0.68761821\n",
            "Iteration 39, loss = 0.68511950\n",
            "Iteration 40, loss = 0.68252852\n",
            "Iteration 41, loss = 0.68002110\n",
            "Iteration 42, loss = 0.67761807\n",
            "Iteration 43, loss = 0.67543891\n",
            "Iteration 44, loss = 0.67320696\n",
            "Iteration 45, loss = 0.67119636\n",
            "Iteration 46, loss = 0.66930743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 47, loss = 0.66722632\n",
            "Iteration 48, loss = 0.66538411\n",
            "Iteration 49, loss = 0.66343125\n",
            "Iteration 50, loss = 0.66151966\n",
            "Iteration 51, loss = 0.65973000\n",
            "Iteration 52, loss = 0.65793582\n",
            "Iteration 53, loss = 0.65622869\n",
            "Iteration 54, loss = 0.65468002\n",
            "Iteration 55, loss = 0.65320535\n",
            "Iteration 56, loss = 0.65181447\n",
            "Iteration 57, loss = 0.65035231\n",
            "Iteration 58, loss = 0.64891425\n",
            "Iteration 59, loss = 0.64755073\n",
            "Iteration 60, loss = 0.64626251\n",
            "Iteration 61, loss = 0.64505130\n",
            "Iteration 62, loss = 0.64379440\n",
            "Iteration 63, loss = 0.64255345\n",
            "Iteration 64, loss = 0.64133698\n",
            "Iteration 65, loss = 0.64017137\n",
            "Iteration 66, loss = 0.63893323\n",
            "Iteration 67, loss = 0.63778882\n",
            "Iteration 68, loss = 0.63673076\n",
            "Iteration 69, loss = 0.63560115\n",
            "Iteration 70, loss = 0.63455761\n",
            "Iteration 71, loss = 0.63353220\n",
            "Iteration 72, loss = 0.63246273\n",
            "Iteration 73, loss = 0.63147427\n",
            "Iteration 74, loss = 0.63055777\n",
            "Iteration 75, loss = 0.62967430\n",
            "Iteration 76, loss = 0.62883840\n",
            "Iteration 77, loss = 0.62794709\n",
            "Iteration 78, loss = 0.62718417\n",
            "Iteration 79, loss = 0.62641941\n",
            "Iteration 80, loss = 0.62567713\n",
            "Iteration 81, loss = 0.62496967\n",
            "Iteration 82, loss = 0.62424083\n",
            "Iteration 83, loss = 0.62351271\n",
            "Iteration 84, loss = 0.62281916\n",
            "Iteration 85, loss = 0.62211463\n",
            "Iteration 86, loss = 0.62139155\n",
            "Iteration 87, loss = 0.62068848\n",
            "Iteration 88, loss = 0.61997756\n",
            "Iteration 89, loss = 0.61929985\n",
            "Iteration 90, loss = 0.61858862\n",
            "Iteration 91, loss = 0.61794400\n",
            "Iteration 92, loss = 0.61723905\n",
            "Iteration 93, loss = 0.61656102\n",
            "Iteration 94, loss = 0.61591205\n",
            "Iteration 95, loss = 0.61519728\n",
            "Iteration 96, loss = 0.61459602\n",
            "Iteration 97, loss = 0.61397021\n",
            "Iteration 98, loss = 0.61336479\n",
            "Iteration 99, loss = 0.61277025\n",
            "Iteration 100, loss = 0.61218560\n",
            "Iteration 101, loss = 0.61165672\n",
            "Iteration 102, loss = 0.61110966\n",
            "Iteration 103, loss = 0.61062357\n",
            "Iteration 104, loss = 0.61004453\n",
            "Iteration 105, loss = 0.60951759\n",
            "Iteration 106, loss = 0.60895887\n",
            "Iteration 107, loss = 0.60842317\n",
            "Iteration 108, loss = 0.60790224\n",
            "Iteration 109, loss = 0.60736399\n",
            "Iteration 110, loss = 0.60687911\n",
            "Iteration 111, loss = 0.60639413\n",
            "Iteration 112, loss = 0.60589977\n",
            "Iteration 113, loss = 0.60544970\n",
            "Iteration 114, loss = 0.60499882\n",
            "Iteration 115, loss = 0.60457310\n",
            "Iteration 116, loss = 0.60414893\n",
            "Iteration 117, loss = 0.60368021\n",
            "Iteration 118, loss = 0.60331520\n",
            "Iteration 119, loss = 0.60294032\n",
            "Iteration 120, loss = 0.60252016\n",
            "Iteration 121, loss = 0.60213903\n",
            "Iteration 122, loss = 0.60174523\n",
            "Iteration 123, loss = 0.60133593\n",
            "Iteration 124, loss = 0.60091023\n",
            "Iteration 125, loss = 0.60050976\n",
            "Iteration 126, loss = 0.60010820\n",
            "Iteration 127, loss = 0.59970696\n",
            "Iteration 128, loss = 0.59934052\n",
            "Iteration 129, loss = 0.59898036\n",
            "Iteration 130, loss = 0.59863160\n",
            "Iteration 131, loss = 0.59831106\n",
            "Iteration 132, loss = 0.59796491\n",
            "Iteration 133, loss = 0.59763822\n",
            "Iteration 134, loss = 0.59729262\n",
            "Iteration 135, loss = 0.59696763\n",
            "Iteration 136, loss = 0.59661883\n",
            "Iteration 137, loss = 0.59630678\n",
            "Iteration 138, loss = 0.59598908\n",
            "Iteration 139, loss = 0.59569112\n",
            "Iteration 140, loss = 0.59536288\n",
            "Iteration 141, loss = 0.59505523\n",
            "Iteration 142, loss = 0.59471383\n",
            "Iteration 143, loss = 0.59436415\n",
            "Iteration 144, loss = 0.59405748\n",
            "Iteration 145, loss = 0.59373360\n",
            "Iteration 146, loss = 0.59341229\n",
            "Iteration 147, loss = 0.59310556\n",
            "Iteration 148, loss = 0.59278314\n",
            "Iteration 149, loss = 0.59243203\n",
            "Iteration 150, loss = 0.59216162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.86923367\n",
            "Iteration 2, loss = 0.86623006\n",
            "Iteration 3, loss = 0.86201844\n",
            "Iteration 4, loss = 0.85633892\n",
            "Iteration 5, loss = 0.85008599\n",
            "Iteration 6, loss = 0.84356157\n",
            "Iteration 7, loss = 0.83652964\n",
            "Iteration 8, loss = 0.82935828\n",
            "Iteration 9, loss = 0.82278608\n",
            "Iteration 10, loss = 0.81571552\n",
            "Iteration 11, loss = 0.80886293\n",
            "Iteration 12, loss = 0.80218902\n",
            "Iteration 13, loss = 0.79599872\n",
            "Iteration 14, loss = 0.78964699\n",
            "Iteration 15, loss = 0.78337376\n",
            "Iteration 16, loss = 0.77753155\n",
            "Iteration 17, loss = 0.77180928\n",
            "Iteration 18, loss = 0.76609001\n",
            "Iteration 19, loss = 0.76110090\n",
            "Iteration 20, loss = 0.75627648\n",
            "Iteration 21, loss = 0.75155283\n",
            "Iteration 22, loss = 0.74704216\n",
            "Iteration 23, loss = 0.74279740\n",
            "Iteration 24, loss = 0.73856804\n",
            "Iteration 25, loss = 0.73462565\n",
            "Iteration 26, loss = 0.73055780\n",
            "Iteration 27, loss = 0.72667968\n",
            "Iteration 28, loss = 0.72311381\n",
            "Iteration 29, loss = 0.71931481\n",
            "Iteration 30, loss = 0.71559199\n",
            "Iteration 31, loss = 0.71214502\n",
            "Iteration 32, loss = 0.70883599\n",
            "Iteration 33, loss = 0.70545153\n",
            "Iteration 34, loss = 0.70226445\n",
            "Iteration 35, loss = 0.69916085\n",
            "Iteration 36, loss = 0.69616862\n",
            "Iteration 37, loss = 0.69307073\n",
            "Iteration 38, loss = 0.69021815\n",
            "Iteration 39, loss = 0.68742445\n",
            "Iteration 40, loss = 0.68467159\n",
            "Iteration 41, loss = 0.68190363\n",
            "Iteration 42, loss = 0.67946455\n",
            "Iteration 43, loss = 0.67717399\n",
            "Iteration 44, loss = 0.67499244\n",
            "Iteration 45, loss = 0.67284265\n",
            "Iteration 46, loss = 0.67090200\n",
            "Iteration 47, loss = 0.66876285\n",
            "Iteration 48, loss = 0.66681518\n",
            "Iteration 49, loss = 0.66482789\n",
            "Iteration 50, loss = 0.66288284\n",
            "Iteration 51, loss = 0.66109203\n",
            "Iteration 52, loss = 0.65916577\n",
            "Iteration 53, loss = 0.65740321\n",
            "Iteration 54, loss = 0.65573998\n",
            "Iteration 55, loss = 0.65410347\n",
            "Iteration 56, loss = 0.65259831\n",
            "Iteration 57, loss = 0.65109783\n",
            "Iteration 58, loss = 0.64958658\n",
            "Iteration 59, loss = 0.64816078\n",
            "Iteration 60, loss = 0.64678991\n",
            "Iteration 61, loss = 0.64549181\n",
            "Iteration 62, loss = 0.64412795\n",
            "Iteration 63, loss = 0.64282614\n",
            "Iteration 64, loss = 0.64150110\n",
            "Iteration 65, loss = 0.64025473\n",
            "Iteration 66, loss = 0.63892570\n",
            "Iteration 67, loss = 0.63768365\n",
            "Iteration 68, loss = 0.63652642\n",
            "Iteration 69, loss = 0.63536168\n",
            "Iteration 70, loss = 0.63421997\n",
            "Iteration 71, loss = 0.63317325\n",
            "Iteration 72, loss = 0.63205658\n",
            "Iteration 73, loss = 0.63100731\n",
            "Iteration 74, loss = 0.63001822\n",
            "Iteration 75, loss = 0.62901823\n",
            "Iteration 76, loss = 0.62809679\n",
            "Iteration 77, loss = 0.62707914\n",
            "Iteration 78, loss = 0.62623111\n",
            "Iteration 79, loss = 0.62533088\n",
            "Iteration 80, loss = 0.62450127\n",
            "Iteration 81, loss = 0.62368795\n",
            "Iteration 82, loss = 0.62285271\n",
            "Iteration 83, loss = 0.62204441\n",
            "Iteration 84, loss = 0.62127094\n",
            "Iteration 85, loss = 0.62052542\n",
            "Iteration 86, loss = 0.61970997\n",
            "Iteration 87, loss = 0.61893977\n",
            "Iteration 88, loss = 0.61815595\n",
            "Iteration 89, loss = 0.61740030\n",
            "Iteration 90, loss = 0.61662252\n",
            "Iteration 91, loss = 0.61594190\n",
            "Iteration 92, loss = 0.61517044\n",
            "Iteration 93, loss = 0.61448395\n",
            "Iteration 94, loss = 0.61380520\n",
            "Iteration 95, loss = 0.61298957\n",
            "Iteration 96, loss = 0.61235810\n",
            "Iteration 97, loss = 0.61163696\n",
            "Iteration 98, loss = 0.61092207\n",
            "Iteration 99, loss = 0.61023909\n",
            "Iteration 100, loss = 0.60959741\n",
            "Iteration 101, loss = 0.60897924\n",
            "Iteration 102, loss = 0.60830994\n",
            "Iteration 103, loss = 0.60768355\n",
            "Iteration 104, loss = 0.60697099\n",
            "Iteration 105, loss = 0.60632363\n",
            "Iteration 106, loss = 0.60567325\n",
            "Iteration 107, loss = 0.60497776\n",
            "Iteration 108, loss = 0.60439037\n",
            "Iteration 109, loss = 0.60375829\n",
            "Iteration 110, loss = 0.60318738\n",
            "Iteration 111, loss = 0.60258742\n",
            "Iteration 112, loss = 0.60197632\n",
            "Iteration 113, loss = 0.60141942\n",
            "Iteration 114, loss = 0.60086993\n",
            "Iteration 115, loss = 0.60029735\n",
            "Iteration 116, loss = 0.59974641\n",
            "Iteration 117, loss = 0.59909337\n",
            "Iteration 118, loss = 0.59858018\n",
            "Iteration 119, loss = 0.59808021\n",
            "Iteration 120, loss = 0.59759884\n",
            "Iteration 121, loss = 0.59712295\n",
            "Iteration 122, loss = 0.59664494\n",
            "Iteration 123, loss = 0.59613269\n",
            "Iteration 124, loss = 0.59559367\n",
            "Iteration 125, loss = 0.59512475\n",
            "Iteration 126, loss = 0.59460867\n",
            "Iteration 127, loss = 0.59411337\n",
            "Iteration 128, loss = 0.59365786\n",
            "Iteration 129, loss = 0.59322298\n",
            "Iteration 130, loss = 0.59276458\n",
            "Iteration 131, loss = 0.59238339\n",
            "Iteration 132, loss = 0.59196273\n",
            "Iteration 133, loss = 0.59154884\n",
            "Iteration 134, loss = 0.59114234\n",
            "Iteration 135, loss = 0.59075022\n",
            "Iteration 136, loss = 0.59032802\n",
            "Iteration 137, loss = 0.58994490\n",
            "Iteration 138, loss = 0.58955952\n",
            "Iteration 139, loss = 0.58920941\n",
            "Iteration 140, loss = 0.58882673\n",
            "Iteration 141, loss = 0.58847080\n",
            "Iteration 142, loss = 0.58807377\n",
            "Iteration 143, loss = 0.58768820\n",
            "Iteration 144, loss = 0.58730221\n",
            "Iteration 145, loss = 0.58692645\n",
            "Iteration 146, loss = 0.58654280\n",
            "Iteration 147, loss = 0.58619159\n",
            "Iteration 148, loss = 0.58579775\n",
            "Iteration 149, loss = 0.58539863\n",
            "Iteration 150, loss = 0.58505482\n",
            "Iteration 1, loss = 0.84858612\n",
            "Iteration 2, loss = 0.84586103\n",
            "Iteration 3, loss = 0.84202808\n",
            "Iteration 4, loss = 0.83677269\n",
            "Iteration 5, loss = 0.83102491\n",
            "Iteration 6, loss = 0.82523503\n",
            "Iteration 7, loss = 0.81887373\n",
            "Iteration 8, loss = 0.81242895\n",
            "Iteration 9, loss = 0.80642299\n",
            "Iteration 10, loss = 0.80007903\n",
            "Iteration 11, loss = 0.79384885\n",
            "Iteration 12, loss = 0.78766457\n",
            "Iteration 13, loss = 0.78212698\n",
            "Iteration 14, loss = 0.77616718\n",
            "Iteration 15, loss = 0.77057092\n",
            "Iteration 16, loss = 0.76520465\n",
            "Iteration 17, loss = 0.76001179\n",
            "Iteration 18, loss = 0.75481795\n",
            "Iteration 19, loss = 0.75019268\n",
            "Iteration 20, loss = 0.74570973\n",
            "Iteration 21, loss = 0.74143312\n",
            "Iteration 22, loss = 0.73714002\n",
            "Iteration 23, loss = 0.73317639\n",
            "Iteration 24, loss = 0.72916594\n",
            "Iteration 25, loss = 0.72564340\n",
            "Iteration 26, loss = 0.72187504\n",
            "Iteration 27, loss = 0.71826418\n",
            "Iteration 28, loss = 0.71475157\n",
            "Iteration 29, loss = 0.71126395\n",
            "Iteration 30, loss = 0.70780464\n",
            "Iteration 31, loss = 0.70433933\n",
            "Iteration 32, loss = 0.70129476\n",
            "Iteration 33, loss = 0.69812788\n",
            "Iteration 34, loss = 0.69512355\n",
            "Iteration 35, loss = 0.69245263\n",
            "Iteration 36, loss = 0.68965835\n",
            "Iteration 37, loss = 0.68690679\n",
            "Iteration 38, loss = 0.68418766\n",
            "Iteration 39, loss = 0.68157247\n",
            "Iteration 40, loss = 0.67890392\n",
            "Iteration 41, loss = 0.67638975\n",
            "Iteration 42, loss = 0.67403004\n",
            "Iteration 43, loss = 0.67200248\n",
            "Iteration 44, loss = 0.66999841\n",
            "Iteration 45, loss = 0.66803721\n",
            "Iteration 46, loss = 0.66615995\n",
            "Iteration 47, loss = 0.66413871\n",
            "Iteration 48, loss = 0.66225463\n",
            "Iteration 49, loss = 0.66048563\n",
            "Iteration 50, loss = 0.65875979\n",
            "Iteration 51, loss = 0.65707940\n",
            "Iteration 52, loss = 0.65546792\n",
            "Iteration 53, loss = 0.65384613\n",
            "Iteration 54, loss = 0.65236260\n",
            "Iteration 55, loss = 0.65091725\n",
            "Iteration 56, loss = 0.64959092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 57, loss = 0.64820250\n",
            "Iteration 58, loss = 0.64691513\n",
            "Iteration 59, loss = 0.64568032\n",
            "Iteration 60, loss = 0.64440258\n",
            "Iteration 61, loss = 0.64327986\n",
            "Iteration 62, loss = 0.64202316\n",
            "Iteration 63, loss = 0.64089945\n",
            "Iteration 64, loss = 0.63978166\n",
            "Iteration 65, loss = 0.63867096\n",
            "Iteration 66, loss = 0.63754716\n",
            "Iteration 67, loss = 0.63647957\n",
            "Iteration 68, loss = 0.63545464\n",
            "Iteration 69, loss = 0.63444067\n",
            "Iteration 70, loss = 0.63343899\n",
            "Iteration 71, loss = 0.63246926\n",
            "Iteration 72, loss = 0.63142338\n",
            "Iteration 73, loss = 0.63047341\n",
            "Iteration 74, loss = 0.62955579\n",
            "Iteration 75, loss = 0.62870095\n",
            "Iteration 76, loss = 0.62785827\n",
            "Iteration 77, loss = 0.62697778\n",
            "Iteration 78, loss = 0.62622000\n",
            "Iteration 79, loss = 0.62543734\n",
            "Iteration 80, loss = 0.62467271\n",
            "Iteration 81, loss = 0.62399221\n",
            "Iteration 82, loss = 0.62324994\n",
            "Iteration 83, loss = 0.62256615\n",
            "Iteration 84, loss = 0.62187724\n",
            "Iteration 85, loss = 0.62128307\n",
            "Iteration 86, loss = 0.62055994\n",
            "Iteration 87, loss = 0.61987967\n",
            "Iteration 88, loss = 0.61925896\n",
            "Iteration 89, loss = 0.61861177\n",
            "Iteration 90, loss = 0.61796749\n",
            "Iteration 91, loss = 0.61735920\n",
            "Iteration 92, loss = 0.61671408\n",
            "Iteration 93, loss = 0.61610607\n",
            "Iteration 94, loss = 0.61551042\n",
            "Iteration 95, loss = 0.61475085\n",
            "Iteration 96, loss = 0.61417133\n",
            "Iteration 97, loss = 0.61357502\n",
            "Iteration 98, loss = 0.61292309\n",
            "Iteration 99, loss = 0.61240326\n",
            "Iteration 100, loss = 0.61185817\n",
            "Iteration 101, loss = 0.61132681\n",
            "Iteration 102, loss = 0.61082051\n",
            "Iteration 103, loss = 0.61033679\n",
            "Iteration 104, loss = 0.60983754\n",
            "Iteration 105, loss = 0.60932550\n",
            "Iteration 106, loss = 0.60880611\n",
            "Iteration 107, loss = 0.60827682\n",
            "Iteration 108, loss = 0.60777675\n",
            "Iteration 109, loss = 0.60728584\n",
            "Iteration 110, loss = 0.60683020\n",
            "Iteration 111, loss = 0.60638657\n",
            "Iteration 112, loss = 0.60590872\n",
            "Iteration 113, loss = 0.60549932\n",
            "Iteration 114, loss = 0.60508998\n",
            "Iteration 115, loss = 0.60466578\n",
            "Iteration 116, loss = 0.60427608\n",
            "Iteration 117, loss = 0.60386024\n",
            "Iteration 118, loss = 0.60351764\n",
            "Iteration 119, loss = 0.60317213\n",
            "Iteration 120, loss = 0.60282647\n",
            "Iteration 121, loss = 0.60252279\n",
            "Iteration 122, loss = 0.60215557\n",
            "Iteration 123, loss = 0.60177600\n",
            "Iteration 124, loss = 0.60138698\n",
            "Iteration 125, loss = 0.60104870\n",
            "Iteration 126, loss = 0.60066440\n",
            "Iteration 127, loss = 0.60029622\n",
            "Iteration 128, loss = 0.59995722\n",
            "Iteration 129, loss = 0.59964275\n",
            "Iteration 130, loss = 0.59931902\n",
            "Iteration 131, loss = 0.59905775\n",
            "Iteration 132, loss = 0.59875957\n",
            "Iteration 133, loss = 0.59846534\n",
            "Iteration 134, loss = 0.59818749\n",
            "Iteration 135, loss = 0.59789258\n",
            "Iteration 136, loss = 0.59757564\n",
            "Iteration 137, loss = 0.59730132\n",
            "Iteration 138, loss = 0.59703448\n",
            "Iteration 139, loss = 0.59675296\n",
            "Iteration 140, loss = 0.59646325\n",
            "Iteration 141, loss = 0.59621349\n",
            "Iteration 142, loss = 0.59592745\n",
            "Iteration 143, loss = 0.59564027\n",
            "Iteration 144, loss = 0.59536162\n",
            "Iteration 145, loss = 0.59508834\n",
            "Iteration 146, loss = 0.59481764\n",
            "Iteration 147, loss = 0.59457382\n",
            "Iteration 148, loss = 0.59429337\n",
            "Iteration 149, loss = 0.59403140\n",
            "Iteration 150, loss = 0.59378019\n",
            "Iteration 1, loss = 0.84407341\n",
            "Iteration 2, loss = 0.84130009\n",
            "Iteration 3, loss = 0.83725245\n",
            "Iteration 4, loss = 0.83218094\n",
            "Iteration 5, loss = 0.82697800\n",
            "Iteration 6, loss = 0.82168607\n",
            "Iteration 7, loss = 0.81584661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8, loss = 0.81000185\n",
            "Iteration 9, loss = 0.80435363\n",
            "Iteration 10, loss = 0.79816107\n",
            "Iteration 11, loss = 0.79237071\n",
            "Iteration 12, loss = 0.78666337\n",
            "Iteration 13, loss = 0.78102998\n",
            "Iteration 14, loss = 0.77550694\n",
            "Iteration 15, loss = 0.77026749\n",
            "Iteration 16, loss = 0.76517323\n",
            "Iteration 17, loss = 0.76032485\n",
            "Iteration 18, loss = 0.75561036\n",
            "Iteration 19, loss = 0.75098454\n",
            "Iteration 20, loss = 0.74658742\n",
            "Iteration 21, loss = 0.74243839\n",
            "Iteration 22, loss = 0.73833552\n",
            "Iteration 23, loss = 0.73436580\n",
            "Iteration 24, loss = 0.73058470\n",
            "Iteration 25, loss = 0.72698704\n",
            "Iteration 26, loss = 0.72315689\n",
            "Iteration 27, loss = 0.71954935\n",
            "Iteration 28, loss = 0.71591667\n",
            "Iteration 29, loss = 0.71260479\n",
            "Iteration 30, loss = 0.70914756\n",
            "Iteration 31, loss = 0.70589731\n",
            "Iteration 32, loss = 0.70265938\n",
            "Iteration 33, loss = 0.69947938\n",
            "Iteration 34, loss = 0.69654095\n",
            "Iteration 35, loss = 0.69348703\n",
            "Iteration 36, loss = 0.69054236\n",
            "Iteration 37, loss = 0.68762425\n",
            "Iteration 38, loss = 0.68489428\n",
            "Iteration 39, loss = 0.68225194\n",
            "Iteration 40, loss = 0.67961680\n",
            "Iteration 41, loss = 0.67710505\n",
            "Iteration 42, loss = 0.67477275\n",
            "Iteration 43, loss = 0.67274744\n",
            "Iteration 44, loss = 0.67056462\n",
            "Iteration 45, loss = 0.66859460\n",
            "Iteration 46, loss = 0.66655832\n",
            "Iteration 47, loss = 0.66454959\n",
            "Iteration 48, loss = 0.66265032\n",
            "Iteration 49, loss = 0.66091053\n",
            "Iteration 50, loss = 0.65917129\n",
            "Iteration 51, loss = 0.65752328\n",
            "Iteration 52, loss = 0.65594358\n",
            "Iteration 53, loss = 0.65432003\n",
            "Iteration 54, loss = 0.65283905\n",
            "Iteration 55, loss = 0.65128565\n",
            "Iteration 56, loss = 0.64996104\n",
            "Iteration 57, loss = 0.64857742\n",
            "Iteration 58, loss = 0.64723652\n",
            "Iteration 59, loss = 0.64599833\n",
            "Iteration 60, loss = 0.64471383\n",
            "Iteration 61, loss = 0.64352032\n",
            "Iteration 62, loss = 0.64224376\n",
            "Iteration 63, loss = 0.64109397\n",
            "Iteration 64, loss = 0.63993373\n",
            "Iteration 65, loss = 0.63886747\n",
            "Iteration 66, loss = 0.63775884\n",
            "Iteration 67, loss = 0.63666581\n",
            "Iteration 68, loss = 0.63561012\n",
            "Iteration 69, loss = 0.63454685\n",
            "Iteration 70, loss = 0.63350548\n",
            "Iteration 71, loss = 0.63253331\n",
            "Iteration 72, loss = 0.63148168\n",
            "Iteration 73, loss = 0.63054464\n",
            "Iteration 74, loss = 0.62962690\n",
            "Iteration 75, loss = 0.62875371\n",
            "Iteration 76, loss = 0.62785837\n",
            "Iteration 77, loss = 0.62703086\n",
            "Iteration 78, loss = 0.62624236\n",
            "Iteration 79, loss = 0.62546140\n",
            "Iteration 80, loss = 0.62474273\n",
            "Iteration 81, loss = 0.62408242\n",
            "Iteration 82, loss = 0.62337742\n",
            "Iteration 83, loss = 0.62269981\n",
            "Iteration 84, loss = 0.62201595\n",
            "Iteration 85, loss = 0.62140697\n",
            "Iteration 86, loss = 0.62071370\n",
            "Iteration 87, loss = 0.62004325\n",
            "Iteration 88, loss = 0.61943810\n",
            "Iteration 89, loss = 0.61880019\n",
            "Iteration 90, loss = 0.61817375\n",
            "Iteration 91, loss = 0.61758277\n",
            "Iteration 92, loss = 0.61693716\n",
            "Iteration 93, loss = 0.61634558\n",
            "Iteration 94, loss = 0.61577297\n",
            "Iteration 95, loss = 0.61512111\n",
            "Iteration 96, loss = 0.61457329\n",
            "Iteration 97, loss = 0.61400662\n",
            "Iteration 98, loss = 0.61334432\n",
            "Iteration 99, loss = 0.61284428\n",
            "Iteration 100, loss = 0.61230045\n",
            "Iteration 101, loss = 0.61178248\n",
            "Iteration 102, loss = 0.61130670\n",
            "Iteration 103, loss = 0.61079670\n",
            "Iteration 104, loss = 0.61029651\n",
            "Iteration 105, loss = 0.60978651\n",
            "Iteration 106, loss = 0.60928542\n",
            "Iteration 107, loss = 0.60876180\n",
            "Iteration 108, loss = 0.60826273\n",
            "Iteration 109, loss = 0.60773135\n",
            "Iteration 110, loss = 0.60722575\n",
            "Iteration 111, loss = 0.60676696\n",
            "Iteration 112, loss = 0.60620469\n",
            "Iteration 113, loss = 0.60571493\n",
            "Iteration 114, loss = 0.60527486\n",
            "Iteration 115, loss = 0.60477620\n",
            "Iteration 116, loss = 0.60433492\n",
            "Iteration 117, loss = 0.60388702\n",
            "Iteration 118, loss = 0.60347394\n",
            "Iteration 119, loss = 0.60307031\n",
            "Iteration 120, loss = 0.60266544\n",
            "Iteration 121, loss = 0.60226998\n",
            "Iteration 122, loss = 0.60185161\n",
            "Iteration 123, loss = 0.60138960\n",
            "Iteration 124, loss = 0.60099822\n",
            "Iteration 125, loss = 0.60059314\n",
            "Iteration 126, loss = 0.60017253\n",
            "Iteration 127, loss = 0.59977016\n",
            "Iteration 128, loss = 0.59938774\n",
            "Iteration 129, loss = 0.59902502\n",
            "Iteration 130, loss = 0.59864652\n",
            "Iteration 131, loss = 0.59832533\n",
            "Iteration 132, loss = 0.59799104\n",
            "Iteration 133, loss = 0.59766587\n",
            "Iteration 134, loss = 0.59736055\n",
            "Iteration 135, loss = 0.59702591\n",
            "Iteration 136, loss = 0.59667199\n",
            "Iteration 137, loss = 0.59635611\n",
            "Iteration 138, loss = 0.59608056\n",
            "Iteration 139, loss = 0.59578235\n",
            "Iteration 140, loss = 0.59548403\n",
            "Iteration 141, loss = 0.59521450\n",
            "Iteration 142, loss = 0.59491029\n",
            "Iteration 143, loss = 0.59461594\n",
            "Iteration 144, loss = 0.59431879\n",
            "Iteration 145, loss = 0.59403609\n",
            "Iteration 146, loss = 0.59375025\n",
            "Iteration 147, loss = 0.59347385\n",
            "Iteration 148, loss = 0.59319147\n",
            "Iteration 149, loss = 0.59291991\n",
            "Iteration 150, loss = 0.59263915\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 150 and for layer number 3 : 0.6987500000000001\n",
            "Iteration 1, loss = 0.92831019\n",
            "Iteration 2, loss = 0.92628737\n",
            "Iteration 3, loss = 0.92292720\n",
            "Iteration 4, loss = 0.91890896\n",
            "Iteration 5, loss = 0.91406816\n",
            "Iteration 6, loss = 0.90896363\n",
            "Iteration 7, loss = 0.90386015\n",
            "Iteration 8, loss = 0.89884936\n",
            "Iteration 9, loss = 0.89376039\n",
            "Iteration 10, loss = 0.88868258\n",
            "Iteration 11, loss = 0.88381471\n",
            "Iteration 12, loss = 0.87878426\n",
            "Iteration 13, loss = 0.87398219\n",
            "Iteration 14, loss = 0.86928989\n",
            "Iteration 15, loss = 0.86471610\n",
            "Iteration 16, loss = 0.86053140\n",
            "Iteration 17, loss = 0.85612820\n",
            "Iteration 18, loss = 0.85197041\n",
            "Iteration 19, loss = 0.84782638\n",
            "Iteration 20, loss = 0.84387295\n",
            "Iteration 21, loss = 0.83992078\n",
            "Iteration 22, loss = 0.83601275\n",
            "Iteration 23, loss = 0.83220154\n",
            "Iteration 24, loss = 0.82811202\n",
            "Iteration 25, loss = 0.82449396\n",
            "Iteration 26, loss = 0.82082671\n",
            "Iteration 27, loss = 0.81721320\n",
            "Iteration 28, loss = 0.81380905\n",
            "Iteration 29, loss = 0.81040528\n",
            "Iteration 30, loss = 0.80705060\n",
            "Iteration 31, loss = 0.80392526\n",
            "Iteration 32, loss = 0.80063213\n",
            "Iteration 33, loss = 0.79760810\n",
            "Iteration 34, loss = 0.79459235\n",
            "Iteration 35, loss = 0.79159397\n",
            "Iteration 36, loss = 0.78886164\n",
            "Iteration 37, loss = 0.78586148\n",
            "Iteration 38, loss = 0.78317255\n",
            "Iteration 39, loss = 0.78022418\n",
            "Iteration 40, loss = 0.77762076\n",
            "Iteration 41, loss = 0.77495681\n",
            "Iteration 42, loss = 0.77229239\n",
            "Iteration 43, loss = 0.76974491\n",
            "Iteration 44, loss = 0.76720757\n",
            "Iteration 45, loss = 0.76456895\n",
            "Iteration 46, loss = 0.76198026\n",
            "Iteration 47, loss = 0.75960833\n",
            "Iteration 48, loss = 0.75721478\n",
            "Iteration 49, loss = 0.75488200\n",
            "Iteration 50, loss = 0.75248586\n",
            "Iteration 51, loss = 0.75018259\n",
            "Iteration 52, loss = 0.74800238\n",
            "Iteration 53, loss = 0.74586725\n",
            "Iteration 54, loss = 0.74362634\n",
            "Iteration 55, loss = 0.74151143\n",
            "Iteration 56, loss = 0.73950295\n",
            "Iteration 57, loss = 0.73741831\n",
            "Iteration 58, loss = 0.73543089\n",
            "Iteration 59, loss = 0.73347890\n",
            "Iteration 60, loss = 0.73143214\n",
            "Iteration 61, loss = 0.72947052\n",
            "Iteration 62, loss = 0.72761557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 63, loss = 0.72575659\n",
            "Iteration 64, loss = 0.72393461\n",
            "Iteration 65, loss = 0.72217066\n",
            "Iteration 66, loss = 0.72037809\n",
            "Iteration 67, loss = 0.71872244\n",
            "Iteration 68, loss = 0.71699545\n",
            "Iteration 69, loss = 0.71528424\n",
            "Iteration 70, loss = 0.71358358\n",
            "Iteration 71, loss = 0.71201015\n",
            "Iteration 72, loss = 0.71043121\n",
            "Iteration 73, loss = 0.70886096\n",
            "Iteration 74, loss = 0.70734836\n",
            "Iteration 75, loss = 0.70577004\n",
            "Iteration 76, loss = 0.70432856\n",
            "Iteration 77, loss = 0.70282172\n",
            "Iteration 78, loss = 0.70132546\n",
            "Iteration 79, loss = 0.69988901\n",
            "Iteration 80, loss = 0.69838408\n",
            "Iteration 81, loss = 0.69691519\n",
            "Iteration 82, loss = 0.69553924\n",
            "Iteration 83, loss = 0.69407292\n",
            "Iteration 84, loss = 0.69278138\n",
            "Iteration 85, loss = 0.69142090\n",
            "Iteration 86, loss = 0.69015005\n",
            "Iteration 87, loss = 0.68883737\n",
            "Iteration 88, loss = 0.68750556\n",
            "Iteration 89, loss = 0.68619651\n",
            "Iteration 90, loss = 0.68485066\n",
            "Iteration 91, loss = 0.68350643\n",
            "Iteration 92, loss = 0.68209430\n",
            "Iteration 93, loss = 0.68077022\n",
            "Iteration 94, loss = 0.67944943\n",
            "Iteration 95, loss = 0.67819217\n",
            "Iteration 96, loss = 0.67696762\n",
            "Iteration 97, loss = 0.67577008\n",
            "Iteration 98, loss = 0.67458090\n",
            "Iteration 99, loss = 0.67341100\n",
            "Iteration 100, loss = 0.67223428\n",
            "Iteration 101, loss = 0.67104506\n",
            "Iteration 102, loss = 0.66978585\n",
            "Iteration 103, loss = 0.66861227\n",
            "Iteration 104, loss = 0.66745578\n",
            "Iteration 105, loss = 0.66627850\n",
            "Iteration 106, loss = 0.66510940\n",
            "Iteration 107, loss = 0.66393676\n",
            "Iteration 108, loss = 0.66281068\n",
            "Iteration 109, loss = 0.66169601\n",
            "Iteration 110, loss = 0.66061268\n",
            "Iteration 111, loss = 0.65960111\n",
            "Iteration 112, loss = 0.65854483\n",
            "Iteration 113, loss = 0.65754644\n",
            "Iteration 114, loss = 0.65647858\n",
            "Iteration 115, loss = 0.65551663\n",
            "Iteration 116, loss = 0.65447314\n",
            "Iteration 117, loss = 0.65349882\n",
            "Iteration 118, loss = 0.65251628\n",
            "Iteration 119, loss = 0.65153519\n",
            "Iteration 120, loss = 0.65054167\n",
            "Iteration 121, loss = 0.64961018\n",
            "Iteration 122, loss = 0.64876723\n",
            "Iteration 123, loss = 0.64784156\n",
            "Iteration 124, loss = 0.64697288\n",
            "Iteration 125, loss = 0.64616527\n",
            "Iteration 126, loss = 0.64527555\n",
            "Iteration 127, loss = 0.64449520\n",
            "Iteration 128, loss = 0.64361690\n",
            "Iteration 129, loss = 0.64280606\n",
            "Iteration 130, loss = 0.64200862\n",
            "Iteration 131, loss = 0.64122482\n",
            "Iteration 132, loss = 0.64042905\n",
            "Iteration 133, loss = 0.63966933\n",
            "Iteration 134, loss = 0.63888969\n",
            "Iteration 135, loss = 0.63813226\n",
            "Iteration 136, loss = 0.63734726\n",
            "Iteration 137, loss = 0.63663371\n",
            "Iteration 138, loss = 0.63588696\n",
            "Iteration 139, loss = 0.63517280\n",
            "Iteration 140, loss = 0.63446451\n",
            "Iteration 141, loss = 0.63377240\n",
            "Iteration 142, loss = 0.63309239\n",
            "Iteration 143, loss = 0.63242575\n",
            "Iteration 144, loss = 0.63171326\n",
            "Iteration 145, loss = 0.63108449\n",
            "Iteration 146, loss = 0.63045197\n",
            "Iteration 147, loss = 0.62980498\n",
            "Iteration 148, loss = 0.62919895\n",
            "Iteration 149, loss = 0.62856276\n",
            "Iteration 150, loss = 0.62792757\n",
            "Iteration 1, loss = 0.92515262\n",
            "Iteration 2, loss = 0.92320793\n",
            "Iteration 3, loss = 0.91997489\n",
            "Iteration 4, loss = 0.91591886\n",
            "Iteration 5, loss = 0.91108617\n",
            "Iteration 6, loss = 0.90611197\n",
            "Iteration 7, loss = 0.90111358\n",
            "Iteration 8, loss = 0.89611580\n",
            "Iteration 9, loss = 0.89103097\n",
            "Iteration 10, loss = 0.88614356\n",
            "Iteration 11, loss = 0.88144205\n",
            "Iteration 12, loss = 0.87657599\n",
            "Iteration 13, loss = 0.87196161\n",
            "Iteration 14, loss = 0.86739565\n",
            "Iteration 15, loss = 0.86293846\n",
            "Iteration 16, loss = 0.85890079\n",
            "Iteration 17, loss = 0.85483203\n",
            "Iteration 18, loss = 0.85098669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 19, loss = 0.84699066\n",
            "Iteration 20, loss = 0.84324236\n",
            "Iteration 21, loss = 0.83945760\n",
            "Iteration 22, loss = 0.83574260\n",
            "Iteration 23, loss = 0.83208773\n",
            "Iteration 24, loss = 0.82831715\n",
            "Iteration 25, loss = 0.82490285\n",
            "Iteration 26, loss = 0.82145077\n",
            "Iteration 27, loss = 0.81803640\n",
            "Iteration 28, loss = 0.81467422\n",
            "Iteration 29, loss = 0.81143074\n",
            "Iteration 30, loss = 0.80809690\n",
            "Iteration 31, loss = 0.80507070\n",
            "Iteration 32, loss = 0.80183450\n",
            "Iteration 33, loss = 0.79879721\n",
            "Iteration 34, loss = 0.79582125\n",
            "Iteration 35, loss = 0.79278224\n",
            "Iteration 36, loss = 0.78996639\n",
            "Iteration 37, loss = 0.78695614\n",
            "Iteration 38, loss = 0.78424656\n",
            "Iteration 39, loss = 0.78127393\n",
            "Iteration 40, loss = 0.77872840\n",
            "Iteration 41, loss = 0.77606929\n",
            "Iteration 42, loss = 0.77339733\n",
            "Iteration 43, loss = 0.77081091\n",
            "Iteration 44, loss = 0.76835674\n",
            "Iteration 45, loss = 0.76574743\n",
            "Iteration 46, loss = 0.76315724\n",
            "Iteration 47, loss = 0.76082949\n",
            "Iteration 48, loss = 0.75845520\n",
            "Iteration 49, loss = 0.75612422\n",
            "Iteration 50, loss = 0.75378236\n",
            "Iteration 51, loss = 0.75155630\n",
            "Iteration 52, loss = 0.74945850\n",
            "Iteration 53, loss = 0.74738251\n",
            "Iteration 54, loss = 0.74516311\n",
            "Iteration 55, loss = 0.74313444\n",
            "Iteration 56, loss = 0.74120140\n",
            "Iteration 57, loss = 0.73915571\n",
            "Iteration 58, loss = 0.73719056\n",
            "Iteration 59, loss = 0.73528087\n",
            "Iteration 60, loss = 0.73334510\n",
            "Iteration 61, loss = 0.73141029\n",
            "Iteration 62, loss = 0.72959204\n",
            "Iteration 63, loss = 0.72775738\n",
            "Iteration 64, loss = 0.72591830\n",
            "Iteration 65, loss = 0.72420142\n",
            "Iteration 66, loss = 0.72236917\n",
            "Iteration 67, loss = 0.72072695\n",
            "Iteration 68, loss = 0.71904202\n",
            "Iteration 69, loss = 0.71728188\n",
            "Iteration 70, loss = 0.71565896\n",
            "Iteration 71, loss = 0.71408007\n",
            "Iteration 72, loss = 0.71245632\n",
            "Iteration 73, loss = 0.71088491\n",
            "Iteration 74, loss = 0.70937593\n",
            "Iteration 75, loss = 0.70783125\n",
            "Iteration 76, loss = 0.70637332\n",
            "Iteration 77, loss = 0.70488077\n",
            "Iteration 78, loss = 0.70340466\n",
            "Iteration 79, loss = 0.70197001\n",
            "Iteration 80, loss = 0.70049560\n",
            "Iteration 81, loss = 0.69904607\n",
            "Iteration 82, loss = 0.69762145\n",
            "Iteration 83, loss = 0.69614817\n",
            "Iteration 84, loss = 0.69479063\n",
            "Iteration 85, loss = 0.69346291\n",
            "Iteration 86, loss = 0.69214622\n",
            "Iteration 87, loss = 0.69083577\n",
            "Iteration 88, loss = 0.68948877\n",
            "Iteration 89, loss = 0.68815146\n",
            "Iteration 90, loss = 0.68677016\n",
            "Iteration 91, loss = 0.68540869\n",
            "Iteration 92, loss = 0.68391370\n",
            "Iteration 93, loss = 0.68257332\n",
            "Iteration 94, loss = 0.68117831\n",
            "Iteration 95, loss = 0.67987551\n",
            "Iteration 96, loss = 0.67862677\n",
            "Iteration 97, loss = 0.67738780\n",
            "Iteration 98, loss = 0.67615795\n",
            "Iteration 99, loss = 0.67497811\n",
            "Iteration 100, loss = 0.67374746\n",
            "Iteration 101, loss = 0.67250444\n",
            "Iteration 102, loss = 0.67126693\n",
            "Iteration 103, loss = 0.67005472\n",
            "Iteration 104, loss = 0.66890725\n",
            "Iteration 105, loss = 0.66774064\n",
            "Iteration 106, loss = 0.66657045\n",
            "Iteration 107, loss = 0.66541036\n",
            "Iteration 108, loss = 0.66432764\n",
            "Iteration 109, loss = 0.66321036\n",
            "Iteration 110, loss = 0.66213977\n",
            "Iteration 111, loss = 0.66112772\n",
            "Iteration 112, loss = 0.66008692\n",
            "Iteration 113, loss = 0.65914036\n",
            "Iteration 114, loss = 0.65809409\n",
            "Iteration 115, loss = 0.65714780\n",
            "Iteration 116, loss = 0.65615028\n",
            "Iteration 117, loss = 0.65520116\n",
            "Iteration 118, loss = 0.65422912\n",
            "Iteration 119, loss = 0.65325035\n",
            "Iteration 120, loss = 0.65225538\n",
            "Iteration 121, loss = 0.65132736\n",
            "Iteration 122, loss = 0.65045346\n",
            "Iteration 123, loss = 0.64954977\n",
            "Iteration 124, loss = 0.64862332\n",
            "Iteration 125, loss = 0.64775884\n",
            "Iteration 126, loss = 0.64684982\n",
            "Iteration 127, loss = 0.64601374\n",
            "Iteration 128, loss = 0.64505299\n",
            "Iteration 129, loss = 0.64418821\n",
            "Iteration 130, loss = 0.64332865\n",
            "Iteration 131, loss = 0.64247441\n",
            "Iteration 132, loss = 0.64162688\n",
            "Iteration 133, loss = 0.64078769\n",
            "Iteration 134, loss = 0.63992572\n",
            "Iteration 135, loss = 0.63907326\n",
            "Iteration 136, loss = 0.63822019\n",
            "Iteration 137, loss = 0.63748852\n",
            "Iteration 138, loss = 0.63671692\n",
            "Iteration 139, loss = 0.63598547\n",
            "Iteration 140, loss = 0.63523103\n",
            "Iteration 141, loss = 0.63449165\n",
            "Iteration 142, loss = 0.63370574\n",
            "Iteration 143, loss = 0.63296622\n",
            "Iteration 144, loss = 0.63219230\n",
            "Iteration 145, loss = 0.63148231\n",
            "Iteration 146, loss = 0.63076723\n",
            "Iteration 147, loss = 0.63004646\n",
            "Iteration 148, loss = 0.62937075\n",
            "Iteration 149, loss = 0.62869006\n",
            "Iteration 150, loss = 0.62796728\n",
            "Iteration 1, loss = 0.93462281\n",
            "Iteration 2, loss = 0.93261224\n",
            "Iteration 3, loss = 0.92941834\n",
            "Iteration 4, loss = 0.92525033\n",
            "Iteration 5, loss = 0.92065853\n",
            "Iteration 6, loss = 0.91555772\n",
            "Iteration 7, loss = 0.91059613\n",
            "Iteration 8, loss = 0.90541505\n",
            "Iteration 9, loss = 0.90029749\n",
            "Iteration 10, loss = 0.89529951\n",
            "Iteration 11, loss = 0.89038858\n",
            "Iteration 12, loss = 0.88547267\n",
            "Iteration 13, loss = 0.88066014\n",
            "Iteration 14, loss = 0.87599931\n",
            "Iteration 15, loss = 0.87133466\n",
            "Iteration 16, loss = 0.86701607\n",
            "Iteration 17, loss = 0.86274708\n",
            "Iteration 18, loss = 0.85864838\n",
            "Iteration 19, loss = 0.85439727\n",
            "Iteration 20, loss = 0.85037709\n",
            "Iteration 21, loss = 0.84634247\n",
            "Iteration 22, loss = 0.84239937\n",
            "Iteration 23, loss = 0.83855448\n",
            "Iteration 24, loss = 0.83465813\n",
            "Iteration 25, loss = 0.83096674\n",
            "Iteration 26, loss = 0.82741978\n",
            "Iteration 27, loss = 0.82371122\n",
            "Iteration 28, loss = 0.82025754\n",
            "Iteration 29, loss = 0.81690640\n",
            "Iteration 30, loss = 0.81348565\n",
            "Iteration 31, loss = 0.81033127\n",
            "Iteration 32, loss = 0.80706458\n",
            "Iteration 33, loss = 0.80388785\n",
            "Iteration 34, loss = 0.80070719\n",
            "Iteration 35, loss = 0.79753268\n",
            "Iteration 36, loss = 0.79445505\n",
            "Iteration 37, loss = 0.79131373\n",
            "Iteration 38, loss = 0.78835925\n",
            "Iteration 39, loss = 0.78525734\n",
            "Iteration 40, loss = 0.78258735\n",
            "Iteration 41, loss = 0.77976841\n",
            "Iteration 42, loss = 0.77689181\n",
            "Iteration 43, loss = 0.77404346\n",
            "Iteration 44, loss = 0.77144905\n",
            "Iteration 45, loss = 0.76875241\n",
            "Iteration 46, loss = 0.76603951\n",
            "Iteration 47, loss = 0.76363139\n",
            "Iteration 48, loss = 0.76122007\n",
            "Iteration 49, loss = 0.75874285\n",
            "Iteration 50, loss = 0.75632330\n",
            "Iteration 51, loss = 0.75401154\n",
            "Iteration 52, loss = 0.75172788\n",
            "Iteration 53, loss = 0.74946183\n",
            "Iteration 54, loss = 0.74711655\n",
            "Iteration 55, loss = 0.74496158\n",
            "Iteration 56, loss = 0.74289829\n",
            "Iteration 57, loss = 0.74075807\n",
            "Iteration 58, loss = 0.73864727\n",
            "Iteration 59, loss = 0.73658594\n",
            "Iteration 60, loss = 0.73447008\n",
            "Iteration 61, loss = 0.73240466\n",
            "Iteration 62, loss = 0.73039789\n",
            "Iteration 63, loss = 0.72845788\n",
            "Iteration 64, loss = 0.72653095\n",
            "Iteration 65, loss = 0.72471988\n",
            "Iteration 66, loss = 0.72273809\n",
            "Iteration 67, loss = 0.72088747\n",
            "Iteration 68, loss = 0.71908629\n",
            "Iteration 69, loss = 0.71713136\n",
            "Iteration 70, loss = 0.71527881\n",
            "Iteration 71, loss = 0.71355173\n",
            "Iteration 72, loss = 0.71174876\n",
            "Iteration 73, loss = 0.70999877\n",
            "Iteration 74, loss = 0.70831828\n",
            "Iteration 75, loss = 0.70659337\n",
            "Iteration 76, loss = 0.70499195\n",
            "Iteration 77, loss = 0.70337414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 78, loss = 0.70177357\n",
            "Iteration 79, loss = 0.70025428\n",
            "Iteration 80, loss = 0.69861256\n",
            "Iteration 81, loss = 0.69707178\n",
            "Iteration 82, loss = 0.69552728\n",
            "Iteration 83, loss = 0.69393161\n",
            "Iteration 84, loss = 0.69241273\n",
            "Iteration 85, loss = 0.69095717\n",
            "Iteration 86, loss = 0.68948161\n",
            "Iteration 87, loss = 0.68804856\n",
            "Iteration 88, loss = 0.68658037\n",
            "Iteration 89, loss = 0.68510199\n",
            "Iteration 90, loss = 0.68363864\n",
            "Iteration 91, loss = 0.68220180\n",
            "Iteration 92, loss = 0.68063484\n",
            "Iteration 93, loss = 0.67918267\n",
            "Iteration 94, loss = 0.67773565\n",
            "Iteration 95, loss = 0.67630324\n",
            "Iteration 96, loss = 0.67493092\n",
            "Iteration 97, loss = 0.67360341\n",
            "Iteration 98, loss = 0.67227445\n",
            "Iteration 99, loss = 0.67098636\n",
            "Iteration 100, loss = 0.66966581\n",
            "Iteration 101, loss = 0.66833240\n",
            "Iteration 102, loss = 0.66705544\n",
            "Iteration 103, loss = 0.66575348\n",
            "Iteration 104, loss = 0.66452828\n",
            "Iteration 105, loss = 0.66327437\n",
            "Iteration 106, loss = 0.66199517\n",
            "Iteration 107, loss = 0.66074385\n",
            "Iteration 108, loss = 0.65955795\n",
            "Iteration 109, loss = 0.65835191\n",
            "Iteration 110, loss = 0.65721563\n",
            "Iteration 111, loss = 0.65614481\n",
            "Iteration 112, loss = 0.65496847\n",
            "Iteration 113, loss = 0.65389317\n",
            "Iteration 114, loss = 0.65273817\n",
            "Iteration 115, loss = 0.65168348\n",
            "Iteration 116, loss = 0.65059551\n",
            "Iteration 117, loss = 0.64957036\n",
            "Iteration 118, loss = 0.64856594\n",
            "Iteration 119, loss = 0.64753817\n",
            "Iteration 120, loss = 0.64651576\n",
            "Iteration 121, loss = 0.64553035\n",
            "Iteration 122, loss = 0.64455996\n",
            "Iteration 123, loss = 0.64351206\n",
            "Iteration 124, loss = 0.64252108\n",
            "Iteration 125, loss = 0.64161128\n",
            "Iteration 126, loss = 0.64068012\n",
            "Iteration 127, loss = 0.63985894\n",
            "Iteration 128, loss = 0.63893508\n",
            "Iteration 129, loss = 0.63808524\n",
            "Iteration 130, loss = 0.63725818\n",
            "Iteration 131, loss = 0.63641797\n",
            "Iteration 132, loss = 0.63561298\n",
            "Iteration 133, loss = 0.63477335\n",
            "Iteration 134, loss = 0.63391441\n",
            "Iteration 135, loss = 0.63304245\n",
            "Iteration 136, loss = 0.63216148\n",
            "Iteration 137, loss = 0.63136898\n",
            "Iteration 138, loss = 0.63057856\n",
            "Iteration 139, loss = 0.62978781\n",
            "Iteration 140, loss = 0.62900919\n",
            "Iteration 141, loss = 0.62822730\n",
            "Iteration 142, loss = 0.62742769\n",
            "Iteration 143, loss = 0.62666233\n",
            "Iteration 144, loss = 0.62583832\n",
            "Iteration 145, loss = 0.62506955\n",
            "Iteration 146, loss = 0.62430315\n",
            "Iteration 147, loss = 0.62354551\n",
            "Iteration 148, loss = 0.62284358\n",
            "Iteration 149, loss = 0.62213004\n",
            "Iteration 150, loss = 0.62138943\n",
            "Iteration 1, loss = 0.93310249\n",
            "Iteration 2, loss = 0.93101092\n",
            "Iteration 3, loss = 0.92770703\n",
            "Iteration 4, loss = 0.92348094\n",
            "Iteration 5, loss = 0.91870634\n",
            "Iteration 6, loss = 0.91350854\n",
            "Iteration 7, loss = 0.90855626\n",
            "Iteration 8, loss = 0.90333343\n",
            "Iteration 9, loss = 0.89816244\n",
            "Iteration 10, loss = 0.89306903\n",
            "Iteration 11, loss = 0.88797547\n",
            "Iteration 12, loss = 0.88302867\n",
            "Iteration 13, loss = 0.87830591\n",
            "Iteration 14, loss = 0.87363722\n",
            "Iteration 15, loss = 0.86901748\n",
            "Iteration 16, loss = 0.86479539\n",
            "Iteration 17, loss = 0.86038833\n",
            "Iteration 18, loss = 0.85639915\n",
            "Iteration 19, loss = 0.85227054\n",
            "Iteration 20, loss = 0.84834947\n",
            "Iteration 21, loss = 0.84451866\n",
            "Iteration 22, loss = 0.84076112\n",
            "Iteration 23, loss = 0.83717459\n",
            "Iteration 24, loss = 0.83341012\n",
            "Iteration 25, loss = 0.82979470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 26, loss = 0.82628064\n",
            "Iteration 27, loss = 0.82271049\n",
            "Iteration 28, loss = 0.81929932\n",
            "Iteration 29, loss = 0.81605612\n",
            "Iteration 30, loss = 0.81276286\n",
            "Iteration 31, loss = 0.80960758\n",
            "Iteration 32, loss = 0.80651049\n",
            "Iteration 33, loss = 0.80331972\n",
            "Iteration 34, loss = 0.80023102\n",
            "Iteration 35, loss = 0.79699869\n",
            "Iteration 36, loss = 0.79400137\n",
            "Iteration 37, loss = 0.79097394\n",
            "Iteration 38, loss = 0.78805094\n",
            "Iteration 39, loss = 0.78511982\n",
            "Iteration 40, loss = 0.78247517\n",
            "Iteration 41, loss = 0.77979332\n",
            "Iteration 42, loss = 0.77699418\n",
            "Iteration 43, loss = 0.77429562\n",
            "Iteration 44, loss = 0.77172579\n",
            "Iteration 45, loss = 0.76916050\n",
            "Iteration 46, loss = 0.76647297\n",
            "Iteration 47, loss = 0.76409478\n",
            "Iteration 48, loss = 0.76166267\n",
            "Iteration 49, loss = 0.75916081\n",
            "Iteration 50, loss = 0.75678157\n",
            "Iteration 51, loss = 0.75448138\n",
            "Iteration 52, loss = 0.75215647\n",
            "Iteration 53, loss = 0.74984933\n",
            "Iteration 54, loss = 0.74751943\n",
            "Iteration 55, loss = 0.74539031\n",
            "Iteration 56, loss = 0.74335546\n",
            "Iteration 57, loss = 0.74121902\n",
            "Iteration 58, loss = 0.73912598\n",
            "Iteration 59, loss = 0.73715404\n",
            "Iteration 60, loss = 0.73507830\n",
            "Iteration 61, loss = 0.73318207\n",
            "Iteration 62, loss = 0.73126676\n",
            "Iteration 63, loss = 0.72939938\n",
            "Iteration 64, loss = 0.72756473\n",
            "Iteration 65, loss = 0.72579634\n",
            "Iteration 66, loss = 0.72394008\n",
            "Iteration 67, loss = 0.72217471\n",
            "Iteration 68, loss = 0.72043589\n",
            "Iteration 69, loss = 0.71850239\n",
            "Iteration 70, loss = 0.71671073\n",
            "Iteration 71, loss = 0.71501354\n",
            "Iteration 72, loss = 0.71325594\n",
            "Iteration 73, loss = 0.71157480\n",
            "Iteration 74, loss = 0.70997376\n",
            "Iteration 75, loss = 0.70834349\n",
            "Iteration 76, loss = 0.70678517\n",
            "Iteration 77, loss = 0.70527284\n",
            "Iteration 78, loss = 0.70373065\n",
            "Iteration 79, loss = 0.70226150\n",
            "Iteration 80, loss = 0.70070864\n",
            "Iteration 81, loss = 0.69912475\n",
            "Iteration 82, loss = 0.69757835\n",
            "Iteration 83, loss = 0.69597960\n",
            "Iteration 84, loss = 0.69442451\n",
            "Iteration 85, loss = 0.69299803\n",
            "Iteration 86, loss = 0.69155555\n",
            "Iteration 87, loss = 0.69017218\n",
            "Iteration 88, loss = 0.68882177\n",
            "Iteration 89, loss = 0.68743527\n",
            "Iteration 90, loss = 0.68603636\n",
            "Iteration 91, loss = 0.68469807\n",
            "Iteration 92, loss = 0.68322054\n",
            "Iteration 93, loss = 0.68188267\n",
            "Iteration 94, loss = 0.68056752\n",
            "Iteration 95, loss = 0.67922529\n",
            "Iteration 96, loss = 0.67794626\n",
            "Iteration 97, loss = 0.67673420\n",
            "Iteration 98, loss = 0.67547574\n",
            "Iteration 99, loss = 0.67423622\n",
            "Iteration 100, loss = 0.67305365\n",
            "Iteration 101, loss = 0.67184034\n",
            "Iteration 102, loss = 0.67071311\n",
            "Iteration 103, loss = 0.66952001\n",
            "Iteration 104, loss = 0.66837398\n",
            "Iteration 105, loss = 0.66719879\n",
            "Iteration 106, loss = 0.66603831\n",
            "Iteration 107, loss = 0.66481475\n",
            "Iteration 108, loss = 0.66370231\n",
            "Iteration 109, loss = 0.66253770\n",
            "Iteration 110, loss = 0.66148177\n",
            "Iteration 111, loss = 0.66037129\n",
            "Iteration 112, loss = 0.65928578\n",
            "Iteration 113, loss = 0.65824376\n",
            "Iteration 114, loss = 0.65714020\n",
            "Iteration 115, loss = 0.65610681\n",
            "Iteration 116, loss = 0.65510969\n",
            "Iteration 117, loss = 0.65415203\n",
            "Iteration 118, loss = 0.65322297\n",
            "Iteration 119, loss = 0.65223789\n",
            "Iteration 120, loss = 0.65126999\n",
            "Iteration 121, loss = 0.65039327\n",
            "Iteration 122, loss = 0.64948728\n",
            "Iteration 123, loss = 0.64856928\n",
            "Iteration 124, loss = 0.64763909\n",
            "Iteration 125, loss = 0.64676217\n",
            "Iteration 126, loss = 0.64587163\n",
            "Iteration 127, loss = 0.64508020\n",
            "Iteration 128, loss = 0.64418209\n",
            "Iteration 129, loss = 0.64334651\n",
            "Iteration 130, loss = 0.64251319\n",
            "Iteration 131, loss = 0.64166131\n",
            "Iteration 132, loss = 0.64084183\n",
            "Iteration 133, loss = 0.64000666\n",
            "Iteration 134, loss = 0.63915158\n",
            "Iteration 135, loss = 0.63828495\n",
            "Iteration 136, loss = 0.63743946\n",
            "Iteration 137, loss = 0.63664379\n",
            "Iteration 138, loss = 0.63587903\n",
            "Iteration 139, loss = 0.63510284\n",
            "Iteration 140, loss = 0.63436477\n",
            "Iteration 141, loss = 0.63362194\n",
            "Iteration 142, loss = 0.63288549\n",
            "Iteration 143, loss = 0.63218612\n",
            "Iteration 144, loss = 0.63138964\n",
            "Iteration 145, loss = 0.63070869\n",
            "Iteration 146, loss = 0.62998211\n",
            "Iteration 147, loss = 0.62929853\n",
            "Iteration 148, loss = 0.62865177\n",
            "Iteration 149, loss = 0.62799051\n",
            "Iteration 150, loss = 0.62730519\n",
            "Iteration 1, loss = 0.93946865\n",
            "Iteration 2, loss = 0.93734548\n",
            "Iteration 3, loss = 0.93396022\n",
            "Iteration 4, loss = 0.92974495\n",
            "Iteration 5, loss = 0.92508441\n",
            "Iteration 6, loss = 0.92006070\n",
            "Iteration 7, loss = 0.91518496\n",
            "Iteration 8, loss = 0.91020915\n",
            "Iteration 9, loss = 0.90529195\n",
            "Iteration 10, loss = 0.90050938\n",
            "Iteration 11, loss = 0.89552786\n",
            "Iteration 12, loss = 0.89092624\n",
            "Iteration 13, loss = 0.88627671\n",
            "Iteration 14, loss = 0.88176332\n",
            "Iteration 15, loss = 0.87748747\n",
            "Iteration 16, loss = 0.87338356\n",
            "Iteration 17, loss = 0.86932619\n",
            "Iteration 18, loss = 0.86557100\n",
            "Iteration 19, loss = 0.86175953\n",
            "Iteration 20, loss = 0.85810708\n",
            "Iteration 21, loss = 0.85457487\n",
            "Iteration 22, loss = 0.85096897\n",
            "Iteration 23, loss = 0.84761148\n",
            "Iteration 24, loss = 0.84404867\n",
            "Iteration 25, loss = 0.84066128\n",
            "Iteration 26, loss = 0.83730958\n",
            "Iteration 27, loss = 0.83388757\n",
            "Iteration 28, loss = 0.83057501\n",
            "Iteration 29, loss = 0.82738517\n",
            "Iteration 30, loss = 0.82415322\n",
            "Iteration 31, loss = 0.82106702\n",
            "Iteration 32, loss = 0.81815273\n",
            "Iteration 33, loss = 0.81509467\n",
            "Iteration 34, loss = 0.81215013\n",
            "Iteration 35, loss = 0.80906566\n",
            "Iteration 36, loss = 0.80619125\n",
            "Iteration 37, loss = 0.80336983\n",
            "Iteration 38, loss = 0.80049635\n",
            "Iteration 39, loss = 0.79772338\n",
            "Iteration 40, loss = 0.79507950\n",
            "Iteration 41, loss = 0.79258288\n",
            "Iteration 42, loss = 0.78995614\n",
            "Iteration 43, loss = 0.78747175\n",
            "Iteration 44, loss = 0.78509049\n",
            "Iteration 45, loss = 0.78279095\n",
            "Iteration 46, loss = 0.78030914\n",
            "Iteration 47, loss = 0.77804290\n",
            "Iteration 48, loss = 0.77571093\n",
            "Iteration 49, loss = 0.77337670\n",
            "Iteration 50, loss = 0.77119120\n",
            "Iteration 51, loss = 0.76906050\n",
            "Iteration 52, loss = 0.76683251\n",
            "Iteration 53, loss = 0.76474393\n",
            "Iteration 54, loss = 0.76252897\n",
            "Iteration 55, loss = 0.76055234\n",
            "Iteration 56, loss = 0.75863057\n",
            "Iteration 57, loss = 0.75661412\n",
            "Iteration 58, loss = 0.75460319\n",
            "Iteration 59, loss = 0.75272004\n",
            "Iteration 60, loss = 0.75074771\n",
            "Iteration 61, loss = 0.74888193\n",
            "Iteration 62, loss = 0.74706429\n",
            "Iteration 63, loss = 0.74522519\n",
            "Iteration 64, loss = 0.74342654\n",
            "Iteration 65, loss = 0.74166896\n",
            "Iteration 66, loss = 0.73980177\n",
            "Iteration 67, loss = 0.73797811\n",
            "Iteration 68, loss = 0.73626208\n",
            "Iteration 69, loss = 0.73436834\n",
            "Iteration 70, loss = 0.73252826\n",
            "Iteration 71, loss = 0.73089009\n",
            "Iteration 72, loss = 0.72915676\n",
            "Iteration 73, loss = 0.72752511\n",
            "Iteration 74, loss = 0.72591320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 75, loss = 0.72434095\n",
            "Iteration 76, loss = 0.72284166\n",
            "Iteration 77, loss = 0.72136264\n",
            "Iteration 78, loss = 0.71987881\n",
            "Iteration 79, loss = 0.71844868\n",
            "Iteration 80, loss = 0.71699466\n",
            "Iteration 81, loss = 0.71551285\n",
            "Iteration 82, loss = 0.71406031\n",
            "Iteration 83, loss = 0.71258023\n",
            "Iteration 84, loss = 0.71109177\n",
            "Iteration 85, loss = 0.70973876\n",
            "Iteration 86, loss = 0.70839930\n",
            "Iteration 87, loss = 0.70708242\n",
            "Iteration 88, loss = 0.70577438\n",
            "Iteration 89, loss = 0.70451113\n",
            "Iteration 90, loss = 0.70322118\n",
            "Iteration 91, loss = 0.70202655\n",
            "Iteration 92, loss = 0.70078451\n",
            "Iteration 93, loss = 0.69961631\n",
            "Iteration 94, loss = 0.69844678\n",
            "Iteration 95, loss = 0.69724830\n",
            "Iteration 96, loss = 0.69613427\n",
            "Iteration 97, loss = 0.69501374\n",
            "Iteration 98, loss = 0.69384144\n",
            "Iteration 99, loss = 0.69264942\n",
            "Iteration 100, loss = 0.69152324\n",
            "Iteration 101, loss = 0.69032139\n",
            "Iteration 102, loss = 0.68921963\n",
            "Iteration 103, loss = 0.68806356\n",
            "Iteration 104, loss = 0.68694547\n",
            "Iteration 105, loss = 0.68580049\n",
            "Iteration 106, loss = 0.68467709\n",
            "Iteration 107, loss = 0.68351202\n",
            "Iteration 108, loss = 0.68244297\n",
            "Iteration 109, loss = 0.68129337\n",
            "Iteration 110, loss = 0.68018769\n",
            "Iteration 111, loss = 0.67911323\n",
            "Iteration 112, loss = 0.67800563\n",
            "Iteration 113, loss = 0.67688688\n",
            "Iteration 114, loss = 0.67577377\n",
            "Iteration 115, loss = 0.67469987\n",
            "Iteration 116, loss = 0.67365680\n",
            "Iteration 117, loss = 0.67266171\n",
            "Iteration 118, loss = 0.67166519\n",
            "Iteration 119, loss = 0.67065051\n",
            "Iteration 120, loss = 0.66964606\n",
            "Iteration 121, loss = 0.66875745\n",
            "Iteration 122, loss = 0.66785397\n",
            "Iteration 123, loss = 0.66691974\n",
            "Iteration 124, loss = 0.66597774\n",
            "Iteration 125, loss = 0.66508293\n",
            "Iteration 126, loss = 0.66416539\n",
            "Iteration 127, loss = 0.66333100\n",
            "Iteration 128, loss = 0.66245092\n",
            "Iteration 129, loss = 0.66161204\n",
            "Iteration 130, loss = 0.66077100\n",
            "Iteration 131, loss = 0.65992557\n",
            "Iteration 132, loss = 0.65910868\n",
            "Iteration 133, loss = 0.65826914\n",
            "Iteration 134, loss = 0.65743825\n",
            "Iteration 135, loss = 0.65660546\n",
            "Iteration 136, loss = 0.65580924\n",
            "Iteration 137, loss = 0.65503717\n",
            "Iteration 138, loss = 0.65433598\n",
            "Iteration 139, loss = 0.65356897\n",
            "Iteration 140, loss = 0.65286764\n",
            "Iteration 141, loss = 0.65214869\n",
            "Iteration 142, loss = 0.65143177\n",
            "Iteration 143, loss = 0.65075158\n",
            "Iteration 144, loss = 0.64999049\n",
            "Iteration 145, loss = 0.64932034\n",
            "Iteration 146, loss = 0.64861187\n",
            "Iteration 147, loss = 0.64793553\n",
            "Iteration 148, loss = 0.64725881\n",
            "Iteration 149, loss = 0.64656754\n",
            "Iteration 150, loss = 0.64585344\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 150 and for layer number 4 : 0.64125\n",
            "Iteration 1, loss = 0.88791930\n",
            "Iteration 2, loss = 0.88478701\n",
            "Iteration 3, loss = 0.87977954\n",
            "Iteration 4, loss = 0.87382746\n",
            "Iteration 5, loss = 0.86693408\n",
            "Iteration 6, loss = 0.85991963\n",
            "Iteration 7, loss = 0.85349317\n",
            "Iteration 8, loss = 0.84609686\n",
            "Iteration 9, loss = 0.83903239\n",
            "Iteration 10, loss = 0.83199676\n",
            "Iteration 11, loss = 0.82476252\n",
            "Iteration 12, loss = 0.81799272\n",
            "Iteration 13, loss = 0.81153730\n",
            "Iteration 14, loss = 0.80501491\n",
            "Iteration 15, loss = 0.79881692\n",
            "Iteration 16, loss = 0.79283969\n",
            "Iteration 17, loss = 0.78704952\n",
            "Iteration 18, loss = 0.78142027\n",
            "Iteration 19, loss = 0.77612815\n",
            "Iteration 20, loss = 0.77058668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 21, loss = 0.76563879\n",
            "Iteration 22, loss = 0.76077411\n",
            "Iteration 23, loss = 0.75622954\n",
            "Iteration 24, loss = 0.75181798\n",
            "Iteration 25, loss = 0.74780850\n",
            "Iteration 26, loss = 0.74367194\n",
            "Iteration 27, loss = 0.73976022\n",
            "Iteration 28, loss = 0.73608417\n",
            "Iteration 29, loss = 0.73214581\n",
            "Iteration 30, loss = 0.72863038\n",
            "Iteration 31, loss = 0.72522531\n",
            "Iteration 32, loss = 0.72208870\n",
            "Iteration 33, loss = 0.71905862\n",
            "Iteration 34, loss = 0.71605055\n",
            "Iteration 35, loss = 0.71338575\n",
            "Iteration 36, loss = 0.71054771\n",
            "Iteration 37, loss = 0.70780651\n",
            "Iteration 38, loss = 0.70507846\n",
            "Iteration 39, loss = 0.70241483\n",
            "Iteration 40, loss = 0.69974696\n",
            "Iteration 41, loss = 0.69719897\n",
            "Iteration 42, loss = 0.69474538\n",
            "Iteration 43, loss = 0.69229605\n",
            "Iteration 44, loss = 0.68998519\n",
            "Iteration 45, loss = 0.68769249\n",
            "Iteration 46, loss = 0.68541216\n",
            "Iteration 47, loss = 0.68334998\n",
            "Iteration 48, loss = 0.68137544\n",
            "Iteration 49, loss = 0.67942804\n",
            "Iteration 50, loss = 0.67769897\n",
            "Iteration 51, loss = 0.67587683\n",
            "Iteration 52, loss = 0.67441020\n",
            "Iteration 53, loss = 0.67273943\n",
            "Iteration 54, loss = 0.67136315\n",
            "Iteration 55, loss = 0.66969470\n",
            "Iteration 56, loss = 0.66819849\n",
            "Iteration 57, loss = 0.66678144\n",
            "Iteration 58, loss = 0.66532375\n",
            "Iteration 59, loss = 0.66394656\n",
            "Iteration 60, loss = 0.66254487\n",
            "Iteration 61, loss = 0.66119003\n",
            "Iteration 62, loss = 0.65990917\n",
            "Iteration 63, loss = 0.65860006\n",
            "Iteration 64, loss = 0.65745658\n",
            "Iteration 65, loss = 0.65639810\n",
            "Iteration 66, loss = 0.65515649\n",
            "Iteration 67, loss = 0.65406199\n",
            "Iteration 68, loss = 0.65300071\n",
            "Iteration 69, loss = 0.65201781\n",
            "Iteration 70, loss = 0.65097950\n",
            "Iteration 71, loss = 0.65009911\n",
            "Iteration 72, loss = 0.64908267\n",
            "Iteration 73, loss = 0.64825233\n",
            "Iteration 74, loss = 0.64739105\n",
            "Iteration 75, loss = 0.64651078\n",
            "Iteration 76, loss = 0.64567306\n",
            "Iteration 77, loss = 0.64489576\n",
            "Iteration 78, loss = 0.64421106\n",
            "Iteration 79, loss = 0.64343015\n",
            "Iteration 80, loss = 0.64269074\n",
            "Iteration 81, loss = 0.64200003\n",
            "Iteration 82, loss = 0.64132052\n",
            "Iteration 83, loss = 0.64064109\n",
            "Iteration 84, loss = 0.63989578\n",
            "Iteration 85, loss = 0.63924996\n",
            "Iteration 86, loss = 0.63849229\n",
            "Iteration 87, loss = 0.63783327\n",
            "Iteration 88, loss = 0.63715252\n",
            "Iteration 89, loss = 0.63649693\n",
            "Iteration 90, loss = 0.63589299\n",
            "Iteration 91, loss = 0.63518930\n",
            "Iteration 92, loss = 0.63457609\n",
            "Iteration 93, loss = 0.63395665\n",
            "Iteration 94, loss = 0.63331641\n",
            "Iteration 95, loss = 0.63264966\n",
            "Iteration 96, loss = 0.63204170\n",
            "Iteration 97, loss = 0.63145729\n",
            "Iteration 98, loss = 0.63083574\n",
            "Iteration 99, loss = 0.63029201\n",
            "Iteration 100, loss = 0.62972358\n",
            "Iteration 101, loss = 0.62916841\n",
            "Iteration 102, loss = 0.62864743\n",
            "Iteration 103, loss = 0.62811693\n",
            "Iteration 104, loss = 0.62753695\n",
            "Iteration 105, loss = 0.62700635\n",
            "Iteration 106, loss = 0.62649924\n",
            "Iteration 107, loss = 0.62599200\n",
            "Iteration 108, loss = 0.62549756\n",
            "Iteration 109, loss = 0.62495530\n",
            "Iteration 110, loss = 0.62447073\n",
            "Iteration 111, loss = 0.62393226\n",
            "Iteration 112, loss = 0.62347574\n",
            "Iteration 113, loss = 0.62301065\n",
            "Iteration 114, loss = 0.62254573\n",
            "Iteration 115, loss = 0.62215107\n",
            "Iteration 116, loss = 0.62173290\n",
            "Iteration 117, loss = 0.62131221\n",
            "Iteration 118, loss = 0.62083700\n",
            "Iteration 119, loss = 0.62046053\n",
            "Iteration 120, loss = 0.62002285\n",
            "Iteration 121, loss = 0.61963050\n",
            "Iteration 122, loss = 0.61921993\n",
            "Iteration 123, loss = 0.61881579\n",
            "Iteration 124, loss = 0.61840680\n",
            "Iteration 125, loss = 0.61801465\n",
            "Iteration 126, loss = 0.61765406\n",
            "Iteration 127, loss = 0.61726355\n",
            "Iteration 128, loss = 0.61687388\n",
            "Iteration 129, loss = 0.61654076\n",
            "Iteration 130, loss = 0.61616167\n",
            "Iteration 131, loss = 0.61579524\n",
            "Iteration 132, loss = 0.61543350\n",
            "Iteration 133, loss = 0.61508540\n",
            "Iteration 134, loss = 0.61473026\n",
            "Iteration 135, loss = 0.61439139\n",
            "Iteration 136, loss = 0.61404298\n",
            "Iteration 137, loss = 0.61372779\n",
            "Iteration 138, loss = 0.61341953\n",
            "Iteration 139, loss = 0.61310868\n",
            "Iteration 140, loss = 0.61276988\n",
            "Iteration 141, loss = 0.61242081\n",
            "Iteration 142, loss = 0.61206685\n",
            "Iteration 143, loss = 0.61169393\n",
            "Iteration 144, loss = 0.61138846\n",
            "Iteration 145, loss = 0.61104682\n",
            "Iteration 146, loss = 0.61072881\n",
            "Iteration 147, loss = 0.61042243\n",
            "Iteration 148, loss = 0.61011410\n",
            "Iteration 149, loss = 0.60980392\n",
            "Iteration 150, loss = 0.60945383\n",
            "Iteration 1, loss = 0.88667506\n",
            "Iteration 2, loss = 0.88351311\n",
            "Iteration 3, loss = 0.87864988\n",
            "Iteration 4, loss = 0.87260271\n",
            "Iteration 5, loss = 0.86575091\n",
            "Iteration 6, loss = 0.85865495\n",
            "Iteration 7, loss = 0.85211290\n",
            "Iteration 8, loss = 0.84485323\n",
            "Iteration 9, loss = 0.83785117\n",
            "Iteration 10, loss = 0.83103528\n",
            "Iteration 11, loss = 0.82391039\n",
            "Iteration 12, loss = 0.81728381\n",
            "Iteration 13, loss = 0.81056237\n",
            "Iteration 14, loss = 0.80417780\n",
            "Iteration 15, loss = 0.79788403\n",
            "Iteration 16, loss = 0.79172048\n",
            "Iteration 17, loss = 0.78589084\n",
            "Iteration 18, loss = 0.78024639\n",
            "Iteration 19, loss = 0.77527683\n",
            "Iteration 20, loss = 0.76983525\n",
            "Iteration 21, loss = 0.76512748\n",
            "Iteration 22, loss = 0.76031041\n",
            "Iteration 23, loss = 0.75566756\n",
            "Iteration 24, loss = 0.75123287\n",
            "Iteration 25, loss = 0.74717290\n",
            "Iteration 26, loss = 0.74309808\n",
            "Iteration 27, loss = 0.73909650\n",
            "Iteration 28, loss = 0.73534184\n",
            "Iteration 29, loss = 0.73146632\n",
            "Iteration 30, loss = 0.72783360\n",
            "Iteration 31, loss = 0.72415824\n",
            "Iteration 32, loss = 0.72081620\n",
            "Iteration 33, loss = 0.71757346\n",
            "Iteration 34, loss = 0.71430564\n",
            "Iteration 35, loss = 0.71143757\n",
            "Iteration 36, loss = 0.70844375\n",
            "Iteration 37, loss = 0.70547686\n",
            "Iteration 38, loss = 0.70270393\n",
            "Iteration 39, loss = 0.70016842\n",
            "Iteration 40, loss = 0.69738412\n",
            "Iteration 41, loss = 0.69481862\n",
            "Iteration 42, loss = 0.69239524\n",
            "Iteration 43, loss = 0.68976645\n",
            "Iteration 44, loss = 0.68741275\n",
            "Iteration 45, loss = 0.68502015\n",
            "Iteration 46, loss = 0.68269702\n",
            "Iteration 47, loss = 0.68062921\n",
            "Iteration 48, loss = 0.67855526\n",
            "Iteration 49, loss = 0.67663369\n",
            "Iteration 50, loss = 0.67475577\n",
            "Iteration 51, loss = 0.67285993\n",
            "Iteration 52, loss = 0.67128721\n",
            "Iteration 53, loss = 0.66942488\n",
            "Iteration 54, loss = 0.66796022\n",
            "Iteration 55, loss = 0.66608528\n",
            "Iteration 56, loss = 0.66438449\n",
            "Iteration 57, loss = 0.66280374\n",
            "Iteration 58, loss = 0.66126445\n",
            "Iteration 59, loss = 0.65981469\n",
            "Iteration 60, loss = 0.65833838\n",
            "Iteration 61, loss = 0.65696295\n",
            "Iteration 62, loss = 0.65566478\n",
            "Iteration 63, loss = 0.65425246\n",
            "Iteration 64, loss = 0.65304333\n",
            "Iteration 65, loss = 0.65190526\n",
            "Iteration 66, loss = 0.65073936\n",
            "Iteration 67, loss = 0.64955138\n",
            "Iteration 68, loss = 0.64845330\n",
            "Iteration 69, loss = 0.64739906\n",
            "Iteration 70, loss = 0.64639450\n",
            "Iteration 71, loss = 0.64546332\n",
            "Iteration 72, loss = 0.64444218\n",
            "Iteration 73, loss = 0.64356265\n",
            "Iteration 74, loss = 0.64269640\n",
            "Iteration 75, loss = 0.64173035\n",
            "Iteration 76, loss = 0.64084432\n",
            "Iteration 77, loss = 0.63998132\n",
            "Iteration 78, loss = 0.63918837\n",
            "Iteration 79, loss = 0.63833688\n",
            "Iteration 80, loss = 0.63748454\n",
            "Iteration 81, loss = 0.63668468\n",
            "Iteration 82, loss = 0.63590312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 83, loss = 0.63515432\n",
            "Iteration 84, loss = 0.63435972\n",
            "Iteration 85, loss = 0.63367797\n",
            "Iteration 86, loss = 0.63287418\n",
            "Iteration 87, loss = 0.63216616\n",
            "Iteration 88, loss = 0.63143260\n",
            "Iteration 89, loss = 0.63074916\n",
            "Iteration 90, loss = 0.63012356\n",
            "Iteration 91, loss = 0.62939877\n",
            "Iteration 92, loss = 0.62878563\n",
            "Iteration 93, loss = 0.62818137\n",
            "Iteration 94, loss = 0.62753163\n",
            "Iteration 95, loss = 0.62687453\n",
            "Iteration 96, loss = 0.62626042\n",
            "Iteration 97, loss = 0.62562589\n",
            "Iteration 98, loss = 0.62499300\n",
            "Iteration 99, loss = 0.62440384\n",
            "Iteration 100, loss = 0.62381887\n",
            "Iteration 101, loss = 0.62324946\n",
            "Iteration 102, loss = 0.62268885\n",
            "Iteration 103, loss = 0.62213448\n",
            "Iteration 104, loss = 0.62156196\n",
            "Iteration 105, loss = 0.62102050\n",
            "Iteration 106, loss = 0.62050980\n",
            "Iteration 107, loss = 0.61999940\n",
            "Iteration 108, loss = 0.61947200\n",
            "Iteration 109, loss = 0.61889228\n",
            "Iteration 110, loss = 0.61837360\n",
            "Iteration 111, loss = 0.61780234\n",
            "Iteration 112, loss = 0.61731808\n",
            "Iteration 113, loss = 0.61682786\n",
            "Iteration 114, loss = 0.61634198\n",
            "Iteration 115, loss = 0.61588132\n",
            "Iteration 116, loss = 0.61539914\n",
            "Iteration 117, loss = 0.61496000\n",
            "Iteration 118, loss = 0.61445999\n",
            "Iteration 119, loss = 0.61405786\n",
            "Iteration 120, loss = 0.61359280\n",
            "Iteration 121, loss = 0.61315311\n",
            "Iteration 122, loss = 0.61272132\n",
            "Iteration 123, loss = 0.61229425\n",
            "Iteration 124, loss = 0.61186651\n",
            "Iteration 125, loss = 0.61149580\n",
            "Iteration 126, loss = 0.61113313\n",
            "Iteration 127, loss = 0.61075871\n",
            "Iteration 128, loss = 0.61038748\n",
            "Iteration 129, loss = 0.61006564\n",
            "Iteration 130, loss = 0.60973529\n",
            "Iteration 131, loss = 0.60940572\n",
            "Iteration 132, loss = 0.60907905\n",
            "Iteration 133, loss = 0.60876726\n",
            "Iteration 134, loss = 0.60845105\n",
            "Iteration 135, loss = 0.60813815\n",
            "Iteration 136, loss = 0.60783352\n",
            "Iteration 137, loss = 0.60753356\n",
            "Iteration 138, loss = 0.60723539\n",
            "Iteration 139, loss = 0.60693029\n",
            "Iteration 140, loss = 0.60661511\n",
            "Iteration 141, loss = 0.60632463\n",
            "Iteration 142, loss = 0.60599017\n",
            "Iteration 143, loss = 0.60568620\n",
            "Iteration 144, loss = 0.60541104\n",
            "Iteration 145, loss = 0.60510121\n",
            "Iteration 146, loss = 0.60480565\n",
            "Iteration 147, loss = 0.60450824\n",
            "Iteration 148, loss = 0.60423121\n",
            "Iteration 149, loss = 0.60393618\n",
            "Iteration 150, loss = 0.60361900\n",
            "Iteration 1, loss = 0.88994796\n",
            "Iteration 2, loss = 0.88647879\n",
            "Iteration 3, loss = 0.88112416\n",
            "Iteration 4, loss = 0.87477056\n",
            "Iteration 5, loss = 0.86759332\n",
            "Iteration 6, loss = 0.86026211\n",
            "Iteration 7, loss = 0.85335631\n",
            "Iteration 8, loss = 0.84586796\n",
            "Iteration 9, loss = 0.83874205\n",
            "Iteration 10, loss = 0.83175349\n",
            "Iteration 11, loss = 0.82448515\n",
            "Iteration 12, loss = 0.81778325\n",
            "Iteration 13, loss = 0.81084241\n",
            "Iteration 14, loss = 0.80421718\n",
            "Iteration 15, loss = 0.79782291\n",
            "Iteration 16, loss = 0.79153897\n",
            "Iteration 17, loss = 0.78578672\n",
            "Iteration 18, loss = 0.78028990\n",
            "Iteration 19, loss = 0.77514713\n",
            "Iteration 20, loss = 0.76965946\n",
            "Iteration 21, loss = 0.76458983\n",
            "Iteration 22, loss = 0.75975845\n",
            "Iteration 23, loss = 0.75512558\n",
            "Iteration 24, loss = 0.75073017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 25, loss = 0.74659412\n",
            "Iteration 26, loss = 0.74241300\n",
            "Iteration 27, loss = 0.73858643\n",
            "Iteration 28, loss = 0.73478476\n",
            "Iteration 29, loss = 0.73086329\n",
            "Iteration 30, loss = 0.72730311\n",
            "Iteration 31, loss = 0.72350860\n",
            "Iteration 32, loss = 0.72016727\n",
            "Iteration 33, loss = 0.71697302\n",
            "Iteration 34, loss = 0.71360923\n",
            "Iteration 35, loss = 0.71068663\n",
            "Iteration 36, loss = 0.70783843\n",
            "Iteration 37, loss = 0.70491116\n",
            "Iteration 38, loss = 0.70215099\n",
            "Iteration 39, loss = 0.69968857\n",
            "Iteration 40, loss = 0.69682383\n",
            "Iteration 41, loss = 0.69427119\n",
            "Iteration 42, loss = 0.69198212\n",
            "Iteration 43, loss = 0.68919723\n",
            "Iteration 44, loss = 0.68691056\n",
            "Iteration 45, loss = 0.68454722\n",
            "Iteration 46, loss = 0.68224448\n",
            "Iteration 47, loss = 0.68018345\n",
            "Iteration 48, loss = 0.67814086\n",
            "Iteration 49, loss = 0.67618343\n",
            "Iteration 50, loss = 0.67429781\n",
            "Iteration 51, loss = 0.67240548\n",
            "Iteration 52, loss = 0.67073257\n",
            "Iteration 53, loss = 0.66887623\n",
            "Iteration 54, loss = 0.66727539\n",
            "Iteration 55, loss = 0.66548070\n",
            "Iteration 56, loss = 0.66375165\n",
            "Iteration 57, loss = 0.66206313\n",
            "Iteration 58, loss = 0.66052993\n",
            "Iteration 59, loss = 0.65902693\n",
            "Iteration 60, loss = 0.65749027\n",
            "Iteration 61, loss = 0.65599758\n",
            "Iteration 62, loss = 0.65470773\n",
            "Iteration 63, loss = 0.65327373\n",
            "Iteration 64, loss = 0.65200876\n",
            "Iteration 65, loss = 0.65082649\n",
            "Iteration 66, loss = 0.64977568\n",
            "Iteration 67, loss = 0.64850480\n",
            "Iteration 68, loss = 0.64743751\n",
            "Iteration 69, loss = 0.64638809\n",
            "Iteration 70, loss = 0.64542021\n",
            "Iteration 71, loss = 0.64443926\n",
            "Iteration 72, loss = 0.64338729\n",
            "Iteration 73, loss = 0.64241557\n",
            "Iteration 74, loss = 0.64150518\n",
            "Iteration 75, loss = 0.64053993\n",
            "Iteration 76, loss = 0.63963000\n",
            "Iteration 77, loss = 0.63876754\n",
            "Iteration 78, loss = 0.63794665\n",
            "Iteration 79, loss = 0.63710295\n",
            "Iteration 80, loss = 0.63618590\n",
            "Iteration 81, loss = 0.63534354\n",
            "Iteration 82, loss = 0.63453306\n",
            "Iteration 83, loss = 0.63371928\n",
            "Iteration 84, loss = 0.63288966\n",
            "Iteration 85, loss = 0.63216956\n",
            "Iteration 86, loss = 0.63129485\n",
            "Iteration 87, loss = 0.63059605\n",
            "Iteration 88, loss = 0.62984039\n",
            "Iteration 89, loss = 0.62914261\n",
            "Iteration 90, loss = 0.62847791\n",
            "Iteration 91, loss = 0.62776941\n",
            "Iteration 92, loss = 0.62715342\n",
            "Iteration 93, loss = 0.62653126\n",
            "Iteration 94, loss = 0.62586031\n",
            "Iteration 95, loss = 0.62520916\n",
            "Iteration 96, loss = 0.62453176\n",
            "Iteration 97, loss = 0.62392353\n",
            "Iteration 98, loss = 0.62329005\n",
            "Iteration 99, loss = 0.62268263\n",
            "Iteration 100, loss = 0.62210977\n",
            "Iteration 101, loss = 0.62153879\n",
            "Iteration 102, loss = 0.62096210\n",
            "Iteration 103, loss = 0.62039536\n",
            "Iteration 104, loss = 0.61979729\n",
            "Iteration 105, loss = 0.61921874\n",
            "Iteration 106, loss = 0.61872081\n",
            "Iteration 107, loss = 0.61819904\n",
            "Iteration 108, loss = 0.61769996\n",
            "Iteration 109, loss = 0.61714989\n",
            "Iteration 110, loss = 0.61665917\n",
            "Iteration 111, loss = 0.61610002\n",
            "Iteration 112, loss = 0.61560955\n",
            "Iteration 113, loss = 0.61511586\n",
            "Iteration 114, loss = 0.61462980\n",
            "Iteration 115, loss = 0.61415450\n",
            "Iteration 116, loss = 0.61366960\n",
            "Iteration 117, loss = 0.61321745\n",
            "Iteration 118, loss = 0.61273741\n",
            "Iteration 119, loss = 0.61233518\n",
            "Iteration 120, loss = 0.61192375\n",
            "Iteration 121, loss = 0.61147456\n",
            "Iteration 122, loss = 0.61104419\n",
            "Iteration 123, loss = 0.61065688\n",
            "Iteration 124, loss = 0.61023305\n",
            "Iteration 125, loss = 0.60986801\n",
            "Iteration 126, loss = 0.60947917\n",
            "Iteration 127, loss = 0.60912309\n",
            "Iteration 128, loss = 0.60875497\n",
            "Iteration 129, loss = 0.60843382\n",
            "Iteration 130, loss = 0.60809005\n",
            "Iteration 131, loss = 0.60775346\n",
            "Iteration 132, loss = 0.60741015\n",
            "Iteration 133, loss = 0.60707433\n",
            "Iteration 134, loss = 0.60673199\n",
            "Iteration 135, loss = 0.60638836\n",
            "Iteration 136, loss = 0.60605999\n",
            "Iteration 137, loss = 0.60574818\n",
            "Iteration 138, loss = 0.60542733\n",
            "Iteration 139, loss = 0.60511922\n",
            "Iteration 140, loss = 0.60481512\n",
            "Iteration 141, loss = 0.60451908\n",
            "Iteration 142, loss = 0.60418285\n",
            "Iteration 143, loss = 0.60388202\n",
            "Iteration 144, loss = 0.60357479\n",
            "Iteration 145, loss = 0.60325144\n",
            "Iteration 146, loss = 0.60294371\n",
            "Iteration 147, loss = 0.60261059\n",
            "Iteration 148, loss = 0.60232324\n",
            "Iteration 149, loss = 0.60202313\n",
            "Iteration 150, loss = 0.60171767\n",
            "Iteration 1, loss = 0.88349736\n",
            "Iteration 2, loss = 0.87995489\n",
            "Iteration 3, loss = 0.87484423\n",
            "Iteration 4, loss = 0.86857561\n",
            "Iteration 5, loss = 0.86173197\n",
            "Iteration 6, loss = 0.85454050\n",
            "Iteration 7, loss = 0.84774058\n",
            "Iteration 8, loss = 0.84088394\n",
            "Iteration 9, loss = 0.83393267\n",
            "Iteration 10, loss = 0.82742553\n",
            "Iteration 11, loss = 0.82086785\n",
            "Iteration 12, loss = 0.81452708\n",
            "Iteration 13, loss = 0.80823130\n",
            "Iteration 14, loss = 0.80203476\n",
            "Iteration 15, loss = 0.79614496\n",
            "Iteration 16, loss = 0.79049779\n",
            "Iteration 17, loss = 0.78504210\n",
            "Iteration 18, loss = 0.77952845\n",
            "Iteration 19, loss = 0.77469909\n",
            "Iteration 20, loss = 0.76936348\n",
            "Iteration 21, loss = 0.76442730\n",
            "Iteration 22, loss = 0.75965800\n",
            "Iteration 23, loss = 0.75500764\n",
            "Iteration 24, loss = 0.75071470\n",
            "Iteration 25, loss = 0.74647049\n",
            "Iteration 26, loss = 0.74261509\n",
            "Iteration 27, loss = 0.73863169\n",
            "Iteration 28, loss = 0.73493685\n",
            "Iteration 29, loss = 0.73128501\n",
            "Iteration 30, loss = 0.72789941\n",
            "Iteration 31, loss = 0.72426266\n",
            "Iteration 32, loss = 0.72101993\n",
            "Iteration 33, loss = 0.71785043\n",
            "Iteration 34, loss = 0.71458345\n",
            "Iteration 35, loss = 0.71165677\n",
            "Iteration 36, loss = 0.70878583\n",
            "Iteration 37, loss = 0.70588880\n",
            "Iteration 38, loss = 0.70314079\n",
            "Iteration 39, loss = 0.70061319\n",
            "Iteration 40, loss = 0.69798247\n",
            "Iteration 41, loss = 0.69549327\n",
            "Iteration 42, loss = 0.69327842\n",
            "Iteration 43, loss = 0.69073254\n",
            "Iteration 44, loss = 0.68867848\n",
            "Iteration 45, loss = 0.68642668\n",
            "Iteration 46, loss = 0.68434546\n",
            "Iteration 47, loss = 0.68236535\n",
            "Iteration 48, loss = 0.68048017\n",
            "Iteration 49, loss = 0.67864669\n",
            "Iteration 50, loss = 0.67680541\n",
            "Iteration 51, loss = 0.67496606\n",
            "Iteration 52, loss = 0.67331816\n",
            "Iteration 53, loss = 0.67162095\n",
            "Iteration 54, loss = 0.67012659\n",
            "Iteration 55, loss = 0.66851262\n",
            "Iteration 56, loss = 0.66698837\n",
            "Iteration 57, loss = 0.66532003\n",
            "Iteration 58, loss = 0.66385469\n",
            "Iteration 59, loss = 0.66236615\n",
            "Iteration 60, loss = 0.66095100\n",
            "Iteration 61, loss = 0.65952010\n",
            "Iteration 62, loss = 0.65829264\n",
            "Iteration 63, loss = 0.65702747\n",
            "Iteration 64, loss = 0.65574388\n",
            "Iteration 65, loss = 0.65461995\n",
            "Iteration 66, loss = 0.65353183\n",
            "Iteration 67, loss = 0.65225074\n",
            "Iteration 68, loss = 0.65117719\n",
            "Iteration 69, loss = 0.64998474\n",
            "Iteration 70, loss = 0.64900805\n",
            "Iteration 71, loss = 0.64805924\n",
            "Iteration 72, loss = 0.64707263\n",
            "Iteration 73, loss = 0.64619598\n",
            "Iteration 74, loss = 0.64539438\n",
            "Iteration 75, loss = 0.64456929\n",
            "Iteration 76, loss = 0.64368399\n",
            "Iteration 77, loss = 0.64289466\n",
            "Iteration 78, loss = 0.64211176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 79, loss = 0.64132455\n",
            "Iteration 80, loss = 0.64048025\n",
            "Iteration 81, loss = 0.63968838\n",
            "Iteration 82, loss = 0.63893461\n",
            "Iteration 83, loss = 0.63821109\n",
            "Iteration 84, loss = 0.63747211\n",
            "Iteration 85, loss = 0.63685252\n",
            "Iteration 86, loss = 0.63609556\n",
            "Iteration 87, loss = 0.63543753\n",
            "Iteration 88, loss = 0.63471118\n",
            "Iteration 89, loss = 0.63404085\n",
            "Iteration 90, loss = 0.63340349\n",
            "Iteration 91, loss = 0.63278193\n",
            "Iteration 92, loss = 0.63214743\n",
            "Iteration 93, loss = 0.63157648\n",
            "Iteration 94, loss = 0.63095984\n",
            "Iteration 95, loss = 0.63041787\n",
            "Iteration 96, loss = 0.62977208\n",
            "Iteration 97, loss = 0.62918399\n",
            "Iteration 98, loss = 0.62857863\n",
            "Iteration 99, loss = 0.62795666\n",
            "Iteration 100, loss = 0.62743278\n",
            "Iteration 101, loss = 0.62685021\n",
            "Iteration 102, loss = 0.62633634\n",
            "Iteration 103, loss = 0.62579168\n",
            "Iteration 104, loss = 0.62524932\n",
            "Iteration 105, loss = 0.62471175\n",
            "Iteration 106, loss = 0.62418637\n",
            "Iteration 107, loss = 0.62365253\n",
            "Iteration 108, loss = 0.62314381\n",
            "Iteration 109, loss = 0.62264512\n",
            "Iteration 110, loss = 0.62216813\n",
            "Iteration 111, loss = 0.62165089\n",
            "Iteration 112, loss = 0.62116648\n",
            "Iteration 113, loss = 0.62068983\n",
            "Iteration 114, loss = 0.62022157\n",
            "Iteration 115, loss = 0.61975804\n",
            "Iteration 116, loss = 0.61928792\n",
            "Iteration 117, loss = 0.61889902\n",
            "Iteration 118, loss = 0.61843594\n",
            "Iteration 119, loss = 0.61802865\n",
            "Iteration 120, loss = 0.61762445\n",
            "Iteration 121, loss = 0.61721078\n",
            "Iteration 122, loss = 0.61681830\n",
            "Iteration 123, loss = 0.61644115\n",
            "Iteration 124, loss = 0.61604421\n",
            "Iteration 125, loss = 0.61570259\n",
            "Iteration 126, loss = 0.61535021\n",
            "Iteration 127, loss = 0.61506127\n",
            "Iteration 128, loss = 0.61473832\n",
            "Iteration 129, loss = 0.61444763\n",
            "Iteration 130, loss = 0.61415855\n",
            "Iteration 131, loss = 0.61385193\n",
            "Iteration 132, loss = 0.61355253\n",
            "Iteration 133, loss = 0.61322731\n",
            "Iteration 134, loss = 0.61292373\n",
            "Iteration 135, loss = 0.61261517\n",
            "Iteration 136, loss = 0.61231101\n",
            "Iteration 137, loss = 0.61202998\n",
            "Iteration 138, loss = 0.61175024\n",
            "Iteration 139, loss = 0.61147013\n",
            "Iteration 140, loss = 0.61120172\n",
            "Iteration 141, loss = 0.61090642\n",
            "Iteration 142, loss = 0.61061183\n",
            "Iteration 143, loss = 0.61032082\n",
            "Iteration 144, loss = 0.61000956\n",
            "Iteration 145, loss = 0.60972792\n",
            "Iteration 146, loss = 0.60945950\n",
            "Iteration 147, loss = 0.60917434\n",
            "Iteration 148, loss = 0.60890758\n",
            "Iteration 149, loss = 0.60864347\n",
            "Iteration 150, loss = 0.60839202\n",
            "Iteration 1, loss = 0.88921381\n",
            "Iteration 2, loss = 0.88561120\n",
            "Iteration 3, loss = 0.88039745\n",
            "Iteration 4, loss = 0.87411510\n",
            "Iteration 5, loss = 0.86695385\n",
            "Iteration 6, loss = 0.85950718\n",
            "Iteration 7, loss = 0.85233922\n",
            "Iteration 8, loss = 0.84527473\n",
            "Iteration 9, loss = 0.83809709\n",
            "Iteration 10, loss = 0.83164349\n",
            "Iteration 11, loss = 0.82493838\n",
            "Iteration 12, loss = 0.81870921\n",
            "Iteration 13, loss = 0.81251318\n",
            "Iteration 14, loss = 0.80607251\n",
            "Iteration 15, loss = 0.79981985\n",
            "Iteration 16, loss = 0.79393281\n",
            "Iteration 17, loss = 0.78828581\n",
            "Iteration 18, loss = 0.78245479\n",
            "Iteration 19, loss = 0.77739177\n",
            "Iteration 20, loss = 0.77198009\n",
            "Iteration 21, loss = 0.76686429\n",
            "Iteration 22, loss = 0.76204430\n",
            "Iteration 23, loss = 0.75723984\n",
            "Iteration 24, loss = 0.75306036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 25, loss = 0.74865567\n",
            "Iteration 26, loss = 0.74459266\n",
            "Iteration 27, loss = 0.74053604\n",
            "Iteration 28, loss = 0.73681378\n",
            "Iteration 29, loss = 0.73320126\n",
            "Iteration 30, loss = 0.72947076\n",
            "Iteration 31, loss = 0.72576794\n",
            "Iteration 32, loss = 0.72239030\n",
            "Iteration 33, loss = 0.71920770\n",
            "Iteration 34, loss = 0.71580616\n",
            "Iteration 35, loss = 0.71274085\n",
            "Iteration 36, loss = 0.70975790\n",
            "Iteration 37, loss = 0.70664366\n",
            "Iteration 38, loss = 0.70366441\n",
            "Iteration 39, loss = 0.70088208\n",
            "Iteration 40, loss = 0.69810738\n",
            "Iteration 41, loss = 0.69560192\n",
            "Iteration 42, loss = 0.69316336\n",
            "Iteration 43, loss = 0.69061309\n",
            "Iteration 44, loss = 0.68855524\n",
            "Iteration 45, loss = 0.68625829\n",
            "Iteration 46, loss = 0.68415410\n",
            "Iteration 47, loss = 0.68216332\n",
            "Iteration 48, loss = 0.68023366\n",
            "Iteration 49, loss = 0.67837345\n",
            "Iteration 50, loss = 0.67652492\n",
            "Iteration 51, loss = 0.67465587\n",
            "Iteration 52, loss = 0.67291367\n",
            "Iteration 53, loss = 0.67120569\n",
            "Iteration 54, loss = 0.66960302\n",
            "Iteration 55, loss = 0.66800315\n",
            "Iteration 56, loss = 0.66652548\n",
            "Iteration 57, loss = 0.66485587\n",
            "Iteration 58, loss = 0.66349741\n",
            "Iteration 59, loss = 0.66208478\n",
            "Iteration 60, loss = 0.66070181\n",
            "Iteration 61, loss = 0.65933908\n",
            "Iteration 62, loss = 0.65815998\n",
            "Iteration 63, loss = 0.65691715\n",
            "Iteration 64, loss = 0.65568373\n",
            "Iteration 65, loss = 0.65459061\n",
            "Iteration 66, loss = 0.65352896\n",
            "Iteration 67, loss = 0.65233664\n",
            "Iteration 68, loss = 0.65132622\n",
            "Iteration 69, loss = 0.65019996\n",
            "Iteration 70, loss = 0.64924598\n",
            "Iteration 71, loss = 0.64821908\n",
            "Iteration 72, loss = 0.64725881\n",
            "Iteration 73, loss = 0.64638887\n",
            "Iteration 74, loss = 0.64557963\n",
            "Iteration 75, loss = 0.64472783\n",
            "Iteration 76, loss = 0.64381309\n",
            "Iteration 77, loss = 0.64297595\n",
            "Iteration 78, loss = 0.64215750\n",
            "Iteration 79, loss = 0.64136949\n",
            "Iteration 80, loss = 0.64060358\n",
            "Iteration 81, loss = 0.63981219\n",
            "Iteration 82, loss = 0.63909431\n",
            "Iteration 83, loss = 0.63840286\n",
            "Iteration 84, loss = 0.63766622\n",
            "Iteration 85, loss = 0.63702776\n",
            "Iteration 86, loss = 0.63626099\n",
            "Iteration 87, loss = 0.63554534\n",
            "Iteration 88, loss = 0.63482509\n",
            "Iteration 89, loss = 0.63410681\n",
            "Iteration 90, loss = 0.63339896\n",
            "Iteration 91, loss = 0.63272694\n",
            "Iteration 92, loss = 0.63204511\n",
            "Iteration 93, loss = 0.63144044\n",
            "Iteration 94, loss = 0.63084003\n",
            "Iteration 95, loss = 0.63028250\n",
            "Iteration 96, loss = 0.62965499\n",
            "Iteration 97, loss = 0.62905022\n",
            "Iteration 98, loss = 0.62844429\n",
            "Iteration 99, loss = 0.62780816\n",
            "Iteration 100, loss = 0.62726404\n",
            "Iteration 101, loss = 0.62667447\n",
            "Iteration 102, loss = 0.62614736\n",
            "Iteration 103, loss = 0.62561211\n",
            "Iteration 104, loss = 0.62507882\n",
            "Iteration 105, loss = 0.62452230\n",
            "Iteration 106, loss = 0.62401096\n",
            "Iteration 107, loss = 0.62343172\n",
            "Iteration 108, loss = 0.62293591\n",
            "Iteration 109, loss = 0.62242508\n",
            "Iteration 110, loss = 0.62197929\n",
            "Iteration 111, loss = 0.62148484\n",
            "Iteration 112, loss = 0.62100910\n",
            "Iteration 113, loss = 0.62055945\n",
            "Iteration 114, loss = 0.62011406\n",
            "Iteration 115, loss = 0.61961416\n",
            "Iteration 116, loss = 0.61915852\n",
            "Iteration 117, loss = 0.61873552\n",
            "Iteration 118, loss = 0.61828072\n",
            "Iteration 119, loss = 0.61786798\n",
            "Iteration 120, loss = 0.61746353\n",
            "Iteration 121, loss = 0.61703375\n",
            "Iteration 122, loss = 0.61663113\n",
            "Iteration 123, loss = 0.61623362\n",
            "Iteration 124, loss = 0.61582066\n",
            "Iteration 125, loss = 0.61543730\n",
            "Iteration 126, loss = 0.61503144\n",
            "Iteration 127, loss = 0.61465035\n",
            "Iteration 128, loss = 0.61429004\n",
            "Iteration 129, loss = 0.61393473\n",
            "Iteration 130, loss = 0.61360773\n",
            "Iteration 131, loss = 0.61325552\n",
            "Iteration 132, loss = 0.61291127\n",
            "Iteration 133, loss = 0.61256136\n",
            "Iteration 134, loss = 0.61222469\n",
            "Iteration 135, loss = 0.61188378\n",
            "Iteration 136, loss = 0.61154140\n",
            "Iteration 137, loss = 0.61123058\n",
            "Iteration 138, loss = 0.61090626\n",
            "Iteration 139, loss = 0.61058499\n",
            "Iteration 140, loss = 0.61029187\n",
            "Iteration 141, loss = 0.60998706\n",
            "Iteration 142, loss = 0.60965266\n",
            "Iteration 143, loss = 0.60933431\n",
            "Iteration 144, loss = 0.60899844\n",
            "Iteration 145, loss = 0.60866336\n",
            "Iteration 146, loss = 0.60834776\n",
            "Iteration 147, loss = 0.60804879\n",
            "Iteration 148, loss = 0.60774415\n",
            "Iteration 149, loss = 0.60745530\n",
            "Iteration 150, loss = 0.60720143\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 150 and for layer number 5 : 0.6987500000000001\n",
            "Iteration 1, loss = 1.38926569\n",
            "Iteration 2, loss = 1.37280501\n",
            "Iteration 3, loss = 1.34746797\n",
            "Iteration 4, loss = 1.31619589\n",
            "Iteration 5, loss = 1.28284600\n",
            "Iteration 6, loss = 1.24835476\n",
            "Iteration 7, loss = 1.21472357\n",
            "Iteration 8, loss = 1.18128774\n",
            "Iteration 9, loss = 1.14886509\n",
            "Iteration 10, loss = 1.11879494\n",
            "Iteration 11, loss = 1.08953649\n",
            "Iteration 12, loss = 1.06187751\n",
            "Iteration 13, loss = 1.03601333\n",
            "Iteration 14, loss = 1.01129749\n",
            "Iteration 15, loss = 0.98904579\n",
            "Iteration 16, loss = 0.96813749\n",
            "Iteration 17, loss = 0.94777197\n",
            "Iteration 18, loss = 0.92847460\n",
            "Iteration 19, loss = 0.91099975\n",
            "Iteration 20, loss = 0.89440660\n",
            "Iteration 21, loss = 0.87873317\n",
            "Iteration 22, loss = 0.86430695\n",
            "Iteration 23, loss = 0.85086938\n",
            "Iteration 24, loss = 0.83820984\n",
            "Iteration 25, loss = 0.82677373\n",
            "Iteration 26, loss = 0.81533009\n",
            "Iteration 27, loss = 0.80526332\n",
            "Iteration 28, loss = 0.79582014\n",
            "Iteration 29, loss = 0.78704031\n",
            "Iteration 30, loss = 0.77874972\n",
            "Iteration 31, loss = 0.77095374\n",
            "Iteration 32, loss = 0.76369540\n",
            "Iteration 33, loss = 0.75713497\n",
            "Iteration 34, loss = 0.75066766\n",
            "Iteration 35, loss = 0.74477483\n",
            "Iteration 36, loss = 0.73962234\n",
            "Iteration 37, loss = 0.73447076\n",
            "Iteration 38, loss = 0.72976653\n",
            "Iteration 39, loss = 0.72535374\n",
            "Iteration 40, loss = 0.72111863\n",
            "Iteration 41, loss = 0.71707931\n",
            "Iteration 42, loss = 0.71336464\n",
            "Iteration 43, loss = 0.70972019\n",
            "Iteration 44, loss = 0.70623239\n",
            "Iteration 45, loss = 0.70314381\n",
            "Iteration 46, loss = 0.69989642\n",
            "Iteration 47, loss = 0.69679974\n",
            "Iteration 48, loss = 0.69401116\n",
            "Iteration 49, loss = 0.69139935\n",
            "Iteration 50, loss = 0.68879462\n",
            "Iteration 51, loss = 0.68651470\n",
            "Iteration 52, loss = 0.68421757\n",
            "Iteration 53, loss = 0.68200484\n",
            "Iteration 54, loss = 0.68007585\n",
            "Iteration 55, loss = 0.67808367\n",
            "Iteration 56, loss = 0.67618600\n",
            "Iteration 57, loss = 0.67429057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 58, loss = 0.67248101\n",
            "Iteration 59, loss = 0.67080894\n",
            "Iteration 60, loss = 0.66927500\n",
            "Iteration 61, loss = 0.66761747\n",
            "Iteration 62, loss = 0.66614964\n",
            "Iteration 63, loss = 0.66454833\n",
            "Iteration 64, loss = 0.66305573\n",
            "Iteration 65, loss = 0.66161184\n",
            "Iteration 66, loss = 0.66013871\n",
            "Iteration 67, loss = 0.65874027\n",
            "Iteration 68, loss = 0.65728994\n",
            "Iteration 69, loss = 0.65605861\n",
            "Iteration 70, loss = 0.65477739\n",
            "Iteration 71, loss = 0.65354960\n",
            "Iteration 72, loss = 0.65235350\n",
            "Iteration 73, loss = 0.65124673\n",
            "Iteration 74, loss = 0.65008456\n",
            "Iteration 75, loss = 0.64907371\n",
            "Iteration 76, loss = 0.64797402\n",
            "Iteration 77, loss = 0.64701224\n",
            "Iteration 78, loss = 0.64597245\n",
            "Iteration 79, loss = 0.64505579\n",
            "Iteration 80, loss = 0.64412907\n",
            "Iteration 81, loss = 0.64320081\n",
            "Iteration 82, loss = 0.64230711\n",
            "Iteration 83, loss = 0.64139953\n",
            "Iteration 84, loss = 0.64054124\n",
            "Iteration 85, loss = 0.63968788\n",
            "Iteration 86, loss = 0.63885672\n",
            "Iteration 87, loss = 0.63805459\n",
            "Iteration 88, loss = 0.63726151\n",
            "Iteration 89, loss = 0.63648889\n",
            "Iteration 90, loss = 0.63578801\n",
            "Iteration 91, loss = 0.63499720\n",
            "Iteration 92, loss = 0.63423864\n",
            "Iteration 93, loss = 0.63348850\n",
            "Iteration 94, loss = 0.63277767\n",
            "Iteration 95, loss = 0.63198463\n",
            "Iteration 96, loss = 0.63126116\n",
            "Iteration 97, loss = 0.63054150\n",
            "Iteration 98, loss = 0.62982044\n",
            "Iteration 99, loss = 0.62910262\n",
            "Iteration 100, loss = 0.62846924\n",
            "Iteration 101, loss = 0.62771756\n",
            "Iteration 102, loss = 0.62705986\n",
            "Iteration 103, loss = 0.62638727\n",
            "Iteration 104, loss = 0.62572004\n",
            "Iteration 105, loss = 0.62509265\n",
            "Iteration 106, loss = 0.62441787\n",
            "Iteration 107, loss = 0.62381414\n",
            "Iteration 108, loss = 0.62318615\n",
            "Iteration 109, loss = 0.62257460\n",
            "Iteration 110, loss = 0.62195155\n",
            "Iteration 111, loss = 0.62131250\n",
            "Iteration 112, loss = 0.62072269\n",
            "Iteration 113, loss = 0.62017960\n",
            "Iteration 114, loss = 0.61958378\n",
            "Iteration 115, loss = 0.61903643\n",
            "Iteration 116, loss = 0.61843817\n",
            "Iteration 117, loss = 0.61785837\n",
            "Iteration 118, loss = 0.61732369\n",
            "Iteration 119, loss = 0.61674102\n",
            "Iteration 120, loss = 0.61622506\n",
            "Iteration 121, loss = 0.61576215\n",
            "Iteration 122, loss = 0.61529013\n",
            "Iteration 123, loss = 0.61483731\n",
            "Iteration 124, loss = 0.61438639\n",
            "Iteration 125, loss = 0.61390245\n",
            "Iteration 126, loss = 0.61345612\n",
            "Iteration 127, loss = 0.61299340\n",
            "Iteration 128, loss = 0.61254224\n",
            "Iteration 129, loss = 0.61210542\n",
            "Iteration 130, loss = 0.61164362\n",
            "Iteration 131, loss = 0.61120969\n",
            "Iteration 132, loss = 0.61073231\n",
            "Iteration 133, loss = 0.61026151\n",
            "Iteration 134, loss = 0.60980717\n",
            "Iteration 135, loss = 0.60934013\n",
            "Iteration 136, loss = 0.60886240\n",
            "Iteration 137, loss = 0.60837324\n",
            "Iteration 138, loss = 0.60790973\n",
            "Iteration 139, loss = 0.60744643\n",
            "Iteration 140, loss = 0.60697494\n",
            "Iteration 141, loss = 0.60651293\n",
            "Iteration 142, loss = 0.60607800\n",
            "Iteration 143, loss = 0.60558981\n",
            "Iteration 144, loss = 0.60514142\n",
            "Iteration 145, loss = 0.60471048\n",
            "Iteration 146, loss = 0.60428408\n",
            "Iteration 147, loss = 0.60388988\n",
            "Iteration 148, loss = 0.60350121\n",
            "Iteration 149, loss = 0.60305622\n",
            "Iteration 150, loss = 0.60264952\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37877399\n",
            "Iteration 2, loss = 1.36256087\n",
            "Iteration 3, loss = 1.33739238\n",
            "Iteration 4, loss = 1.30659378\n",
            "Iteration 5, loss = 1.27335856\n",
            "Iteration 6, loss = 1.23889435\n",
            "Iteration 7, loss = 1.20499233\n",
            "Iteration 8, loss = 1.17181650\n",
            "Iteration 9, loss = 1.13971284\n",
            "Iteration 10, loss = 1.10956324\n",
            "Iteration 11, loss = 1.08045416\n",
            "Iteration 12, loss = 1.05294176\n",
            "Iteration 13, loss = 1.02710830\n",
            "Iteration 14, loss = 1.00261165\n",
            "Iteration 15, loss = 0.98070591\n",
            "Iteration 16, loss = 0.95987920\n",
            "Iteration 17, loss = 0.94020854\n",
            "Iteration 18, loss = 0.92122763\n",
            "Iteration 19, loss = 0.90383330\n",
            "Iteration 20, loss = 0.88748408\n",
            "Iteration 21, loss = 0.87191253\n",
            "Iteration 22, loss = 0.85761649\n",
            "Iteration 23, loss = 0.84438059\n",
            "Iteration 24, loss = 0.83166341\n",
            "Iteration 25, loss = 0.82034630\n",
            "Iteration 26, loss = 0.80900028\n",
            "Iteration 27, loss = 0.79924699\n",
            "Iteration 28, loss = 0.78975345\n",
            "Iteration 29, loss = 0.78110653\n",
            "Iteration 30, loss = 0.77320177\n",
            "Iteration 31, loss = 0.76575073\n",
            "Iteration 32, loss = 0.75849142\n",
            "Iteration 33, loss = 0.75189914\n",
            "Iteration 34, loss = 0.74559673\n",
            "Iteration 35, loss = 0.73983119\n",
            "Iteration 36, loss = 0.73485730\n",
            "Iteration 37, loss = 0.72990421\n",
            "Iteration 38, loss = 0.72544570\n",
            "Iteration 39, loss = 0.72117603\n",
            "Iteration 40, loss = 0.71703850\n",
            "Iteration 41, loss = 0.71322451\n",
            "Iteration 42, loss = 0.70966686\n",
            "Iteration 43, loss = 0.70626584\n",
            "Iteration 44, loss = 0.70282098\n",
            "Iteration 45, loss = 0.69979339\n",
            "Iteration 46, loss = 0.69664159\n",
            "Iteration 47, loss = 0.69359879\n",
            "Iteration 48, loss = 0.69084866\n",
            "Iteration 49, loss = 0.68833276\n",
            "Iteration 50, loss = 0.68587860\n",
            "Iteration 51, loss = 0.68370393\n",
            "Iteration 52, loss = 0.68146700\n",
            "Iteration 53, loss = 0.67934255\n",
            "Iteration 54, loss = 0.67741969\n",
            "Iteration 55, loss = 0.67540396\n",
            "Iteration 56, loss = 0.67352010\n",
            "Iteration 57, loss = 0.67177485\n",
            "Iteration 58, loss = 0.67002014\n",
            "Iteration 59, loss = 0.66846780\n",
            "Iteration 60, loss = 0.66695573\n",
            "Iteration 61, loss = 0.66542439\n",
            "Iteration 62, loss = 0.66394678\n",
            "Iteration 63, loss = 0.66235577\n",
            "Iteration 64, loss = 0.66088106\n",
            "Iteration 65, loss = 0.65935602\n",
            "Iteration 66, loss = 0.65793334\n",
            "Iteration 67, loss = 0.65652492\n",
            "Iteration 68, loss = 0.65508253\n",
            "Iteration 69, loss = 0.65383545\n",
            "Iteration 70, loss = 0.65253073\n",
            "Iteration 71, loss = 0.65132221\n",
            "Iteration 72, loss = 0.65005109\n",
            "Iteration 73, loss = 0.64889076\n",
            "Iteration 74, loss = 0.64770514\n",
            "Iteration 75, loss = 0.64660515\n",
            "Iteration 76, loss = 0.64544655\n",
            "Iteration 77, loss = 0.64443220\n",
            "Iteration 78, loss = 0.64331724\n",
            "Iteration 79, loss = 0.64235456\n",
            "Iteration 80, loss = 0.64132897\n",
            "Iteration 81, loss = 0.64038882\n",
            "Iteration 82, loss = 0.63947120\n",
            "Iteration 83, loss = 0.63852183\n",
            "Iteration 84, loss = 0.63767949\n",
            "Iteration 85, loss = 0.63682970\n",
            "Iteration 86, loss = 0.63597486\n",
            "Iteration 87, loss = 0.63514300\n",
            "Iteration 88, loss = 0.63435968\n",
            "Iteration 89, loss = 0.63354415\n",
            "Iteration 90, loss = 0.63282646\n",
            "Iteration 91, loss = 0.63199980\n",
            "Iteration 92, loss = 0.63122688\n",
            "Iteration 93, loss = 0.63045721\n",
            "Iteration 94, loss = 0.62970582\n",
            "Iteration 95, loss = 0.62887494\n",
            "Iteration 96, loss = 0.62811898\n",
            "Iteration 97, loss = 0.62738428\n",
            "Iteration 98, loss = 0.62665124\n",
            "Iteration 99, loss = 0.62594012\n",
            "Iteration 100, loss = 0.62527980\n",
            "Iteration 101, loss = 0.62456794\n",
            "Iteration 102, loss = 0.62386710\n",
            "Iteration 103, loss = 0.62320385\n",
            "Iteration 104, loss = 0.62253996\n",
            "Iteration 105, loss = 0.62189415\n",
            "Iteration 106, loss = 0.62120134\n",
            "Iteration 107, loss = 0.62057750\n",
            "Iteration 108, loss = 0.61992975\n",
            "Iteration 109, loss = 0.61930433\n",
            "Iteration 110, loss = 0.61866132\n",
            "Iteration 111, loss = 0.61802694\n",
            "Iteration 112, loss = 0.61740918\n",
            "Iteration 113, loss = 0.61684939\n",
            "Iteration 114, loss = 0.61623249\n",
            "Iteration 115, loss = 0.61565796\n",
            "Iteration 116, loss = 0.61504501\n",
            "Iteration 117, loss = 0.61446406\n",
            "Iteration 118, loss = 0.61390667\n",
            "Iteration 119, loss = 0.61329184\n",
            "Iteration 120, loss = 0.61276745\n",
            "Iteration 121, loss = 0.61226539\n",
            "Iteration 122, loss = 0.61178911\n",
            "Iteration 123, loss = 0.61131873\n",
            "Iteration 124, loss = 0.61082428\n",
            "Iteration 125, loss = 0.61033579\n",
            "Iteration 126, loss = 0.60984979\n",
            "Iteration 127, loss = 0.60935164\n",
            "Iteration 128, loss = 0.60887902\n",
            "Iteration 129, loss = 0.60840988\n",
            "Iteration 130, loss = 0.60792078\n",
            "Iteration 131, loss = 0.60746030\n",
            "Iteration 132, loss = 0.60695595\n",
            "Iteration 133, loss = 0.60645273\n",
            "Iteration 134, loss = 0.60595229\n",
            "Iteration 135, loss = 0.60546307\n",
            "Iteration 136, loss = 0.60495070\n",
            "Iteration 137, loss = 0.60449123\n",
            "Iteration 138, loss = 0.60401363\n",
            "Iteration 139, loss = 0.60356238\n",
            "Iteration 140, loss = 0.60309091\n",
            "Iteration 141, loss = 0.60264393\n",
            "Iteration 142, loss = 0.60224576\n",
            "Iteration 143, loss = 0.60177122\n",
            "Iteration 144, loss = 0.60137298\n",
            "Iteration 145, loss = 0.60096009\n",
            "Iteration 146, loss = 0.60056936\n",
            "Iteration 147, loss = 0.60019633\n",
            "Iteration 148, loss = 0.59982456\n",
            "Iteration 149, loss = 0.59941932\n",
            "Iteration 150, loss = 0.59899122\n",
            "Iteration 1, loss = 1.37951553\n",
            "Iteration 2, loss = 1.36345276\n",
            "Iteration 3, loss = 1.33844177\n",
            "Iteration 4, loss = 1.30796298\n",
            "Iteration 5, loss = 1.27459126\n",
            "Iteration 6, loss = 1.24040492\n",
            "Iteration 7, loss = 1.20671426\n",
            "Iteration 8, loss = 1.17282486\n",
            "Iteration 9, loss = 1.14067304\n",
            "Iteration 10, loss = 1.10992688\n",
            "Iteration 11, loss = 1.08024299\n",
            "Iteration 12, loss = 1.05249805\n",
            "Iteration 13, loss = 1.02632044\n",
            "Iteration 14, loss = 1.00219240\n",
            "Iteration 15, loss = 0.97949832\n",
            "Iteration 16, loss = 0.95866806\n",
            "Iteration 17, loss = 0.93831233\n",
            "Iteration 18, loss = 0.91955176\n",
            "Iteration 19, loss = 0.90227991\n",
            "Iteration 20, loss = 0.88594294\n",
            "Iteration 21, loss = 0.87069089\n",
            "Iteration 22, loss = 0.85667586\n",
            "Iteration 23, loss = 0.84359414\n",
            "Iteration 24, loss = 0.83093219\n",
            "Iteration 25, loss = 0.81954024\n",
            "Iteration 26, loss = 0.80797193\n",
            "Iteration 27, loss = 0.79813972\n",
            "Iteration 28, loss = 0.78827489\n",
            "Iteration 29, loss = 0.77945534\n",
            "Iteration 30, loss = 0.77167073\n",
            "Iteration 31, loss = 0.76379821\n",
            "Iteration 32, loss = 0.75668072\n",
            "Iteration 33, loss = 0.74977028\n",
            "Iteration 34, loss = 0.74333202\n",
            "Iteration 35, loss = 0.73744851\n",
            "Iteration 36, loss = 0.73224584\n",
            "Iteration 37, loss = 0.72713655\n",
            "Iteration 38, loss = 0.72251036\n",
            "Iteration 39, loss = 0.71809401\n",
            "Iteration 40, loss = 0.71381178\n",
            "Iteration 41, loss = 0.71006817\n",
            "Iteration 42, loss = 0.70642663\n",
            "Iteration 43, loss = 0.70297709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 44, loss = 0.69956808\n",
            "Iteration 45, loss = 0.69665075\n",
            "Iteration 46, loss = 0.69345907\n",
            "Iteration 47, loss = 0.69050376\n",
            "Iteration 48, loss = 0.68787513\n",
            "Iteration 49, loss = 0.68529710\n",
            "Iteration 50, loss = 0.68283550\n",
            "Iteration 51, loss = 0.68057320\n",
            "Iteration 52, loss = 0.67831132\n",
            "Iteration 53, loss = 0.67622728\n",
            "Iteration 54, loss = 0.67421572\n",
            "Iteration 55, loss = 0.67206718\n",
            "Iteration 56, loss = 0.67019947\n",
            "Iteration 57, loss = 0.66839904\n",
            "Iteration 58, loss = 0.66666072\n",
            "Iteration 59, loss = 0.66504789\n",
            "Iteration 60, loss = 0.66353676\n",
            "Iteration 61, loss = 0.66204168\n",
            "Iteration 62, loss = 0.66055577\n",
            "Iteration 63, loss = 0.65901870\n",
            "Iteration 64, loss = 0.65756080\n",
            "Iteration 65, loss = 0.65607636\n",
            "Iteration 66, loss = 0.65469643\n",
            "Iteration 67, loss = 0.65336830\n",
            "Iteration 68, loss = 0.65205637\n",
            "Iteration 69, loss = 0.65084949\n",
            "Iteration 70, loss = 0.64960277\n",
            "Iteration 71, loss = 0.64837394\n",
            "Iteration 72, loss = 0.64713407\n",
            "Iteration 73, loss = 0.64596524\n",
            "Iteration 74, loss = 0.64480252\n",
            "Iteration 75, loss = 0.64368401\n",
            "Iteration 76, loss = 0.64254765\n",
            "Iteration 77, loss = 0.64150617\n",
            "Iteration 78, loss = 0.64039261\n",
            "Iteration 79, loss = 0.63941413\n",
            "Iteration 80, loss = 0.63838386\n",
            "Iteration 81, loss = 0.63743753\n",
            "Iteration 82, loss = 0.63650474\n",
            "Iteration 83, loss = 0.63557633\n",
            "Iteration 84, loss = 0.63469972\n",
            "Iteration 85, loss = 0.63385219\n",
            "Iteration 86, loss = 0.63298910\n",
            "Iteration 87, loss = 0.63211509\n",
            "Iteration 88, loss = 0.63130517\n",
            "Iteration 89, loss = 0.63045727\n",
            "Iteration 90, loss = 0.62971541\n",
            "Iteration 91, loss = 0.62884744\n",
            "Iteration 92, loss = 0.62806282\n",
            "Iteration 93, loss = 0.62730010\n",
            "Iteration 94, loss = 0.62658916\n",
            "Iteration 95, loss = 0.62581543\n",
            "Iteration 96, loss = 0.62512103\n",
            "Iteration 97, loss = 0.62443816\n",
            "Iteration 98, loss = 0.62373486\n",
            "Iteration 99, loss = 0.62304011\n",
            "Iteration 100, loss = 0.62238348\n",
            "Iteration 101, loss = 0.62169762\n",
            "Iteration 102, loss = 0.62100535\n",
            "Iteration 103, loss = 0.62032091\n",
            "Iteration 104, loss = 0.61961880\n",
            "Iteration 105, loss = 0.61893901\n",
            "Iteration 106, loss = 0.61820553\n",
            "Iteration 107, loss = 0.61757926\n",
            "Iteration 108, loss = 0.61690315\n",
            "Iteration 109, loss = 0.61627656\n",
            "Iteration 110, loss = 0.61564412\n",
            "Iteration 111, loss = 0.61502192\n",
            "Iteration 112, loss = 0.61441879\n",
            "Iteration 113, loss = 0.61384866\n",
            "Iteration 114, loss = 0.61324066\n",
            "Iteration 115, loss = 0.61265048\n",
            "Iteration 116, loss = 0.61203885\n",
            "Iteration 117, loss = 0.61143757\n",
            "Iteration 118, loss = 0.61083038\n",
            "Iteration 119, loss = 0.61019121\n",
            "Iteration 120, loss = 0.60962254\n",
            "Iteration 121, loss = 0.60904441\n",
            "Iteration 122, loss = 0.60850742\n",
            "Iteration 123, loss = 0.60796362\n",
            "Iteration 124, loss = 0.60739810\n",
            "Iteration 125, loss = 0.60686236\n",
            "Iteration 126, loss = 0.60632434\n",
            "Iteration 127, loss = 0.60582041\n",
            "Iteration 128, loss = 0.60527198\n",
            "Iteration 129, loss = 0.60476835\n",
            "Iteration 130, loss = 0.60423221\n",
            "Iteration 131, loss = 0.60373134\n",
            "Iteration 132, loss = 0.60320781\n",
            "Iteration 133, loss = 0.60265126\n",
            "Iteration 134, loss = 0.60212403\n",
            "Iteration 135, loss = 0.60161982\n",
            "Iteration 136, loss = 0.60110169\n",
            "Iteration 137, loss = 0.60061041\n",
            "Iteration 138, loss = 0.60013873\n",
            "Iteration 139, loss = 0.59963681\n",
            "Iteration 140, loss = 0.59914936\n",
            "Iteration 141, loss = 0.59866027\n",
            "Iteration 142, loss = 0.59821810\n",
            "Iteration 143, loss = 0.59770969\n",
            "Iteration 144, loss = 0.59729074\n",
            "Iteration 145, loss = 0.59684055\n",
            "Iteration 146, loss = 0.59639239\n",
            "Iteration 147, loss = 0.59598622\n",
            "Iteration 148, loss = 0.59557002\n",
            "Iteration 149, loss = 0.59513057\n",
            "Iteration 150, loss = 0.59465560\n",
            "Iteration 1, loss = 1.39138454\n",
            "Iteration 2, loss = 1.37512562\n",
            "Iteration 3, loss = 1.34984865\n",
            "Iteration 4, loss = 1.31828804\n",
            "Iteration 5, loss = 1.28379813\n",
            "Iteration 6, loss = 1.24919730\n",
            "Iteration 7, loss = 1.21383002\n",
            "Iteration 8, loss = 1.17922096\n",
            "Iteration 9, loss = 1.14576061\n",
            "Iteration 10, loss = 1.11442987\n",
            "Iteration 11, loss = 1.08433757\n",
            "Iteration 12, loss = 1.05600586\n",
            "Iteration 13, loss = 1.02936580\n",
            "Iteration 14, loss = 1.00449374\n",
            "Iteration 15, loss = 0.98126814\n",
            "Iteration 16, loss = 0.96045322\n",
            "Iteration 17, loss = 0.93985076\n",
            "Iteration 18, loss = 0.92131534\n",
            "Iteration 19, loss = 0.90427757\n",
            "Iteration 20, loss = 0.88794149\n",
            "Iteration 21, loss = 0.87257105\n",
            "Iteration 22, loss = 0.85865619\n",
            "Iteration 23, loss = 0.84553420\n",
            "Iteration 24, loss = 0.83288102\n",
            "Iteration 25, loss = 0.82164075\n",
            "Iteration 26, loss = 0.81031884\n",
            "Iteration 27, loss = 0.80044622\n",
            "Iteration 28, loss = 0.79078083\n",
            "Iteration 29, loss = 0.78199497\n",
            "Iteration 30, loss = 0.77411266\n",
            "Iteration 31, loss = 0.76624354\n",
            "Iteration 32, loss = 0.75938182\n",
            "Iteration 33, loss = 0.75241517\n",
            "Iteration 34, loss = 0.74614686\n",
            "Iteration 35, loss = 0.74020298\n",
            "Iteration 36, loss = 0.73504213\n",
            "Iteration 37, loss = 0.72986011\n",
            "Iteration 38, loss = 0.72514082\n",
            "Iteration 39, loss = 0.72075279\n",
            "Iteration 40, loss = 0.71636099\n",
            "Iteration 41, loss = 0.71262350\n",
            "Iteration 42, loss = 0.70892549\n",
            "Iteration 43, loss = 0.70543635\n",
            "Iteration 44, loss = 0.70195493\n",
            "Iteration 45, loss = 0.69894456\n",
            "Iteration 46, loss = 0.69579898\n",
            "Iteration 47, loss = 0.69295315\n",
            "Iteration 48, loss = 0.69035026\n",
            "Iteration 49, loss = 0.68781951\n",
            "Iteration 50, loss = 0.68550089\n",
            "Iteration 51, loss = 0.68324006\n",
            "Iteration 52, loss = 0.68114943\n",
            "Iteration 53, loss = 0.67907425\n",
            "Iteration 54, loss = 0.67705865\n",
            "Iteration 55, loss = 0.67497031\n",
            "Iteration 56, loss = 0.67311956\n",
            "Iteration 57, loss = 0.67123379\n",
            "Iteration 58, loss = 0.66945703\n",
            "Iteration 59, loss = 0.66776204\n",
            "Iteration 60, loss = 0.66623540\n",
            "Iteration 61, loss = 0.66472561\n",
            "Iteration 62, loss = 0.66332128\n",
            "Iteration 63, loss = 0.66181759\n",
            "Iteration 64, loss = 0.66047596\n",
            "Iteration 65, loss = 0.65908815\n",
            "Iteration 66, loss = 0.65776076\n",
            "Iteration 67, loss = 0.65648902\n",
            "Iteration 68, loss = 0.65521748\n",
            "Iteration 69, loss = 0.65400813\n",
            "Iteration 70, loss = 0.65278035\n",
            "Iteration 71, loss = 0.65149094\n",
            "Iteration 72, loss = 0.65028077\n",
            "Iteration 73, loss = 0.64905842\n",
            "Iteration 74, loss = 0.64790424\n",
            "Iteration 75, loss = 0.64677162\n",
            "Iteration 76, loss = 0.64562035\n",
            "Iteration 77, loss = 0.64452933\n",
            "Iteration 78, loss = 0.64345644\n",
            "Iteration 79, loss = 0.64241040\n",
            "Iteration 80, loss = 0.64140986\n",
            "Iteration 81, loss = 0.64043956\n",
            "Iteration 82, loss = 0.63949342\n",
            "Iteration 83, loss = 0.63859004\n",
            "Iteration 84, loss = 0.63771080\n",
            "Iteration 85, loss = 0.63683352\n",
            "Iteration 86, loss = 0.63594807\n",
            "Iteration 87, loss = 0.63506056\n",
            "Iteration 88, loss = 0.63421892\n",
            "Iteration 89, loss = 0.63337974\n",
            "Iteration 90, loss = 0.63261508\n",
            "Iteration 91, loss = 0.63177660\n",
            "Iteration 92, loss = 0.63099911\n",
            "Iteration 93, loss = 0.63024576\n",
            "Iteration 94, loss = 0.62950227\n",
            "Iteration 95, loss = 0.62874205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 96, loss = 0.62802933\n",
            "Iteration 97, loss = 0.62731275\n",
            "Iteration 98, loss = 0.62657765\n",
            "Iteration 99, loss = 0.62582972\n",
            "Iteration 100, loss = 0.62512388\n",
            "Iteration 101, loss = 0.62440886\n",
            "Iteration 102, loss = 0.62369126\n",
            "Iteration 103, loss = 0.62301138\n",
            "Iteration 104, loss = 0.62231346\n",
            "Iteration 105, loss = 0.62164193\n",
            "Iteration 106, loss = 0.62091321\n",
            "Iteration 107, loss = 0.62027969\n",
            "Iteration 108, loss = 0.61963396\n",
            "Iteration 109, loss = 0.61900211\n",
            "Iteration 110, loss = 0.61838819\n",
            "Iteration 111, loss = 0.61778817\n",
            "Iteration 112, loss = 0.61718123\n",
            "Iteration 113, loss = 0.61662347\n",
            "Iteration 114, loss = 0.61606190\n",
            "Iteration 115, loss = 0.61547712\n",
            "Iteration 116, loss = 0.61488928\n",
            "Iteration 117, loss = 0.61433814\n",
            "Iteration 118, loss = 0.61377771\n",
            "Iteration 119, loss = 0.61320796\n",
            "Iteration 120, loss = 0.61267437\n",
            "Iteration 121, loss = 0.61214127\n",
            "Iteration 122, loss = 0.61166081\n",
            "Iteration 123, loss = 0.61117147\n",
            "Iteration 124, loss = 0.61067739\n",
            "Iteration 125, loss = 0.61019738\n",
            "Iteration 126, loss = 0.60970918\n",
            "Iteration 127, loss = 0.60927424\n",
            "Iteration 128, loss = 0.60878360\n",
            "Iteration 129, loss = 0.60829878\n",
            "Iteration 130, loss = 0.60781020\n",
            "Iteration 131, loss = 0.60733642\n",
            "Iteration 132, loss = 0.60684527\n",
            "Iteration 133, loss = 0.60634604\n",
            "Iteration 134, loss = 0.60588950\n",
            "Iteration 135, loss = 0.60540424\n",
            "Iteration 136, loss = 0.60493219\n",
            "Iteration 137, loss = 0.60446340\n",
            "Iteration 138, loss = 0.60405119\n",
            "Iteration 139, loss = 0.60358573\n",
            "Iteration 140, loss = 0.60316252\n",
            "Iteration 141, loss = 0.60271924\n",
            "Iteration 142, loss = 0.60230785\n",
            "Iteration 143, loss = 0.60185146\n",
            "Iteration 144, loss = 0.60144689\n",
            "Iteration 145, loss = 0.60102438\n",
            "Iteration 146, loss = 0.60057588\n",
            "Iteration 147, loss = 0.60019066\n",
            "Iteration 148, loss = 0.59977412\n",
            "Iteration 149, loss = 0.59935245\n",
            "Iteration 150, loss = 0.59893304\n",
            "Iteration 1, loss = 1.36378915\n",
            "Iteration 2, loss = 1.34879364\n",
            "Iteration 3, loss = 1.32549217\n",
            "Iteration 4, loss = 1.29708034\n",
            "Iteration 5, loss = 1.26554033\n",
            "Iteration 6, loss = 1.23383872\n",
            "Iteration 7, loss = 1.20200469\n",
            "Iteration 8, loss = 1.17053414\n",
            "Iteration 9, loss = 1.14064775\n",
            "Iteration 10, loss = 1.11151975\n",
            "Iteration 11, loss = 1.08406253\n",
            "Iteration 12, loss = 1.05814511\n",
            "Iteration 13, loss = 1.03354346\n",
            "Iteration 14, loss = 1.01070814\n",
            "Iteration 15, loss = 0.98890098\n",
            "Iteration 16, loss = 0.96886449\n",
            "Iteration 17, loss = 0.94923104\n",
            "Iteration 18, loss = 0.93151074\n",
            "Iteration 19, loss = 0.91502500\n",
            "Iteration 20, loss = 0.89945880\n",
            "Iteration 21, loss = 0.88472298\n",
            "Iteration 22, loss = 0.87121456\n",
            "Iteration 23, loss = 0.85846639\n",
            "Iteration 24, loss = 0.84606854\n",
            "Iteration 25, loss = 0.83470170\n",
            "Iteration 26, loss = 0.82388112\n",
            "Iteration 27, loss = 0.81424012\n",
            "Iteration 28, loss = 0.80487305\n",
            "Iteration 29, loss = 0.79605592\n",
            "Iteration 30, loss = 0.78816723\n",
            "Iteration 31, loss = 0.78027235\n",
            "Iteration 32, loss = 0.77329705\n",
            "Iteration 33, loss = 0.76630727\n",
            "Iteration 34, loss = 0.75998863\n",
            "Iteration 35, loss = 0.75396852\n",
            "Iteration 36, loss = 0.74833678\n",
            "Iteration 37, loss = 0.74285837\n",
            "Iteration 38, loss = 0.73803442\n",
            "Iteration 39, loss = 0.73341573\n",
            "Iteration 40, loss = 0.72891424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 41, loss = 0.72504769\n",
            "Iteration 42, loss = 0.72109686\n",
            "Iteration 43, loss = 0.71757315\n",
            "Iteration 44, loss = 0.71400332\n",
            "Iteration 45, loss = 0.71081284\n",
            "Iteration 46, loss = 0.70768563\n",
            "Iteration 47, loss = 0.70471192\n",
            "Iteration 48, loss = 0.70194281\n",
            "Iteration 49, loss = 0.69925315\n",
            "Iteration 50, loss = 0.69669744\n",
            "Iteration 51, loss = 0.69424636\n",
            "Iteration 52, loss = 0.69189830\n",
            "Iteration 53, loss = 0.68973931\n",
            "Iteration 54, loss = 0.68760345\n",
            "Iteration 55, loss = 0.68543024\n",
            "Iteration 56, loss = 0.68342348\n",
            "Iteration 57, loss = 0.68147262\n",
            "Iteration 58, loss = 0.67964124\n",
            "Iteration 59, loss = 0.67787307\n",
            "Iteration 60, loss = 0.67621673\n",
            "Iteration 61, loss = 0.67462365\n",
            "Iteration 62, loss = 0.67309221\n",
            "Iteration 63, loss = 0.67150391\n",
            "Iteration 64, loss = 0.67007641\n",
            "Iteration 65, loss = 0.66858230\n",
            "Iteration 66, loss = 0.66714332\n",
            "Iteration 67, loss = 0.66581116\n",
            "Iteration 68, loss = 0.66446415\n",
            "Iteration 69, loss = 0.66313131\n",
            "Iteration 70, loss = 0.66182390\n",
            "Iteration 71, loss = 0.66046432\n",
            "Iteration 72, loss = 0.65919492\n",
            "Iteration 73, loss = 0.65790677\n",
            "Iteration 74, loss = 0.65671474\n",
            "Iteration 75, loss = 0.65551452\n",
            "Iteration 76, loss = 0.65432494\n",
            "Iteration 77, loss = 0.65319837\n",
            "Iteration 78, loss = 0.65212463\n",
            "Iteration 79, loss = 0.65105388\n",
            "Iteration 80, loss = 0.65003300\n",
            "Iteration 81, loss = 0.64903759\n",
            "Iteration 82, loss = 0.64809585\n",
            "Iteration 83, loss = 0.64717154\n",
            "Iteration 84, loss = 0.64626768\n",
            "Iteration 85, loss = 0.64537314\n",
            "Iteration 86, loss = 0.64449899\n",
            "Iteration 87, loss = 0.64359621\n",
            "Iteration 88, loss = 0.64275708\n",
            "Iteration 89, loss = 0.64191812\n",
            "Iteration 90, loss = 0.64113195\n",
            "Iteration 91, loss = 0.64030779\n",
            "Iteration 92, loss = 0.63949773\n",
            "Iteration 93, loss = 0.63875682\n",
            "Iteration 94, loss = 0.63795612\n",
            "Iteration 95, loss = 0.63717242\n",
            "Iteration 96, loss = 0.63645109\n",
            "Iteration 97, loss = 0.63570073\n",
            "Iteration 98, loss = 0.63498115\n",
            "Iteration 99, loss = 0.63421861\n",
            "Iteration 100, loss = 0.63348999\n",
            "Iteration 101, loss = 0.63275874\n",
            "Iteration 102, loss = 0.63202608\n",
            "Iteration 103, loss = 0.63134886\n",
            "Iteration 104, loss = 0.63066372\n",
            "Iteration 105, loss = 0.63001472\n",
            "Iteration 106, loss = 0.62930887\n",
            "Iteration 107, loss = 0.62867911\n",
            "Iteration 108, loss = 0.62807512\n",
            "Iteration 109, loss = 0.62747870\n",
            "Iteration 110, loss = 0.62685895\n",
            "Iteration 111, loss = 0.62630162\n",
            "Iteration 112, loss = 0.62570594\n",
            "Iteration 113, loss = 0.62516088\n",
            "Iteration 114, loss = 0.62457410\n",
            "Iteration 115, loss = 0.62400773\n",
            "Iteration 116, loss = 0.62342274\n",
            "Iteration 117, loss = 0.62285866\n",
            "Iteration 118, loss = 0.62229865\n",
            "Iteration 119, loss = 0.62175565\n",
            "Iteration 120, loss = 0.62119369\n",
            "Iteration 121, loss = 0.62065287\n",
            "Iteration 122, loss = 0.62014015\n",
            "Iteration 123, loss = 0.61962987\n",
            "Iteration 124, loss = 0.61911487\n",
            "Iteration 125, loss = 0.61863745\n",
            "Iteration 126, loss = 0.61813522\n",
            "Iteration 127, loss = 0.61768179\n",
            "Iteration 128, loss = 0.61719308\n",
            "Iteration 129, loss = 0.61668202\n",
            "Iteration 130, loss = 0.61622575\n",
            "Iteration 131, loss = 0.61574837\n",
            "Iteration 132, loss = 0.61529571\n",
            "Iteration 133, loss = 0.61482482\n",
            "Iteration 134, loss = 0.61439165\n",
            "Iteration 135, loss = 0.61393492\n",
            "Iteration 136, loss = 0.61349984\n",
            "Iteration 137, loss = 0.61306190\n",
            "Iteration 138, loss = 0.61265032\n",
            "Iteration 139, loss = 0.61218768\n",
            "Iteration 140, loss = 0.61176824\n",
            "Iteration 141, loss = 0.61134064\n",
            "Iteration 142, loss = 0.61094127\n",
            "Iteration 143, loss = 0.61051579\n",
            "Iteration 144, loss = 0.61011162\n",
            "Iteration 145, loss = 0.60970606\n",
            "Iteration 146, loss = 0.60928174\n",
            "Iteration 147, loss = 0.60888100\n",
            "Iteration 148, loss = 0.60846789\n",
            "Iteration 149, loss = 0.60803492\n",
            "Iteration 150, loss = 0.60762284\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 150 and for layer number 6 : 0.6962499999999998\n",
            "Iteration 1, loss = 0.61693719\n",
            "Iteration 2, loss = 0.61686447\n",
            "Iteration 3, loss = 0.61675151\n",
            "Iteration 4, loss = 0.61661842\n",
            "Iteration 5, loss = 0.61646426\n",
            "Iteration 6, loss = 0.61631571\n",
            "Iteration 7, loss = 0.61614581\n",
            "Iteration 8, loss = 0.61596065\n",
            "Iteration 9, loss = 0.61578398\n",
            "Iteration 10, loss = 0.61560733\n",
            "Iteration 11, loss = 0.61543375\n",
            "Iteration 12, loss = 0.61527975\n",
            "Iteration 13, loss = 0.61511094\n",
            "Iteration 14, loss = 0.61495375\n",
            "Iteration 15, loss = 0.61478984\n",
            "Iteration 16, loss = 0.61463306\n",
            "Iteration 17, loss = 0.61447873\n",
            "Iteration 18, loss = 0.61432621\n",
            "Iteration 19, loss = 0.61417436\n",
            "Iteration 20, loss = 0.61400728\n",
            "Iteration 21, loss = 0.61383297\n",
            "Iteration 22, loss = 0.61367462\n",
            "Iteration 23, loss = 0.61350652\n",
            "Iteration 24, loss = 0.61335359\n",
            "Iteration 25, loss = 0.61319555\n",
            "Iteration 26, loss = 0.61306514\n",
            "Iteration 27, loss = 0.61290532\n",
            "Iteration 28, loss = 0.61277722\n",
            "Iteration 29, loss = 0.61265346\n",
            "Iteration 30, loss = 0.61251235\n",
            "Iteration 31, loss = 0.61238930\n",
            "Iteration 32, loss = 0.61226155\n",
            "Iteration 33, loss = 0.61213591\n",
            "Iteration 34, loss = 0.61202127\n",
            "Iteration 35, loss = 0.61189676\n",
            "Iteration 36, loss = 0.61176314\n",
            "Iteration 37, loss = 0.61162506\n",
            "Iteration 38, loss = 0.61149544\n",
            "Iteration 39, loss = 0.61137006\n",
            "Iteration 40, loss = 0.61124508\n",
            "Iteration 41, loss = 0.61110902\n",
            "Iteration 42, loss = 0.61099032\n",
            "Iteration 43, loss = 0.61085784\n",
            "Iteration 44, loss = 0.61073928\n",
            "Iteration 45, loss = 0.61062182\n",
            "Iteration 46, loss = 0.61048916\n",
            "Iteration 47, loss = 0.61036991\n",
            "Iteration 48, loss = 0.61024277\n",
            "Iteration 49, loss = 0.61013546\n",
            "Iteration 50, loss = 0.61002089\n",
            "Iteration 51, loss = 0.60990687\n",
            "Iteration 52, loss = 0.60979970\n",
            "Iteration 53, loss = 0.60969865\n",
            "Iteration 54, loss = 0.60959045\n",
            "Iteration 55, loss = 0.60948292\n",
            "Iteration 56, loss = 0.60937738\n",
            "Iteration 57, loss = 0.60926927\n",
            "Iteration 58, loss = 0.60917448\n",
            "Iteration 59, loss = 0.60906914\n",
            "Iteration 60, loss = 0.60896820\n",
            "Iteration 61, loss = 0.60886422\n",
            "Iteration 62, loss = 0.60877775\n",
            "Iteration 63, loss = 0.60867478\n",
            "Iteration 64, loss = 0.60857955\n",
            "Iteration 65, loss = 0.60848439\n",
            "Iteration 66, loss = 0.60838570\n",
            "Iteration 67, loss = 0.60829814\n",
            "Iteration 68, loss = 0.60820577\n",
            "Iteration 69, loss = 0.60812774\n",
            "Iteration 70, loss = 0.60803664\n",
            "Iteration 71, loss = 0.60794173\n",
            "Iteration 72, loss = 0.60785282\n",
            "Iteration 73, loss = 0.60776810\n",
            "Iteration 74, loss = 0.60766500\n",
            "Iteration 75, loss = 0.60758161\n",
            "Iteration 76, loss = 0.60748356\n",
            "Iteration 77, loss = 0.60739894\n",
            "Iteration 78, loss = 0.60731203\n",
            "Iteration 79, loss = 0.60722005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 80, loss = 0.60712968\n",
            "Iteration 81, loss = 0.60704893\n",
            "Iteration 82, loss = 0.60696141\n",
            "Iteration 83, loss = 0.60688701\n",
            "Iteration 84, loss = 0.60679662\n",
            "Iteration 85, loss = 0.60670958\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61771206\n",
            "Iteration 2, loss = 0.61763256\n",
            "Iteration 3, loss = 0.61750896\n",
            "Iteration 4, loss = 0.61736013\n",
            "Iteration 5, loss = 0.61720601\n",
            "Iteration 6, loss = 0.61703925\n",
            "Iteration 7, loss = 0.61687983\n",
            "Iteration 8, loss = 0.61668458\n",
            "Iteration 9, loss = 0.61650897\n",
            "Iteration 10, loss = 0.61634191\n",
            "Iteration 11, loss = 0.61616438\n",
            "Iteration 12, loss = 0.61600889\n",
            "Iteration 13, loss = 0.61582381\n",
            "Iteration 14, loss = 0.61566827\n",
            "Iteration 15, loss = 0.61549674\n",
            "Iteration 16, loss = 0.61534090\n",
            "Iteration 17, loss = 0.61517681\n",
            "Iteration 18, loss = 0.61502298\n",
            "Iteration 19, loss = 0.61486598\n",
            "Iteration 20, loss = 0.61470307\n",
            "Iteration 21, loss = 0.61454151\n",
            "Iteration 22, loss = 0.61438853\n",
            "Iteration 23, loss = 0.61422513\n",
            "Iteration 24, loss = 0.61407653\n",
            "Iteration 25, loss = 0.61391791\n",
            "Iteration 26, loss = 0.61377394\n",
            "Iteration 27, loss = 0.61362218\n",
            "Iteration 28, loss = 0.61348831\n",
            "Iteration 29, loss = 0.61334584\n",
            "Iteration 30, loss = 0.61321506\n",
            "Iteration 31, loss = 0.61309244\n",
            "Iteration 32, loss = 0.61295282\n",
            "Iteration 33, loss = 0.61282899\n",
            "Iteration 34, loss = 0.61269835\n",
            "Iteration 35, loss = 0.61258404\n",
            "Iteration 36, loss = 0.61244344\n",
            "Iteration 37, loss = 0.61230626\n",
            "Iteration 38, loss = 0.61216372\n",
            "Iteration 39, loss = 0.61203699\n",
            "Iteration 40, loss = 0.61189655\n",
            "Iteration 41, loss = 0.61175462\n",
            "Iteration 42, loss = 0.61161427\n",
            "Iteration 43, loss = 0.61148385\n",
            "Iteration 44, loss = 0.61135062\n",
            "Iteration 45, loss = 0.61122322\n",
            "Iteration 46, loss = 0.61108342\n",
            "Iteration 47, loss = 0.61094533\n",
            "Iteration 48, loss = 0.61081445\n",
            "Iteration 49, loss = 0.61068681\n",
            "Iteration 50, loss = 0.61056174\n",
            "Iteration 51, loss = 0.61044813\n",
            "Iteration 52, loss = 0.61033493\n",
            "Iteration 53, loss = 0.61021829\n",
            "Iteration 54, loss = 0.61010919\n",
            "Iteration 55, loss = 0.61000586\n",
            "Iteration 56, loss = 0.60988739\n",
            "Iteration 57, loss = 0.60976884\n",
            "Iteration 58, loss = 0.60966597\n",
            "Iteration 59, loss = 0.60955587\n",
            "Iteration 60, loss = 0.60944274\n",
            "Iteration 61, loss = 0.60932539\n",
            "Iteration 62, loss = 0.60923146\n",
            "Iteration 63, loss = 0.60909925\n",
            "Iteration 64, loss = 0.60898774\n",
            "Iteration 65, loss = 0.60887739\n",
            "Iteration 66, loss = 0.60875534\n",
            "Iteration 67, loss = 0.60864213\n",
            "Iteration 68, loss = 0.60852801\n",
            "Iteration 69, loss = 0.60842906\n",
            "Iteration 70, loss = 0.60832290\n",
            "Iteration 71, loss = 0.60821626\n",
            "Iteration 72, loss = 0.60811197\n",
            "Iteration 73, loss = 0.60800907\n",
            "Iteration 74, loss = 0.60789491\n",
            "Iteration 75, loss = 0.60780669\n",
            "Iteration 76, loss = 0.60769204\n",
            "Iteration 77, loss = 0.60759716\n",
            "Iteration 78, loss = 0.60749301\n",
            "Iteration 79, loss = 0.60738373\n",
            "Iteration 80, loss = 0.60728635\n",
            "Iteration 81, loss = 0.60719595\n",
            "Iteration 82, loss = 0.60709942\n",
            "Iteration 83, loss = 0.60701638\n",
            "Iteration 84, loss = 0.60692635\n",
            "Iteration 85, loss = 0.60683945\n",
            "Iteration 86, loss = 0.60675609\n",
            "Iteration 87, loss = 0.60667504\n",
            "Iteration 88, loss = 0.60659137\n",
            "Iteration 89, loss = 0.60651963\n",
            "Iteration 90, loss = 0.60642717\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61763916\n",
            "Iteration 2, loss = 0.61755997\n",
            "Iteration 3, loss = 0.61742232\n",
            "Iteration 4, loss = 0.61725393\n",
            "Iteration 5, loss = 0.61708619\n",
            "Iteration 6, loss = 0.61689799\n",
            "Iteration 7, loss = 0.61671618\n",
            "Iteration 8, loss = 0.61650164\n",
            "Iteration 9, loss = 0.61630973\n",
            "Iteration 10, loss = 0.61611724\n",
            "Iteration 11, loss = 0.61593663\n",
            "Iteration 12, loss = 0.61576363\n",
            "Iteration 13, loss = 0.61556226\n",
            "Iteration 14, loss = 0.61538666\n",
            "Iteration 15, loss = 0.61520889\n",
            "Iteration 16, loss = 0.61503323\n",
            "Iteration 17, loss = 0.61486521\n",
            "Iteration 18, loss = 0.61470006\n",
            "Iteration 19, loss = 0.61453828\n",
            "Iteration 20, loss = 0.61437299\n",
            "Iteration 21, loss = 0.61421653\n",
            "Iteration 22, loss = 0.61405569\n",
            "Iteration 23, loss = 0.61388640\n",
            "Iteration 24, loss = 0.61373448\n",
            "Iteration 25, loss = 0.61357646\n",
            "Iteration 26, loss = 0.61342231\n",
            "Iteration 27, loss = 0.61327118\n",
            "Iteration 28, loss = 0.61311335\n",
            "Iteration 29, loss = 0.61297079\n",
            "Iteration 30, loss = 0.61282693\n",
            "Iteration 31, loss = 0.61269890\n",
            "Iteration 32, loss = 0.61255474\n",
            "Iteration 33, loss = 0.61241223\n",
            "Iteration 34, loss = 0.61226940\n",
            "Iteration 35, loss = 0.61213160\n",
            "Iteration 36, loss = 0.61198811\n",
            "Iteration 37, loss = 0.61184581\n",
            "Iteration 38, loss = 0.61170049\n",
            "Iteration 39, loss = 0.61157611\n",
            "Iteration 40, loss = 0.61142730\n",
            "Iteration 41, loss = 0.61127667\n",
            "Iteration 42, loss = 0.61113732\n",
            "Iteration 43, loss = 0.61100411\n",
            "Iteration 44, loss = 0.61086913\n",
            "Iteration 45, loss = 0.61073484\n",
            "Iteration 46, loss = 0.61060145\n",
            "Iteration 47, loss = 0.61046314\n",
            "Iteration 48, loss = 0.61033730\n",
            "Iteration 49, loss = 0.61020822\n",
            "Iteration 50, loss = 0.61007740\n",
            "Iteration 51, loss = 0.60996338\n",
            "Iteration 52, loss = 0.60985178\n",
            "Iteration 53, loss = 0.60973070\n",
            "Iteration 54, loss = 0.60961527\n",
            "Iteration 55, loss = 0.60949856\n",
            "Iteration 56, loss = 0.60938041\n",
            "Iteration 57, loss = 0.60926013\n",
            "Iteration 58, loss = 0.60914580\n",
            "Iteration 59, loss = 0.60902755\n",
            "Iteration 60, loss = 0.60890665\n",
            "Iteration 61, loss = 0.60878651\n",
            "Iteration 62, loss = 0.60867591\n",
            "Iteration 63, loss = 0.60854401\n",
            "Iteration 64, loss = 0.60842057\n",
            "Iteration 65, loss = 0.60829508\n",
            "Iteration 66, loss = 0.60817432\n",
            "Iteration 67, loss = 0.60805562\n",
            "Iteration 68, loss = 0.60794420\n",
            "Iteration 69, loss = 0.60783815\n",
            "Iteration 70, loss = 0.60773163\n",
            "Iteration 71, loss = 0.60763356\n",
            "Iteration 72, loss = 0.60751712\n",
            "Iteration 73, loss = 0.60740617\n",
            "Iteration 74, loss = 0.60728371\n",
            "Iteration 75, loss = 0.60719874\n",
            "Iteration 76, loss = 0.60707233\n",
            "Iteration 77, loss = 0.60697847\n",
            "Iteration 78, loss = 0.60686966\n",
            "Iteration 79, loss = 0.60676477\n",
            "Iteration 80, loss = 0.60666713\n",
            "Iteration 81, loss = 0.60657029\n",
            "Iteration 82, loss = 0.60647206\n",
            "Iteration 83, loss = 0.60638523\n",
            "Iteration 84, loss = 0.60629166\n",
            "Iteration 85, loss = 0.60620042\n",
            "Iteration 86, loss = 0.60612214\n",
            "Iteration 87, loss = 0.60604223\n",
            "Iteration 88, loss = 0.60596838\n",
            "Iteration 89, loss = 0.60589523\n",
            "Iteration 90, loss = 0.60580521\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62088357\n",
            "Iteration 2, loss = 0.62080758\n",
            "Iteration 3, loss = 0.62069495\n",
            "Iteration 4, loss = 0.62054238\n",
            "Iteration 5, loss = 0.62038891\n",
            "Iteration 6, loss = 0.62021289\n",
            "Iteration 7, loss = 0.62003518\n",
            "Iteration 8, loss = 0.61983724\n",
            "Iteration 9, loss = 0.61965125\n",
            "Iteration 10, loss = 0.61946574\n",
            "Iteration 11, loss = 0.61928678\n",
            "Iteration 12, loss = 0.61912370\n",
            "Iteration 13, loss = 0.61894901\n",
            "Iteration 14, loss = 0.61877305\n",
            "Iteration 15, loss = 0.61860692\n",
            "Iteration 16, loss = 0.61844169\n",
            "Iteration 17, loss = 0.61826895\n",
            "Iteration 18, loss = 0.61810540\n",
            "Iteration 19, loss = 0.61794667\n",
            "Iteration 20, loss = 0.61779409\n",
            "Iteration 21, loss = 0.61765462\n",
            "Iteration 22, loss = 0.61749969\n",
            "Iteration 23, loss = 0.61734374\n",
            "Iteration 24, loss = 0.61718970\n",
            "Iteration 25, loss = 0.61702731\n",
            "Iteration 26, loss = 0.61686454\n",
            "Iteration 27, loss = 0.61672331\n",
            "Iteration 28, loss = 0.61655046\n",
            "Iteration 29, loss = 0.61640417\n",
            "Iteration 30, loss = 0.61625657\n",
            "Iteration 31, loss = 0.61612342\n",
            "Iteration 32, loss = 0.61597996\n",
            "Iteration 33, loss = 0.61583342\n",
            "Iteration 34, loss = 0.61569987\n",
            "Iteration 35, loss = 0.61556063\n",
            "Iteration 36, loss = 0.61542958\n",
            "Iteration 37, loss = 0.61530817\n",
            "Iteration 38, loss = 0.61517959\n",
            "Iteration 39, loss = 0.61506558\n",
            "Iteration 40, loss = 0.61494687\n",
            "Iteration 41, loss = 0.61483037\n",
            "Iteration 42, loss = 0.61470883\n",
            "Iteration 43, loss = 0.61460197\n",
            "Iteration 44, loss = 0.61447871\n",
            "Iteration 45, loss = 0.61436047\n",
            "Iteration 46, loss = 0.61425494\n",
            "Iteration 47, loss = 0.61414539\n",
            "Iteration 48, loss = 0.61403406\n",
            "Iteration 49, loss = 0.61392290\n",
            "Iteration 50, loss = 0.61380453\n",
            "Iteration 51, loss = 0.61371073\n",
            "Iteration 52, loss = 0.61360963\n",
            "Iteration 53, loss = 0.61350809\n",
            "Iteration 54, loss = 0.61341021\n",
            "Iteration 55, loss = 0.61331563\n",
            "Iteration 56, loss = 0.61322032\n",
            "Iteration 57, loss = 0.61312362\n",
            "Iteration 58, loss = 0.61303187\n",
            "Iteration 59, loss = 0.61293793\n",
            "Iteration 60, loss = 0.61284660\n",
            "Iteration 61, loss = 0.61275442\n",
            "Iteration 62, loss = 0.61266764\n",
            "Iteration 63, loss = 0.61256719\n",
            "Iteration 64, loss = 0.61246187\n",
            "Iteration 65, loss = 0.61235746\n",
            "Iteration 66, loss = 0.61225596\n",
            "Iteration 67, loss = 0.61215694\n",
            "Iteration 68, loss = 0.61206416\n",
            "Iteration 69, loss = 0.61197032\n",
            "Iteration 70, loss = 0.61187100\n",
            "Iteration 71, loss = 0.61178403\n",
            "Iteration 72, loss = 0.61167796\n",
            "Iteration 73, loss = 0.61157994\n",
            "Iteration 74, loss = 0.61147541\n",
            "Iteration 75, loss = 0.61140259\n",
            "Iteration 76, loss = 0.61129541\n",
            "Iteration 77, loss = 0.61120835\n",
            "Iteration 78, loss = 0.61111527\n",
            "Iteration 79, loss = 0.61103216\n",
            "Iteration 80, loss = 0.61095465\n",
            "Iteration 81, loss = 0.61087089\n",
            "Iteration 82, loss = 0.61079131\n",
            "Iteration 83, loss = 0.61071503\n",
            "Iteration 84, loss = 0.61063483\n",
            "Iteration 85, loss = 0.61055422\n",
            "Iteration 86, loss = 0.61048168\n",
            "Iteration 87, loss = 0.61040607\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62018025\n",
            "Iteration 2, loss = 0.62009819\n",
            "Iteration 3, loss = 0.61998317\n",
            "Iteration 4, loss = 0.61981666\n",
            "Iteration 5, loss = 0.61963710\n",
            "Iteration 6, loss = 0.61944478\n",
            "Iteration 7, loss = 0.61925290\n",
            "Iteration 8, loss = 0.61903845\n",
            "Iteration 9, loss = 0.61885557\n",
            "Iteration 10, loss = 0.61866268\n",
            "Iteration 11, loss = 0.61848604\n",
            "Iteration 12, loss = 0.61833134\n",
            "Iteration 13, loss = 0.61815251\n",
            "Iteration 14, loss = 0.61796856\n",
            "Iteration 15, loss = 0.61780050\n",
            "Iteration 16, loss = 0.61763388\n",
            "Iteration 17, loss = 0.61745805\n",
            "Iteration 18, loss = 0.61727872\n",
            "Iteration 19, loss = 0.61709604\n",
            "Iteration 20, loss = 0.61693584\n",
            "Iteration 21, loss = 0.61678432\n",
            "Iteration 22, loss = 0.61660435\n",
            "Iteration 23, loss = 0.61645117\n",
            "Iteration 24, loss = 0.61629760\n",
            "Iteration 25, loss = 0.61613359\n",
            "Iteration 26, loss = 0.61598174\n",
            "Iteration 27, loss = 0.61583842\n",
            "Iteration 28, loss = 0.61567659\n",
            "Iteration 29, loss = 0.61552571\n",
            "Iteration 30, loss = 0.61538139\n",
            "Iteration 31, loss = 0.61524275\n",
            "Iteration 32, loss = 0.61509965\n",
            "Iteration 33, loss = 0.61494620\n",
            "Iteration 34, loss = 0.61481205\n",
            "Iteration 35, loss = 0.61466606\n",
            "Iteration 36, loss = 0.61454011\n",
            "Iteration 37, loss = 0.61442324\n",
            "Iteration 38, loss = 0.61429471\n",
            "Iteration 39, loss = 0.61417885\n",
            "Iteration 40, loss = 0.61406637\n",
            "Iteration 41, loss = 0.61394071\n",
            "Iteration 42, loss = 0.61382750\n",
            "Iteration 43, loss = 0.61370365\n",
            "Iteration 44, loss = 0.61359746\n",
            "Iteration 45, loss = 0.61346493\n",
            "Iteration 46, loss = 0.61335820\n",
            "Iteration 47, loss = 0.61324493\n",
            "Iteration 48, loss = 0.61312768\n",
            "Iteration 49, loss = 0.61300162\n",
            "Iteration 50, loss = 0.61287863\n",
            "Iteration 51, loss = 0.61278578\n",
            "Iteration 52, loss = 0.61266496\n",
            "Iteration 53, loss = 0.61256068\n",
            "Iteration 54, loss = 0.61244833\n",
            "Iteration 55, loss = 0.61235408\n",
            "Iteration 56, loss = 0.61225595\n",
            "Iteration 57, loss = 0.61215760\n",
            "Iteration 58, loss = 0.61206425\n",
            "Iteration 59, loss = 0.61197433\n",
            "Iteration 60, loss = 0.61188279\n",
            "Iteration 61, loss = 0.61179557\n",
            "Iteration 62, loss = 0.61170404\n",
            "Iteration 63, loss = 0.61161334\n",
            "Iteration 64, loss = 0.61151561\n",
            "Iteration 65, loss = 0.61141306\n",
            "Iteration 66, loss = 0.61130998\n",
            "Iteration 67, loss = 0.61122290\n",
            "Iteration 68, loss = 0.61112557\n",
            "Iteration 69, loss = 0.61103638\n",
            "Iteration 70, loss = 0.61095068\n",
            "Iteration 71, loss = 0.61087186\n",
            "Iteration 72, loss = 0.61077796\n",
            "Iteration 73, loss = 0.61069137\n",
            "Iteration 74, loss = 0.61060678\n",
            "Iteration 75, loss = 0.61052912\n",
            "Iteration 76, loss = 0.61045012\n",
            "Iteration 77, loss = 0.61037871\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 200 and for layer number 2 : 0.7\n",
            "Iteration 1, loss = 0.84801653\n",
            "Iteration 2, loss = 0.84528764\n",
            "Iteration 3, loss = 0.84117560\n",
            "Iteration 4, loss = 0.83582942\n",
            "Iteration 5, loss = 0.82975974\n",
            "Iteration 6, loss = 0.82353252\n",
            "Iteration 7, loss = 0.81664243\n",
            "Iteration 8, loss = 0.81002340\n",
            "Iteration 9, loss = 0.80368440\n",
            "Iteration 10, loss = 0.79741786\n",
            "Iteration 11, loss = 0.79116889\n",
            "Iteration 12, loss = 0.78479499\n",
            "Iteration 13, loss = 0.77897234\n",
            "Iteration 14, loss = 0.77276067\n",
            "Iteration 15, loss = 0.76728918\n",
            "Iteration 16, loss = 0.76206401\n",
            "Iteration 17, loss = 0.75702104\n",
            "Iteration 18, loss = 0.75199301\n",
            "Iteration 19, loss = 0.74737027\n",
            "Iteration 20, loss = 0.74276500\n",
            "Iteration 21, loss = 0.73837289\n",
            "Iteration 22, loss = 0.73407699\n",
            "Iteration 23, loss = 0.73007924\n",
            "Iteration 24, loss = 0.72625222\n",
            "Iteration 25, loss = 0.72244250\n",
            "Iteration 26, loss = 0.71862878\n",
            "Iteration 27, loss = 0.71502063\n",
            "Iteration 28, loss = 0.71165062\n",
            "Iteration 29, loss = 0.70825508\n",
            "Iteration 30, loss = 0.70488572\n",
            "Iteration 31, loss = 0.70184174\n",
            "Iteration 32, loss = 0.69891904\n",
            "Iteration 33, loss = 0.69611030\n",
            "Iteration 34, loss = 0.69321969\n",
            "Iteration 35, loss = 0.69047473\n",
            "Iteration 36, loss = 0.68810891\n",
            "Iteration 37, loss = 0.68540721\n",
            "Iteration 38, loss = 0.68303292\n",
            "Iteration 39, loss = 0.68048045\n",
            "Iteration 40, loss = 0.67779155\n",
            "Iteration 41, loss = 0.67527131\n",
            "Iteration 42, loss = 0.67280099\n",
            "Iteration 43, loss = 0.67046862\n",
            "Iteration 44, loss = 0.66827526\n",
            "Iteration 45, loss = 0.66614495\n",
            "Iteration 46, loss = 0.66422713\n",
            "Iteration 47, loss = 0.66229631\n",
            "Iteration 48, loss = 0.66059117\n",
            "Iteration 49, loss = 0.65875777\n",
            "Iteration 50, loss = 0.65706317\n",
            "Iteration 51, loss = 0.65538023\n",
            "Iteration 52, loss = 0.65371124\n",
            "Iteration 53, loss = 0.65216698\n",
            "Iteration 54, loss = 0.65065006\n",
            "Iteration 55, loss = 0.64920995\n",
            "Iteration 56, loss = 0.64790729\n",
            "Iteration 57, loss = 0.64652529\n",
            "Iteration 58, loss = 0.64516241\n",
            "Iteration 59, loss = 0.64389573\n",
            "Iteration 60, loss = 0.64264924\n",
            "Iteration 61, loss = 0.64145436\n",
            "Iteration 62, loss = 0.64022518\n",
            "Iteration 63, loss = 0.63902072\n",
            "Iteration 64, loss = 0.63780270\n",
            "Iteration 65, loss = 0.63673321\n",
            "Iteration 66, loss = 0.63556199\n",
            "Iteration 67, loss = 0.63443060\n",
            "Iteration 68, loss = 0.63346065\n",
            "Iteration 69, loss = 0.63238086\n",
            "Iteration 70, loss = 0.63142079\n",
            "Iteration 71, loss = 0.63054257\n",
            "Iteration 72, loss = 0.62958069\n",
            "Iteration 73, loss = 0.62866353\n",
            "Iteration 74, loss = 0.62786278\n",
            "Iteration 75, loss = 0.62707678\n",
            "Iteration 76, loss = 0.62631715\n",
            "Iteration 77, loss = 0.62554014\n",
            "Iteration 78, loss = 0.62477088\n",
            "Iteration 79, loss = 0.62400197\n",
            "Iteration 80, loss = 0.62327855\n",
            "Iteration 81, loss = 0.62255956\n",
            "Iteration 82, loss = 0.62183722\n",
            "Iteration 83, loss = 0.62108874\n",
            "Iteration 84, loss = 0.62039759\n",
            "Iteration 85, loss = 0.61967253\n",
            "Iteration 86, loss = 0.61896942\n",
            "Iteration 87, loss = 0.61829945\n",
            "Iteration 88, loss = 0.61761280\n",
            "Iteration 89, loss = 0.61697162\n",
            "Iteration 90, loss = 0.61630797\n",
            "Iteration 91, loss = 0.61566924\n",
            "Iteration 92, loss = 0.61500381\n",
            "Iteration 93, loss = 0.61436935\n",
            "Iteration 94, loss = 0.61373605\n",
            "Iteration 95, loss = 0.61307396\n",
            "Iteration 96, loss = 0.61248474\n",
            "Iteration 97, loss = 0.61183704\n",
            "Iteration 98, loss = 0.61128115\n",
            "Iteration 99, loss = 0.61063105\n",
            "Iteration 100, loss = 0.61004000\n",
            "Iteration 101, loss = 0.60947662\n",
            "Iteration 102, loss = 0.60889804\n",
            "Iteration 103, loss = 0.60843826\n",
            "Iteration 104, loss = 0.60783015\n",
            "Iteration 105, loss = 0.60731472\n",
            "Iteration 106, loss = 0.60679356\n",
            "Iteration 107, loss = 0.60628627\n",
            "Iteration 108, loss = 0.60578700\n",
            "Iteration 109, loss = 0.60530442\n",
            "Iteration 110, loss = 0.60483648\n",
            "Iteration 111, loss = 0.60436338\n",
            "Iteration 112, loss = 0.60393270\n",
            "Iteration 113, loss = 0.60352094\n",
            "Iteration 114, loss = 0.60307740\n",
            "Iteration 115, loss = 0.60268764\n",
            "Iteration 116, loss = 0.60225422\n",
            "Iteration 117, loss = 0.60183950\n",
            "Iteration 118, loss = 0.60144200\n",
            "Iteration 119, loss = 0.60106225\n",
            "Iteration 120, loss = 0.60063420\n",
            "Iteration 121, loss = 0.60023915\n",
            "Iteration 122, loss = 0.59983608\n",
            "Iteration 123, loss = 0.59942553\n",
            "Iteration 124, loss = 0.59901972\n",
            "Iteration 125, loss = 0.59862884\n",
            "Iteration 126, loss = 0.59824233\n",
            "Iteration 127, loss = 0.59785970\n",
            "Iteration 128, loss = 0.59750168\n",
            "Iteration 129, loss = 0.59713842\n",
            "Iteration 130, loss = 0.59680408\n",
            "Iteration 131, loss = 0.59648184\n",
            "Iteration 132, loss = 0.59612641\n",
            "Iteration 133, loss = 0.59579540\n",
            "Iteration 134, loss = 0.59545269\n",
            "Iteration 135, loss = 0.59513843\n",
            "Iteration 136, loss = 0.59481077\n",
            "Iteration 137, loss = 0.59450553\n",
            "Iteration 138, loss = 0.59417295\n",
            "Iteration 139, loss = 0.59386987\n",
            "Iteration 140, loss = 0.59352151\n",
            "Iteration 141, loss = 0.59320636\n",
            "Iteration 142, loss = 0.59288570\n",
            "Iteration 143, loss = 0.59254332\n",
            "Iteration 144, loss = 0.59224173\n",
            "Iteration 145, loss = 0.59192463\n",
            "Iteration 146, loss = 0.59160331\n",
            "Iteration 147, loss = 0.59128449\n",
            "Iteration 148, loss = 0.59096323\n",
            "Iteration 149, loss = 0.59063084\n",
            "Iteration 150, loss = 0.59038802\n",
            "Iteration 151, loss = 0.59008751\n",
            "Iteration 152, loss = 0.58981761\n",
            "Iteration 153, loss = 0.58954686\n",
            "Iteration 154, loss = 0.58929396\n",
            "Iteration 155, loss = 0.58905256\n",
            "Iteration 156, loss = 0.58879207\n",
            "Iteration 157, loss = 0.58854333\n",
            "Iteration 158, loss = 0.58827484\n",
            "Iteration 159, loss = 0.58804633\n",
            "Iteration 160, loss = 0.58781168\n",
            "Iteration 161, loss = 0.58759210\n",
            "Iteration 162, loss = 0.58738600\n",
            "Iteration 163, loss = 0.58716915\n",
            "Iteration 164, loss = 0.58696599\n",
            "Iteration 165, loss = 0.58674771\n",
            "Iteration 166, loss = 0.58651408\n",
            "Iteration 167, loss = 0.58629909\n",
            "Iteration 168, loss = 0.58608911\n",
            "Iteration 169, loss = 0.58586344\n",
            "Iteration 170, loss = 0.58567720\n",
            "Iteration 171, loss = 0.58546245\n",
            "Iteration 172, loss = 0.58521621\n",
            "Iteration 173, loss = 0.58502864\n",
            "Iteration 174, loss = 0.58481075\n",
            "Iteration 175, loss = 0.58460457\n",
            "Iteration 176, loss = 0.58439746\n",
            "Iteration 177, loss = 0.58420359\n",
            "Iteration 178, loss = 0.58401735\n",
            "Iteration 179, loss = 0.58380576\n",
            "Iteration 180, loss = 0.58362774\n",
            "Iteration 181, loss = 0.58342924\n",
            "Iteration 182, loss = 0.58323680\n",
            "Iteration 183, loss = 0.58304079\n",
            "Iteration 184, loss = 0.58286730\n",
            "Iteration 185, loss = 0.58269420\n",
            "Iteration 186, loss = 0.58253006\n",
            "Iteration 187, loss = 0.58235621\n",
            "Iteration 188, loss = 0.58218110\n",
            "Iteration 189, loss = 0.58200841\n",
            "Iteration 190, loss = 0.58182101\n",
            "Iteration 191, loss = 0.58163037\n",
            "Iteration 192, loss = 0.58144884\n",
            "Iteration 193, loss = 0.58127192\n",
            "Iteration 194, loss = 0.58110636\n",
            "Iteration 195, loss = 0.58093690\n",
            "Iteration 196, loss = 0.58077830\n",
            "Iteration 197, loss = 0.58063196\n",
            "Iteration 198, loss = 0.58047848\n",
            "Iteration 199, loss = 0.58031425\n",
            "Iteration 200, loss = 0.58016699\n",
            "Iteration 1, loss = 0.84884434\n",
            "Iteration 2, loss = 0.84610604\n",
            "Iteration 3, loss = 0.84230708\n",
            "Iteration 4, loss = 0.83712877\n",
            "Iteration 5, loss = 0.83146006\n",
            "Iteration 6, loss = 0.82550296\n",
            "Iteration 7, loss = 0.81909492\n",
            "Iteration 8, loss = 0.81265443\n",
            "Iteration 9, loss = 0.80669577\n",
            "Iteration 10, loss = 0.80039728\n",
            "Iteration 11, loss = 0.79425360\n",
            "Iteration 12, loss = 0.78824072\n",
            "Iteration 13, loss = 0.78261576\n",
            "Iteration 14, loss = 0.77671008\n",
            "Iteration 15, loss = 0.77114439\n",
            "Iteration 16, loss = 0.76606181\n",
            "Iteration 17, loss = 0.76108403\n",
            "Iteration 18, loss = 0.75613587\n",
            "Iteration 19, loss = 0.75170254\n",
            "Iteration 20, loss = 0.74713289\n",
            "Iteration 21, loss = 0.74293770\n",
            "Iteration 22, loss = 0.73884508\n",
            "Iteration 23, loss = 0.73495863\n",
            "Iteration 24, loss = 0.73114961\n",
            "Iteration 25, loss = 0.72759086\n",
            "Iteration 26, loss = 0.72382245\n",
            "Iteration 27, loss = 0.72028484\n",
            "Iteration 28, loss = 0.71690495\n",
            "Iteration 29, loss = 0.71349934\n",
            "Iteration 30, loss = 0.71009845\n",
            "Iteration 31, loss = 0.70704925\n",
            "Iteration 32, loss = 0.70404036\n",
            "Iteration 33, loss = 0.70113012\n",
            "Iteration 34, loss = 0.69813861\n",
            "Iteration 35, loss = 0.69541121\n",
            "Iteration 36, loss = 0.69283424\n",
            "Iteration 37, loss = 0.69007100\n",
            "Iteration 38, loss = 0.68761821\n",
            "Iteration 39, loss = 0.68511950\n",
            "Iteration 40, loss = 0.68252852\n",
            "Iteration 41, loss = 0.68002110\n",
            "Iteration 42, loss = 0.67761807\n",
            "Iteration 43, loss = 0.67543891\n",
            "Iteration 44, loss = 0.67320696\n",
            "Iteration 45, loss = 0.67119636\n",
            "Iteration 46, loss = 0.66930743\n",
            "Iteration 47, loss = 0.66722632\n",
            "Iteration 48, loss = 0.66538411\n",
            "Iteration 49, loss = 0.66343125\n",
            "Iteration 50, loss = 0.66151966\n",
            "Iteration 51, loss = 0.65973000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 52, loss = 0.65793582\n",
            "Iteration 53, loss = 0.65622869\n",
            "Iteration 54, loss = 0.65468002\n",
            "Iteration 55, loss = 0.65320535\n",
            "Iteration 56, loss = 0.65181447\n",
            "Iteration 57, loss = 0.65035231\n",
            "Iteration 58, loss = 0.64891425\n",
            "Iteration 59, loss = 0.64755073\n",
            "Iteration 60, loss = 0.64626251\n",
            "Iteration 61, loss = 0.64505130\n",
            "Iteration 62, loss = 0.64379440\n",
            "Iteration 63, loss = 0.64255345\n",
            "Iteration 64, loss = 0.64133698\n",
            "Iteration 65, loss = 0.64017137\n",
            "Iteration 66, loss = 0.63893323\n",
            "Iteration 67, loss = 0.63778882\n",
            "Iteration 68, loss = 0.63673076\n",
            "Iteration 69, loss = 0.63560115\n",
            "Iteration 70, loss = 0.63455761\n",
            "Iteration 71, loss = 0.63353220\n",
            "Iteration 72, loss = 0.63246273\n",
            "Iteration 73, loss = 0.63147427\n",
            "Iteration 74, loss = 0.63055777\n",
            "Iteration 75, loss = 0.62967430\n",
            "Iteration 76, loss = 0.62883840\n",
            "Iteration 77, loss = 0.62794709\n",
            "Iteration 78, loss = 0.62718417\n",
            "Iteration 79, loss = 0.62641941\n",
            "Iteration 80, loss = 0.62567713\n",
            "Iteration 81, loss = 0.62496967\n",
            "Iteration 82, loss = 0.62424083\n",
            "Iteration 83, loss = 0.62351271\n",
            "Iteration 84, loss = 0.62281916\n",
            "Iteration 85, loss = 0.62211463\n",
            "Iteration 86, loss = 0.62139155\n",
            "Iteration 87, loss = 0.62068848\n",
            "Iteration 88, loss = 0.61997756\n",
            "Iteration 89, loss = 0.61929985\n",
            "Iteration 90, loss = 0.61858862\n",
            "Iteration 91, loss = 0.61794400\n",
            "Iteration 92, loss = 0.61723905\n",
            "Iteration 93, loss = 0.61656102\n",
            "Iteration 94, loss = 0.61591205\n",
            "Iteration 95, loss = 0.61519728\n",
            "Iteration 96, loss = 0.61459602\n",
            "Iteration 97, loss = 0.61397021\n",
            "Iteration 98, loss = 0.61336479\n",
            "Iteration 99, loss = 0.61277025\n",
            "Iteration 100, loss = 0.61218560\n",
            "Iteration 101, loss = 0.61165672\n",
            "Iteration 102, loss = 0.61110966\n",
            "Iteration 103, loss = 0.61062357\n",
            "Iteration 104, loss = 0.61004453\n",
            "Iteration 105, loss = 0.60951759\n",
            "Iteration 106, loss = 0.60895887\n",
            "Iteration 107, loss = 0.60842317\n",
            "Iteration 108, loss = 0.60790224\n",
            "Iteration 109, loss = 0.60736399\n",
            "Iteration 110, loss = 0.60687911\n",
            "Iteration 111, loss = 0.60639413\n",
            "Iteration 112, loss = 0.60589977\n",
            "Iteration 113, loss = 0.60544970\n",
            "Iteration 114, loss = 0.60499882\n",
            "Iteration 115, loss = 0.60457310\n",
            "Iteration 116, loss = 0.60414893\n",
            "Iteration 117, loss = 0.60368021\n",
            "Iteration 118, loss = 0.60331520\n",
            "Iteration 119, loss = 0.60294032\n",
            "Iteration 120, loss = 0.60252016\n",
            "Iteration 121, loss = 0.60213903\n",
            "Iteration 122, loss = 0.60174523\n",
            "Iteration 123, loss = 0.60133593\n",
            "Iteration 124, loss = 0.60091023\n",
            "Iteration 125, loss = 0.60050976\n",
            "Iteration 126, loss = 0.60010820\n",
            "Iteration 127, loss = 0.59970696\n",
            "Iteration 128, loss = 0.59934052\n",
            "Iteration 129, loss = 0.59898036\n",
            "Iteration 130, loss = 0.59863160\n",
            "Iteration 131, loss = 0.59831106\n",
            "Iteration 132, loss = 0.59796491\n",
            "Iteration 133, loss = 0.59763822\n",
            "Iteration 134, loss = 0.59729262\n",
            "Iteration 135, loss = 0.59696763\n",
            "Iteration 136, loss = 0.59661883\n",
            "Iteration 137, loss = 0.59630678\n",
            "Iteration 138, loss = 0.59598908\n",
            "Iteration 139, loss = 0.59569112\n",
            "Iteration 140, loss = 0.59536288\n",
            "Iteration 141, loss = 0.59505523\n",
            "Iteration 142, loss = 0.59471383\n",
            "Iteration 143, loss = 0.59436415\n",
            "Iteration 144, loss = 0.59405748\n",
            "Iteration 145, loss = 0.59373360\n",
            "Iteration 146, loss = 0.59341229\n",
            "Iteration 147, loss = 0.59310556\n",
            "Iteration 148, loss = 0.59278314\n",
            "Iteration 149, loss = 0.59243203\n",
            "Iteration 150, loss = 0.59216162\n",
            "Iteration 151, loss = 0.59183948\n",
            "Iteration 152, loss = 0.59152583\n",
            "Iteration 153, loss = 0.59122354\n",
            "Iteration 154, loss = 0.59094689\n",
            "Iteration 155, loss = 0.59062364\n",
            "Iteration 156, loss = 0.59034079\n",
            "Iteration 157, loss = 0.59003310\n",
            "Iteration 158, loss = 0.58973518\n",
            "Iteration 159, loss = 0.58946881\n",
            "Iteration 160, loss = 0.58921332\n",
            "Iteration 161, loss = 0.58896173\n",
            "Iteration 162, loss = 0.58873790\n",
            "Iteration 163, loss = 0.58849665\n",
            "Iteration 164, loss = 0.58826871\n",
            "Iteration 165, loss = 0.58801024\n",
            "Iteration 166, loss = 0.58775940\n",
            "Iteration 167, loss = 0.58751073\n",
            "Iteration 168, loss = 0.58728124\n",
            "Iteration 169, loss = 0.58701916\n",
            "Iteration 170, loss = 0.58680194\n",
            "Iteration 171, loss = 0.58655512\n",
            "Iteration 172, loss = 0.58630942\n",
            "Iteration 173, loss = 0.58610079\n",
            "Iteration 174, loss = 0.58588253\n",
            "Iteration 175, loss = 0.58566431\n",
            "Iteration 176, loss = 0.58545213\n",
            "Iteration 177, loss = 0.58524128\n",
            "Iteration 178, loss = 0.58503744\n",
            "Iteration 179, loss = 0.58479710\n",
            "Iteration 180, loss = 0.58458663\n",
            "Iteration 181, loss = 0.58435643\n",
            "Iteration 182, loss = 0.58415343\n",
            "Iteration 183, loss = 0.58392523\n",
            "Iteration 184, loss = 0.58373347\n",
            "Iteration 185, loss = 0.58355026\n",
            "Iteration 186, loss = 0.58335871\n",
            "Iteration 187, loss = 0.58317377\n",
            "Iteration 188, loss = 0.58296934\n",
            "Iteration 189, loss = 0.58278168\n",
            "Iteration 190, loss = 0.58259214\n",
            "Iteration 191, loss = 0.58239186\n",
            "Iteration 192, loss = 0.58221350\n",
            "Iteration 193, loss = 0.58203516\n",
            "Iteration 194, loss = 0.58186028\n",
            "Iteration 195, loss = 0.58168141\n",
            "Iteration 196, loss = 0.58150987\n",
            "Iteration 197, loss = 0.58132432\n",
            "Iteration 198, loss = 0.58114499\n",
            "Iteration 199, loss = 0.58097392\n",
            "Iteration 200, loss = 0.58080477\n",
            "Iteration 1, loss = 0.86923367\n",
            "Iteration 2, loss = 0.86623006\n",
            "Iteration 3, loss = 0.86201844\n",
            "Iteration 4, loss = 0.85633892\n",
            "Iteration 5, loss = 0.85008599\n",
            "Iteration 6, loss = 0.84356157\n",
            "Iteration 7, loss = 0.83652964\n",
            "Iteration 8, loss = 0.82935828\n",
            "Iteration 9, loss = 0.82278608\n",
            "Iteration 10, loss = 0.81571552\n",
            "Iteration 11, loss = 0.80886293\n",
            "Iteration 12, loss = 0.80218902\n",
            "Iteration 13, loss = 0.79599872\n",
            "Iteration 14, loss = 0.78964699\n",
            "Iteration 15, loss = 0.78337376\n",
            "Iteration 16, loss = 0.77753155\n",
            "Iteration 17, loss = 0.77180928\n",
            "Iteration 18, loss = 0.76609001\n",
            "Iteration 19, loss = 0.76110090\n",
            "Iteration 20, loss = 0.75627648\n",
            "Iteration 21, loss = 0.75155283\n",
            "Iteration 22, loss = 0.74704216\n",
            "Iteration 23, loss = 0.74279740\n",
            "Iteration 24, loss = 0.73856804\n",
            "Iteration 25, loss = 0.73462565\n",
            "Iteration 26, loss = 0.73055780\n",
            "Iteration 27, loss = 0.72667968\n",
            "Iteration 28, loss = 0.72311381\n",
            "Iteration 29, loss = 0.71931481\n",
            "Iteration 30, loss = 0.71559199\n",
            "Iteration 31, loss = 0.71214502\n",
            "Iteration 32, loss = 0.70883599\n",
            "Iteration 33, loss = 0.70545153\n",
            "Iteration 34, loss = 0.70226445\n",
            "Iteration 35, loss = 0.69916085\n",
            "Iteration 36, loss = 0.69616862\n",
            "Iteration 37, loss = 0.69307073\n",
            "Iteration 38, loss = 0.69021815\n",
            "Iteration 39, loss = 0.68742445\n",
            "Iteration 40, loss = 0.68467159\n",
            "Iteration 41, loss = 0.68190363\n",
            "Iteration 42, loss = 0.67946455\n",
            "Iteration 43, loss = 0.67717399\n",
            "Iteration 44, loss = 0.67499244\n",
            "Iteration 45, loss = 0.67284265\n",
            "Iteration 46, loss = 0.67090200\n",
            "Iteration 47, loss = 0.66876285\n",
            "Iteration 48, loss = 0.66681518\n",
            "Iteration 49, loss = 0.66482789\n",
            "Iteration 50, loss = 0.66288284\n",
            "Iteration 51, loss = 0.66109203\n",
            "Iteration 52, loss = 0.65916577\n",
            "Iteration 53, loss = 0.65740321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 54, loss = 0.65573998\n",
            "Iteration 55, loss = 0.65410347\n",
            "Iteration 56, loss = 0.65259831\n",
            "Iteration 57, loss = 0.65109783\n",
            "Iteration 58, loss = 0.64958658\n",
            "Iteration 59, loss = 0.64816078\n",
            "Iteration 60, loss = 0.64678991\n",
            "Iteration 61, loss = 0.64549181\n",
            "Iteration 62, loss = 0.64412795\n",
            "Iteration 63, loss = 0.64282614\n",
            "Iteration 64, loss = 0.64150110\n",
            "Iteration 65, loss = 0.64025473\n",
            "Iteration 66, loss = 0.63892570\n",
            "Iteration 67, loss = 0.63768365\n",
            "Iteration 68, loss = 0.63652642\n",
            "Iteration 69, loss = 0.63536168\n",
            "Iteration 70, loss = 0.63421997\n",
            "Iteration 71, loss = 0.63317325\n",
            "Iteration 72, loss = 0.63205658\n",
            "Iteration 73, loss = 0.63100731\n",
            "Iteration 74, loss = 0.63001822\n",
            "Iteration 75, loss = 0.62901823\n",
            "Iteration 76, loss = 0.62809679\n",
            "Iteration 77, loss = 0.62707914\n",
            "Iteration 78, loss = 0.62623111\n",
            "Iteration 79, loss = 0.62533088\n",
            "Iteration 80, loss = 0.62450127\n",
            "Iteration 81, loss = 0.62368795\n",
            "Iteration 82, loss = 0.62285271\n",
            "Iteration 83, loss = 0.62204441\n",
            "Iteration 84, loss = 0.62127094\n",
            "Iteration 85, loss = 0.62052542\n",
            "Iteration 86, loss = 0.61970997\n",
            "Iteration 87, loss = 0.61893977\n",
            "Iteration 88, loss = 0.61815595\n",
            "Iteration 89, loss = 0.61740030\n",
            "Iteration 90, loss = 0.61662252\n",
            "Iteration 91, loss = 0.61594190\n",
            "Iteration 92, loss = 0.61517044\n",
            "Iteration 93, loss = 0.61448395\n",
            "Iteration 94, loss = 0.61380520\n",
            "Iteration 95, loss = 0.61298957\n",
            "Iteration 96, loss = 0.61235810\n",
            "Iteration 97, loss = 0.61163696\n",
            "Iteration 98, loss = 0.61092207\n",
            "Iteration 99, loss = 0.61023909\n",
            "Iteration 100, loss = 0.60959741\n",
            "Iteration 101, loss = 0.60897924\n",
            "Iteration 102, loss = 0.60830994\n",
            "Iteration 103, loss = 0.60768355\n",
            "Iteration 104, loss = 0.60697099\n",
            "Iteration 105, loss = 0.60632363\n",
            "Iteration 106, loss = 0.60567325\n",
            "Iteration 107, loss = 0.60497776\n",
            "Iteration 108, loss = 0.60439037\n",
            "Iteration 109, loss = 0.60375829\n",
            "Iteration 110, loss = 0.60318738\n",
            "Iteration 111, loss = 0.60258742\n",
            "Iteration 112, loss = 0.60197632\n",
            "Iteration 113, loss = 0.60141942\n",
            "Iteration 114, loss = 0.60086993\n",
            "Iteration 115, loss = 0.60029735\n",
            "Iteration 116, loss = 0.59974641\n",
            "Iteration 117, loss = 0.59909337\n",
            "Iteration 118, loss = 0.59858018\n",
            "Iteration 119, loss = 0.59808021\n",
            "Iteration 120, loss = 0.59759884\n",
            "Iteration 121, loss = 0.59712295\n",
            "Iteration 122, loss = 0.59664494\n",
            "Iteration 123, loss = 0.59613269\n",
            "Iteration 124, loss = 0.59559367\n",
            "Iteration 125, loss = 0.59512475\n",
            "Iteration 126, loss = 0.59460867\n",
            "Iteration 127, loss = 0.59411337\n",
            "Iteration 128, loss = 0.59365786\n",
            "Iteration 129, loss = 0.59322298\n",
            "Iteration 130, loss = 0.59276458\n",
            "Iteration 131, loss = 0.59238339\n",
            "Iteration 132, loss = 0.59196273\n",
            "Iteration 133, loss = 0.59154884\n",
            "Iteration 134, loss = 0.59114234\n",
            "Iteration 135, loss = 0.59075022\n",
            "Iteration 136, loss = 0.59032802\n",
            "Iteration 137, loss = 0.58994490\n",
            "Iteration 138, loss = 0.58955952\n",
            "Iteration 139, loss = 0.58920941\n",
            "Iteration 140, loss = 0.58882673\n",
            "Iteration 141, loss = 0.58847080\n",
            "Iteration 142, loss = 0.58807377\n",
            "Iteration 143, loss = 0.58768820\n",
            "Iteration 144, loss = 0.58730221\n",
            "Iteration 145, loss = 0.58692645\n",
            "Iteration 146, loss = 0.58654280\n",
            "Iteration 147, loss = 0.58619159\n",
            "Iteration 148, loss = 0.58579775\n",
            "Iteration 149, loss = 0.58539863\n",
            "Iteration 150, loss = 0.58505482\n",
            "Iteration 151, loss = 0.58467476\n",
            "Iteration 152, loss = 0.58429695\n",
            "Iteration 153, loss = 0.58394553\n",
            "Iteration 154, loss = 0.58358963\n",
            "Iteration 155, loss = 0.58321133\n",
            "Iteration 156, loss = 0.58288150\n",
            "Iteration 157, loss = 0.58252365\n",
            "Iteration 158, loss = 0.58218811\n",
            "Iteration 159, loss = 0.58189613\n",
            "Iteration 160, loss = 0.58160914\n",
            "Iteration 161, loss = 0.58130793\n",
            "Iteration 162, loss = 0.58103253\n",
            "Iteration 163, loss = 0.58075001\n",
            "Iteration 164, loss = 0.58047517\n",
            "Iteration 165, loss = 0.58018892\n",
            "Iteration 166, loss = 0.57992538\n",
            "Iteration 167, loss = 0.57963637\n",
            "Iteration 168, loss = 0.57938675\n",
            "Iteration 169, loss = 0.57909789\n",
            "Iteration 170, loss = 0.57882554\n",
            "Iteration 171, loss = 0.57855363\n",
            "Iteration 172, loss = 0.57829767\n",
            "Iteration 173, loss = 0.57804112\n",
            "Iteration 174, loss = 0.57780951\n",
            "Iteration 175, loss = 0.57755053\n",
            "Iteration 176, loss = 0.57730370\n",
            "Iteration 177, loss = 0.57705146\n",
            "Iteration 178, loss = 0.57681810\n",
            "Iteration 179, loss = 0.57655681\n",
            "Iteration 180, loss = 0.57632833\n",
            "Iteration 181, loss = 0.57608327\n",
            "Iteration 182, loss = 0.57584442\n",
            "Iteration 183, loss = 0.57559388\n",
            "Iteration 184, loss = 0.57537209\n",
            "Iteration 185, loss = 0.57514602\n",
            "Iteration 186, loss = 0.57491080\n",
            "Iteration 187, loss = 0.57469425\n",
            "Iteration 188, loss = 0.57447699\n",
            "Iteration 189, loss = 0.57425421\n",
            "Iteration 190, loss = 0.57401494\n",
            "Iteration 191, loss = 0.57378281\n",
            "Iteration 192, loss = 0.57356778\n",
            "Iteration 193, loss = 0.57335559\n",
            "Iteration 194, loss = 0.57314874\n",
            "Iteration 195, loss = 0.57293664\n",
            "Iteration 196, loss = 0.57275212\n",
            "Iteration 197, loss = 0.57253941\n",
            "Iteration 198, loss = 0.57234058\n",
            "Iteration 199, loss = 0.57211390\n",
            "Iteration 200, loss = 0.57191341\n",
            "Iteration 1, loss = 0.84858612\n",
            "Iteration 2, loss = 0.84586103\n",
            "Iteration 3, loss = 0.84202808\n",
            "Iteration 4, loss = 0.83677269\n",
            "Iteration 5, loss = 0.83102491\n",
            "Iteration 6, loss = 0.82523503\n",
            "Iteration 7, loss = 0.81887373\n",
            "Iteration 8, loss = 0.81242895\n",
            "Iteration 9, loss = 0.80642299\n",
            "Iteration 10, loss = 0.80007903\n",
            "Iteration 11, loss = 0.79384885\n",
            "Iteration 12, loss = 0.78766457\n",
            "Iteration 13, loss = 0.78212698\n",
            "Iteration 14, loss = 0.77616718\n",
            "Iteration 15, loss = 0.77057092\n",
            "Iteration 16, loss = 0.76520465\n",
            "Iteration 17, loss = 0.76001179\n",
            "Iteration 18, loss = 0.75481795\n",
            "Iteration 19, loss = 0.75019268\n",
            "Iteration 20, loss = 0.74570973\n",
            "Iteration 21, loss = 0.74143312\n",
            "Iteration 22, loss = 0.73714002\n",
            "Iteration 23, loss = 0.73317639\n",
            "Iteration 24, loss = 0.72916594\n",
            "Iteration 25, loss = 0.72564340\n",
            "Iteration 26, loss = 0.72187504\n",
            "Iteration 27, loss = 0.71826418\n",
            "Iteration 28, loss = 0.71475157\n",
            "Iteration 29, loss = 0.71126395\n",
            "Iteration 30, loss = 0.70780464\n",
            "Iteration 31, loss = 0.70433933\n",
            "Iteration 32, loss = 0.70129476\n",
            "Iteration 33, loss = 0.69812788\n",
            "Iteration 34, loss = 0.69512355\n",
            "Iteration 35, loss = 0.69245263\n",
            "Iteration 36, loss = 0.68965835\n",
            "Iteration 37, loss = 0.68690679\n",
            "Iteration 38, loss = 0.68418766\n",
            "Iteration 39, loss = 0.68157247\n",
            "Iteration 40, loss = 0.67890392\n",
            "Iteration 41, loss = 0.67638975\n",
            "Iteration 42, loss = 0.67403004\n",
            "Iteration 43, loss = 0.67200248\n",
            "Iteration 44, loss = 0.66999841\n",
            "Iteration 45, loss = 0.66803721\n",
            "Iteration 46, loss = 0.66615995\n",
            "Iteration 47, loss = 0.66413871\n",
            "Iteration 48, loss = 0.66225463\n",
            "Iteration 49, loss = 0.66048563\n",
            "Iteration 50, loss = 0.65875979\n",
            "Iteration 51, loss = 0.65707940\n",
            "Iteration 52, loss = 0.65546792\n",
            "Iteration 53, loss = 0.65384613\n",
            "Iteration 54, loss = 0.65236260\n",
            "Iteration 55, loss = 0.65091725\n",
            "Iteration 56, loss = 0.64959092\n",
            "Iteration 57, loss = 0.64820250\n",
            "Iteration 58, loss = 0.64691513\n",
            "Iteration 59, loss = 0.64568032\n",
            "Iteration 60, loss = 0.64440258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 61, loss = 0.64327986\n",
            "Iteration 62, loss = 0.64202316\n",
            "Iteration 63, loss = 0.64089945\n",
            "Iteration 64, loss = 0.63978166\n",
            "Iteration 65, loss = 0.63867096\n",
            "Iteration 66, loss = 0.63754716\n",
            "Iteration 67, loss = 0.63647957\n",
            "Iteration 68, loss = 0.63545464\n",
            "Iteration 69, loss = 0.63444067\n",
            "Iteration 70, loss = 0.63343899\n",
            "Iteration 71, loss = 0.63246926\n",
            "Iteration 72, loss = 0.63142338\n",
            "Iteration 73, loss = 0.63047341\n",
            "Iteration 74, loss = 0.62955579\n",
            "Iteration 75, loss = 0.62870095\n",
            "Iteration 76, loss = 0.62785827\n",
            "Iteration 77, loss = 0.62697778\n",
            "Iteration 78, loss = 0.62622000\n",
            "Iteration 79, loss = 0.62543734\n",
            "Iteration 80, loss = 0.62467271\n",
            "Iteration 81, loss = 0.62399221\n",
            "Iteration 82, loss = 0.62324994\n",
            "Iteration 83, loss = 0.62256615\n",
            "Iteration 84, loss = 0.62187724\n",
            "Iteration 85, loss = 0.62128307\n",
            "Iteration 86, loss = 0.62055994\n",
            "Iteration 87, loss = 0.61987967\n",
            "Iteration 88, loss = 0.61925896\n",
            "Iteration 89, loss = 0.61861177\n",
            "Iteration 90, loss = 0.61796749\n",
            "Iteration 91, loss = 0.61735920\n",
            "Iteration 92, loss = 0.61671408\n",
            "Iteration 93, loss = 0.61610607\n",
            "Iteration 94, loss = 0.61551042\n",
            "Iteration 95, loss = 0.61475085\n",
            "Iteration 96, loss = 0.61417133\n",
            "Iteration 97, loss = 0.61357502\n",
            "Iteration 98, loss = 0.61292309\n",
            "Iteration 99, loss = 0.61240326\n",
            "Iteration 100, loss = 0.61185817\n",
            "Iteration 101, loss = 0.61132681\n",
            "Iteration 102, loss = 0.61082051\n",
            "Iteration 103, loss = 0.61033679\n",
            "Iteration 104, loss = 0.60983754\n",
            "Iteration 105, loss = 0.60932550\n",
            "Iteration 106, loss = 0.60880611\n",
            "Iteration 107, loss = 0.60827682\n",
            "Iteration 108, loss = 0.60777675\n",
            "Iteration 109, loss = 0.60728584\n",
            "Iteration 110, loss = 0.60683020\n",
            "Iteration 111, loss = 0.60638657\n",
            "Iteration 112, loss = 0.60590872\n",
            "Iteration 113, loss = 0.60549932\n",
            "Iteration 114, loss = 0.60508998\n",
            "Iteration 115, loss = 0.60466578\n",
            "Iteration 116, loss = 0.60427608\n",
            "Iteration 117, loss = 0.60386024\n",
            "Iteration 118, loss = 0.60351764\n",
            "Iteration 119, loss = 0.60317213\n",
            "Iteration 120, loss = 0.60282647\n",
            "Iteration 121, loss = 0.60252279\n",
            "Iteration 122, loss = 0.60215557\n",
            "Iteration 123, loss = 0.60177600\n",
            "Iteration 124, loss = 0.60138698\n",
            "Iteration 125, loss = 0.60104870\n",
            "Iteration 126, loss = 0.60066440\n",
            "Iteration 127, loss = 0.60029622\n",
            "Iteration 128, loss = 0.59995722\n",
            "Iteration 129, loss = 0.59964275\n",
            "Iteration 130, loss = 0.59931902\n",
            "Iteration 131, loss = 0.59905775\n",
            "Iteration 132, loss = 0.59875957\n",
            "Iteration 133, loss = 0.59846534\n",
            "Iteration 134, loss = 0.59818749\n",
            "Iteration 135, loss = 0.59789258\n",
            "Iteration 136, loss = 0.59757564\n",
            "Iteration 137, loss = 0.59730132\n",
            "Iteration 138, loss = 0.59703448\n",
            "Iteration 139, loss = 0.59675296\n",
            "Iteration 140, loss = 0.59646325\n",
            "Iteration 141, loss = 0.59621349\n",
            "Iteration 142, loss = 0.59592745\n",
            "Iteration 143, loss = 0.59564027\n",
            "Iteration 144, loss = 0.59536162\n",
            "Iteration 145, loss = 0.59508834\n",
            "Iteration 146, loss = 0.59481764\n",
            "Iteration 147, loss = 0.59457382\n",
            "Iteration 148, loss = 0.59429337\n",
            "Iteration 149, loss = 0.59403140\n",
            "Iteration 150, loss = 0.59378019\n",
            "Iteration 151, loss = 0.59351883\n",
            "Iteration 152, loss = 0.59324684\n",
            "Iteration 153, loss = 0.59298803\n",
            "Iteration 154, loss = 0.59274647\n",
            "Iteration 155, loss = 0.59246327\n",
            "Iteration 156, loss = 0.59223741\n",
            "Iteration 157, loss = 0.59195932\n",
            "Iteration 158, loss = 0.59173650\n",
            "Iteration 159, loss = 0.59150695\n",
            "Iteration 160, loss = 0.59129473\n",
            "Iteration 161, loss = 0.59105468\n",
            "Iteration 162, loss = 0.59084932\n",
            "Iteration 163, loss = 0.59064474\n",
            "Iteration 164, loss = 0.59043675\n",
            "Iteration 165, loss = 0.59022354\n",
            "Iteration 166, loss = 0.59002231\n",
            "Iteration 167, loss = 0.58980346\n",
            "Iteration 168, loss = 0.58959525\n",
            "Iteration 169, loss = 0.58938385\n",
            "Iteration 170, loss = 0.58915162\n",
            "Iteration 171, loss = 0.58893284\n",
            "Iteration 172, loss = 0.58873316\n",
            "Iteration 173, loss = 0.58851302\n",
            "Iteration 174, loss = 0.58834254\n",
            "Iteration 175, loss = 0.58811887\n",
            "Iteration 176, loss = 0.58792879\n",
            "Iteration 177, loss = 0.58774321\n",
            "Iteration 178, loss = 0.58756283\n",
            "Iteration 179, loss = 0.58734702\n",
            "Iteration 180, loss = 0.58718299\n",
            "Iteration 181, loss = 0.58696382\n",
            "Iteration 182, loss = 0.58678926\n",
            "Iteration 183, loss = 0.58659905\n",
            "Iteration 184, loss = 0.58643580\n",
            "Iteration 185, loss = 0.58625711\n",
            "Iteration 186, loss = 0.58609266\n",
            "Iteration 187, loss = 0.58592706\n",
            "Iteration 188, loss = 0.58575609\n",
            "Iteration 189, loss = 0.58558891\n",
            "Iteration 190, loss = 0.58540802\n",
            "Iteration 191, loss = 0.58523423\n",
            "Iteration 192, loss = 0.58507152\n",
            "Iteration 193, loss = 0.58491719\n",
            "Iteration 194, loss = 0.58477430\n",
            "Iteration 195, loss = 0.58462679\n",
            "Iteration 196, loss = 0.58448415\n",
            "Iteration 197, loss = 0.58432513\n",
            "Iteration 198, loss = 0.58416263\n",
            "Iteration 199, loss = 0.58401465\n",
            "Iteration 200, loss = 0.58385186\n",
            "Iteration 1, loss = 0.84407341\n",
            "Iteration 2, loss = 0.84130009\n",
            "Iteration 3, loss = 0.83725245\n",
            "Iteration 4, loss = 0.83218094\n",
            "Iteration 5, loss = 0.82697800\n",
            "Iteration 6, loss = 0.82168607\n",
            "Iteration 7, loss = 0.81584661\n",
            "Iteration 8, loss = 0.81000185\n",
            "Iteration 9, loss = 0.80435363\n",
            "Iteration 10, loss = 0.79816107\n",
            "Iteration 11, loss = 0.79237071\n",
            "Iteration 12, loss = 0.78666337\n",
            "Iteration 13, loss = 0.78102998\n",
            "Iteration 14, loss = 0.77550694\n",
            "Iteration 15, loss = 0.77026749\n",
            "Iteration 16, loss = 0.76517323\n",
            "Iteration 17, loss = 0.76032485\n",
            "Iteration 18, loss = 0.75561036\n",
            "Iteration 19, loss = 0.75098454\n",
            "Iteration 20, loss = 0.74658742\n",
            "Iteration 21, loss = 0.74243839\n",
            "Iteration 22, loss = 0.73833552\n",
            "Iteration 23, loss = 0.73436580\n",
            "Iteration 24, loss = 0.73058470\n",
            "Iteration 25, loss = 0.72698704\n",
            "Iteration 26, loss = 0.72315689\n",
            "Iteration 27, loss = 0.71954935\n",
            "Iteration 28, loss = 0.71591667\n",
            "Iteration 29, loss = 0.71260479\n",
            "Iteration 30, loss = 0.70914756\n",
            "Iteration 31, loss = 0.70589731\n",
            "Iteration 32, loss = 0.70265938\n",
            "Iteration 33, loss = 0.69947938\n",
            "Iteration 34, loss = 0.69654095\n",
            "Iteration 35, loss = 0.69348703\n",
            "Iteration 36, loss = 0.69054236\n",
            "Iteration 37, loss = 0.68762425\n",
            "Iteration 38, loss = 0.68489428\n",
            "Iteration 39, loss = 0.68225194\n",
            "Iteration 40, loss = 0.67961680\n",
            "Iteration 41, loss = 0.67710505\n",
            "Iteration 42, loss = 0.67477275\n",
            "Iteration 43, loss = 0.67274744\n",
            "Iteration 44, loss = 0.67056462\n",
            "Iteration 45, loss = 0.66859460\n",
            "Iteration 46, loss = 0.66655832\n",
            "Iteration 47, loss = 0.66454959\n",
            "Iteration 48, loss = 0.66265032\n",
            "Iteration 49, loss = 0.66091053\n",
            "Iteration 50, loss = 0.65917129\n",
            "Iteration 51, loss = 0.65752328\n",
            "Iteration 52, loss = 0.65594358\n",
            "Iteration 53, loss = 0.65432003\n",
            "Iteration 54, loss = 0.65283905\n",
            "Iteration 55, loss = 0.65128565\n",
            "Iteration 56, loss = 0.64996104\n",
            "Iteration 57, loss = 0.64857742\n",
            "Iteration 58, loss = 0.64723652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 59, loss = 0.64599833\n",
            "Iteration 60, loss = 0.64471383\n",
            "Iteration 61, loss = 0.64352032\n",
            "Iteration 62, loss = 0.64224376\n",
            "Iteration 63, loss = 0.64109397\n",
            "Iteration 64, loss = 0.63993373\n",
            "Iteration 65, loss = 0.63886747\n",
            "Iteration 66, loss = 0.63775884\n",
            "Iteration 67, loss = 0.63666581\n",
            "Iteration 68, loss = 0.63561012\n",
            "Iteration 69, loss = 0.63454685\n",
            "Iteration 70, loss = 0.63350548\n",
            "Iteration 71, loss = 0.63253331\n",
            "Iteration 72, loss = 0.63148168\n",
            "Iteration 73, loss = 0.63054464\n",
            "Iteration 74, loss = 0.62962690\n",
            "Iteration 75, loss = 0.62875371\n",
            "Iteration 76, loss = 0.62785837\n",
            "Iteration 77, loss = 0.62703086\n",
            "Iteration 78, loss = 0.62624236\n",
            "Iteration 79, loss = 0.62546140\n",
            "Iteration 80, loss = 0.62474273\n",
            "Iteration 81, loss = 0.62408242\n",
            "Iteration 82, loss = 0.62337742\n",
            "Iteration 83, loss = 0.62269981\n",
            "Iteration 84, loss = 0.62201595\n",
            "Iteration 85, loss = 0.62140697\n",
            "Iteration 86, loss = 0.62071370\n",
            "Iteration 87, loss = 0.62004325\n",
            "Iteration 88, loss = 0.61943810\n",
            "Iteration 89, loss = 0.61880019\n",
            "Iteration 90, loss = 0.61817375\n",
            "Iteration 91, loss = 0.61758277\n",
            "Iteration 92, loss = 0.61693716\n",
            "Iteration 93, loss = 0.61634558\n",
            "Iteration 94, loss = 0.61577297\n",
            "Iteration 95, loss = 0.61512111\n",
            "Iteration 96, loss = 0.61457329\n",
            "Iteration 97, loss = 0.61400662\n",
            "Iteration 98, loss = 0.61334432\n",
            "Iteration 99, loss = 0.61284428\n",
            "Iteration 100, loss = 0.61230045\n",
            "Iteration 101, loss = 0.61178248\n",
            "Iteration 102, loss = 0.61130670\n",
            "Iteration 103, loss = 0.61079670\n",
            "Iteration 104, loss = 0.61029651\n",
            "Iteration 105, loss = 0.60978651\n",
            "Iteration 106, loss = 0.60928542\n",
            "Iteration 107, loss = 0.60876180\n",
            "Iteration 108, loss = 0.60826273\n",
            "Iteration 109, loss = 0.60773135\n",
            "Iteration 110, loss = 0.60722575\n",
            "Iteration 111, loss = 0.60676696\n",
            "Iteration 112, loss = 0.60620469\n",
            "Iteration 113, loss = 0.60571493\n",
            "Iteration 114, loss = 0.60527486\n",
            "Iteration 115, loss = 0.60477620\n",
            "Iteration 116, loss = 0.60433492\n",
            "Iteration 117, loss = 0.60388702\n",
            "Iteration 118, loss = 0.60347394\n",
            "Iteration 119, loss = 0.60307031\n",
            "Iteration 120, loss = 0.60266544\n",
            "Iteration 121, loss = 0.60226998\n",
            "Iteration 122, loss = 0.60185161\n",
            "Iteration 123, loss = 0.60138960\n",
            "Iteration 124, loss = 0.60099822\n",
            "Iteration 125, loss = 0.60059314\n",
            "Iteration 126, loss = 0.60017253\n",
            "Iteration 127, loss = 0.59977016\n",
            "Iteration 128, loss = 0.59938774\n",
            "Iteration 129, loss = 0.59902502\n",
            "Iteration 130, loss = 0.59864652\n",
            "Iteration 131, loss = 0.59832533\n",
            "Iteration 132, loss = 0.59799104\n",
            "Iteration 133, loss = 0.59766587\n",
            "Iteration 134, loss = 0.59736055\n",
            "Iteration 135, loss = 0.59702591\n",
            "Iteration 136, loss = 0.59667199\n",
            "Iteration 137, loss = 0.59635611\n",
            "Iteration 138, loss = 0.59608056\n",
            "Iteration 139, loss = 0.59578235\n",
            "Iteration 140, loss = 0.59548403\n",
            "Iteration 141, loss = 0.59521450\n",
            "Iteration 142, loss = 0.59491029\n",
            "Iteration 143, loss = 0.59461594\n",
            "Iteration 144, loss = 0.59431879\n",
            "Iteration 145, loss = 0.59403609\n",
            "Iteration 146, loss = 0.59375025\n",
            "Iteration 147, loss = 0.59347385\n",
            "Iteration 148, loss = 0.59319147\n",
            "Iteration 149, loss = 0.59291991\n",
            "Iteration 150, loss = 0.59263915\n",
            "Iteration 151, loss = 0.59235449\n",
            "Iteration 152, loss = 0.59205361\n",
            "Iteration 153, loss = 0.59178152\n",
            "Iteration 154, loss = 0.59152328\n",
            "Iteration 155, loss = 0.59123631\n",
            "Iteration 156, loss = 0.59097670\n",
            "Iteration 157, loss = 0.59070864\n",
            "Iteration 158, loss = 0.59047385\n",
            "Iteration 159, loss = 0.59022275\n",
            "Iteration 160, loss = 0.58999782\n",
            "Iteration 161, loss = 0.58973176\n",
            "Iteration 162, loss = 0.58950729\n",
            "Iteration 163, loss = 0.58929229\n",
            "Iteration 164, loss = 0.58904123\n",
            "Iteration 165, loss = 0.58882014\n",
            "Iteration 166, loss = 0.58861401\n",
            "Iteration 167, loss = 0.58839697\n",
            "Iteration 168, loss = 0.58817000\n",
            "Iteration 169, loss = 0.58796628\n",
            "Iteration 170, loss = 0.58773167\n",
            "Iteration 171, loss = 0.58750522\n",
            "Iteration 172, loss = 0.58730681\n",
            "Iteration 173, loss = 0.58707169\n",
            "Iteration 174, loss = 0.58688823\n",
            "Iteration 175, loss = 0.58667755\n",
            "Iteration 176, loss = 0.58650037\n",
            "Iteration 177, loss = 0.58632151\n",
            "Iteration 178, loss = 0.58614904\n",
            "Iteration 179, loss = 0.58594676\n",
            "Iteration 180, loss = 0.58576109\n",
            "Iteration 181, loss = 0.58556065\n",
            "Iteration 182, loss = 0.58536427\n",
            "Iteration 183, loss = 0.58516704\n",
            "Iteration 184, loss = 0.58499852\n",
            "Iteration 185, loss = 0.58480437\n",
            "Iteration 186, loss = 0.58463367\n",
            "Iteration 187, loss = 0.58446392\n",
            "Iteration 188, loss = 0.58429171\n",
            "Iteration 189, loss = 0.58411291\n",
            "Iteration 190, loss = 0.58392177\n",
            "Iteration 191, loss = 0.58376057\n",
            "Iteration 192, loss = 0.58359527\n",
            "Iteration 193, loss = 0.58344437\n",
            "Iteration 194, loss = 0.58328742\n",
            "Iteration 195, loss = 0.58312929\n",
            "Iteration 196, loss = 0.58299545\n",
            "Iteration 197, loss = 0.58282952\n",
            "Iteration 198, loss = 0.58268235\n",
            "Iteration 199, loss = 0.58255022\n",
            "Iteration 200, loss = 0.58240461\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 200 and for layer number 3 : 0.6987500000000001\n",
            "Iteration 1, loss = 0.92831019\n",
            "Iteration 2, loss = 0.92628737\n",
            "Iteration 3, loss = 0.92292720\n",
            "Iteration 4, loss = 0.91890896\n",
            "Iteration 5, loss = 0.91406816\n",
            "Iteration 6, loss = 0.90896363\n",
            "Iteration 7, loss = 0.90386015\n",
            "Iteration 8, loss = 0.89884936\n",
            "Iteration 9, loss = 0.89376039\n",
            "Iteration 10, loss = 0.88868258\n",
            "Iteration 11, loss = 0.88381471\n",
            "Iteration 12, loss = 0.87878426\n",
            "Iteration 13, loss = 0.87398219\n",
            "Iteration 14, loss = 0.86928989\n",
            "Iteration 15, loss = 0.86471610\n",
            "Iteration 16, loss = 0.86053140\n",
            "Iteration 17, loss = 0.85612820\n",
            "Iteration 18, loss = 0.85197041\n",
            "Iteration 19, loss = 0.84782638\n",
            "Iteration 20, loss = 0.84387295\n",
            "Iteration 21, loss = 0.83992078\n",
            "Iteration 22, loss = 0.83601275\n",
            "Iteration 23, loss = 0.83220154\n",
            "Iteration 24, loss = 0.82811202\n",
            "Iteration 25, loss = 0.82449396\n",
            "Iteration 26, loss = 0.82082671\n",
            "Iteration 27, loss = 0.81721320\n",
            "Iteration 28, loss = 0.81380905\n",
            "Iteration 29, loss = 0.81040528\n",
            "Iteration 30, loss = 0.80705060\n",
            "Iteration 31, loss = 0.80392526\n",
            "Iteration 32, loss = 0.80063213\n",
            "Iteration 33, loss = 0.79760810\n",
            "Iteration 34, loss = 0.79459235\n",
            "Iteration 35, loss = 0.79159397\n",
            "Iteration 36, loss = 0.78886164\n",
            "Iteration 37, loss = 0.78586148\n",
            "Iteration 38, loss = 0.78317255\n",
            "Iteration 39, loss = 0.78022418\n",
            "Iteration 40, loss = 0.77762076"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 41, loss = 0.77495681\n",
            "Iteration 42, loss = 0.77229239\n",
            "Iteration 43, loss = 0.76974491\n",
            "Iteration 44, loss = 0.76720757\n",
            "Iteration 45, loss = 0.76456895\n",
            "Iteration 46, loss = 0.76198026\n",
            "Iteration 47, loss = 0.75960833\n",
            "Iteration 48, loss = 0.75721478\n",
            "Iteration 49, loss = 0.75488200\n",
            "Iteration 50, loss = 0.75248586\n",
            "Iteration 51, loss = 0.75018259\n",
            "Iteration 52, loss = 0.74800238\n",
            "Iteration 53, loss = 0.74586725\n",
            "Iteration 54, loss = 0.74362634\n",
            "Iteration 55, loss = 0.74151143\n",
            "Iteration 56, loss = 0.73950295\n",
            "Iteration 57, loss = 0.73741831\n",
            "Iteration 58, loss = 0.73543089\n",
            "Iteration 59, loss = 0.73347890\n",
            "Iteration 60, loss = 0.73143214\n",
            "Iteration 61, loss = 0.72947052\n",
            "Iteration 62, loss = 0.72761557\n",
            "Iteration 63, loss = 0.72575659\n",
            "Iteration 64, loss = 0.72393461\n",
            "Iteration 65, loss = 0.72217066\n",
            "Iteration 66, loss = 0.72037809\n",
            "Iteration 67, loss = 0.71872244\n",
            "Iteration 68, loss = 0.71699545\n",
            "Iteration 69, loss = 0.71528424\n",
            "Iteration 70, loss = 0.71358358\n",
            "Iteration 71, loss = 0.71201015\n",
            "Iteration 72, loss = 0.71043121\n",
            "Iteration 73, loss = 0.70886096\n",
            "Iteration 74, loss = 0.70734836\n",
            "Iteration 75, loss = 0.70577004\n",
            "Iteration 76, loss = 0.70432856\n",
            "Iteration 77, loss = 0.70282172\n",
            "Iteration 78, loss = 0.70132546\n",
            "Iteration 79, loss = 0.69988901\n",
            "Iteration 80, loss = 0.69838408\n",
            "Iteration 81, loss = 0.69691519\n",
            "Iteration 82, loss = 0.69553924\n",
            "Iteration 83, loss = 0.69407292\n",
            "Iteration 84, loss = 0.69278138\n",
            "Iteration 85, loss = 0.69142090\n",
            "Iteration 86, loss = 0.69015005\n",
            "Iteration 87, loss = 0.68883737\n",
            "Iteration 88, loss = 0.68750556\n",
            "Iteration 89, loss = 0.68619651\n",
            "Iteration 90, loss = 0.68485066\n",
            "Iteration 91, loss = 0.68350643\n",
            "Iteration 92, loss = 0.68209430\n",
            "Iteration 93, loss = 0.68077022\n",
            "Iteration 94, loss = 0.67944943\n",
            "Iteration 95, loss = 0.67819217\n",
            "Iteration 96, loss = 0.67696762\n",
            "Iteration 97, loss = 0.67577008\n",
            "Iteration 98, loss = 0.67458090\n",
            "Iteration 99, loss = 0.67341100\n",
            "Iteration 100, loss = 0.67223428\n",
            "Iteration 101, loss = 0.67104506\n",
            "Iteration 102, loss = 0.66978585\n",
            "Iteration 103, loss = 0.66861227\n",
            "Iteration 104, loss = 0.66745578\n",
            "Iteration 105, loss = 0.66627850\n",
            "Iteration 106, loss = 0.66510940\n",
            "Iteration 107, loss = 0.66393676\n",
            "Iteration 108, loss = 0.66281068\n",
            "Iteration 109, loss = 0.66169601\n",
            "Iteration 110, loss = 0.66061268\n",
            "Iteration 111, loss = 0.65960111\n",
            "Iteration 112, loss = 0.65854483\n",
            "Iteration 113, loss = 0.65754644\n",
            "Iteration 114, loss = 0.65647858\n",
            "Iteration 115, loss = 0.65551663\n",
            "Iteration 116, loss = 0.65447314\n",
            "Iteration 117, loss = 0.65349882\n",
            "Iteration 118, loss = 0.65251628\n",
            "Iteration 119, loss = 0.65153519\n",
            "Iteration 120, loss = 0.65054167\n",
            "Iteration 121, loss = 0.64961018\n",
            "Iteration 122, loss = 0.64876723\n",
            "Iteration 123, loss = 0.64784156\n",
            "Iteration 124, loss = 0.64697288\n",
            "Iteration 125, loss = 0.64616527\n",
            "Iteration 126, loss = 0.64527555\n",
            "Iteration 127, loss = 0.64449520\n",
            "Iteration 128, loss = 0.64361690\n",
            "Iteration 129, loss = 0.64280606\n",
            "Iteration 130, loss = 0.64200862\n",
            "Iteration 131, loss = 0.64122482\n",
            "Iteration 132, loss = 0.64042905\n",
            "Iteration 133, loss = 0.63966933\n",
            "Iteration 134, loss = 0.63888969\n",
            "Iteration 135, loss = 0.63813226\n",
            "Iteration 136, loss = 0.63734726\n",
            "Iteration 137, loss = 0.63663371\n",
            "Iteration 138, loss = 0.63588696\n",
            "Iteration 139, loss = 0.63517280\n",
            "Iteration 140, loss = 0.63446451\n",
            "Iteration 141, loss = 0.63377240\n",
            "Iteration 142, loss = 0.63309239\n",
            "Iteration 143, loss = 0.63242575\n",
            "Iteration 144, loss = 0.63171326\n",
            "Iteration 145, loss = 0.63108449\n",
            "Iteration 146, loss = 0.63045197\n",
            "Iteration 147, loss = 0.62980498\n",
            "Iteration 148, loss = 0.62919895\n",
            "Iteration 149, loss = 0.62856276\n",
            "Iteration 150, loss = 0.62792757\n",
            "Iteration 151, loss = 0.62728295\n",
            "Iteration 152, loss = 0.62668567\n",
            "Iteration 153, loss = 0.62605320\n",
            "Iteration 154, loss = 0.62546216\n",
            "Iteration 155, loss = 0.62489327\n",
            "Iteration 156, loss = 0.62430820\n",
            "Iteration 157, loss = 0.62376765\n",
            "Iteration 158, loss = 0.62323552\n",
            "Iteration 159, loss = 0.62263925\n",
            "Iteration 160, loss = 0.62209814\n",
            "Iteration 161, loss = 0.62156367\n",
            "Iteration 162, loss = 0.62102885\n",
            "Iteration 163, loss = 0.62053098\n",
            "Iteration 164, loss = 0.62002699\n",
            "Iteration 165, loss = 0.61953419\n",
            "Iteration 166, loss = 0.61903164\n",
            "Iteration 167, loss = 0.61857634\n",
            "Iteration 168, loss = 0.61806699\n",
            "Iteration 169, loss = 0.61763124\n",
            "Iteration 170, loss = 0.61720855\n",
            "Iteration 171, loss = 0.61678803\n",
            "Iteration 172, loss = 0.61631638\n",
            "Iteration 173, loss = 0.61592079\n",
            "Iteration 174, loss = 0.61550139\n",
            "Iteration 175, loss = 0.61506420\n",
            "Iteration 176, loss = 0.61463952\n",
            "Iteration 177, loss = 0.61420145\n",
            "Iteration 178, loss = 0.61381566\n",
            "Iteration 179, loss = 0.61337462\n",
            "Iteration 180, loss = 0.61294324\n",
            "Iteration 181, loss = 0.61254902\n",
            "Iteration 182, loss = 0.61213546\n",
            "Iteration 183, loss = 0.61171487\n",
            "Iteration 184, loss = 0.61129479\n",
            "Iteration 185, loss = 0.61090305\n",
            "Iteration 186, loss = 0.61052582\n",
            "Iteration 187, loss = 0.61017439\n",
            "Iteration 188, loss = 0.60981990\n",
            "Iteration 189, loss = 0.60946219\n",
            "Iteration 190, loss = 0.60909915\n",
            "Iteration 191, loss = 0.60873686\n",
            "Iteration 192, loss = 0.60836788\n",
            "Iteration 193, loss = 0.60798240\n",
            "Iteration 194, loss = 0.60760311\n",
            "Iteration 195, loss = 0.60724688\n",
            "Iteration 196, loss = 0.60685989\n",
            "Iteration 197, loss = 0.60648148\n",
            "Iteration 198, loss = 0.60614240\n",
            "Iteration 199, loss = 0.60579278\n",
            "Iteration 200, loss = 0.60546356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.92515262\n",
            "Iteration 2, loss = 0.92320793\n",
            "Iteration 3, loss = 0.91997489\n",
            "Iteration 4, loss = 0.91591886\n",
            "Iteration 5, loss = 0.91108617\n",
            "Iteration 6, loss = 0.90611197\n",
            "Iteration 7, loss = 0.90111358\n",
            "Iteration 8, loss = 0.89611580\n",
            "Iteration 9, loss = 0.89103097\n",
            "Iteration 10, loss = 0.88614356\n",
            "Iteration 11, loss = 0.88144205\n",
            "Iteration 12, loss = 0.87657599\n",
            "Iteration 13, loss = 0.87196161\n",
            "Iteration 14, loss = 0.86739565\n",
            "Iteration 15, loss = 0.86293846\n",
            "Iteration 16, loss = 0.85890079\n",
            "Iteration 17, loss = 0.85483203\n",
            "Iteration 18, loss = 0.85098669\n",
            "Iteration 19, loss = 0.84699066\n",
            "Iteration 20, loss = 0.84324236\n",
            "Iteration 21, loss = 0.83945760\n",
            "Iteration 22, loss = 0.83574260\n",
            "Iteration 23, loss = 0.83208773\n",
            "Iteration 24, loss = 0.82831715\n",
            "Iteration 25, loss = 0.82490285\n",
            "Iteration 26, loss = 0.82145077\n",
            "Iteration 27, loss = 0.81803640\n",
            "Iteration 28, loss = 0.81467422\n",
            "Iteration 29, loss = 0.81143074\n",
            "Iteration 30, loss = 0.80809690\n",
            "Iteration 31, loss = 0.80507070\n",
            "Iteration 32, loss = 0.80183450\n",
            "Iteration 33, loss = 0.79879721\n",
            "Iteration 34, loss = 0.79582125\n",
            "Iteration 35, loss = 0.79278224\n",
            "Iteration 36, loss = 0.78996639\n",
            "Iteration 37, loss = 0.78695614\n",
            "Iteration 38, loss = 0.78424656\n",
            "Iteration 39, loss = 0.78127393\n",
            "Iteration 40, loss = 0.77872840\n",
            "Iteration 41, loss = 0.77606929\n",
            "Iteration 42, loss = 0.77339733\n",
            "Iteration 43, loss = 0.77081091\n",
            "Iteration 44, loss = 0.76835674\n",
            "Iteration 45, loss = 0.76574743\n",
            "Iteration 46, loss = 0.76315724\n",
            "Iteration 47, loss = 0.76082949\n",
            "Iteration 48, loss = 0.75845520\n",
            "Iteration 49, loss = 0.75612422\n",
            "Iteration 50, loss = 0.75378236\n",
            "Iteration 51, loss = 0.75155630\n",
            "Iteration 52, loss = 0.74945850\n",
            "Iteration 53, loss = 0.74738251\n",
            "Iteration 54, loss = 0.74516311\n",
            "Iteration 55, loss = 0.74313444\n",
            "Iteration 56, loss = 0.74120140\n",
            "Iteration 57, loss = 0.73915571\n",
            "Iteration 58, loss = 0.73719056\n",
            "Iteration 59, loss = 0.73528087\n",
            "Iteration 60, loss = 0.73334510\n",
            "Iteration 61, loss = 0.73141029\n",
            "Iteration 62, loss = 0.72959204\n",
            "Iteration 63, loss = 0.72775738\n",
            "Iteration 64, loss = 0.72591830\n",
            "Iteration 65, loss = 0.72420142\n",
            "Iteration 66, loss = 0.72236917\n",
            "Iteration 67, loss = 0.72072695\n",
            "Iteration 68, loss = 0.71904202\n",
            "Iteration 69, loss = 0.71728188\n",
            "Iteration 70, loss = 0.71565896\n",
            "Iteration 71, loss = 0.71408007\n",
            "Iteration 72, loss = 0.71245632\n",
            "Iteration 73, loss = 0.71088491\n",
            "Iteration 74, loss = 0.70937593\n",
            "Iteration 75, loss = 0.70783125\n",
            "Iteration 76, loss = 0.70637332\n",
            "Iteration 77, loss = 0.70488077\n",
            "Iteration 78, loss = 0.70340466\n",
            "Iteration 79, loss = 0.70197001\n",
            "Iteration 80, loss = 0.70049560\n",
            "Iteration 81, loss = 0.69904607\n",
            "Iteration 82, loss = 0.69762145\n",
            "Iteration 83, loss = 0.69614817\n",
            "Iteration 84, loss = 0.69479063\n",
            "Iteration 85, loss = 0.69346291\n",
            "Iteration 86, loss = 0.69214622\n",
            "Iteration 87, loss = 0.69083577\n",
            "Iteration 88, loss = 0.68948877\n",
            "Iteration 89, loss = 0.68815146\n",
            "Iteration 90, loss = 0.68677016\n",
            "Iteration 91, loss = 0.68540869\n",
            "Iteration 92, loss = 0.68391370\n",
            "Iteration 93, loss = 0.68257332\n",
            "Iteration 94, loss = 0.68117831\n",
            "Iteration 95, loss = 0.67987551\n",
            "Iteration 96, loss = 0.67862677\n",
            "Iteration 97, loss = 0.67738780\n",
            "Iteration 98, loss = 0.67615795\n",
            "Iteration 99, loss = 0.67497811\n",
            "Iteration 100, loss = 0.67374746\n",
            "Iteration 101, loss = 0.67250444\n",
            "Iteration 102, loss = 0.67126693\n",
            "Iteration 103, loss = 0.67005472\n",
            "Iteration 104, loss = 0.66890725\n",
            "Iteration 105, loss = 0.66774064\n",
            "Iteration 106, loss = 0.66657045\n",
            "Iteration 107, loss = 0.66541036\n",
            "Iteration 108, loss = 0.66432764\n",
            "Iteration 109, loss = 0.66321036\n",
            "Iteration 110, loss = 0.66213977\n",
            "Iteration 111, loss = 0.66112772\n",
            "Iteration 112, loss = 0.66008692\n",
            "Iteration 113, loss = 0.65914036\n",
            "Iteration 114, loss = 0.65809409\n",
            "Iteration 115, loss = 0.65714780\n",
            "Iteration 116, loss = 0.65615028\n",
            "Iteration 117, loss = 0.65520116\n",
            "Iteration 118, loss = 0.65422912\n",
            "Iteration 119, loss = 0.65325035\n",
            "Iteration 120, loss = 0.65225538\n",
            "Iteration 121, loss = 0.65132736\n",
            "Iteration 122, loss = 0.65045346\n",
            "Iteration 123, loss = 0.64954977\n",
            "Iteration 124, loss = 0.64862332\n",
            "Iteration 125, loss = 0.64775884\n",
            "Iteration 126, loss = 0.64684982\n",
            "Iteration 127, loss = 0.64601374\n",
            "Iteration 128, loss = 0.64505299\n",
            "Iteration 129, loss = 0.64418821\n",
            "Iteration 130, loss = 0.64332865\n",
            "Iteration 131, loss = 0.64247441\n",
            "Iteration 132, loss = 0.64162688\n",
            "Iteration 133, loss = 0.64078769\n",
            "Iteration 134, loss = 0.63992572\n",
            "Iteration 135, loss = 0.63907326\n",
            "Iteration 136, loss = 0.63822019\n",
            "Iteration 137, loss = 0.63748852\n",
            "Iteration 138, loss = 0.63671692\n",
            "Iteration 139, loss = 0.63598547\n",
            "Iteration 140, loss = 0.63523103\n",
            "Iteration 141, loss = 0.63449165\n",
            "Iteration 142, loss = 0.63370574\n",
            "Iteration 143, loss = 0.63296622\n",
            "Iteration 144, loss = 0.63219230\n",
            "Iteration 145, loss = 0.63148231\n",
            "Iteration 146, loss = 0.63076723\n",
            "Iteration 147, loss = 0.63004646\n",
            "Iteration 148, loss = 0.62937075\n",
            "Iteration 149, loss = 0.62869006\n",
            "Iteration 150, loss = 0.62796728\n",
            "Iteration 151, loss = 0.62729451\n",
            "Iteration 152, loss = 0.62660678\n",
            "Iteration 153, loss = 0.62590347\n",
            "Iteration 154, loss = 0.62522243\n",
            "Iteration 155, loss = 0.62458056\n",
            "Iteration 156, loss = 0.62393185\n",
            "Iteration 157, loss = 0.62329807\n",
            "Iteration 158, loss = 0.62267699\n",
            "Iteration 159, loss = 0.62198637\n",
            "Iteration 160, loss = 0.62137726\n",
            "Iteration 161, loss = 0.62076873\n",
            "Iteration 162, loss = 0.62021435\n",
            "Iteration 163, loss = 0.61964814\n",
            "Iteration 164, loss = 0.61909061\n",
            "Iteration 165, loss = 0.61855327\n",
            "Iteration 166, loss = 0.61798790\n",
            "Iteration 167, loss = 0.61747390\n",
            "Iteration 168, loss = 0.61694193\n",
            "Iteration 169, loss = 0.61646207\n",
            "Iteration 170, loss = 0.61597248\n",
            "Iteration 171, loss = 0.61547669\n",
            "Iteration 172, loss = 0.61495934\n",
            "Iteration 173, loss = 0.61449388\n",
            "Iteration 174, loss = 0.61400510\n",
            "Iteration 175, loss = 0.61349986\n",
            "Iteration 176, loss = 0.61300698\n",
            "Iteration 177, loss = 0.61250294\n",
            "Iteration 178, loss = 0.61202343\n",
            "Iteration 179, loss = 0.61154230\n",
            "Iteration 180, loss = 0.61100797\n",
            "Iteration 181, loss = 0.61051376\n",
            "Iteration 182, loss = 0.61004297\n",
            "Iteration 183, loss = 0.60953382\n",
            "Iteration 184, loss = 0.60905911\n",
            "Iteration 185, loss = 0.60858798\n",
            "Iteration 186, loss = 0.60816841\n",
            "Iteration 187, loss = 0.60774487\n",
            "Iteration 188, loss = 0.60733655\n",
            "Iteration 189, loss = 0.60691087\n",
            "Iteration 190, loss = 0.60654567\n",
            "Iteration 191, loss = 0.60611222\n",
            "Iteration 192, loss = 0.60570740\n",
            "Iteration 193, loss = 0.60527708\n",
            "Iteration 194, loss = 0.60482520\n",
            "Iteration 195, loss = 0.60443089\n",
            "Iteration 196, loss = 0.60398392\n",
            "Iteration 197, loss = 0.60355145\n",
            "Iteration 198, loss = 0.60316406\n",
            "Iteration 199, loss = 0.60277230\n",
            "Iteration 200, loss = 0.60238668\n",
            "Iteration 1, loss = 0.93462281\n",
            "Iteration 2, loss = 0.93261224\n",
            "Iteration 3, loss = 0.92941834\n",
            "Iteration 4, loss = 0.92525033\n",
            "Iteration 5, loss = 0.92065853\n",
            "Iteration 6, loss = 0.91555772\n",
            "Iteration 7, loss = 0.91059613\n",
            "Iteration 8, loss = 0.90541505\n",
            "Iteration 9, loss = 0.90029749\n",
            "Iteration 10, loss = 0.89529951\n",
            "Iteration 11, loss = 0.89038858\n",
            "Iteration 12, loss = 0.88547267\n",
            "Iteration 13, loss = 0.88066014\n",
            "Iteration 14, loss = 0.87599931\n",
            "Iteration 15, loss = 0.87133466\n",
            "Iteration 16, loss = 0.86701607\n",
            "Iteration 17, loss = 0.86274708\n",
            "Iteration 18, loss = 0.85864838\n",
            "Iteration 19, loss = 0.85439727\n",
            "Iteration 20, loss = 0.85037709\n",
            "Iteration 21, loss = 0.84634247\n",
            "Iteration 22, loss = 0.84239937\n",
            "Iteration 23, loss = 0.83855448\n",
            "Iteration 24, loss = 0.83465813\n",
            "Iteration 25, loss = 0.83096674\n",
            "Iteration 26, loss = 0.82741978\n",
            "Iteration 27, loss = 0.82371122\n",
            "Iteration 28, loss = 0.82025754\n",
            "Iteration 29, loss = 0.81690640\n",
            "Iteration 30, loss = 0.81348565\n",
            "Iteration 31, loss = 0.81033127\n",
            "Iteration 32, loss = 0.80706458\n",
            "Iteration 33, loss = 0.80388785\n",
            "Iteration 34, loss = 0.80070719\n",
            "Iteration 35, loss = 0.79753268\n",
            "Iteration 36, loss = 0.79445505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 37, loss = 0.79131373\n",
            "Iteration 38, loss = 0.78835925\n",
            "Iteration 39, loss = 0.78525734\n",
            "Iteration 40, loss = 0.78258735\n",
            "Iteration 41, loss = 0.77976841\n",
            "Iteration 42, loss = 0.77689181\n",
            "Iteration 43, loss = 0.77404346\n",
            "Iteration 44, loss = 0.77144905\n",
            "Iteration 45, loss = 0.76875241\n",
            "Iteration 46, loss = 0.76603951\n",
            "Iteration 47, loss = 0.76363139\n",
            "Iteration 48, loss = 0.76122007\n",
            "Iteration 49, loss = 0.75874285\n",
            "Iteration 50, loss = 0.75632330\n",
            "Iteration 51, loss = 0.75401154\n",
            "Iteration 52, loss = 0.75172788\n",
            "Iteration 53, loss = 0.74946183\n",
            "Iteration 54, loss = 0.74711655\n",
            "Iteration 55, loss = 0.74496158\n",
            "Iteration 56, loss = 0.74289829\n",
            "Iteration 57, loss = 0.74075807\n",
            "Iteration 58, loss = 0.73864727\n",
            "Iteration 59, loss = 0.73658594\n",
            "Iteration 60, loss = 0.73447008\n",
            "Iteration 61, loss = 0.73240466\n",
            "Iteration 62, loss = 0.73039789\n",
            "Iteration 63, loss = 0.72845788\n",
            "Iteration 64, loss = 0.72653095\n",
            "Iteration 65, loss = 0.72471988\n",
            "Iteration 66, loss = 0.72273809\n",
            "Iteration 67, loss = 0.72088747\n",
            "Iteration 68, loss = 0.71908629\n",
            "Iteration 69, loss = 0.71713136\n",
            "Iteration 70, loss = 0.71527881\n",
            "Iteration 71, loss = 0.71355173\n",
            "Iteration 72, loss = 0.71174876\n",
            "Iteration 73, loss = 0.70999877\n",
            "Iteration 74, loss = 0.70831828\n",
            "Iteration 75, loss = 0.70659337\n",
            "Iteration 76, loss = 0.70499195\n",
            "Iteration 77, loss = 0.70337414\n",
            "Iteration 78, loss = 0.70177357\n",
            "Iteration 79, loss = 0.70025428\n",
            "Iteration 80, loss = 0.69861256\n",
            "Iteration 81, loss = 0.69707178\n",
            "Iteration 82, loss = 0.69552728\n",
            "Iteration 83, loss = 0.69393161\n",
            "Iteration 84, loss = 0.69241273\n",
            "Iteration 85, loss = 0.69095717\n",
            "Iteration 86, loss = 0.68948161\n",
            "Iteration 87, loss = 0.68804856\n",
            "Iteration 88, loss = 0.68658037\n",
            "Iteration 89, loss = 0.68510199\n",
            "Iteration 90, loss = 0.68363864\n",
            "Iteration 91, loss = 0.68220180\n",
            "Iteration 92, loss = 0.68063484\n",
            "Iteration 93, loss = 0.67918267\n",
            "Iteration 94, loss = 0.67773565\n",
            "Iteration 95, loss = 0.67630324\n",
            "Iteration 96, loss = 0.67493092\n",
            "Iteration 97, loss = 0.67360341\n",
            "Iteration 98, loss = 0.67227445\n",
            "Iteration 99, loss = 0.67098636\n",
            "Iteration 100, loss = 0.66966581\n",
            "Iteration 101, loss = 0.66833240\n",
            "Iteration 102, loss = 0.66705544\n",
            "Iteration 103, loss = 0.66575348\n",
            "Iteration 104, loss = 0.66452828\n",
            "Iteration 105, loss = 0.66327437\n",
            "Iteration 106, loss = 0.66199517\n",
            "Iteration 107, loss = 0.66074385\n",
            "Iteration 108, loss = 0.65955795\n",
            "Iteration 109, loss = 0.65835191\n",
            "Iteration 110, loss = 0.65721563\n",
            "Iteration 111, loss = 0.65614481\n",
            "Iteration 112, loss = 0.65496847\n",
            "Iteration 113, loss = 0.65389317\n",
            "Iteration 114, loss = 0.65273817\n",
            "Iteration 115, loss = 0.65168348\n",
            "Iteration 116, loss = 0.65059551\n",
            "Iteration 117, loss = 0.64957036\n",
            "Iteration 118, loss = 0.64856594\n",
            "Iteration 119, loss = 0.64753817\n",
            "Iteration 120, loss = 0.64651576\n",
            "Iteration 121, loss = 0.64553035\n",
            "Iteration 122, loss = 0.64455996\n",
            "Iteration 123, loss = 0.64351206\n",
            "Iteration 124, loss = 0.64252108\n",
            "Iteration 125, loss = 0.64161128\n",
            "Iteration 126, loss = 0.64068012\n",
            "Iteration 127, loss = 0.63985894\n",
            "Iteration 128, loss = 0.63893508\n",
            "Iteration 129, loss = 0.63808524\n",
            "Iteration 130, loss = 0.63725818\n",
            "Iteration 131, loss = 0.63641797\n",
            "Iteration 132, loss = 0.63561298\n",
            "Iteration 133, loss = 0.63477335\n",
            "Iteration 134, loss = 0.63391441\n",
            "Iteration 135, loss = 0.63304245\n",
            "Iteration 136, loss = 0.63216148\n",
            "Iteration 137, loss = 0.63136898\n",
            "Iteration 138, loss = 0.63057856\n",
            "Iteration 139, loss = 0.62978781\n",
            "Iteration 140, loss = 0.62900919\n",
            "Iteration 141, loss = 0.62822730\n",
            "Iteration 142, loss = 0.62742769\n",
            "Iteration 143, loss = 0.62666233\n",
            "Iteration 144, loss = 0.62583832\n",
            "Iteration 145, loss = 0.62506955\n",
            "Iteration 146, loss = 0.62430315\n",
            "Iteration 147, loss = 0.62354551\n",
            "Iteration 148, loss = 0.62284358\n",
            "Iteration 149, loss = 0.62213004\n",
            "Iteration 150, loss = 0.62138943\n",
            "Iteration 151, loss = 0.62070810\n",
            "Iteration 152, loss = 0.62003486\n",
            "Iteration 153, loss = 0.61933758\n",
            "Iteration 154, loss = 0.61866106\n",
            "Iteration 155, loss = 0.61806783\n",
            "Iteration 156, loss = 0.61742271\n",
            "Iteration 157, loss = 0.61680810\n",
            "Iteration 158, loss = 0.61624284\n",
            "Iteration 159, loss = 0.61561695\n",
            "Iteration 160, loss = 0.61504814\n",
            "Iteration 161, loss = 0.61447339\n",
            "Iteration 162, loss = 0.61393348\n",
            "Iteration 163, loss = 0.61338422\n",
            "Iteration 164, loss = 0.61282368\n",
            "Iteration 165, loss = 0.61227801\n",
            "Iteration 166, loss = 0.61174106\n",
            "Iteration 167, loss = 0.61121644\n",
            "Iteration 168, loss = 0.61069513\n",
            "Iteration 169, loss = 0.61022305\n",
            "Iteration 170, loss = 0.60973026\n",
            "Iteration 171, loss = 0.60925344\n",
            "Iteration 172, loss = 0.60876788\n",
            "Iteration 173, loss = 0.60831122\n",
            "Iteration 174, loss = 0.60782154\n",
            "Iteration 175, loss = 0.60732081\n",
            "Iteration 176, loss = 0.60681934\n",
            "Iteration 177, loss = 0.60630945\n",
            "Iteration 178, loss = 0.60582926\n",
            "Iteration 179, loss = 0.60531436\n",
            "Iteration 180, loss = 0.60478088\n",
            "Iteration 181, loss = 0.60425562\n",
            "Iteration 182, loss = 0.60377797\n",
            "Iteration 183, loss = 0.60325576\n",
            "Iteration 184, loss = 0.60278176\n",
            "Iteration 185, loss = 0.60229687\n",
            "Iteration 186, loss = 0.60185511\n",
            "Iteration 187, loss = 0.60139929\n",
            "Iteration 188, loss = 0.60096361\n",
            "Iteration 189, loss = 0.60053984\n",
            "Iteration 190, loss = 0.60015279\n",
            "Iteration 191, loss = 0.59970373\n",
            "Iteration 192, loss = 0.59931360\n",
            "Iteration 193, loss = 0.59887315\n",
            "Iteration 194, loss = 0.59841493\n",
            "Iteration 195, loss = 0.59799575\n",
            "Iteration 196, loss = 0.59756929\n",
            "Iteration 197, loss = 0.59715635\n",
            "Iteration 198, loss = 0.59678665\n",
            "Iteration 199, loss = 0.59639952\n",
            "Iteration 200, loss = 0.59605798\n",
            "Iteration 1, loss = 0.93310249\n",
            "Iteration 2, loss = 0.93101092\n",
            "Iteration 3, loss = 0.92770703\n",
            "Iteration 4, loss = 0.92348094\n",
            "Iteration 5, loss = 0.91870634\n",
            "Iteration 6, loss = 0.91350854\n",
            "Iteration 7, loss = 0.90855626\n",
            "Iteration 8, loss = 0.90333343\n",
            "Iteration 9, loss = 0.89816244\n",
            "Iteration 10, loss = 0.89306903\n",
            "Iteration 11, loss = 0.88797547\n",
            "Iteration 12, loss = 0.88302867\n",
            "Iteration 13, loss = 0.87830591\n",
            "Iteration 14, loss = 0.87363722\n",
            "Iteration 15, loss = 0.86901748\n",
            "Iteration 16, loss = 0.86479539\n",
            "Iteration 17, loss = 0.86038833\n",
            "Iteration 18, loss = 0.85639915\n",
            "Iteration 19, loss = 0.85227054\n",
            "Iteration 20, loss = 0.84834947\n",
            "Iteration 21, loss = 0.84451866\n",
            "Iteration 22, loss = 0.84076112\n",
            "Iteration 23, loss = 0.83717459\n",
            "Iteration 24, loss = 0.83341012\n",
            "Iteration 25, loss = 0.82979470\n",
            "Iteration 26, loss = 0.82628064\n",
            "Iteration 27, loss = 0.82271049\n",
            "Iteration 28, loss = 0.81929932\n",
            "Iteration 29, loss = 0.81605612\n",
            "Iteration 30, loss = 0.81276286\n",
            "Iteration 31, loss = 0.80960758\n",
            "Iteration 32, loss = 0.80651049\n",
            "Iteration 33, loss = 0.80331972\n",
            "Iteration 34, loss = 0.80023102\n",
            "Iteration 35, loss = 0.79699869\n",
            "Iteration 36, loss = 0.79400137\n",
            "Iteration 37, loss = 0.79097394\n",
            "Iteration 38, loss = 0.78805094\n",
            "Iteration 39, loss = 0.78511982\n",
            "Iteration 40, loss = 0.78247517\n",
            "Iteration 41, loss = 0.77979332\n",
            "Iteration 42, loss = 0.77699418\n",
            "Iteration 43, loss = 0.77429562\n",
            "Iteration 44, loss = 0.77172579\n",
            "Iteration 45, loss = 0.76916050\n",
            "Iteration 46, loss = 0.76647297\n",
            "Iteration 47, loss = 0.76409478\n",
            "Iteration 48, loss = 0.76166267\n",
            "Iteration 49, loss = 0.75916081\n",
            "Iteration 50, loss = 0.75678157\n",
            "Iteration 51, loss = 0.75448138\n",
            "Iteration 52, loss = 0.75215647\n",
            "Iteration 53, loss = 0.74984933\n",
            "Iteration 54, loss = 0.74751943\n",
            "Iteration 55, loss = 0.74539031\n",
            "Iteration 56, loss = 0.74335546\n",
            "Iteration 57, loss = 0.74121902\n",
            "Iteration 58, loss = 0.73912598\n",
            "Iteration 59, loss = 0.73715404\n",
            "Iteration 60, loss = 0.73507830\n",
            "Iteration 61, loss = 0.73318207\n",
            "Iteration 62, loss = 0.73126676"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 63, loss = 0.72939938\n",
            "Iteration 64, loss = 0.72756473\n",
            "Iteration 65, loss = 0.72579634\n",
            "Iteration 66, loss = 0.72394008\n",
            "Iteration 67, loss = 0.72217471\n",
            "Iteration 68, loss = 0.72043589\n",
            "Iteration 69, loss = 0.71850239\n",
            "Iteration 70, loss = 0.71671073\n",
            "Iteration 71, loss = 0.71501354\n",
            "Iteration 72, loss = 0.71325594\n",
            "Iteration 73, loss = 0.71157480\n",
            "Iteration 74, loss = 0.70997376\n",
            "Iteration 75, loss = 0.70834349\n",
            "Iteration 76, loss = 0.70678517\n",
            "Iteration 77, loss = 0.70527284\n",
            "Iteration 78, loss = 0.70373065\n",
            "Iteration 79, loss = 0.70226150\n",
            "Iteration 80, loss = 0.70070864\n",
            "Iteration 81, loss = 0.69912475\n",
            "Iteration 82, loss = 0.69757835\n",
            "Iteration 83, loss = 0.69597960\n",
            "Iteration 84, loss = 0.69442451\n",
            "Iteration 85, loss = 0.69299803\n",
            "Iteration 86, loss = 0.69155555\n",
            "Iteration 87, loss = 0.69017218\n",
            "Iteration 88, loss = 0.68882177\n",
            "Iteration 89, loss = 0.68743527\n",
            "Iteration 90, loss = 0.68603636\n",
            "Iteration 91, loss = 0.68469807\n",
            "Iteration 92, loss = 0.68322054\n",
            "Iteration 93, loss = 0.68188267\n",
            "Iteration 94, loss = 0.68056752\n",
            "Iteration 95, loss = 0.67922529\n",
            "Iteration 96, loss = 0.67794626\n",
            "Iteration 97, loss = 0.67673420\n",
            "Iteration 98, loss = 0.67547574\n",
            "Iteration 99, loss = 0.67423622\n",
            "Iteration 100, loss = 0.67305365\n",
            "Iteration 101, loss = 0.67184034\n",
            "Iteration 102, loss = 0.67071311\n",
            "Iteration 103, loss = 0.66952001\n",
            "Iteration 104, loss = 0.66837398\n",
            "Iteration 105, loss = 0.66719879\n",
            "Iteration 106, loss = 0.66603831\n",
            "Iteration 107, loss = 0.66481475\n",
            "Iteration 108, loss = 0.66370231\n",
            "Iteration 109, loss = 0.66253770\n",
            "Iteration 110, loss = 0.66148177\n",
            "Iteration 111, loss = 0.66037129\n",
            "Iteration 112, loss = 0.65928578\n",
            "Iteration 113, loss = 0.65824376\n",
            "Iteration 114, loss = 0.65714020\n",
            "Iteration 115, loss = 0.65610681\n",
            "Iteration 116, loss = 0.65510969\n",
            "Iteration 117, loss = 0.65415203\n",
            "Iteration 118, loss = 0.65322297\n",
            "Iteration 119, loss = 0.65223789\n",
            "Iteration 120, loss = 0.65126999\n",
            "Iteration 121, loss = 0.65039327\n",
            "Iteration 122, loss = 0.64948728\n",
            "Iteration 123, loss = 0.64856928\n",
            "Iteration 124, loss = 0.64763909\n",
            "Iteration 125, loss = 0.64676217\n",
            "Iteration 126, loss = 0.64587163\n",
            "Iteration 127, loss = 0.64508020\n",
            "Iteration 128, loss = 0.64418209\n",
            "Iteration 129, loss = 0.64334651\n",
            "Iteration 130, loss = 0.64251319\n",
            "Iteration 131, loss = 0.64166131\n",
            "Iteration 132, loss = 0.64084183\n",
            "Iteration 133, loss = 0.64000666\n",
            "Iteration 134, loss = 0.63915158\n",
            "Iteration 135, loss = 0.63828495\n",
            "Iteration 136, loss = 0.63743946\n",
            "Iteration 137, loss = 0.63664379\n",
            "Iteration 138, loss = 0.63587903\n",
            "Iteration 139, loss = 0.63510284\n",
            "Iteration 140, loss = 0.63436477\n",
            "Iteration 141, loss = 0.63362194\n",
            "Iteration 142, loss = 0.63288549\n",
            "Iteration 143, loss = 0.63218612\n",
            "Iteration 144, loss = 0.63138964\n",
            "Iteration 145, loss = 0.63070869\n",
            "Iteration 146, loss = 0.62998211\n",
            "Iteration 147, loss = 0.62929853\n",
            "Iteration 148, loss = 0.62865177\n",
            "Iteration 149, loss = 0.62799051\n",
            "Iteration 150, loss = 0.62730519\n",
            "Iteration 151, loss = 0.62669492\n",
            "Iteration 152, loss = 0.62603760\n",
            "Iteration 153, loss = 0.62537564\n",
            "Iteration 154, loss = 0.62473405\n",
            "Iteration 155, loss = 0.62414264\n",
            "Iteration 156, loss = 0.62350340\n",
            "Iteration 157, loss = 0.62289037\n",
            "Iteration 158, loss = 0.62231641\n",
            "Iteration 159, loss = 0.62174108\n",
            "Iteration 160, loss = 0.62120407\n",
            "Iteration 161, loss = 0.62066035\n",
            "Iteration 162, loss = 0.62013935\n",
            "Iteration 163, loss = 0.61959465\n",
            "Iteration 164, loss = 0.61906516\n",
            "Iteration 165, loss = 0.61850232\n",
            "Iteration 166, loss = 0.61796057\n",
            "Iteration 167, loss = 0.61744452\n",
            "Iteration 168, loss = 0.61693136\n",
            "Iteration 169, loss = 0.61646205\n",
            "Iteration 170, loss = 0.61597786\n",
            "Iteration 171, loss = 0.61553054\n",
            "Iteration 172, loss = 0.61508273\n",
            "Iteration 173, loss = 0.61465326\n",
            "Iteration 174, loss = 0.61419772\n",
            "Iteration 175, loss = 0.61374751\n",
            "Iteration 176, loss = 0.61327769\n",
            "Iteration 177, loss = 0.61283197\n",
            "Iteration 178, loss = 0.61237918\n",
            "Iteration 179, loss = 0.61192631\n",
            "Iteration 180, loss = 0.61142976\n",
            "Iteration 181, loss = 0.61096015\n",
            "Iteration 182, loss = 0.61051523\n",
            "Iteration 183, loss = 0.61005702\n",
            "Iteration 184, loss = 0.60960340\n",
            "Iteration 185, loss = 0.60918261\n",
            "Iteration 186, loss = 0.60877521\n",
            "Iteration 187, loss = 0.60837667\n",
            "Iteration 188, loss = 0.60799180\n",
            "Iteration 189, loss = 0.60762399\n",
            "Iteration 190, loss = 0.60728135\n",
            "Iteration 191, loss = 0.60689327\n",
            "Iteration 192, loss = 0.60654312\n",
            "Iteration 193, loss = 0.60616760\n",
            "Iteration 194, loss = 0.60578300\n",
            "Iteration 195, loss = 0.60541803\n",
            "Iteration 196, loss = 0.60505822\n",
            "Iteration 197, loss = 0.60472181\n",
            "Iteration 198, loss = 0.60435601\n",
            "Iteration 199, loss = 0.60402445\n",
            "Iteration 200, loss = 0.60371389\n",
            "Iteration 1, loss = 0.93946865\n",
            "Iteration 2, loss = 0.93734548\n",
            "Iteration 3, loss = 0.93396022\n",
            "Iteration 4, loss = 0.92974495\n",
            "Iteration 5, loss = 0.92508441\n",
            "Iteration 6, loss = 0.92006070\n",
            "Iteration 7, loss = 0.91518496\n",
            "Iteration 8, loss = 0.91020915\n",
            "Iteration 9, loss = 0.90529195\n",
            "Iteration 10, loss = 0.90050938\n",
            "Iteration 11, loss = 0.89552786\n",
            "Iteration 12, loss = 0.89092624\n",
            "Iteration 13, loss = 0.88627671\n",
            "Iteration 14, loss = 0.88176332\n",
            "Iteration 15, loss = 0.87748747\n",
            "Iteration 16, loss = 0.87338356\n",
            "Iteration 17, loss = 0.86932619\n",
            "Iteration 18, loss = 0.86557100"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 19, loss = 0.86175953\n",
            "Iteration 20, loss = 0.85810708\n",
            "Iteration 21, loss = 0.85457487\n",
            "Iteration 22, loss = 0.85096897\n",
            "Iteration 23, loss = 0.84761148\n",
            "Iteration 24, loss = 0.84404867\n",
            "Iteration 25, loss = 0.84066128\n",
            "Iteration 26, loss = 0.83730958\n",
            "Iteration 27, loss = 0.83388757\n",
            "Iteration 28, loss = 0.83057501\n",
            "Iteration 29, loss = 0.82738517\n",
            "Iteration 30, loss = 0.82415322\n",
            "Iteration 31, loss = 0.82106702\n",
            "Iteration 32, loss = 0.81815273\n",
            "Iteration 33, loss = 0.81509467\n",
            "Iteration 34, loss = 0.81215013\n",
            "Iteration 35, loss = 0.80906566\n",
            "Iteration 36, loss = 0.80619125\n",
            "Iteration 37, loss = 0.80336983\n",
            "Iteration 38, loss = 0.80049635\n",
            "Iteration 39, loss = 0.79772338\n",
            "Iteration 40, loss = 0.79507950\n",
            "Iteration 41, loss = 0.79258288\n",
            "Iteration 42, loss = 0.78995614\n",
            "Iteration 43, loss = 0.78747175\n",
            "Iteration 44, loss = 0.78509049\n",
            "Iteration 45, loss = 0.78279095\n",
            "Iteration 46, loss = 0.78030914\n",
            "Iteration 47, loss = 0.77804290\n",
            "Iteration 48, loss = 0.77571093\n",
            "Iteration 49, loss = 0.77337670\n",
            "Iteration 50, loss = 0.77119120\n",
            "Iteration 51, loss = 0.76906050\n",
            "Iteration 52, loss = 0.76683251\n",
            "Iteration 53, loss = 0.76474393\n",
            "Iteration 54, loss = 0.76252897\n",
            "Iteration 55, loss = 0.76055234\n",
            "Iteration 56, loss = 0.75863057\n",
            "Iteration 57, loss = 0.75661412\n",
            "Iteration 58, loss = 0.75460319\n",
            "Iteration 59, loss = 0.75272004\n",
            "Iteration 60, loss = 0.75074771\n",
            "Iteration 61, loss = 0.74888193\n",
            "Iteration 62, loss = 0.74706429\n",
            "Iteration 63, loss = 0.74522519\n",
            "Iteration 64, loss = 0.74342654\n",
            "Iteration 65, loss = 0.74166896\n",
            "Iteration 66, loss = 0.73980177\n",
            "Iteration 67, loss = 0.73797811\n",
            "Iteration 68, loss = 0.73626208\n",
            "Iteration 69, loss = 0.73436834\n",
            "Iteration 70, loss = 0.73252826\n",
            "Iteration 71, loss = 0.73089009\n",
            "Iteration 72, loss = 0.72915676\n",
            "Iteration 73, loss = 0.72752511\n",
            "Iteration 74, loss = 0.72591320\n",
            "Iteration 75, loss = 0.72434095\n",
            "Iteration 76, loss = 0.72284166\n",
            "Iteration 77, loss = 0.72136264\n",
            "Iteration 78, loss = 0.71987881\n",
            "Iteration 79, loss = 0.71844868\n",
            "Iteration 80, loss = 0.71699466\n",
            "Iteration 81, loss = 0.71551285\n",
            "Iteration 82, loss = 0.71406031\n",
            "Iteration 83, loss = 0.71258023\n",
            "Iteration 84, loss = 0.71109177\n",
            "Iteration 85, loss = 0.70973876\n",
            "Iteration 86, loss = 0.70839930\n",
            "Iteration 87, loss = 0.70708242\n",
            "Iteration 88, loss = 0.70577438\n",
            "Iteration 89, loss = 0.70451113\n",
            "Iteration 90, loss = 0.70322118\n",
            "Iteration 91, loss = 0.70202655\n",
            "Iteration 92, loss = 0.70078451\n",
            "Iteration 93, loss = 0.69961631\n",
            "Iteration 94, loss = 0.69844678\n",
            "Iteration 95, loss = 0.69724830\n",
            "Iteration 96, loss = 0.69613427\n",
            "Iteration 97, loss = 0.69501374\n",
            "Iteration 98, loss = 0.69384144\n",
            "Iteration 99, loss = 0.69264942\n",
            "Iteration 100, loss = 0.69152324\n",
            "Iteration 101, loss = 0.69032139\n",
            "Iteration 102, loss = 0.68921963\n",
            "Iteration 103, loss = 0.68806356\n",
            "Iteration 104, loss = 0.68694547\n",
            "Iteration 105, loss = 0.68580049\n",
            "Iteration 106, loss = 0.68467709\n",
            "Iteration 107, loss = 0.68351202\n",
            "Iteration 108, loss = 0.68244297\n",
            "Iteration 109, loss = 0.68129337\n",
            "Iteration 110, loss = 0.68018769\n",
            "Iteration 111, loss = 0.67911323\n",
            "Iteration 112, loss = 0.67800563\n",
            "Iteration 113, loss = 0.67688688\n",
            "Iteration 114, loss = 0.67577377\n",
            "Iteration 115, loss = 0.67469987\n",
            "Iteration 116, loss = 0.67365680\n",
            "Iteration 117, loss = 0.67266171\n",
            "Iteration 118, loss = 0.67166519\n",
            "Iteration 119, loss = 0.67065051\n",
            "Iteration 120, loss = 0.66964606\n",
            "Iteration 121, loss = 0.66875745\n",
            "Iteration 122, loss = 0.66785397\n",
            "Iteration 123, loss = 0.66691974\n",
            "Iteration 124, loss = 0.66597774\n",
            "Iteration 125, loss = 0.66508293\n",
            "Iteration 126, loss = 0.66416539\n",
            "Iteration 127, loss = 0.66333100\n",
            "Iteration 128, loss = 0.66245092\n",
            "Iteration 129, loss = 0.66161204\n",
            "Iteration 130, loss = 0.66077100\n",
            "Iteration 131, loss = 0.65992557\n",
            "Iteration 132, loss = 0.65910868\n",
            "Iteration 133, loss = 0.65826914\n",
            "Iteration 134, loss = 0.65743825\n",
            "Iteration 135, loss = 0.65660546\n",
            "Iteration 136, loss = 0.65580924\n",
            "Iteration 137, loss = 0.65503717\n",
            "Iteration 138, loss = 0.65433598\n",
            "Iteration 139, loss = 0.65356897\n",
            "Iteration 140, loss = 0.65286764\n",
            "Iteration 141, loss = 0.65214869\n",
            "Iteration 142, loss = 0.65143177\n",
            "Iteration 143, loss = 0.65075158\n",
            "Iteration 144, loss = 0.64999049\n",
            "Iteration 145, loss = 0.64932034\n",
            "Iteration 146, loss = 0.64861187\n",
            "Iteration 147, loss = 0.64793553\n",
            "Iteration 148, loss = 0.64725881\n",
            "Iteration 149, loss = 0.64656754\n",
            "Iteration 150, loss = 0.64585344\n",
            "Iteration 151, loss = 0.64521098\n",
            "Iteration 152, loss = 0.64453091\n",
            "Iteration 153, loss = 0.64387464\n",
            "Iteration 154, loss = 0.64322345\n",
            "Iteration 155, loss = 0.64257659\n",
            "Iteration 156, loss = 0.64193601\n",
            "Iteration 157, loss = 0.64126137\n",
            "Iteration 158, loss = 0.64067131\n",
            "Iteration 159, loss = 0.64006415\n",
            "Iteration 160, loss = 0.63951919\n",
            "Iteration 161, loss = 0.63896076\n",
            "Iteration 162, loss = 0.63843980\n",
            "Iteration 163, loss = 0.63790762\n",
            "Iteration 164, loss = 0.63737248\n",
            "Iteration 165, loss = 0.63682077\n",
            "Iteration 166, loss = 0.63629540\n",
            "Iteration 167, loss = 0.63578067\n",
            "Iteration 168, loss = 0.63528664\n",
            "Iteration 169, loss = 0.63478826\n",
            "Iteration 170, loss = 0.63429989\n",
            "Iteration 171, loss = 0.63383149\n",
            "Iteration 172, loss = 0.63337751\n",
            "Iteration 173, loss = 0.63292391\n",
            "Iteration 174, loss = 0.63245834\n",
            "Iteration 175, loss = 0.63198994\n",
            "Iteration 176, loss = 0.63152790\n",
            "Iteration 177, loss = 0.63106858\n",
            "Iteration 178, loss = 0.63058891\n",
            "Iteration 179, loss = 0.63013966\n",
            "Iteration 180, loss = 0.62966603\n",
            "Iteration 181, loss = 0.62919883\n",
            "Iteration 182, loss = 0.62875810\n",
            "Iteration 183, loss = 0.62828319\n",
            "Iteration 184, loss = 0.62782626\n",
            "Iteration 185, loss = 0.62740296\n",
            "Iteration 186, loss = 0.62697229\n",
            "Iteration 187, loss = 0.62652175\n",
            "Iteration 188, loss = 0.62612073\n",
            "Iteration 189, loss = 0.62573812\n",
            "Iteration 190, loss = 0.62537260\n",
            "Iteration 191, loss = 0.62496575\n",
            "Iteration 192, loss = 0.62457609\n",
            "Iteration 193, loss = 0.62417561\n",
            "Iteration 194, loss = 0.62377016\n",
            "Iteration 195, loss = 0.62335560\n",
            "Iteration 196, loss = 0.62296276\n",
            "Iteration 197, loss = 0.62257833\n",
            "Iteration 198, loss = 0.62217987\n",
            "Iteration 199, loss = 0.62179790\n",
            "Iteration 200, loss = 0.62146252\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 200 and for layer number 4 : 0.6624999999999999\n",
            "Iteration 1, loss = 0.88791930\n",
            "Iteration 2, loss = 0.88478701\n",
            "Iteration 3, loss = 0.87977954\n",
            "Iteration 4, loss = 0.87382746\n",
            "Iteration 5, loss = 0.86693408\n",
            "Iteration 6, loss = 0.85991963\n",
            "Iteration 7, loss = 0.85349317\n",
            "Iteration 8, loss = 0.84609686\n",
            "Iteration 9, loss = 0.83903239\n",
            "Iteration 10, loss = 0.83199676\n",
            "Iteration 11, loss = 0.82476252\n",
            "Iteration 12, loss = 0.81799272\n",
            "Iteration 13, loss = 0.81153730\n",
            "Iteration 14, loss = 0.80501491\n",
            "Iteration 15, loss = 0.79881692\n",
            "Iteration 16, loss = 0.79283969\n",
            "Iteration 17, loss = 0.78704952\n",
            "Iteration 18, loss = 0.78142027\n",
            "Iteration 19, loss = 0.77612815\n",
            "Iteration 20, loss = 0.77058668\n",
            "Iteration 21, loss = 0.76563879\n",
            "Iteration 22, loss = 0.76077411\n",
            "Iteration 23, loss = 0.75622954\n",
            "Iteration 24, loss = 0.75181798\n",
            "Iteration 25, loss = 0.74780850\n",
            "Iteration 26, loss = 0.74367194\n",
            "Iteration 27, loss = 0.73976022\n",
            "Iteration 28, loss = 0.73608417\n",
            "Iteration 29, loss = 0.73214581\n",
            "Iteration 30, loss = 0.72863038\n",
            "Iteration 31, loss = 0.72522531\n",
            "Iteration 32, loss = 0.72208870\n",
            "Iteration 33, loss = 0.71905862\n",
            "Iteration 34, loss = 0.71605055\n",
            "Iteration 35, loss = 0.71338575\n",
            "Iteration 36, loss = 0.71054771\n",
            "Iteration 37, loss = 0.70780651\n",
            "Iteration 38, loss = 0.70507846\n",
            "Iteration 39, loss = 0.70241483\n",
            "Iteration 40, loss = 0.69974696\n",
            "Iteration 41, loss = 0.69719897\n",
            "Iteration 42, loss = 0.69474538\n",
            "Iteration 43, loss = 0.69229605\n",
            "Iteration 44, loss = 0.68998519\n",
            "Iteration 45, loss = 0.68769249\n",
            "Iteration 46, loss = 0.68541216\n",
            "Iteration 47, loss = 0.68334998\n",
            "Iteration 48, loss = 0.68137544\n",
            "Iteration 49, loss = 0.67942804\n",
            "Iteration 50, loss = 0.67769897\n",
            "Iteration 51, loss = 0.67587683\n",
            "Iteration 52, loss = 0.67441020\n",
            "Iteration 53, loss = 0.67273943\n",
            "Iteration 54, loss = 0.67136315\n",
            "Iteration 55, loss = 0.66969470\n",
            "Iteration 56, loss = 0.66819849\n",
            "Iteration 57, loss = 0.66678144\n",
            "Iteration 58, loss = 0.66532375"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 59, loss = 0.66394656\n",
            "Iteration 60, loss = 0.66254487\n",
            "Iteration 61, loss = 0.66119003\n",
            "Iteration 62, loss = 0.65990917\n",
            "Iteration 63, loss = 0.65860006\n",
            "Iteration 64, loss = 0.65745658\n",
            "Iteration 65, loss = 0.65639810\n",
            "Iteration 66, loss = 0.65515649\n",
            "Iteration 67, loss = 0.65406199\n",
            "Iteration 68, loss = 0.65300071\n",
            "Iteration 69, loss = 0.65201781\n",
            "Iteration 70, loss = 0.65097950\n",
            "Iteration 71, loss = 0.65009911\n",
            "Iteration 72, loss = 0.64908267\n",
            "Iteration 73, loss = 0.64825233\n",
            "Iteration 74, loss = 0.64739105\n",
            "Iteration 75, loss = 0.64651078\n",
            "Iteration 76, loss = 0.64567306\n",
            "Iteration 77, loss = 0.64489576\n",
            "Iteration 78, loss = 0.64421106\n",
            "Iteration 79, loss = 0.64343015\n",
            "Iteration 80, loss = 0.64269074\n",
            "Iteration 81, loss = 0.64200003\n",
            "Iteration 82, loss = 0.64132052\n",
            "Iteration 83, loss = 0.64064109\n",
            "Iteration 84, loss = 0.63989578\n",
            "Iteration 85, loss = 0.63924996\n",
            "Iteration 86, loss = 0.63849229\n",
            "Iteration 87, loss = 0.63783327\n",
            "Iteration 88, loss = 0.63715252\n",
            "Iteration 89, loss = 0.63649693\n",
            "Iteration 90, loss = 0.63589299\n",
            "Iteration 91, loss = 0.63518930\n",
            "Iteration 92, loss = 0.63457609\n",
            "Iteration 93, loss = 0.63395665\n",
            "Iteration 94, loss = 0.63331641\n",
            "Iteration 95, loss = 0.63264966\n",
            "Iteration 96, loss = 0.63204170\n",
            "Iteration 97, loss = 0.63145729\n",
            "Iteration 98, loss = 0.63083574\n",
            "Iteration 99, loss = 0.63029201\n",
            "Iteration 100, loss = 0.62972358\n",
            "Iteration 101, loss = 0.62916841\n",
            "Iteration 102, loss = 0.62864743\n",
            "Iteration 103, loss = 0.62811693\n",
            "Iteration 104, loss = 0.62753695\n",
            "Iteration 105, loss = 0.62700635\n",
            "Iteration 106, loss = 0.62649924\n",
            "Iteration 107, loss = 0.62599200\n",
            "Iteration 108, loss = 0.62549756\n",
            "Iteration 109, loss = 0.62495530\n",
            "Iteration 110, loss = 0.62447073\n",
            "Iteration 111, loss = 0.62393226\n",
            "Iteration 112, loss = 0.62347574\n",
            "Iteration 113, loss = 0.62301065\n",
            "Iteration 114, loss = 0.62254573\n",
            "Iteration 115, loss = 0.62215107\n",
            "Iteration 116, loss = 0.62173290\n",
            "Iteration 117, loss = 0.62131221\n",
            "Iteration 118, loss = 0.62083700\n",
            "Iteration 119, loss = 0.62046053\n",
            "Iteration 120, loss = 0.62002285\n",
            "Iteration 121, loss = 0.61963050\n",
            "Iteration 122, loss = 0.61921993\n",
            "Iteration 123, loss = 0.61881579\n",
            "Iteration 124, loss = 0.61840680\n",
            "Iteration 125, loss = 0.61801465\n",
            "Iteration 126, loss = 0.61765406\n",
            "Iteration 127, loss = 0.61726355\n",
            "Iteration 128, loss = 0.61687388\n",
            "Iteration 129, loss = 0.61654076\n",
            "Iteration 130, loss = 0.61616167\n",
            "Iteration 131, loss = 0.61579524\n",
            "Iteration 132, loss = 0.61543350\n",
            "Iteration 133, loss = 0.61508540\n",
            "Iteration 134, loss = 0.61473026\n",
            "Iteration 135, loss = 0.61439139\n",
            "Iteration 136, loss = 0.61404298\n",
            "Iteration 137, loss = 0.61372779\n",
            "Iteration 138, loss = 0.61341953\n",
            "Iteration 139, loss = 0.61310868\n",
            "Iteration 140, loss = 0.61276988\n",
            "Iteration 141, loss = 0.61242081\n",
            "Iteration 142, loss = 0.61206685\n",
            "Iteration 143, loss = 0.61169393\n",
            "Iteration 144, loss = 0.61138846\n",
            "Iteration 145, loss = 0.61104682\n",
            "Iteration 146, loss = 0.61072881\n",
            "Iteration 147, loss = 0.61042243\n",
            "Iteration 148, loss = 0.61011410\n",
            "Iteration 149, loss = 0.60980392\n",
            "Iteration 150, loss = 0.60945383\n",
            "Iteration 151, loss = 0.60914143\n",
            "Iteration 152, loss = 0.60883958\n",
            "Iteration 153, loss = 0.60851290\n",
            "Iteration 154, loss = 0.60821514\n",
            "Iteration 155, loss = 0.60787110\n",
            "Iteration 156, loss = 0.60756271\n",
            "Iteration 157, loss = 0.60726763\n",
            "Iteration 158, loss = 0.60696357\n",
            "Iteration 159, loss = 0.60667989\n",
            "Iteration 160, loss = 0.60637682\n",
            "Iteration 161, loss = 0.60607746\n",
            "Iteration 162, loss = 0.60580819\n",
            "Iteration 163, loss = 0.60549342\n",
            "Iteration 164, loss = 0.60522086\n",
            "Iteration 165, loss = 0.60494172\n",
            "Iteration 166, loss = 0.60468410\n",
            "Iteration 167, loss = 0.60442890\n",
            "Iteration 168, loss = 0.60419657\n",
            "Iteration 169, loss = 0.60394767\n",
            "Iteration 170, loss = 0.60370146\n",
            "Iteration 171, loss = 0.60345721\n",
            "Iteration 172, loss = 0.60322870\n",
            "Iteration 173, loss = 0.60298054\n",
            "Iteration 174, loss = 0.60275245\n",
            "Iteration 175, loss = 0.60254950\n",
            "Iteration 176, loss = 0.60230616\n",
            "Iteration 177, loss = 0.60209045\n",
            "Iteration 178, loss = 0.60186878\n",
            "Iteration 179, loss = 0.60164130\n",
            "Iteration 180, loss = 0.60140268\n",
            "Iteration 181, loss = 0.60117165\n",
            "Iteration 182, loss = 0.60095455\n",
            "Iteration 183, loss = 0.60070854\n",
            "Iteration 184, loss = 0.60050614\n",
            "Iteration 185, loss = 0.60024558\n",
            "Iteration 186, loss = 0.60004448\n",
            "Iteration 187, loss = 0.59984445\n",
            "Iteration 188, loss = 0.59962281\n",
            "Iteration 189, loss = 0.59942377\n",
            "Iteration 190, loss = 0.59921879\n",
            "Iteration 191, loss = 0.59901873\n",
            "Iteration 192, loss = 0.59881232\n",
            "Iteration 193, loss = 0.59862066\n",
            "Iteration 194, loss = 0.59841907\n",
            "Iteration 195, loss = 0.59823296\n",
            "Iteration 196, loss = 0.59805454\n",
            "Iteration 197, loss = 0.59785025\n",
            "Iteration 198, loss = 0.59764457\n",
            "Iteration 199, loss = 0.59743038\n",
            "Iteration 200, loss = 0.59721128\n",
            "Iteration 1, loss = 0.88667506\n",
            "Iteration 2, loss = 0.88351311\n",
            "Iteration 3, loss = 0.87864988\n",
            "Iteration 4, loss = 0.87260271\n",
            "Iteration 5, loss = 0.86575091\n",
            "Iteration 6, loss = 0.85865495\n",
            "Iteration 7, loss = 0.85211290\n",
            "Iteration 8, loss = 0.84485323\n",
            "Iteration 9, loss = 0.83785117\n",
            "Iteration 10, loss = 0.83103528\n",
            "Iteration 11, loss = 0.82391039\n",
            "Iteration 12, loss = 0.81728381\n",
            "Iteration 13, loss = 0.81056237\n",
            "Iteration 14, loss = 0.80417780\n",
            "Iteration 15, loss = 0.79788403\n",
            "Iteration 16, loss = 0.79172048"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 17, loss = 0.78589084\n",
            "Iteration 18, loss = 0.78024639\n",
            "Iteration 19, loss = 0.77527683\n",
            "Iteration 20, loss = 0.76983525\n",
            "Iteration 21, loss = 0.76512748\n",
            "Iteration 22, loss = 0.76031041\n",
            "Iteration 23, loss = 0.75566756\n",
            "Iteration 24, loss = 0.75123287\n",
            "Iteration 25, loss = 0.74717290\n",
            "Iteration 26, loss = 0.74309808\n",
            "Iteration 27, loss = 0.73909650\n",
            "Iteration 28, loss = 0.73534184\n",
            "Iteration 29, loss = 0.73146632\n",
            "Iteration 30, loss = 0.72783360\n",
            "Iteration 31, loss = 0.72415824\n",
            "Iteration 32, loss = 0.72081620\n",
            "Iteration 33, loss = 0.71757346\n",
            "Iteration 34, loss = 0.71430564\n",
            "Iteration 35, loss = 0.71143757\n",
            "Iteration 36, loss = 0.70844375\n",
            "Iteration 37, loss = 0.70547686\n",
            "Iteration 38, loss = 0.70270393\n",
            "Iteration 39, loss = 0.70016842\n",
            "Iteration 40, loss = 0.69738412\n",
            "Iteration 41, loss = 0.69481862\n",
            "Iteration 42, loss = 0.69239524\n",
            "Iteration 43, loss = 0.68976645\n",
            "Iteration 44, loss = 0.68741275\n",
            "Iteration 45, loss = 0.68502015\n",
            "Iteration 46, loss = 0.68269702\n",
            "Iteration 47, loss = 0.68062921\n",
            "Iteration 48, loss = 0.67855526\n",
            "Iteration 49, loss = 0.67663369\n",
            "Iteration 50, loss = 0.67475577\n",
            "Iteration 51, loss = 0.67285993\n",
            "Iteration 52, loss = 0.67128721\n",
            "Iteration 53, loss = 0.66942488\n",
            "Iteration 54, loss = 0.66796022\n",
            "Iteration 55, loss = 0.66608528\n",
            "Iteration 56, loss = 0.66438449\n",
            "Iteration 57, loss = 0.66280374\n",
            "Iteration 58, loss = 0.66126445\n",
            "Iteration 59, loss = 0.65981469\n",
            "Iteration 60, loss = 0.65833838\n",
            "Iteration 61, loss = 0.65696295\n",
            "Iteration 62, loss = 0.65566478\n",
            "Iteration 63, loss = 0.65425246\n",
            "Iteration 64, loss = 0.65304333\n",
            "Iteration 65, loss = 0.65190526\n",
            "Iteration 66, loss = 0.65073936\n",
            "Iteration 67, loss = 0.64955138\n",
            "Iteration 68, loss = 0.64845330\n",
            "Iteration 69, loss = 0.64739906\n",
            "Iteration 70, loss = 0.64639450\n",
            "Iteration 71, loss = 0.64546332\n",
            "Iteration 72, loss = 0.64444218\n",
            "Iteration 73, loss = 0.64356265\n",
            "Iteration 74, loss = 0.64269640\n",
            "Iteration 75, loss = 0.64173035\n",
            "Iteration 76, loss = 0.64084432\n",
            "Iteration 77, loss = 0.63998132\n",
            "Iteration 78, loss = 0.63918837\n",
            "Iteration 79, loss = 0.63833688\n",
            "Iteration 80, loss = 0.63748454\n",
            "Iteration 81, loss = 0.63668468\n",
            "Iteration 82, loss = 0.63590312\n",
            "Iteration 83, loss = 0.63515432\n",
            "Iteration 84, loss = 0.63435972\n",
            "Iteration 85, loss = 0.63367797\n",
            "Iteration 86, loss = 0.63287418\n",
            "Iteration 87, loss = 0.63216616\n",
            "Iteration 88, loss = 0.63143260\n",
            "Iteration 89, loss = 0.63074916\n",
            "Iteration 90, loss = 0.63012356\n",
            "Iteration 91, loss = 0.62939877\n",
            "Iteration 92, loss = 0.62878563\n",
            "Iteration 93, loss = 0.62818137\n",
            "Iteration 94, loss = 0.62753163\n",
            "Iteration 95, loss = 0.62687453\n",
            "Iteration 96, loss = 0.62626042\n",
            "Iteration 97, loss = 0.62562589\n",
            "Iteration 98, loss = 0.62499300\n",
            "Iteration 99, loss = 0.62440384\n",
            "Iteration 100, loss = 0.62381887\n",
            "Iteration 101, loss = 0.62324946\n",
            "Iteration 102, loss = 0.62268885\n",
            "Iteration 103, loss = 0.62213448\n",
            "Iteration 104, loss = 0.62156196\n",
            "Iteration 105, loss = 0.62102050\n",
            "Iteration 106, loss = 0.62050980\n",
            "Iteration 107, loss = 0.61999940\n",
            "Iteration 108, loss = 0.61947200\n",
            "Iteration 109, loss = 0.61889228\n",
            "Iteration 110, loss = 0.61837360\n",
            "Iteration 111, loss = 0.61780234\n",
            "Iteration 112, loss = 0.61731808\n",
            "Iteration 113, loss = 0.61682786\n",
            "Iteration 114, loss = 0.61634198\n",
            "Iteration 115, loss = 0.61588132\n",
            "Iteration 116, loss = 0.61539914\n",
            "Iteration 117, loss = 0.61496000\n",
            "Iteration 118, loss = 0.61445999\n",
            "Iteration 119, loss = 0.61405786\n",
            "Iteration 120, loss = 0.61359280\n",
            "Iteration 121, loss = 0.61315311\n",
            "Iteration 122, loss = 0.61272132\n",
            "Iteration 123, loss = 0.61229425\n",
            "Iteration 124, loss = 0.61186651\n",
            "Iteration 125, loss = 0.61149580\n",
            "Iteration 126, loss = 0.61113313\n",
            "Iteration 127, loss = 0.61075871\n",
            "Iteration 128, loss = 0.61038748\n",
            "Iteration 129, loss = 0.61006564\n",
            "Iteration 130, loss = 0.60973529\n",
            "Iteration 131, loss = 0.60940572\n",
            "Iteration 132, loss = 0.60907905\n",
            "Iteration 133, loss = 0.60876726\n",
            "Iteration 134, loss = 0.60845105\n",
            "Iteration 135, loss = 0.60813815\n",
            "Iteration 136, loss = 0.60783352\n",
            "Iteration 137, loss = 0.60753356\n",
            "Iteration 138, loss = 0.60723539\n",
            "Iteration 139, loss = 0.60693029\n",
            "Iteration 140, loss = 0.60661511\n",
            "Iteration 141, loss = 0.60632463\n",
            "Iteration 142, loss = 0.60599017\n",
            "Iteration 143, loss = 0.60568620\n",
            "Iteration 144, loss = 0.60541104\n",
            "Iteration 145, loss = 0.60510121\n",
            "Iteration 146, loss = 0.60480565\n",
            "Iteration 147, loss = 0.60450824\n",
            "Iteration 148, loss = 0.60423121\n",
            "Iteration 149, loss = 0.60393618\n",
            "Iteration 150, loss = 0.60361900\n",
            "Iteration 151, loss = 0.60333554\n",
            "Iteration 152, loss = 0.60305321\n",
            "Iteration 153, loss = 0.60274115\n",
            "Iteration 154, loss = 0.60245413\n",
            "Iteration 155, loss = 0.60212651\n",
            "Iteration 156, loss = 0.60184832\n",
            "Iteration 157, loss = 0.60155171\n",
            "Iteration 158, loss = 0.60126582\n",
            "Iteration 159, loss = 0.60100262\n",
            "Iteration 160, loss = 0.60068187\n",
            "Iteration 161, loss = 0.60039488\n",
            "Iteration 162, loss = 0.60013446\n",
            "Iteration 163, loss = 0.59983366\n",
            "Iteration 164, loss = 0.59954878\n",
            "Iteration 165, loss = 0.59927854\n",
            "Iteration 166, loss = 0.59901146\n",
            "Iteration 167, loss = 0.59876075\n",
            "Iteration 168, loss = 0.59851650\n",
            "Iteration 169, loss = 0.59825617\n",
            "Iteration 170, loss = 0.59801253\n",
            "Iteration 171, loss = 0.59774458\n",
            "Iteration 172, loss = 0.59748128\n",
            "Iteration 173, loss = 0.59722409\n",
            "Iteration 174, loss = 0.59697695\n",
            "Iteration 175, loss = 0.59673937\n",
            "Iteration 176, loss = 0.59645825\n",
            "Iteration 177, loss = 0.59619640\n",
            "Iteration 178, loss = 0.59594427\n",
            "Iteration 179, loss = 0.59567512\n",
            "Iteration 180, loss = 0.59541360\n",
            "Iteration 181, loss = 0.59514124\n",
            "Iteration 182, loss = 0.59487859\n",
            "Iteration 183, loss = 0.59461068\n",
            "Iteration 184, loss = 0.59437536\n",
            "Iteration 185, loss = 0.59409224\n",
            "Iteration 186, loss = 0.59387115\n",
            "Iteration 187, loss = 0.59364717\n",
            "Iteration 188, loss = 0.59340668\n",
            "Iteration 189, loss = 0.59319492\n",
            "Iteration 190, loss = 0.59297039\n",
            "Iteration 191, loss = 0.59275976\n",
            "Iteration 192, loss = 0.59253535\n",
            "Iteration 193, loss = 0.59232778\n",
            "Iteration 194, loss = 0.59210413\n",
            "Iteration 195, loss = 0.59189420\n",
            "Iteration 196, loss = 0.59170085\n",
            "Iteration 197, loss = 0.59147506\n",
            "Iteration 198, loss = 0.59125193\n",
            "Iteration 199, loss = 0.59102154\n",
            "Iteration 200, loss = 0.59078368\n",
            "Iteration 1, loss = 0.88994796\n",
            "Iteration 2, loss = 0.88647879\n",
            "Iteration 3, loss = 0.88112416\n",
            "Iteration 4, loss = 0.87477056\n",
            "Iteration 5, loss = 0.86759332\n",
            "Iteration 6, loss = 0.86026211\n",
            "Iteration 7, loss = 0.85335631\n",
            "Iteration 8, loss = 0.84586796\n",
            "Iteration 9, loss = 0.83874205\n",
            "Iteration 10, loss = 0.83175349\n",
            "Iteration 11, loss = 0.82448515\n",
            "Iteration 12, loss = 0.81778325\n",
            "Iteration 13, loss = 0.81084241\n",
            "Iteration 14, loss = 0.80421718\n",
            "Iteration 15, loss = 0.79782291\n",
            "Iteration 16, loss = 0.79153897\n",
            "Iteration 17, loss = 0.78578672\n",
            "Iteration 18, loss = 0.78028990\n",
            "Iteration 19, loss = 0.77514713\n",
            "Iteration 20, loss = 0.76965946\n",
            "Iteration 21, loss = 0.76458983\n",
            "Iteration 22, loss = 0.75975845\n",
            "Iteration 23, loss = 0.75512558\n",
            "Iteration 24, loss = 0.75073017\n",
            "Iteration 25, loss = 0.74659412\n",
            "Iteration 26, loss = 0.74241300\n",
            "Iteration 27, loss = 0.73858643\n",
            "Iteration 28, loss = 0.73478476\n",
            "Iteration 29, loss = 0.73086329\n",
            "Iteration 30, loss = 0.72730311\n",
            "Iteration 31, loss = 0.72350860\n",
            "Iteration 32, loss = 0.72016727\n",
            "Iteration 33, loss = 0.71697302\n",
            "Iteration 34, loss = 0.71360923\n",
            "Iteration 35, loss = 0.71068663\n",
            "Iteration 36, loss = 0.70783843\n",
            "Iteration 37, loss = 0.70491116\n",
            "Iteration 38, loss = 0.70215099\n",
            "Iteration 39, loss = 0.69968857\n",
            "Iteration 40, loss = 0.69682383\n",
            "Iteration 41, loss = 0.69427119\n",
            "Iteration 42, loss = 0.69198212\n",
            "Iteration 43, loss = 0.68919723\n",
            "Iteration 44, loss = 0.68691056\n",
            "Iteration 45, loss = 0.68454722\n",
            "Iteration 46, loss = 0.68224448\n",
            "Iteration 47, loss = 0.68018345\n",
            "Iteration 48, loss = 0.67814086\n",
            "Iteration 49, loss = 0.67618343\n",
            "Iteration 50, loss = 0.67429781"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 51, loss = 0.67240548\n",
            "Iteration 52, loss = 0.67073257\n",
            "Iteration 53, loss = 0.66887623\n",
            "Iteration 54, loss = 0.66727539\n",
            "Iteration 55, loss = 0.66548070\n",
            "Iteration 56, loss = 0.66375165\n",
            "Iteration 57, loss = 0.66206313\n",
            "Iteration 58, loss = 0.66052993\n",
            "Iteration 59, loss = 0.65902693\n",
            "Iteration 60, loss = 0.65749027\n",
            "Iteration 61, loss = 0.65599758\n",
            "Iteration 62, loss = 0.65470773\n",
            "Iteration 63, loss = 0.65327373\n",
            "Iteration 64, loss = 0.65200876\n",
            "Iteration 65, loss = 0.65082649\n",
            "Iteration 66, loss = 0.64977568\n",
            "Iteration 67, loss = 0.64850480\n",
            "Iteration 68, loss = 0.64743751\n",
            "Iteration 69, loss = 0.64638809\n",
            "Iteration 70, loss = 0.64542021\n",
            "Iteration 71, loss = 0.64443926\n",
            "Iteration 72, loss = 0.64338729\n",
            "Iteration 73, loss = 0.64241557\n",
            "Iteration 74, loss = 0.64150518\n",
            "Iteration 75, loss = 0.64053993\n",
            "Iteration 76, loss = 0.63963000\n",
            "Iteration 77, loss = 0.63876754\n",
            "Iteration 78, loss = 0.63794665\n",
            "Iteration 79, loss = 0.63710295\n",
            "Iteration 80, loss = 0.63618590\n",
            "Iteration 81, loss = 0.63534354\n",
            "Iteration 82, loss = 0.63453306\n",
            "Iteration 83, loss = 0.63371928\n",
            "Iteration 84, loss = 0.63288966\n",
            "Iteration 85, loss = 0.63216956\n",
            "Iteration 86, loss = 0.63129485\n",
            "Iteration 87, loss = 0.63059605\n",
            "Iteration 88, loss = 0.62984039\n",
            "Iteration 89, loss = 0.62914261\n",
            "Iteration 90, loss = 0.62847791\n",
            "Iteration 91, loss = 0.62776941\n",
            "Iteration 92, loss = 0.62715342\n",
            "Iteration 93, loss = 0.62653126\n",
            "Iteration 94, loss = 0.62586031\n",
            "Iteration 95, loss = 0.62520916\n",
            "Iteration 96, loss = 0.62453176\n",
            "Iteration 97, loss = 0.62392353\n",
            "Iteration 98, loss = 0.62329005\n",
            "Iteration 99, loss = 0.62268263\n",
            "Iteration 100, loss = 0.62210977\n",
            "Iteration 101, loss = 0.62153879\n",
            "Iteration 102, loss = 0.62096210\n",
            "Iteration 103, loss = 0.62039536\n",
            "Iteration 104, loss = 0.61979729\n",
            "Iteration 105, loss = 0.61921874\n",
            "Iteration 106, loss = 0.61872081\n",
            "Iteration 107, loss = 0.61819904\n",
            "Iteration 108, loss = 0.61769996\n",
            "Iteration 109, loss = 0.61714989\n",
            "Iteration 110, loss = 0.61665917\n",
            "Iteration 111, loss = 0.61610002\n",
            "Iteration 112, loss = 0.61560955\n",
            "Iteration 113, loss = 0.61511586\n",
            "Iteration 114, loss = 0.61462980\n",
            "Iteration 115, loss = 0.61415450\n",
            "Iteration 116, loss = 0.61366960\n",
            "Iteration 117, loss = 0.61321745\n",
            "Iteration 118, loss = 0.61273741\n",
            "Iteration 119, loss = 0.61233518\n",
            "Iteration 120, loss = 0.61192375\n",
            "Iteration 121, loss = 0.61147456\n",
            "Iteration 122, loss = 0.61104419\n",
            "Iteration 123, loss = 0.61065688\n",
            "Iteration 124, loss = 0.61023305\n",
            "Iteration 125, loss = 0.60986801\n",
            "Iteration 126, loss = 0.60947917\n",
            "Iteration 127, loss = 0.60912309\n",
            "Iteration 128, loss = 0.60875497\n",
            "Iteration 129, loss = 0.60843382\n",
            "Iteration 130, loss = 0.60809005\n",
            "Iteration 131, loss = 0.60775346\n",
            "Iteration 132, loss = 0.60741015\n",
            "Iteration 133, loss = 0.60707433\n",
            "Iteration 134, loss = 0.60673199\n",
            "Iteration 135, loss = 0.60638836\n",
            "Iteration 136, loss = 0.60605999\n",
            "Iteration 137, loss = 0.60574818\n",
            "Iteration 138, loss = 0.60542733\n",
            "Iteration 139, loss = 0.60511922\n",
            "Iteration 140, loss = 0.60481512\n",
            "Iteration 141, loss = 0.60451908\n",
            "Iteration 142, loss = 0.60418285\n",
            "Iteration 143, loss = 0.60388202\n",
            "Iteration 144, loss = 0.60357479\n",
            "Iteration 145, loss = 0.60325144\n",
            "Iteration 146, loss = 0.60294371\n",
            "Iteration 147, loss = 0.60261059\n",
            "Iteration 148, loss = 0.60232324\n",
            "Iteration 149, loss = 0.60202313\n",
            "Iteration 150, loss = 0.60171767\n",
            "Iteration 151, loss = 0.60142944\n",
            "Iteration 152, loss = 0.60115186\n",
            "Iteration 153, loss = 0.60083333\n",
            "Iteration 154, loss = 0.60052540\n",
            "Iteration 155, loss = 0.60019635\n",
            "Iteration 156, loss = 0.59990429\n",
            "Iteration 157, loss = 0.59958906\n",
            "Iteration 158, loss = 0.59926592\n",
            "Iteration 159, loss = 0.59898832\n",
            "Iteration 160, loss = 0.59865708\n",
            "Iteration 161, loss = 0.59835231\n",
            "Iteration 162, loss = 0.59808585\n",
            "Iteration 163, loss = 0.59778898\n",
            "Iteration 164, loss = 0.59750615\n",
            "Iteration 165, loss = 0.59722219\n",
            "Iteration 166, loss = 0.59696026\n",
            "Iteration 167, loss = 0.59669025\n",
            "Iteration 168, loss = 0.59641560\n",
            "Iteration 169, loss = 0.59615613\n",
            "Iteration 170, loss = 0.59588431\n",
            "Iteration 171, loss = 0.59560134\n",
            "Iteration 172, loss = 0.59531639\n",
            "Iteration 173, loss = 0.59504823\n",
            "Iteration 174, loss = 0.59477155\n",
            "Iteration 175, loss = 0.59449986\n",
            "Iteration 176, loss = 0.59421497\n",
            "Iteration 177, loss = 0.59394613\n",
            "Iteration 178, loss = 0.59368823\n",
            "Iteration 179, loss = 0.59343269\n",
            "Iteration 180, loss = 0.59317814\n",
            "Iteration 181, loss = 0.59291144\n",
            "Iteration 182, loss = 0.59265383\n",
            "Iteration 183, loss = 0.59237597\n",
            "Iteration 184, loss = 0.59212673\n",
            "Iteration 185, loss = 0.59184754\n",
            "Iteration 186, loss = 0.59159807\n",
            "Iteration 187, loss = 0.59135200\n",
            "Iteration 188, loss = 0.59110612\n",
            "Iteration 189, loss = 0.59086219\n",
            "Iteration 190, loss = 0.59062950\n",
            "Iteration 191, loss = 0.59039656\n",
            "Iteration 192, loss = 0.59014839\n",
            "Iteration 193, loss = 0.58993496\n",
            "Iteration 194, loss = 0.58969934\n",
            "Iteration 195, loss = 0.58946922\n",
            "Iteration 196, loss = 0.58925289\n",
            "Iteration 197, loss = 0.58902386\n",
            "Iteration 198, loss = 0.58879472\n",
            "Iteration 199, loss = 0.58858062\n",
            "Iteration 200, loss = 0.58834958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.88349736\n",
            "Iteration 2, loss = 0.87995489\n",
            "Iteration 3, loss = 0.87484423\n",
            "Iteration 4, loss = 0.86857561\n",
            "Iteration 5, loss = 0.86173197\n",
            "Iteration 6, loss = 0.85454050\n",
            "Iteration 7, loss = 0.84774058\n",
            "Iteration 8, loss = 0.84088394\n",
            "Iteration 9, loss = 0.83393267\n",
            "Iteration 10, loss = 0.82742553\n",
            "Iteration 11, loss = 0.82086785\n",
            "Iteration 12, loss = 0.81452708\n",
            "Iteration 13, loss = 0.80823130\n",
            "Iteration 14, loss = 0.80203476\n",
            "Iteration 15, loss = 0.79614496\n",
            "Iteration 16, loss = 0.79049779\n",
            "Iteration 17, loss = 0.78504210\n",
            "Iteration 18, loss = 0.77952845\n",
            "Iteration 19, loss = 0.77469909\n",
            "Iteration 20, loss = 0.76936348\n",
            "Iteration 21, loss = 0.76442730\n",
            "Iteration 22, loss = 0.75965800\n",
            "Iteration 23, loss = 0.75500764\n",
            "Iteration 24, loss = 0.75071470\n",
            "Iteration 25, loss = 0.74647049\n",
            "Iteration 26, loss = 0.74261509\n",
            "Iteration 27, loss = 0.73863169\n",
            "Iteration 28, loss = 0.73493685\n",
            "Iteration 29, loss = 0.73128501\n",
            "Iteration 30, loss = 0.72789941\n",
            "Iteration 31, loss = 0.72426266\n",
            "Iteration 32, loss = 0.72101993\n",
            "Iteration 33, loss = 0.71785043\n",
            "Iteration 34, loss = 0.71458345\n",
            "Iteration 35, loss = 0.71165677\n",
            "Iteration 36, loss = 0.70878583\n",
            "Iteration 37, loss = 0.70588880\n",
            "Iteration 38, loss = 0.70314079\n",
            "Iteration 39, loss = 0.70061319\n",
            "Iteration 40, loss = 0.69798247\n",
            "Iteration 41, loss = 0.69549327\n",
            "Iteration 42, loss = 0.69327842\n",
            "Iteration 43, loss = 0.69073254\n",
            "Iteration 44, loss = 0.68867848\n",
            "Iteration 45, loss = 0.68642668\n",
            "Iteration 46, loss = 0.68434546\n",
            "Iteration 47, loss = 0.68236535\n",
            "Iteration 48, loss = 0.68048017\n",
            "Iteration 49, loss = 0.67864669\n",
            "Iteration 50, loss = 0.67680541\n",
            "Iteration 51, loss = 0.67496606\n",
            "Iteration 52, loss = 0.67331816\n",
            "Iteration 53, loss = 0.67162095\n",
            "Iteration 54, loss = 0.67012659\n",
            "Iteration 55, loss = 0.66851262\n",
            "Iteration 56, loss = 0.66698837\n",
            "Iteration 57, loss = 0.66532003\n",
            "Iteration 58, loss = 0.66385469\n",
            "Iteration 59, loss = 0.66236615\n",
            "Iteration 60, loss = 0.66095100\n",
            "Iteration 61, loss = 0.65952010\n",
            "Iteration 62, loss = 0.65829264\n",
            "Iteration 63, loss = 0.65702747\n",
            "Iteration 64, loss = 0.65574388\n",
            "Iteration 65, loss = 0.65461995\n",
            "Iteration 66, loss = 0.65353183\n",
            "Iteration 67, loss = 0.65225074\n",
            "Iteration 68, loss = 0.65117719\n",
            "Iteration 69, loss = 0.64998474\n",
            "Iteration 70, loss = 0.64900805\n",
            "Iteration 71, loss = 0.64805924\n",
            "Iteration 72, loss = 0.64707263\n",
            "Iteration 73, loss = 0.64619598\n",
            "Iteration 74, loss = 0.64539438\n",
            "Iteration 75, loss = 0.64456929\n",
            "Iteration 76, loss = 0.64368399\n",
            "Iteration 77, loss = 0.64289466\n",
            "Iteration 78, loss = 0.64211176\n",
            "Iteration 79, loss = 0.64132455\n",
            "Iteration 80, loss = 0.64048025\n",
            "Iteration 81, loss = 0.63968838\n",
            "Iteration 82, loss = 0.63893461\n",
            "Iteration 83, loss = 0.63821109\n",
            "Iteration 84, loss = 0.63747211\n",
            "Iteration 85, loss = 0.63685252\n",
            "Iteration 86, loss = 0.63609556\n",
            "Iteration 87, loss = 0.63543753\n",
            "Iteration 88, loss = 0.63471118\n",
            "Iteration 89, loss = 0.63404085\n",
            "Iteration 90, loss = 0.63340349\n",
            "Iteration 91, loss = 0.63278193\n",
            "Iteration 92, loss = 0.63214743\n",
            "Iteration 93, loss = 0.63157648\n",
            "Iteration 94, loss = 0.63095984\n",
            "Iteration 95, loss = 0.63041787\n",
            "Iteration 96, loss = 0.62977208\n",
            "Iteration 97, loss = 0.62918399\n",
            "Iteration 98, loss = 0.62857863\n",
            "Iteration 99, loss = 0.62795666\n",
            "Iteration 100, loss = 0.62743278\n",
            "Iteration 101, loss = 0.62685021\n",
            "Iteration 102, loss = 0.62633634\n",
            "Iteration 103, loss = 0.62579168\n",
            "Iteration 104, loss = 0.62524932\n",
            "Iteration 105, loss = 0.62471175\n",
            "Iteration 106, loss = 0.62418637\n",
            "Iteration 107, loss = 0.62365253\n",
            "Iteration 108, loss = 0.62314381\n",
            "Iteration 109, loss = 0.62264512\n",
            "Iteration 110, loss = 0.62216813\n",
            "Iteration 111, loss = 0.62165089\n",
            "Iteration 112, loss = 0.62116648\n",
            "Iteration 113, loss = 0.62068983\n",
            "Iteration 114, loss = 0.62022157\n",
            "Iteration 115, loss = 0.61975804\n",
            "Iteration 116, loss = 0.61928792\n",
            "Iteration 117, loss = 0.61889902\n",
            "Iteration 118, loss = 0.61843594\n",
            "Iteration 119, loss = 0.61802865\n",
            "Iteration 120, loss = 0.61762445\n",
            "Iteration 121, loss = 0.61721078\n",
            "Iteration 122, loss = 0.61681830\n",
            "Iteration 123, loss = 0.61644115\n",
            "Iteration 124, loss = 0.61604421\n",
            "Iteration 125, loss = 0.61570259\n",
            "Iteration 126, loss = 0.61535021\n",
            "Iteration 127, loss = 0.61506127\n",
            "Iteration 128, loss = 0.61473832\n",
            "Iteration 129, loss = 0.61444763\n",
            "Iteration 130, loss = 0.61415855\n",
            "Iteration 131, loss = 0.61385193\n",
            "Iteration 132, loss = 0.61355253\n",
            "Iteration 133, loss = 0.61322731\n",
            "Iteration 134, loss = 0.61292373\n",
            "Iteration 135, loss = 0.61261517\n",
            "Iteration 136, loss = 0.61231101\n",
            "Iteration 137, loss = 0.61202998\n",
            "Iteration 138, loss = 0.61175024\n",
            "Iteration 139, loss = 0.61147013\n",
            "Iteration 140, loss = 0.61120172\n",
            "Iteration 141, loss = 0.61090642\n",
            "Iteration 142, loss = 0.61061183\n",
            "Iteration 143, loss = 0.61032082\n",
            "Iteration 144, loss = 0.61000956\n",
            "Iteration 145, loss = 0.60972792\n",
            "Iteration 146, loss = 0.60945950\n",
            "Iteration 147, loss = 0.60917434\n",
            "Iteration 148, loss = 0.60890758\n",
            "Iteration 149, loss = 0.60864347\n",
            "Iteration 150, loss = 0.60839202\n",
            "Iteration 151, loss = 0.60811992\n",
            "Iteration 152, loss = 0.60786268\n",
            "Iteration 153, loss = 0.60760296\n",
            "Iteration 154, loss = 0.60733963\n",
            "Iteration 155, loss = 0.60707550\n",
            "Iteration 156, loss = 0.60684228\n",
            "Iteration 157, loss = 0.60657545\n",
            "Iteration 158, loss = 0.60631319\n",
            "Iteration 159, loss = 0.60608399\n",
            "Iteration 160, loss = 0.60581160\n",
            "Iteration 161, loss = 0.60556069\n",
            "Iteration 162, loss = 0.60529163\n",
            "Iteration 163, loss = 0.60502466\n",
            "Iteration 164, loss = 0.60478753\n",
            "Iteration 165, loss = 0.60454314\n",
            "Iteration 166, loss = 0.60431867\n",
            "Iteration 167, loss = 0.60407499\n",
            "Iteration 168, loss = 0.60383628\n",
            "Iteration 169, loss = 0.60361231\n",
            "Iteration 170, loss = 0.60336176\n",
            "Iteration 171, loss = 0.60313222\n",
            "Iteration 172, loss = 0.60290132\n",
            "Iteration 173, loss = 0.60268011\n",
            "Iteration 174, loss = 0.60245866\n",
            "Iteration 175, loss = 0.60223474\n",
            "Iteration 176, loss = 0.60201685\n",
            "Iteration 177, loss = 0.60180890\n",
            "Iteration 178, loss = 0.60159975\n",
            "Iteration 179, loss = 0.60139617\n",
            "Iteration 180, loss = 0.60120760\n",
            "Iteration 181, loss = 0.60100129\n",
            "Iteration 182, loss = 0.60081331\n",
            "Iteration 183, loss = 0.60060786\n",
            "Iteration 184, loss = 0.60040209\n",
            "Iteration 185, loss = 0.60021868\n",
            "Iteration 186, loss = 0.60002204\n",
            "Iteration 187, loss = 0.59984107\n",
            "Iteration 188, loss = 0.59966458\n",
            "Iteration 189, loss = 0.59947868\n",
            "Iteration 190, loss = 0.59929886\n",
            "Iteration 191, loss = 0.59911707\n",
            "Iteration 192, loss = 0.59892208\n",
            "Iteration 193, loss = 0.59874415\n",
            "Iteration 194, loss = 0.59855266\n",
            "Iteration 195, loss = 0.59837945\n",
            "Iteration 196, loss = 0.59819202\n",
            "Iteration 197, loss = 0.59801147\n",
            "Iteration 198, loss = 0.59783531\n",
            "Iteration 199, loss = 0.59765422\n",
            "Iteration 200, loss = 0.59747330\n",
            "Iteration 1, loss = 0.88921381\n",
            "Iteration 2, loss = 0.88561120\n",
            "Iteration 3, loss = 0.88039745\n",
            "Iteration 4, loss = 0.87411510\n",
            "Iteration 5, loss = 0.86695385\n",
            "Iteration 6, loss = 0.85950718\n",
            "Iteration 7, loss = 0.85233922\n",
            "Iteration 8, loss = 0.84527473\n",
            "Iteration 9, loss = 0.83809709\n",
            "Iteration 10, loss = 0.83164349\n",
            "Iteration 11, loss = 0.82493838\n",
            "Iteration 12, loss = 0.81870921\n",
            "Iteration 13, loss = 0.81251318\n",
            "Iteration 14, loss = 0.80607251\n",
            "Iteration 15, loss = 0.79981985\n",
            "Iteration 16, loss = 0.79393281\n",
            "Iteration 17, loss = 0.78828581\n",
            "Iteration 18, loss = 0.78245479\n",
            "Iteration 19, loss = 0.77739177\n",
            "Iteration 20, loss = 0.77198009\n",
            "Iteration 21, loss = 0.76686429"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 22, loss = 0.76204430\n",
            "Iteration 23, loss = 0.75723984\n",
            "Iteration 24, loss = 0.75306036\n",
            "Iteration 25, loss = 0.74865567\n",
            "Iteration 26, loss = 0.74459266\n",
            "Iteration 27, loss = 0.74053604\n",
            "Iteration 28, loss = 0.73681378\n",
            "Iteration 29, loss = 0.73320126\n",
            "Iteration 30, loss = 0.72947076\n",
            "Iteration 31, loss = 0.72576794\n",
            "Iteration 32, loss = 0.72239030\n",
            "Iteration 33, loss = 0.71920770\n",
            "Iteration 34, loss = 0.71580616\n",
            "Iteration 35, loss = 0.71274085\n",
            "Iteration 36, loss = 0.70975790\n",
            "Iteration 37, loss = 0.70664366\n",
            "Iteration 38, loss = 0.70366441\n",
            "Iteration 39, loss = 0.70088208\n",
            "Iteration 40, loss = 0.69810738\n",
            "Iteration 41, loss = 0.69560192\n",
            "Iteration 42, loss = 0.69316336\n",
            "Iteration 43, loss = 0.69061309\n",
            "Iteration 44, loss = 0.68855524\n",
            "Iteration 45, loss = 0.68625829\n",
            "Iteration 46, loss = 0.68415410\n",
            "Iteration 47, loss = 0.68216332\n",
            "Iteration 48, loss = 0.68023366\n",
            "Iteration 49, loss = 0.67837345\n",
            "Iteration 50, loss = 0.67652492\n",
            "Iteration 51, loss = 0.67465587\n",
            "Iteration 52, loss = 0.67291367\n",
            "Iteration 53, loss = 0.67120569\n",
            "Iteration 54, loss = 0.66960302\n",
            "Iteration 55, loss = 0.66800315\n",
            "Iteration 56, loss = 0.66652548\n",
            "Iteration 57, loss = 0.66485587\n",
            "Iteration 58, loss = 0.66349741\n",
            "Iteration 59, loss = 0.66208478\n",
            "Iteration 60, loss = 0.66070181\n",
            "Iteration 61, loss = 0.65933908\n",
            "Iteration 62, loss = 0.65815998\n",
            "Iteration 63, loss = 0.65691715\n",
            "Iteration 64, loss = 0.65568373\n",
            "Iteration 65, loss = 0.65459061\n",
            "Iteration 66, loss = 0.65352896\n",
            "Iteration 67, loss = 0.65233664\n",
            "Iteration 68, loss = 0.65132622\n",
            "Iteration 69, loss = 0.65019996\n",
            "Iteration 70, loss = 0.64924598\n",
            "Iteration 71, loss = 0.64821908\n",
            "Iteration 72, loss = 0.64725881\n",
            "Iteration 73, loss = 0.64638887\n",
            "Iteration 74, loss = 0.64557963\n",
            "Iteration 75, loss = 0.64472783\n",
            "Iteration 76, loss = 0.64381309\n",
            "Iteration 77, loss = 0.64297595\n",
            "Iteration 78, loss = 0.64215750\n",
            "Iteration 79, loss = 0.64136949\n",
            "Iteration 80, loss = 0.64060358\n",
            "Iteration 81, loss = 0.63981219\n",
            "Iteration 82, loss = 0.63909431\n",
            "Iteration 83, loss = 0.63840286\n",
            "Iteration 84, loss = 0.63766622\n",
            "Iteration 85, loss = 0.63702776\n",
            "Iteration 86, loss = 0.63626099\n",
            "Iteration 87, loss = 0.63554534\n",
            "Iteration 88, loss = 0.63482509\n",
            "Iteration 89, loss = 0.63410681\n",
            "Iteration 90, loss = 0.63339896\n",
            "Iteration 91, loss = 0.63272694\n",
            "Iteration 92, loss = 0.63204511\n",
            "Iteration 93, loss = 0.63144044\n",
            "Iteration 94, loss = 0.63084003\n",
            "Iteration 95, loss = 0.63028250\n",
            "Iteration 96, loss = 0.62965499\n",
            "Iteration 97, loss = 0.62905022\n",
            "Iteration 98, loss = 0.62844429\n",
            "Iteration 99, loss = 0.62780816\n",
            "Iteration 100, loss = 0.62726404\n",
            "Iteration 101, loss = 0.62667447\n",
            "Iteration 102, loss = 0.62614736\n",
            "Iteration 103, loss = 0.62561211\n",
            "Iteration 104, loss = 0.62507882\n",
            "Iteration 105, loss = 0.62452230\n",
            "Iteration 106, loss = 0.62401096\n",
            "Iteration 107, loss = 0.62343172\n",
            "Iteration 108, loss = 0.62293591\n",
            "Iteration 109, loss = 0.62242508\n",
            "Iteration 110, loss = 0.62197929\n",
            "Iteration 111, loss = 0.62148484\n",
            "Iteration 112, loss = 0.62100910\n",
            "Iteration 113, loss = 0.62055945\n",
            "Iteration 114, loss = 0.62011406\n",
            "Iteration 115, loss = 0.61961416\n",
            "Iteration 116, loss = 0.61915852\n",
            "Iteration 117, loss = 0.61873552\n",
            "Iteration 118, loss = 0.61828072\n",
            "Iteration 119, loss = 0.61786798\n",
            "Iteration 120, loss = 0.61746353\n",
            "Iteration 121, loss = 0.61703375\n",
            "Iteration 122, loss = 0.61663113\n",
            "Iteration 123, loss = 0.61623362\n",
            "Iteration 124, loss = 0.61582066\n",
            "Iteration 125, loss = 0.61543730\n",
            "Iteration 126, loss = 0.61503144\n",
            "Iteration 127, loss = 0.61465035\n",
            "Iteration 128, loss = 0.61429004\n",
            "Iteration 129, loss = 0.61393473\n",
            "Iteration 130, loss = 0.61360773\n",
            "Iteration 131, loss = 0.61325552\n",
            "Iteration 132, loss = 0.61291127\n",
            "Iteration 133, loss = 0.61256136\n",
            "Iteration 134, loss = 0.61222469\n",
            "Iteration 135, loss = 0.61188378\n",
            "Iteration 136, loss = 0.61154140\n",
            "Iteration 137, loss = 0.61123058\n",
            "Iteration 138, loss = 0.61090626\n",
            "Iteration 139, loss = 0.61058499\n",
            "Iteration 140, loss = 0.61029187\n",
            "Iteration 141, loss = 0.60998706\n",
            "Iteration 142, loss = 0.60965266\n",
            "Iteration 143, loss = 0.60933431\n",
            "Iteration 144, loss = 0.60899844\n",
            "Iteration 145, loss = 0.60866336\n",
            "Iteration 146, loss = 0.60834776\n",
            "Iteration 147, loss = 0.60804879\n",
            "Iteration 148, loss = 0.60774415\n",
            "Iteration 149, loss = 0.60745530\n",
            "Iteration 150, loss = 0.60720143\n",
            "Iteration 151, loss = 0.60690900\n",
            "Iteration 152, loss = 0.60661669\n",
            "Iteration 153, loss = 0.60635691\n",
            "Iteration 154, loss = 0.60606572\n",
            "Iteration 155, loss = 0.60579646\n",
            "Iteration 156, loss = 0.60554041\n",
            "Iteration 157, loss = 0.60524318\n",
            "Iteration 158, loss = 0.60496307\n",
            "Iteration 159, loss = 0.60471253\n",
            "Iteration 160, loss = 0.60443430\n",
            "Iteration 161, loss = 0.60417556\n",
            "Iteration 162, loss = 0.60389384\n",
            "Iteration 163, loss = 0.60362849\n",
            "Iteration 164, loss = 0.60336430\n",
            "Iteration 165, loss = 0.60310983\n",
            "Iteration 166, loss = 0.60286222\n",
            "Iteration 167, loss = 0.60260034\n",
            "Iteration 168, loss = 0.60235280\n",
            "Iteration 169, loss = 0.60210638\n",
            "Iteration 170, loss = 0.60184359\n",
            "Iteration 171, loss = 0.60160474\n",
            "Iteration 172, loss = 0.60135206\n",
            "Iteration 173, loss = 0.60111741\n",
            "Iteration 174, loss = 0.60086754\n",
            "Iteration 175, loss = 0.60062921\n",
            "Iteration 176, loss = 0.60042049\n",
            "Iteration 177, loss = 0.60019989\n",
            "Iteration 178, loss = 0.59996135\n",
            "Iteration 179, loss = 0.59972880\n",
            "Iteration 180, loss = 0.59951067\n",
            "Iteration 181, loss = 0.59928351\n",
            "Iteration 182, loss = 0.59908205\n",
            "Iteration 183, loss = 0.59886021\n",
            "Iteration 184, loss = 0.59861248\n",
            "Iteration 185, loss = 0.59839858\n",
            "Iteration 186, loss = 0.59817611\n",
            "Iteration 187, loss = 0.59796455\n",
            "Iteration 188, loss = 0.59776394\n",
            "Iteration 189, loss = 0.59755618\n",
            "Iteration 190, loss = 0.59735454\n",
            "Iteration 191, loss = 0.59716118\n",
            "Iteration 192, loss = 0.59694838\n",
            "Iteration 193, loss = 0.59676518\n",
            "Iteration 194, loss = 0.59657703\n",
            "Iteration 195, loss = 0.59640539\n",
            "Iteration 196, loss = 0.59619902\n",
            "Iteration 197, loss = 0.59602469\n",
            "Iteration 198, loss = 0.59583830\n",
            "Iteration 199, loss = 0.59564561\n",
            "Iteration 200, loss = 0.59545688\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 200 and for layer number 5 : 0.6987500000000001\n",
            "Iteration 1, loss = 1.38926569\n",
            "Iteration 2, loss = 1.37280501\n",
            "Iteration 3, loss = 1.34746797\n",
            "Iteration 4, loss = 1.31619589\n",
            "Iteration 5, loss = 1.28284600\n",
            "Iteration 6, loss = 1.24835476\n",
            "Iteration 7, loss = 1.21472357\n",
            "Iteration 8, loss = 1.18128774\n",
            "Iteration 9, loss = 1.14886509\n",
            "Iteration 10, loss = 1.11879494\n",
            "Iteration 11, loss = 1.08953649\n",
            "Iteration 12, loss = 1.06187751\n",
            "Iteration 13, loss = 1.03601333\n",
            "Iteration 14, loss = 1.01129749\n",
            "Iteration 15, loss = 0.98904579\n",
            "Iteration 16, loss = 0.96813749\n",
            "Iteration 17, loss = 0.94777197\n",
            "Iteration 18, loss = 0.92847460\n",
            "Iteration 19, loss = 0.91099975\n",
            "Iteration 20, loss = 0.89440660\n",
            "Iteration 21, loss = 0.87873317\n",
            "Iteration 22, loss = 0.86430695\n",
            "Iteration 23, loss = 0.85086938\n",
            "Iteration 24, loss = 0.83820984\n",
            "Iteration 25, loss = 0.82677373\n",
            "Iteration 26, loss = 0.81533009\n",
            "Iteration 27, loss = 0.80526332\n",
            "Iteration 28, loss = 0.79582014\n",
            "Iteration 29, loss = 0.78704031\n",
            "Iteration 30, loss = 0.77874972\n",
            "Iteration 31, loss = 0.77095374\n",
            "Iteration 32, loss = 0.76369540\n",
            "Iteration 33, loss = 0.75713497\n",
            "Iteration 34, loss = 0.75066766\n",
            "Iteration 35, loss = 0.74477483\n",
            "Iteration 36, loss = 0.73962234\n",
            "Iteration 37, loss = 0.73447076\n",
            "Iteration 38, loss = 0.72976653\n",
            "Iteration 39, loss = 0.72535374\n",
            "Iteration 40, loss = 0.72111863\n",
            "Iteration 41, loss = 0.71707931\n",
            "Iteration 42, loss = 0.71336464\n",
            "Iteration 43, loss = 0.70972019\n",
            "Iteration 44, loss = 0.70623239\n",
            "Iteration 45, loss = 0.70314381\n",
            "Iteration 46, loss = 0.69989642\n",
            "Iteration 47, loss = 0.69679974\n",
            "Iteration 48, loss = 0.69401116\n",
            "Iteration 49, loss = 0.69139935\n",
            "Iteration 50, loss = 0.68879462\n",
            "Iteration 51, loss = 0.68651470\n",
            "Iteration 52, loss = 0.68421757\n",
            "Iteration 53, loss = 0.68200484\n",
            "Iteration 54, loss = 0.68007585\n",
            "Iteration 55, loss = 0.67808367\n",
            "Iteration 56, loss = 0.67618600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 57, loss = 0.67429057\n",
            "Iteration 58, loss = 0.67248101\n",
            "Iteration 59, loss = 0.67080894\n",
            "Iteration 60, loss = 0.66927500\n",
            "Iteration 61, loss = 0.66761747\n",
            "Iteration 62, loss = 0.66614964\n",
            "Iteration 63, loss = 0.66454833\n",
            "Iteration 64, loss = 0.66305573\n",
            "Iteration 65, loss = 0.66161184\n",
            "Iteration 66, loss = 0.66013871\n",
            "Iteration 67, loss = 0.65874027\n",
            "Iteration 68, loss = 0.65728994\n",
            "Iteration 69, loss = 0.65605861\n",
            "Iteration 70, loss = 0.65477739\n",
            "Iteration 71, loss = 0.65354960\n",
            "Iteration 72, loss = 0.65235350\n",
            "Iteration 73, loss = 0.65124673\n",
            "Iteration 74, loss = 0.65008456\n",
            "Iteration 75, loss = 0.64907371\n",
            "Iteration 76, loss = 0.64797402\n",
            "Iteration 77, loss = 0.64701224\n",
            "Iteration 78, loss = 0.64597245\n",
            "Iteration 79, loss = 0.64505579\n",
            "Iteration 80, loss = 0.64412907\n",
            "Iteration 81, loss = 0.64320081\n",
            "Iteration 82, loss = 0.64230711\n",
            "Iteration 83, loss = 0.64139953\n",
            "Iteration 84, loss = 0.64054124\n",
            "Iteration 85, loss = 0.63968788\n",
            "Iteration 86, loss = 0.63885672\n",
            "Iteration 87, loss = 0.63805459\n",
            "Iteration 88, loss = 0.63726151\n",
            "Iteration 89, loss = 0.63648889\n",
            "Iteration 90, loss = 0.63578801\n",
            "Iteration 91, loss = 0.63499720\n",
            "Iteration 92, loss = 0.63423864\n",
            "Iteration 93, loss = 0.63348850\n",
            "Iteration 94, loss = 0.63277767\n",
            "Iteration 95, loss = 0.63198463\n",
            "Iteration 96, loss = 0.63126116\n",
            "Iteration 97, loss = 0.63054150\n",
            "Iteration 98, loss = 0.62982044\n",
            "Iteration 99, loss = 0.62910262\n",
            "Iteration 100, loss = 0.62846924\n",
            "Iteration 101, loss = 0.62771756\n",
            "Iteration 102, loss = 0.62705986\n",
            "Iteration 103, loss = 0.62638727\n",
            "Iteration 104, loss = 0.62572004\n",
            "Iteration 105, loss = 0.62509265\n",
            "Iteration 106, loss = 0.62441787\n",
            "Iteration 107, loss = 0.62381414\n",
            "Iteration 108, loss = 0.62318615\n",
            "Iteration 109, loss = 0.62257460\n",
            "Iteration 110, loss = 0.62195155\n",
            "Iteration 111, loss = 0.62131250\n",
            "Iteration 112, loss = 0.62072269\n",
            "Iteration 113, loss = 0.62017960\n",
            "Iteration 114, loss = 0.61958378\n",
            "Iteration 115, loss = 0.61903643\n",
            "Iteration 116, loss = 0.61843817\n",
            "Iteration 117, loss = 0.61785837\n",
            "Iteration 118, loss = 0.61732369\n",
            "Iteration 119, loss = 0.61674102\n",
            "Iteration 120, loss = 0.61622506\n",
            "Iteration 121, loss = 0.61576215\n",
            "Iteration 122, loss = 0.61529013\n",
            "Iteration 123, loss = 0.61483731\n",
            "Iteration 124, loss = 0.61438639\n",
            "Iteration 125, loss = 0.61390245\n",
            "Iteration 126, loss = 0.61345612\n",
            "Iteration 127, loss = 0.61299340\n",
            "Iteration 128, loss = 0.61254224\n",
            "Iteration 129, loss = 0.61210542\n",
            "Iteration 130, loss = 0.61164362\n",
            "Iteration 131, loss = 0.61120969\n",
            "Iteration 132, loss = 0.61073231\n",
            "Iteration 133, loss = 0.61026151\n",
            "Iteration 134, loss = 0.60980717\n",
            "Iteration 135, loss = 0.60934013\n",
            "Iteration 136, loss = 0.60886240\n",
            "Iteration 137, loss = 0.60837324\n",
            "Iteration 138, loss = 0.60790973\n",
            "Iteration 139, loss = 0.60744643\n",
            "Iteration 140, loss = 0.60697494\n",
            "Iteration 141, loss = 0.60651293\n",
            "Iteration 142, loss = 0.60607800\n",
            "Iteration 143, loss = 0.60558981\n",
            "Iteration 144, loss = 0.60514142\n",
            "Iteration 145, loss = 0.60471048\n",
            "Iteration 146, loss = 0.60428408\n",
            "Iteration 147, loss = 0.60388988\n",
            "Iteration 148, loss = 0.60350121\n",
            "Iteration 149, loss = 0.60305622\n",
            "Iteration 150, loss = 0.60264952\n",
            "Iteration 151, loss = 0.60226329\n",
            "Iteration 152, loss = 0.60184344\n",
            "Iteration 153, loss = 0.60141317\n",
            "Iteration 154, loss = 0.60101958\n",
            "Iteration 155, loss = 0.60059526\n",
            "Iteration 156, loss = 0.60021206\n",
            "Iteration 157, loss = 0.59982640\n",
            "Iteration 158, loss = 0.59942785\n",
            "Iteration 159, loss = 0.59905748\n",
            "Iteration 160, loss = 0.59869435\n",
            "Iteration 161, loss = 0.59835766\n",
            "Iteration 162, loss = 0.59803055\n",
            "Iteration 163, loss = 0.59768243\n",
            "Iteration 164, loss = 0.59736333\n",
            "Iteration 165, loss = 0.59704623\n",
            "Iteration 166, loss = 0.59667919\n",
            "Iteration 167, loss = 0.59634940\n",
            "Iteration 168, loss = 0.59600875\n",
            "Iteration 169, loss = 0.59569654\n",
            "Iteration 170, loss = 0.59535122\n",
            "Iteration 171, loss = 0.59503962\n",
            "Iteration 172, loss = 0.59471082\n",
            "Iteration 173, loss = 0.59437875\n",
            "Iteration 174, loss = 0.59404223\n",
            "Iteration 175, loss = 0.59369966\n",
            "Iteration 176, loss = 0.59338444\n",
            "Iteration 177, loss = 0.59306672\n",
            "Iteration 178, loss = 0.59275374\n",
            "Iteration 179, loss = 0.59246291\n",
            "Iteration 180, loss = 0.59214536\n",
            "Iteration 181, loss = 0.59185390\n",
            "Iteration 182, loss = 0.59156543\n",
            "Iteration 183, loss = 0.59128214\n",
            "Iteration 184, loss = 0.59095869\n",
            "Iteration 185, loss = 0.59068455\n",
            "Iteration 186, loss = 0.59037852\n",
            "Iteration 187, loss = 0.59006571\n",
            "Iteration 188, loss = 0.58982558\n",
            "Iteration 189, loss = 0.58956873\n",
            "Iteration 190, loss = 0.58932398\n",
            "Iteration 191, loss = 0.58908908\n",
            "Iteration 192, loss = 0.58886339\n",
            "Iteration 193, loss = 0.58861219\n",
            "Iteration 194, loss = 0.58835646\n",
            "Iteration 195, loss = 0.58808979\n",
            "Iteration 196, loss = 0.58780606\n",
            "Iteration 197, loss = 0.58753925\n",
            "Iteration 198, loss = 0.58730038\n",
            "Iteration 199, loss = 0.58705267\n",
            "Iteration 200, loss = 0.58679714\n",
            "Iteration 1, loss = 1.37877399\n",
            "Iteration 2, loss = 1.36256087\n",
            "Iteration 3, loss = 1.33739238\n",
            "Iteration 4, loss = 1.30659378\n",
            "Iteration 5, loss = 1.27335856\n",
            "Iteration 6, loss = 1.23889435\n",
            "Iteration 7, loss = 1.20499233\n",
            "Iteration 8, loss = 1.17181650\n",
            "Iteration 9, loss = 1.13971284\n",
            "Iteration 10, loss = 1.10956324\n",
            "Iteration 11, loss = 1.08045416\n",
            "Iteration 12, loss = 1.05294176\n",
            "Iteration 13, loss = 1.02710830\n",
            "Iteration 14, loss = 1.00261165\n",
            "Iteration 15, loss = 0.98070591\n",
            "Iteration 16, loss = 0.95987920\n",
            "Iteration 17, loss = 0.94020854\n",
            "Iteration 18, loss = 0.92122763\n",
            "Iteration 19, loss = 0.90383330\n",
            "Iteration 20, loss = 0.88748408\n",
            "Iteration 21, loss = 0.87191253\n",
            "Iteration 22, loss = 0.85761649\n",
            "Iteration 23, loss = 0.84438059\n",
            "Iteration 24, loss = 0.83166341\n",
            "Iteration 25, loss = 0.82034630\n",
            "Iteration 26, loss = 0.80900028\n",
            "Iteration 27, loss = 0.79924699\n",
            "Iteration 28, loss = 0.78975345\n",
            "Iteration 29, loss = 0.78110653\n",
            "Iteration 30, loss = 0.77320177\n",
            "Iteration 31, loss = 0.76575073\n",
            "Iteration 32, loss = 0.75849142\n",
            "Iteration 33, loss = 0.75189914\n",
            "Iteration 34, loss = 0.74559673\n",
            "Iteration 35, loss = 0.73983119\n",
            "Iteration 36, loss = 0.73485730\n",
            "Iteration 37, loss = 0.72990421\n",
            "Iteration 38, loss = 0.72544570\n",
            "Iteration 39, loss = 0.72117603\n",
            "Iteration 40, loss = 0.71703850\n",
            "Iteration 41, loss = 0.71322451\n",
            "Iteration 42, loss = 0.70966686\n",
            "Iteration 43, loss = 0.70626584\n",
            "Iteration 44, loss = 0.70282098\n",
            "Iteration 45, loss = 0.69979339\n",
            "Iteration 46, loss = 0.69664159\n",
            "Iteration 47, loss = 0.69359879\n",
            "Iteration 48, loss = 0.69084866\n",
            "Iteration 49, loss = 0.68833276\n",
            "Iteration 50, loss = 0.68587860\n",
            "Iteration 51, loss = 0.68370393\n",
            "Iteration 52, loss = 0.68146700\n",
            "Iteration 53, loss = 0.67934255\n",
            "Iteration 54, loss = 0.67741969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 55, loss = 0.67540396\n",
            "Iteration 56, loss = 0.67352010\n",
            "Iteration 57, loss = 0.67177485\n",
            "Iteration 58, loss = 0.67002014\n",
            "Iteration 59, loss = 0.66846780\n",
            "Iteration 60, loss = 0.66695573\n",
            "Iteration 61, loss = 0.66542439\n",
            "Iteration 62, loss = 0.66394678\n",
            "Iteration 63, loss = 0.66235577\n",
            "Iteration 64, loss = 0.66088106\n",
            "Iteration 65, loss = 0.65935602\n",
            "Iteration 66, loss = 0.65793334\n",
            "Iteration 67, loss = 0.65652492\n",
            "Iteration 68, loss = 0.65508253\n",
            "Iteration 69, loss = 0.65383545\n",
            "Iteration 70, loss = 0.65253073\n",
            "Iteration 71, loss = 0.65132221\n",
            "Iteration 72, loss = 0.65005109\n",
            "Iteration 73, loss = 0.64889076\n",
            "Iteration 74, loss = 0.64770514\n",
            "Iteration 75, loss = 0.64660515\n",
            "Iteration 76, loss = 0.64544655\n",
            "Iteration 77, loss = 0.64443220\n",
            "Iteration 78, loss = 0.64331724\n",
            "Iteration 79, loss = 0.64235456\n",
            "Iteration 80, loss = 0.64132897\n",
            "Iteration 81, loss = 0.64038882\n",
            "Iteration 82, loss = 0.63947120\n",
            "Iteration 83, loss = 0.63852183\n",
            "Iteration 84, loss = 0.63767949\n",
            "Iteration 85, loss = 0.63682970\n",
            "Iteration 86, loss = 0.63597486\n",
            "Iteration 87, loss = 0.63514300\n",
            "Iteration 88, loss = 0.63435968\n",
            "Iteration 89, loss = 0.63354415\n",
            "Iteration 90, loss = 0.63282646\n",
            "Iteration 91, loss = 0.63199980\n",
            "Iteration 92, loss = 0.63122688\n",
            "Iteration 93, loss = 0.63045721\n",
            "Iteration 94, loss = 0.62970582\n",
            "Iteration 95, loss = 0.62887494\n",
            "Iteration 96, loss = 0.62811898\n",
            "Iteration 97, loss = 0.62738428\n",
            "Iteration 98, loss = 0.62665124\n",
            "Iteration 99, loss = 0.62594012\n",
            "Iteration 100, loss = 0.62527980\n",
            "Iteration 101, loss = 0.62456794\n",
            "Iteration 102, loss = 0.62386710\n",
            "Iteration 103, loss = 0.62320385\n",
            "Iteration 104, loss = 0.62253996\n",
            "Iteration 105, loss = 0.62189415\n",
            "Iteration 106, loss = 0.62120134\n",
            "Iteration 107, loss = 0.62057750\n",
            "Iteration 108, loss = 0.61992975\n",
            "Iteration 109, loss = 0.61930433\n",
            "Iteration 110, loss = 0.61866132\n",
            "Iteration 111, loss = 0.61802694\n",
            "Iteration 112, loss = 0.61740918\n",
            "Iteration 113, loss = 0.61684939\n",
            "Iteration 114, loss = 0.61623249\n",
            "Iteration 115, loss = 0.61565796\n",
            "Iteration 116, loss = 0.61504501\n",
            "Iteration 117, loss = 0.61446406\n",
            "Iteration 118, loss = 0.61390667\n",
            "Iteration 119, loss = 0.61329184\n",
            "Iteration 120, loss = 0.61276745\n",
            "Iteration 121, loss = 0.61226539\n",
            "Iteration 122, loss = 0.61178911\n",
            "Iteration 123, loss = 0.61131873\n",
            "Iteration 124, loss = 0.61082428\n",
            "Iteration 125, loss = 0.61033579\n",
            "Iteration 126, loss = 0.60984979\n",
            "Iteration 127, loss = 0.60935164\n",
            "Iteration 128, loss = 0.60887902\n",
            "Iteration 129, loss = 0.60840988\n",
            "Iteration 130, loss = 0.60792078\n",
            "Iteration 131, loss = 0.60746030\n",
            "Iteration 132, loss = 0.60695595\n",
            "Iteration 133, loss = 0.60645273\n",
            "Iteration 134, loss = 0.60595229\n",
            "Iteration 135, loss = 0.60546307\n",
            "Iteration 136, loss = 0.60495070\n",
            "Iteration 137, loss = 0.60449123\n",
            "Iteration 138, loss = 0.60401363\n",
            "Iteration 139, loss = 0.60356238\n",
            "Iteration 140, loss = 0.60309091\n",
            "Iteration 141, loss = 0.60264393\n",
            "Iteration 142, loss = 0.60224576\n",
            "Iteration 143, loss = 0.60177122\n",
            "Iteration 144, loss = 0.60137298\n",
            "Iteration 145, loss = 0.60096009\n",
            "Iteration 146, loss = 0.60056936\n",
            "Iteration 147, loss = 0.60019633\n",
            "Iteration 148, loss = 0.59982456\n",
            "Iteration 149, loss = 0.59941932\n",
            "Iteration 150, loss = 0.59899122\n",
            "Iteration 151, loss = 0.59859589\n",
            "Iteration 152, loss = 0.59818445\n",
            "Iteration 153, loss = 0.59777471\n",
            "Iteration 154, loss = 0.59741094\n",
            "Iteration 155, loss = 0.59701005\n",
            "Iteration 156, loss = 0.59663178\n",
            "Iteration 157, loss = 0.59625552\n",
            "Iteration 158, loss = 0.59587864\n",
            "Iteration 159, loss = 0.59553984\n",
            "Iteration 160, loss = 0.59521080\n",
            "Iteration 161, loss = 0.59488221\n",
            "Iteration 162, loss = 0.59458009\n",
            "Iteration 163, loss = 0.59425744\n",
            "Iteration 164, loss = 0.59394011\n",
            "Iteration 165, loss = 0.59363717\n",
            "Iteration 166, loss = 0.59326735\n",
            "Iteration 167, loss = 0.59293728\n",
            "Iteration 168, loss = 0.59261492\n",
            "Iteration 169, loss = 0.59231779\n",
            "Iteration 170, loss = 0.59198436\n",
            "Iteration 171, loss = 0.59166940\n",
            "Iteration 172, loss = 0.59134855\n",
            "Iteration 173, loss = 0.59100645\n",
            "Iteration 174, loss = 0.59067415\n",
            "Iteration 175, loss = 0.59034669\n",
            "Iteration 176, loss = 0.59000864\n",
            "Iteration 177, loss = 0.58967364\n",
            "Iteration 178, loss = 0.58932761\n",
            "Iteration 179, loss = 0.58901187\n",
            "Iteration 180, loss = 0.58867165\n",
            "Iteration 181, loss = 0.58836517\n",
            "Iteration 182, loss = 0.58805065\n",
            "Iteration 183, loss = 0.58773935\n",
            "Iteration 184, loss = 0.58741684\n",
            "Iteration 185, loss = 0.58711122\n",
            "Iteration 186, loss = 0.58679375\n",
            "Iteration 187, loss = 0.58646867\n",
            "Iteration 188, loss = 0.58618550\n",
            "Iteration 189, loss = 0.58589695\n",
            "Iteration 190, loss = 0.58561923\n",
            "Iteration 191, loss = 0.58535488\n",
            "Iteration 192, loss = 0.58509026\n",
            "Iteration 193, loss = 0.58483283\n",
            "Iteration 194, loss = 0.58454814\n",
            "Iteration 195, loss = 0.58426541\n",
            "Iteration 196, loss = 0.58397641\n",
            "Iteration 197, loss = 0.58370621\n",
            "Iteration 198, loss = 0.58345101\n",
            "Iteration 199, loss = 0.58317530\n",
            "Iteration 200, loss = 0.58290522\n",
            "Iteration 1, loss = 1.37951553\n",
            "Iteration 2, loss = 1.36345276\n",
            "Iteration 3, loss = 1.33844177\n",
            "Iteration 4, loss = 1.30796298\n",
            "Iteration 5, loss = 1.27459126\n",
            "Iteration 6, loss = 1.24040492\n",
            "Iteration 7, loss = 1.20671426\n",
            "Iteration 8, loss = 1.17282486\n",
            "Iteration 9, loss = 1.14067304\n",
            "Iteration 10, loss = 1.10992688\n",
            "Iteration 11, loss = 1.08024299\n",
            "Iteration 12, loss = 1.05249805\n",
            "Iteration 13, loss = 1.02632044\n",
            "Iteration 14, loss = 1.00219240\n",
            "Iteration 15, loss = 0.97949832\n",
            "Iteration 16, loss = 0.95866806\n",
            "Iteration 17, loss = 0.93831233\n",
            "Iteration 18, loss = 0.91955176\n",
            "Iteration 19, loss = 0.90227991\n",
            "Iteration 20, loss = 0.88594294\n",
            "Iteration 21, loss = 0.87069089\n",
            "Iteration 22, loss = 0.85667586\n",
            "Iteration 23, loss = 0.84359414\n",
            "Iteration 24, loss = 0.83093219\n",
            "Iteration 25, loss = 0.81954024\n",
            "Iteration 26, loss = 0.80797193\n",
            "Iteration 27, loss = 0.79813972\n",
            "Iteration 28, loss = 0.78827489\n",
            "Iteration 29, loss = 0.77945534\n",
            "Iteration 30, loss = 0.77167073\n",
            "Iteration 31, loss = 0.76379821\n",
            "Iteration 32, loss = 0.75668072\n",
            "Iteration 33, loss = 0.74977028\n",
            "Iteration 34, loss = 0.74333202\n",
            "Iteration 35, loss = 0.73744851\n",
            "Iteration 36, loss = 0.73224584\n",
            "Iteration 37, loss = 0.72713655\n",
            "Iteration 38, loss = 0.72251036\n",
            "Iteration 39, loss = 0.71809401\n",
            "Iteration 40, loss = 0.71381178\n",
            "Iteration 41, loss = 0.71006817\n",
            "Iteration 42, loss = 0.70642663\n",
            "Iteration 43, loss = 0.70297709\n",
            "Iteration 44, loss = 0.69956808\n",
            "Iteration 45, loss = 0.69665075\n",
            "Iteration 46, loss = 0.69345907\n",
            "Iteration 47, loss = 0.69050376\n",
            "Iteration 48, loss = 0.68787513\n",
            "Iteration 49, loss = 0.68529710\n",
            "Iteration 50, loss = 0.68283550\n",
            "Iteration 51, loss = 0.68057320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 52, loss = 0.67831132\n",
            "Iteration 53, loss = 0.67622728\n",
            "Iteration 54, loss = 0.67421572\n",
            "Iteration 55, loss = 0.67206718\n",
            "Iteration 56, loss = 0.67019947\n",
            "Iteration 57, loss = 0.66839904\n",
            "Iteration 58, loss = 0.66666072\n",
            "Iteration 59, loss = 0.66504789\n",
            "Iteration 60, loss = 0.66353676\n",
            "Iteration 61, loss = 0.66204168\n",
            "Iteration 62, loss = 0.66055577\n",
            "Iteration 63, loss = 0.65901870\n",
            "Iteration 64, loss = 0.65756080\n",
            "Iteration 65, loss = 0.65607636\n",
            "Iteration 66, loss = 0.65469643\n",
            "Iteration 67, loss = 0.65336830\n",
            "Iteration 68, loss = 0.65205637\n",
            "Iteration 69, loss = 0.65084949\n",
            "Iteration 70, loss = 0.64960277\n",
            "Iteration 71, loss = 0.64837394\n",
            "Iteration 72, loss = 0.64713407\n",
            "Iteration 73, loss = 0.64596524\n",
            "Iteration 74, loss = 0.64480252\n",
            "Iteration 75, loss = 0.64368401\n",
            "Iteration 76, loss = 0.64254765\n",
            "Iteration 77, loss = 0.64150617\n",
            "Iteration 78, loss = 0.64039261\n",
            "Iteration 79, loss = 0.63941413\n",
            "Iteration 80, loss = 0.63838386\n",
            "Iteration 81, loss = 0.63743753\n",
            "Iteration 82, loss = 0.63650474\n",
            "Iteration 83, loss = 0.63557633\n",
            "Iteration 84, loss = 0.63469972\n",
            "Iteration 85, loss = 0.63385219\n",
            "Iteration 86, loss = 0.63298910\n",
            "Iteration 87, loss = 0.63211509\n",
            "Iteration 88, loss = 0.63130517\n",
            "Iteration 89, loss = 0.63045727\n",
            "Iteration 90, loss = 0.62971541\n",
            "Iteration 91, loss = 0.62884744\n",
            "Iteration 92, loss = 0.62806282\n",
            "Iteration 93, loss = 0.62730010\n",
            "Iteration 94, loss = 0.62658916\n",
            "Iteration 95, loss = 0.62581543\n",
            "Iteration 96, loss = 0.62512103\n",
            "Iteration 97, loss = 0.62443816\n",
            "Iteration 98, loss = 0.62373486\n",
            "Iteration 99, loss = 0.62304011\n",
            "Iteration 100, loss = 0.62238348\n",
            "Iteration 101, loss = 0.62169762\n",
            "Iteration 102, loss = 0.62100535\n",
            "Iteration 103, loss = 0.62032091\n",
            "Iteration 104, loss = 0.61961880\n",
            "Iteration 105, loss = 0.61893901\n",
            "Iteration 106, loss = 0.61820553\n",
            "Iteration 107, loss = 0.61757926\n",
            "Iteration 108, loss = 0.61690315\n",
            "Iteration 109, loss = 0.61627656\n",
            "Iteration 110, loss = 0.61564412\n",
            "Iteration 111, loss = 0.61502192\n",
            "Iteration 112, loss = 0.61441879\n",
            "Iteration 113, loss = 0.61384866\n",
            "Iteration 114, loss = 0.61324066\n",
            "Iteration 115, loss = 0.61265048\n",
            "Iteration 116, loss = 0.61203885\n",
            "Iteration 117, loss = 0.61143757\n",
            "Iteration 118, loss = 0.61083038\n",
            "Iteration 119, loss = 0.61019121\n",
            "Iteration 120, loss = 0.60962254\n",
            "Iteration 121, loss = 0.60904441\n",
            "Iteration 122, loss = 0.60850742\n",
            "Iteration 123, loss = 0.60796362\n",
            "Iteration 124, loss = 0.60739810\n",
            "Iteration 125, loss = 0.60686236\n",
            "Iteration 126, loss = 0.60632434\n",
            "Iteration 127, loss = 0.60582041\n",
            "Iteration 128, loss = 0.60527198\n",
            "Iteration 129, loss = 0.60476835\n",
            "Iteration 130, loss = 0.60423221\n",
            "Iteration 131, loss = 0.60373134\n",
            "Iteration 132, loss = 0.60320781\n",
            "Iteration 133, loss = 0.60265126\n",
            "Iteration 134, loss = 0.60212403\n",
            "Iteration 135, loss = 0.60161982\n",
            "Iteration 136, loss = 0.60110169\n",
            "Iteration 137, loss = 0.60061041\n",
            "Iteration 138, loss = 0.60013873\n",
            "Iteration 139, loss = 0.59963681\n",
            "Iteration 140, loss = 0.59914936\n",
            "Iteration 141, loss = 0.59866027\n",
            "Iteration 142, loss = 0.59821810\n",
            "Iteration 143, loss = 0.59770969\n",
            "Iteration 144, loss = 0.59729074\n",
            "Iteration 145, loss = 0.59684055\n",
            "Iteration 146, loss = 0.59639239\n",
            "Iteration 147, loss = 0.59598622\n",
            "Iteration 148, loss = 0.59557002\n",
            "Iteration 149, loss = 0.59513057\n",
            "Iteration 150, loss = 0.59465560\n",
            "Iteration 151, loss = 0.59421569\n",
            "Iteration 152, loss = 0.59376454\n",
            "Iteration 153, loss = 0.59332523\n",
            "Iteration 154, loss = 0.59290748\n",
            "Iteration 155, loss = 0.59246859\n",
            "Iteration 156, loss = 0.59205294\n",
            "Iteration 157, loss = 0.59162856\n",
            "Iteration 158, loss = 0.59118886\n",
            "Iteration 159, loss = 0.59077922\n",
            "Iteration 160, loss = 0.59039004\n",
            "Iteration 161, loss = 0.59000902\n",
            "Iteration 162, loss = 0.58962546\n",
            "Iteration 163, loss = 0.58926966\n",
            "Iteration 164, loss = 0.58888854\n",
            "Iteration 165, loss = 0.58853074\n",
            "Iteration 166, loss = 0.58813874\n",
            "Iteration 167, loss = 0.58776302\n",
            "Iteration 168, loss = 0.58737658\n",
            "Iteration 169, loss = 0.58703757\n",
            "Iteration 170, loss = 0.58664457\n",
            "Iteration 171, loss = 0.58627862\n",
            "Iteration 172, loss = 0.58590590\n",
            "Iteration 173, loss = 0.58553044\n",
            "Iteration 174, loss = 0.58516114\n",
            "Iteration 175, loss = 0.58480732\n",
            "Iteration 176, loss = 0.58444066\n",
            "Iteration 177, loss = 0.58408786\n",
            "Iteration 178, loss = 0.58372734\n",
            "Iteration 179, loss = 0.58336305\n",
            "Iteration 180, loss = 0.58298927\n",
            "Iteration 181, loss = 0.58265563\n",
            "Iteration 182, loss = 0.58229694\n",
            "Iteration 183, loss = 0.58194131\n",
            "Iteration 184, loss = 0.58157458\n",
            "Iteration 185, loss = 0.58122149\n",
            "Iteration 186, loss = 0.58086295\n",
            "Iteration 187, loss = 0.58050201\n",
            "Iteration 188, loss = 0.58016811\n",
            "Iteration 189, loss = 0.57980168\n",
            "Iteration 190, loss = 0.57949534\n",
            "Iteration 191, loss = 0.57916765\n",
            "Iteration 192, loss = 0.57886601\n",
            "Iteration 193, loss = 0.57858799\n",
            "Iteration 194, loss = 0.57826395\n",
            "Iteration 195, loss = 0.57794561\n",
            "Iteration 196, loss = 0.57763663\n",
            "Iteration 197, loss = 0.57731809\n",
            "Iteration 198, loss = 0.57701732\n",
            "Iteration 199, loss = 0.57669483\n",
            "Iteration 200, loss = 0.57637690\n",
            "Iteration 1, loss = 1.39138454\n",
            "Iteration 2, loss = 1.37512562\n",
            "Iteration 3, loss = 1.34984865\n",
            "Iteration 4, loss = 1.31828804\n",
            "Iteration 5, loss = 1.28379813\n",
            "Iteration 6, loss = 1.24919730\n",
            "Iteration 7, loss = 1.21383002\n",
            "Iteration 8, loss = 1.17922096\n",
            "Iteration 9, loss = 1.14576061\n",
            "Iteration 10, loss = 1.11442987\n",
            "Iteration 11, loss = 1.08433757\n",
            "Iteration 12, loss = 1.05600586\n",
            "Iteration 13, loss = 1.02936580\n",
            "Iteration 14, loss = 1.00449374\n",
            "Iteration 15, loss = 0.98126814\n",
            "Iteration 16, loss = 0.96045322\n",
            "Iteration 17, loss = 0.93985076\n",
            "Iteration 18, loss = 0.92131534\n",
            "Iteration 19, loss = 0.90427757\n",
            "Iteration 20, loss = 0.88794149\n",
            "Iteration 21, loss = 0.87257105\n",
            "Iteration 22, loss = 0.85865619\n",
            "Iteration 23, loss = 0.84553420\n",
            "Iteration 24, loss = 0.83288102\n",
            "Iteration 25, loss = 0.82164075\n",
            "Iteration 26, loss = 0.81031884\n",
            "Iteration 27, loss = 0.80044622\n",
            "Iteration 28, loss = 0.79078083\n",
            "Iteration 29, loss = 0.78199497\n",
            "Iteration 30, loss = 0.77411266\n",
            "Iteration 31, loss = 0.76624354\n",
            "Iteration 32, loss = 0.75938182\n",
            "Iteration 33, loss = 0.75241517\n",
            "Iteration 34, loss = 0.74614686\n",
            "Iteration 35, loss = 0.74020298\n",
            "Iteration 36, loss = 0.73504213\n",
            "Iteration 37, loss = 0.72986011\n",
            "Iteration 38, loss = 0.72514082\n",
            "Iteration 39, loss = 0.72075279\n",
            "Iteration 40, loss = 0.71636099\n",
            "Iteration 41, loss = 0.71262350\n",
            "Iteration 42, loss = 0.70892549\n",
            "Iteration 43, loss = 0.70543635\n",
            "Iteration 44, loss = 0.70195493\n",
            "Iteration 45, loss = 0.69894456\n",
            "Iteration 46, loss = 0.69579898\n",
            "Iteration 47, loss = 0.69295315\n",
            "Iteration 48, loss = 0.69035026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 49, loss = 0.68781951\n",
            "Iteration 50, loss = 0.68550089\n",
            "Iteration 51, loss = 0.68324006\n",
            "Iteration 52, loss = 0.68114943\n",
            "Iteration 53, loss = 0.67907425\n",
            "Iteration 54, loss = 0.67705865\n",
            "Iteration 55, loss = 0.67497031\n",
            "Iteration 56, loss = 0.67311956\n",
            "Iteration 57, loss = 0.67123379\n",
            "Iteration 58, loss = 0.66945703\n",
            "Iteration 59, loss = 0.66776204\n",
            "Iteration 60, loss = 0.66623540\n",
            "Iteration 61, loss = 0.66472561\n",
            "Iteration 62, loss = 0.66332128\n",
            "Iteration 63, loss = 0.66181759\n",
            "Iteration 64, loss = 0.66047596\n",
            "Iteration 65, loss = 0.65908815\n",
            "Iteration 66, loss = 0.65776076\n",
            "Iteration 67, loss = 0.65648902\n",
            "Iteration 68, loss = 0.65521748\n",
            "Iteration 69, loss = 0.65400813\n",
            "Iteration 70, loss = 0.65278035\n",
            "Iteration 71, loss = 0.65149094\n",
            "Iteration 72, loss = 0.65028077\n",
            "Iteration 73, loss = 0.64905842\n",
            "Iteration 74, loss = 0.64790424\n",
            "Iteration 75, loss = 0.64677162\n",
            "Iteration 76, loss = 0.64562035\n",
            "Iteration 77, loss = 0.64452933\n",
            "Iteration 78, loss = 0.64345644\n",
            "Iteration 79, loss = 0.64241040\n",
            "Iteration 80, loss = 0.64140986\n",
            "Iteration 81, loss = 0.64043956\n",
            "Iteration 82, loss = 0.63949342\n",
            "Iteration 83, loss = 0.63859004\n",
            "Iteration 84, loss = 0.63771080\n",
            "Iteration 85, loss = 0.63683352\n",
            "Iteration 86, loss = 0.63594807\n",
            "Iteration 87, loss = 0.63506056\n",
            "Iteration 88, loss = 0.63421892\n",
            "Iteration 89, loss = 0.63337974\n",
            "Iteration 90, loss = 0.63261508\n",
            "Iteration 91, loss = 0.63177660\n",
            "Iteration 92, loss = 0.63099911\n",
            "Iteration 93, loss = 0.63024576\n",
            "Iteration 94, loss = 0.62950227\n",
            "Iteration 95, loss = 0.62874205\n",
            "Iteration 96, loss = 0.62802933\n",
            "Iteration 97, loss = 0.62731275\n",
            "Iteration 98, loss = 0.62657765\n",
            "Iteration 99, loss = 0.62582972\n",
            "Iteration 100, loss = 0.62512388\n",
            "Iteration 101, loss = 0.62440886\n",
            "Iteration 102, loss = 0.62369126\n",
            "Iteration 103, loss = 0.62301138\n",
            "Iteration 104, loss = 0.62231346\n",
            "Iteration 105, loss = 0.62164193\n",
            "Iteration 106, loss = 0.62091321\n",
            "Iteration 107, loss = 0.62027969\n",
            "Iteration 108, loss = 0.61963396\n",
            "Iteration 109, loss = 0.61900211\n",
            "Iteration 110, loss = 0.61838819\n",
            "Iteration 111, loss = 0.61778817\n",
            "Iteration 112, loss = 0.61718123\n",
            "Iteration 113, loss = 0.61662347\n",
            "Iteration 114, loss = 0.61606190\n",
            "Iteration 115, loss = 0.61547712\n",
            "Iteration 116, loss = 0.61488928\n",
            "Iteration 117, loss = 0.61433814\n",
            "Iteration 118, loss = 0.61377771\n",
            "Iteration 119, loss = 0.61320796\n",
            "Iteration 120, loss = 0.61267437\n",
            "Iteration 121, loss = 0.61214127\n",
            "Iteration 122, loss = 0.61166081\n",
            "Iteration 123, loss = 0.61117147\n",
            "Iteration 124, loss = 0.61067739\n",
            "Iteration 125, loss = 0.61019738\n",
            "Iteration 126, loss = 0.60970918\n",
            "Iteration 127, loss = 0.60927424\n",
            "Iteration 128, loss = 0.60878360\n",
            "Iteration 129, loss = 0.60829878\n",
            "Iteration 130, loss = 0.60781020\n",
            "Iteration 131, loss = 0.60733642\n",
            "Iteration 132, loss = 0.60684527\n",
            "Iteration 133, loss = 0.60634604\n",
            "Iteration 134, loss = 0.60588950\n",
            "Iteration 135, loss = 0.60540424\n",
            "Iteration 136, loss = 0.60493219\n",
            "Iteration 137, loss = 0.60446340\n",
            "Iteration 138, loss = 0.60405119\n",
            "Iteration 139, loss = 0.60358573\n",
            "Iteration 140, loss = 0.60316252\n",
            "Iteration 141, loss = 0.60271924\n",
            "Iteration 142, loss = 0.60230785\n",
            "Iteration 143, loss = 0.60185146\n",
            "Iteration 144, loss = 0.60144689\n",
            "Iteration 145, loss = 0.60102438\n",
            "Iteration 146, loss = 0.60057588\n",
            "Iteration 147, loss = 0.60019066\n",
            "Iteration 148, loss = 0.59977412\n",
            "Iteration 149, loss = 0.59935245\n",
            "Iteration 150, loss = 0.59893304\n",
            "Iteration 151, loss = 0.59850764\n",
            "Iteration 152, loss = 0.59810748\n",
            "Iteration 153, loss = 0.59771272\n",
            "Iteration 154, loss = 0.59732168\n",
            "Iteration 155, loss = 0.59690309\n",
            "Iteration 156, loss = 0.59651319\n",
            "Iteration 157, loss = 0.59609727\n",
            "Iteration 158, loss = 0.59568627\n",
            "Iteration 159, loss = 0.59527740\n",
            "Iteration 160, loss = 0.59489084\n",
            "Iteration 161, loss = 0.59451146\n",
            "Iteration 162, loss = 0.59414111\n",
            "Iteration 163, loss = 0.59382351\n",
            "Iteration 164, loss = 0.59347342\n",
            "Iteration 165, loss = 0.59316499\n",
            "Iteration 166, loss = 0.59280595\n",
            "Iteration 167, loss = 0.59246808\n",
            "Iteration 168, loss = 0.59209276\n",
            "Iteration 169, loss = 0.59177597\n",
            "Iteration 170, loss = 0.59138530\n",
            "Iteration 171, loss = 0.59102396\n",
            "Iteration 172, loss = 0.59067631\n",
            "Iteration 173, loss = 0.59032790\n",
            "Iteration 174, loss = 0.58998561\n",
            "Iteration 175, loss = 0.58965719\n",
            "Iteration 176, loss = 0.58931282\n",
            "Iteration 177, loss = 0.58897694\n",
            "Iteration 178, loss = 0.58862537\n",
            "Iteration 179, loss = 0.58827694\n",
            "Iteration 180, loss = 0.58793072\n",
            "Iteration 181, loss = 0.58761554\n",
            "Iteration 182, loss = 0.58729607\n",
            "Iteration 183, loss = 0.58696715\n",
            "Iteration 184, loss = 0.58665574\n",
            "Iteration 185, loss = 0.58629769\n",
            "Iteration 186, loss = 0.58597816\n",
            "Iteration 187, loss = 0.58565617\n",
            "Iteration 188, loss = 0.58535530\n",
            "Iteration 189, loss = 0.58501471\n",
            "Iteration 190, loss = 0.58473598\n",
            "Iteration 191, loss = 0.58442454\n",
            "Iteration 192, loss = 0.58413628\n",
            "Iteration 193, loss = 0.58387189\n",
            "Iteration 194, loss = 0.58357290\n",
            "Iteration 195, loss = 0.58328531\n",
            "Iteration 196, loss = 0.58298410\n",
            "Iteration 197, loss = 0.58267924\n",
            "Iteration 198, loss = 0.58238710\n",
            "Iteration 199, loss = 0.58207347\n",
            "Iteration 200, loss = 0.58177106\n",
            "Iteration 1, loss = 1.36378915\n",
            "Iteration 2, loss = 1.34879364\n",
            "Iteration 3, loss = 1.32549217\n",
            "Iteration 4, loss = 1.29708034\n",
            "Iteration 5, loss = 1.26554033\n",
            "Iteration 6, loss = 1.23383872\n",
            "Iteration 7, loss = 1.20200469\n",
            "Iteration 8, loss = 1.17053414\n",
            "Iteration 9, loss = 1.14064775\n",
            "Iteration 10, loss = 1.11151975\n",
            "Iteration 11, loss = 1.08406253\n",
            "Iteration 12, loss = 1.05814511\n",
            "Iteration 13, loss = 1.03354346\n",
            "Iteration 14, loss = 1.01070814\n",
            "Iteration 15, loss = 0.98890098\n",
            "Iteration 16, loss = 0.96886449\n",
            "Iteration 17, loss = 0.94923104\n",
            "Iteration 18, loss = 0.93151074\n",
            "Iteration 19, loss = 0.91502500\n",
            "Iteration 20, loss = 0.89945880\n",
            "Iteration 21, loss = 0.88472298\n",
            "Iteration 22, loss = 0.87121456\n",
            "Iteration 23, loss = 0.85846639\n",
            "Iteration 24, loss = 0.84606854\n",
            "Iteration 25, loss = 0.83470170\n",
            "Iteration 26, loss = 0.82388112\n",
            "Iteration 27, loss = 0.81424012\n",
            "Iteration 28, loss = 0.80487305\n",
            "Iteration 29, loss = 0.79605592\n",
            "Iteration 30, loss = 0.78816723\n",
            "Iteration 31, loss = 0.78027235\n",
            "Iteration 32, loss = 0.77329705\n",
            "Iteration 33, loss = 0.76630727\n",
            "Iteration 34, loss = 0.75998863\n",
            "Iteration 35, loss = 0.75396852\n",
            "Iteration 36, loss = 0.74833678\n",
            "Iteration 37, loss = 0.74285837\n",
            "Iteration 38, loss = 0.73803442\n",
            "Iteration 39, loss = 0.73341573\n",
            "Iteration 40, loss = 0.72891424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 41, loss = 0.72504769\n",
            "Iteration 42, loss = 0.72109686\n",
            "Iteration 43, loss = 0.71757315\n",
            "Iteration 44, loss = 0.71400332\n",
            "Iteration 45, loss = 0.71081284\n",
            "Iteration 46, loss = 0.70768563\n",
            "Iteration 47, loss = 0.70471192\n",
            "Iteration 48, loss = 0.70194281\n",
            "Iteration 49, loss = 0.69925315\n",
            "Iteration 50, loss = 0.69669744\n",
            "Iteration 51, loss = 0.69424636\n",
            "Iteration 52, loss = 0.69189830\n",
            "Iteration 53, loss = 0.68973931\n",
            "Iteration 54, loss = 0.68760345\n",
            "Iteration 55, loss = 0.68543024\n",
            "Iteration 56, loss = 0.68342348\n",
            "Iteration 57, loss = 0.68147262\n",
            "Iteration 58, loss = 0.67964124\n",
            "Iteration 59, loss = 0.67787307\n",
            "Iteration 60, loss = 0.67621673\n",
            "Iteration 61, loss = 0.67462365\n",
            "Iteration 62, loss = 0.67309221\n",
            "Iteration 63, loss = 0.67150391\n",
            "Iteration 64, loss = 0.67007641\n",
            "Iteration 65, loss = 0.66858230\n",
            "Iteration 66, loss = 0.66714332\n",
            "Iteration 67, loss = 0.66581116\n",
            "Iteration 68, loss = 0.66446415\n",
            "Iteration 69, loss = 0.66313131\n",
            "Iteration 70, loss = 0.66182390\n",
            "Iteration 71, loss = 0.66046432\n",
            "Iteration 72, loss = 0.65919492\n",
            "Iteration 73, loss = 0.65790677\n",
            "Iteration 74, loss = 0.65671474\n",
            "Iteration 75, loss = 0.65551452\n",
            "Iteration 76, loss = 0.65432494\n",
            "Iteration 77, loss = 0.65319837\n",
            "Iteration 78, loss = 0.65212463\n",
            "Iteration 79, loss = 0.65105388\n",
            "Iteration 80, loss = 0.65003300\n",
            "Iteration 81, loss = 0.64903759\n",
            "Iteration 82, loss = 0.64809585\n",
            "Iteration 83, loss = 0.64717154\n",
            "Iteration 84, loss = 0.64626768\n",
            "Iteration 85, loss = 0.64537314\n",
            "Iteration 86, loss = 0.64449899\n",
            "Iteration 87, loss = 0.64359621\n",
            "Iteration 88, loss = 0.64275708\n",
            "Iteration 89, loss = 0.64191812\n",
            "Iteration 90, loss = 0.64113195\n",
            "Iteration 91, loss = 0.64030779\n",
            "Iteration 92, loss = 0.63949773\n",
            "Iteration 93, loss = 0.63875682\n",
            "Iteration 94, loss = 0.63795612\n",
            "Iteration 95, loss = 0.63717242\n",
            "Iteration 96, loss = 0.63645109\n",
            "Iteration 97, loss = 0.63570073\n",
            "Iteration 98, loss = 0.63498115\n",
            "Iteration 99, loss = 0.63421861\n",
            "Iteration 100, loss = 0.63348999\n",
            "Iteration 101, loss = 0.63275874\n",
            "Iteration 102, loss = 0.63202608\n",
            "Iteration 103, loss = 0.63134886\n",
            "Iteration 104, loss = 0.63066372\n",
            "Iteration 105, loss = 0.63001472\n",
            "Iteration 106, loss = 0.62930887\n",
            "Iteration 107, loss = 0.62867911\n",
            "Iteration 108, loss = 0.62807512\n",
            "Iteration 109, loss = 0.62747870\n",
            "Iteration 110, loss = 0.62685895\n",
            "Iteration 111, loss = 0.62630162\n",
            "Iteration 112, loss = 0.62570594\n",
            "Iteration 113, loss = 0.62516088\n",
            "Iteration 114, loss = 0.62457410\n",
            "Iteration 115, loss = 0.62400773\n",
            "Iteration 116, loss = 0.62342274\n",
            "Iteration 117, loss = 0.62285866\n",
            "Iteration 118, loss = 0.62229865\n",
            "Iteration 119, loss = 0.62175565\n",
            "Iteration 120, loss = 0.62119369\n",
            "Iteration 121, loss = 0.62065287\n",
            "Iteration 122, loss = 0.62014015\n",
            "Iteration 123, loss = 0.61962987\n",
            "Iteration 124, loss = 0.61911487\n",
            "Iteration 125, loss = 0.61863745\n",
            "Iteration 126, loss = 0.61813522\n",
            "Iteration 127, loss = 0.61768179\n",
            "Iteration 128, loss = 0.61719308\n",
            "Iteration 129, loss = 0.61668202\n",
            "Iteration 130, loss = 0.61622575\n",
            "Iteration 131, loss = 0.61574837\n",
            "Iteration 132, loss = 0.61529571\n",
            "Iteration 133, loss = 0.61482482\n",
            "Iteration 134, loss = 0.61439165\n",
            "Iteration 135, loss = 0.61393492\n",
            "Iteration 136, loss = 0.61349984\n",
            "Iteration 137, loss = 0.61306190\n",
            "Iteration 138, loss = 0.61265032\n",
            "Iteration 139, loss = 0.61218768\n",
            "Iteration 140, loss = 0.61176824\n",
            "Iteration 141, loss = 0.61134064\n",
            "Iteration 142, loss = 0.61094127\n",
            "Iteration 143, loss = 0.61051579\n",
            "Iteration 144, loss = 0.61011162\n",
            "Iteration 145, loss = 0.60970606\n",
            "Iteration 146, loss = 0.60928174\n",
            "Iteration 147, loss = 0.60888100\n",
            "Iteration 148, loss = 0.60846789\n",
            "Iteration 149, loss = 0.60803492\n",
            "Iteration 150, loss = 0.60762284\n",
            "Iteration 151, loss = 0.60719954\n",
            "Iteration 152, loss = 0.60679411\n",
            "Iteration 153, loss = 0.60641267\n",
            "Iteration 154, loss = 0.60602227\n",
            "Iteration 155, loss = 0.60563330\n",
            "Iteration 156, loss = 0.60528969\n",
            "Iteration 157, loss = 0.60490894\n",
            "Iteration 158, loss = 0.60454348\n",
            "Iteration 159, loss = 0.60415489\n",
            "Iteration 160, loss = 0.60378039\n",
            "Iteration 161, loss = 0.60341914\n",
            "Iteration 162, loss = 0.60303635\n",
            "Iteration 163, loss = 0.60269236\n",
            "Iteration 164, loss = 0.60233687\n",
            "Iteration 165, loss = 0.60200957\n",
            "Iteration 166, loss = 0.60165254\n",
            "Iteration 167, loss = 0.60130815\n",
            "Iteration 168, loss = 0.60094719\n",
            "Iteration 169, loss = 0.60061875\n",
            "Iteration 170, loss = 0.60026172\n",
            "Iteration 171, loss = 0.59990238\n",
            "Iteration 172, loss = 0.59953859\n",
            "Iteration 173, loss = 0.59919669\n",
            "Iteration 174, loss = 0.59885553\n",
            "Iteration 175, loss = 0.59853789\n",
            "Iteration 176, loss = 0.59818744\n",
            "Iteration 177, loss = 0.59784538\n",
            "Iteration 178, loss = 0.59752370\n",
            "Iteration 179, loss = 0.59716771\n",
            "Iteration 180, loss = 0.59684887\n",
            "Iteration 181, loss = 0.59653215\n",
            "Iteration 182, loss = 0.59620998\n",
            "Iteration 183, loss = 0.59588931\n",
            "Iteration 184, loss = 0.59559595\n",
            "Iteration 185, loss = 0.59524715\n",
            "Iteration 186, loss = 0.59491951\n",
            "Iteration 187, loss = 0.59461223\n",
            "Iteration 188, loss = 0.59429729\n",
            "Iteration 189, loss = 0.59399093\n",
            "Iteration 190, loss = 0.59370681\n",
            "Iteration 191, loss = 0.59340244\n",
            "Iteration 192, loss = 0.59311298\n",
            "Iteration 193, loss = 0.59284117\n",
            "Iteration 194, loss = 0.59253337\n",
            "Iteration 195, loss = 0.59224873\n",
            "Iteration 196, loss = 0.59194932\n",
            "Iteration 197, loss = 0.59164598\n",
            "Iteration 198, loss = 0.59134717\n",
            "Iteration 199, loss = 0.59103068\n",
            "Iteration 200, loss = 0.59075643\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 200 and for layer number 6 : 0.7074999999999999\n",
            "Iteration 1, loss = 0.61693719\n",
            "Iteration 2, loss = 0.61686447\n",
            "Iteration 3, loss = 0.61675151\n",
            "Iteration 4, loss = 0.61661842\n",
            "Iteration 5, loss = 0.61646426\n",
            "Iteration 6, loss = 0.61631571\n",
            "Iteration 7, loss = 0.61614581\n",
            "Iteration 8, loss = 0.61596065\n",
            "Iteration 9, loss = 0.61578398\n",
            "Iteration 10, loss = 0.61560733\n",
            "Iteration 11, loss = 0.61543375\n",
            "Iteration 12, loss = 0.61527975\n",
            "Iteration 13, loss = 0.61511094\n",
            "Iteration 14, loss = 0.61495375\n",
            "Iteration 15, loss = 0.61478984\n",
            "Iteration 16, loss = 0.61463306\n",
            "Iteration 17, loss = 0.61447873\n",
            "Iteration 18, loss = 0.61432621\n",
            "Iteration 19, loss = 0.61417436\n",
            "Iteration 20, loss = 0.61400728\n",
            "Iteration 21, loss = 0.61383297\n",
            "Iteration 22, loss = 0.61367462\n",
            "Iteration 23, loss = 0.61350652\n",
            "Iteration 24, loss = 0.61335359\n",
            "Iteration 25, loss = 0.61319555\n",
            "Iteration 26, loss = 0.61306514\n",
            "Iteration 27, loss = 0.61290532\n",
            "Iteration 28, loss = 0.61277722\n",
            "Iteration 29, loss = 0.61265346\n",
            "Iteration 30, loss = 0.61251235\n",
            "Iteration 31, loss = 0.61238930\n",
            "Iteration 32, loss = 0.61226155\n",
            "Iteration 33, loss = 0.61213591\n",
            "Iteration 34, loss = 0.61202127\n",
            "Iteration 35, loss = 0.61189676\n",
            "Iteration 36, loss = 0.61176314\n",
            "Iteration 37, loss = 0.61162506\n",
            "Iteration 38, loss = 0.61149544\n",
            "Iteration 39, loss = 0.61137006\n",
            "Iteration 40, loss = 0.61124508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 41, loss = 0.61110902\n",
            "Iteration 42, loss = 0.61099032\n",
            "Iteration 43, loss = 0.61085784\n",
            "Iteration 44, loss = 0.61073928\n",
            "Iteration 45, loss = 0.61062182\n",
            "Iteration 46, loss = 0.61048916\n",
            "Iteration 47, loss = 0.61036991\n",
            "Iteration 48, loss = 0.61024277\n",
            "Iteration 49, loss = 0.61013546\n",
            "Iteration 50, loss = 0.61002089\n",
            "Iteration 51, loss = 0.60990687\n",
            "Iteration 52, loss = 0.60979970\n",
            "Iteration 53, loss = 0.60969865\n",
            "Iteration 54, loss = 0.60959045\n",
            "Iteration 55, loss = 0.60948292\n",
            "Iteration 56, loss = 0.60937738\n",
            "Iteration 57, loss = 0.60926927\n",
            "Iteration 58, loss = 0.60917448\n",
            "Iteration 59, loss = 0.60906914\n",
            "Iteration 60, loss = 0.60896820\n",
            "Iteration 61, loss = 0.60886422\n",
            "Iteration 62, loss = 0.60877775\n",
            "Iteration 63, loss = 0.60867478\n",
            "Iteration 64, loss = 0.60857955\n",
            "Iteration 65, loss = 0.60848439\n",
            "Iteration 66, loss = 0.60838570\n",
            "Iteration 67, loss = 0.60829814\n",
            "Iteration 68, loss = 0.60820577\n",
            "Iteration 69, loss = 0.60812774\n",
            "Iteration 70, loss = 0.60803664\n",
            "Iteration 71, loss = 0.60794173\n",
            "Iteration 72, loss = 0.60785282\n",
            "Iteration 73, loss = 0.60776810\n",
            "Iteration 74, loss = 0.60766500\n",
            "Iteration 75, loss = 0.60758161\n",
            "Iteration 76, loss = 0.60748356\n",
            "Iteration 77, loss = 0.60739894\n",
            "Iteration 78, loss = 0.60731203\n",
            "Iteration 79, loss = 0.60722005\n",
            "Iteration 80, loss = 0.60712968\n",
            "Iteration 81, loss = 0.60704893\n",
            "Iteration 82, loss = 0.60696141\n",
            "Iteration 83, loss = 0.60688701\n",
            "Iteration 84, loss = 0.60679662\n",
            "Iteration 85, loss = 0.60670958\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61771206\n",
            "Iteration 2, loss = 0.61763256\n",
            "Iteration 3, loss = 0.61750896\n",
            "Iteration 4, loss = 0.61736013\n",
            "Iteration 5, loss = 0.61720601\n",
            "Iteration 6, loss = 0.61703925\n",
            "Iteration 7, loss = 0.61687983\n",
            "Iteration 8, loss = 0.61668458\n",
            "Iteration 9, loss = 0.61650897\n",
            "Iteration 10, loss = 0.61634191\n",
            "Iteration 11, loss = 0.61616438\n",
            "Iteration 12, loss = 0.61600889\n",
            "Iteration 13, loss = 0.61582381\n",
            "Iteration 14, loss = 0.61566827\n",
            "Iteration 15, loss = 0.61549674\n",
            "Iteration 16, loss = 0.61534090\n",
            "Iteration 17, loss = 0.61517681\n",
            "Iteration 18, loss = 0.61502298\n",
            "Iteration 19, loss = 0.61486598\n",
            "Iteration 20, loss = 0.61470307\n",
            "Iteration 21, loss = 0.61454151\n",
            "Iteration 22, loss = 0.61438853\n",
            "Iteration 23, loss = 0.61422513\n",
            "Iteration 24, loss = 0.61407653\n",
            "Iteration 25, loss = 0.61391791\n",
            "Iteration 26, loss = 0.61377394\n",
            "Iteration 27, loss = 0.61362218\n",
            "Iteration 28, loss = 0.61348831\n",
            "Iteration 29, loss = 0.61334584\n",
            "Iteration 30, loss = 0.61321506\n",
            "Iteration 31, loss = 0.61309244\n",
            "Iteration 32, loss = 0.61295282\n",
            "Iteration 33, loss = 0.61282899\n",
            "Iteration 34, loss = 0.61269835\n",
            "Iteration 35, loss = 0.61258404\n",
            "Iteration 36, loss = 0.61244344\n",
            "Iteration 37, loss = 0.61230626\n",
            "Iteration 38, loss = 0.61216372\n",
            "Iteration 39, loss = 0.61203699\n",
            "Iteration 40, loss = 0.61189655\n",
            "Iteration 41, loss = 0.61175462\n",
            "Iteration 42, loss = 0.61161427\n",
            "Iteration 43, loss = 0.61148385\n",
            "Iteration 44, loss = 0.61135062\n",
            "Iteration 45, loss = 0.61122322\n",
            "Iteration 46, loss = 0.61108342\n",
            "Iteration 47, loss = 0.61094533\n",
            "Iteration 48, loss = 0.61081445\n",
            "Iteration 49, loss = 0.61068681\n",
            "Iteration 50, loss = 0.61056174\n",
            "Iteration 51, loss = 0.61044813\n",
            "Iteration 52, loss = 0.61033493\n",
            "Iteration 53, loss = 0.61021829\n",
            "Iteration 54, loss = 0.61010919\n",
            "Iteration 55, loss = 0.61000586\n",
            "Iteration 56, loss = 0.60988739\n",
            "Iteration 57, loss = 0.60976884\n",
            "Iteration 58, loss = 0.60966597\n",
            "Iteration 59, loss = 0.60955587\n",
            "Iteration 60, loss = 0.60944274\n",
            "Iteration 61, loss = 0.60932539\n",
            "Iteration 62, loss = 0.60923146\n",
            "Iteration 63, loss = 0.60909925\n",
            "Iteration 64, loss = 0.60898774\n",
            "Iteration 65, loss = 0.60887739\n",
            "Iteration 66, loss = 0.60875534\n",
            "Iteration 67, loss = 0.60864213\n",
            "Iteration 68, loss = 0.60852801\n",
            "Iteration 69, loss = 0.60842906\n",
            "Iteration 70, loss = 0.60832290\n",
            "Iteration 71, loss = 0.60821626\n",
            "Iteration 72, loss = 0.60811197\n",
            "Iteration 73, loss = 0.60800907\n",
            "Iteration 74, loss = 0.60789491\n",
            "Iteration 75, loss = 0.60780669\n",
            "Iteration 76, loss = 0.60769204\n",
            "Iteration 77, loss = 0.60759716\n",
            "Iteration 78, loss = 0.60749301\n",
            "Iteration 79, loss = 0.60738373\n",
            "Iteration 80, loss = 0.60728635\n",
            "Iteration 81, loss = 0.60719595\n",
            "Iteration 82, loss = 0.60709942\n",
            "Iteration 83, loss = 0.60701638\n",
            "Iteration 84, loss = 0.60692635\n",
            "Iteration 85, loss = 0.60683945\n",
            "Iteration 86, loss = 0.60675609\n",
            "Iteration 87, loss = 0.60667504\n",
            "Iteration 88, loss = 0.60659137\n",
            "Iteration 89, loss = 0.60651963\n",
            "Iteration 90, loss = 0.60642717\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.61763916\n",
            "Iteration 2, loss = 0.61755997\n",
            "Iteration 3, loss = 0.61742232\n",
            "Iteration 4, loss = 0.61725393\n",
            "Iteration 5, loss = 0.61708619\n",
            "Iteration 6, loss = 0.61689799\n",
            "Iteration 7, loss = 0.61671618\n",
            "Iteration 8, loss = 0.61650164\n",
            "Iteration 9, loss = 0.61630973\n",
            "Iteration 10, loss = 0.61611724\n",
            "Iteration 11, loss = 0.61593663\n",
            "Iteration 12, loss = 0.61576363\n",
            "Iteration 13, loss = 0.61556226\n",
            "Iteration 14, loss = 0.61538666\n",
            "Iteration 15, loss = 0.61520889\n",
            "Iteration 16, loss = 0.61503323\n",
            "Iteration 17, loss = 0.61486521\n",
            "Iteration 18, loss = 0.61470006\n",
            "Iteration 19, loss = 0.61453828\n",
            "Iteration 20, loss = 0.61437299\n",
            "Iteration 21, loss = 0.61421653\n",
            "Iteration 22, loss = 0.61405569\n",
            "Iteration 23, loss = 0.61388640\n",
            "Iteration 24, loss = 0.61373448\n",
            "Iteration 25, loss = 0.61357646\n",
            "Iteration 26, loss = 0.61342231\n",
            "Iteration 27, loss = 0.61327118\n",
            "Iteration 28, loss = 0.61311335\n",
            "Iteration 29, loss = 0.61297079\n",
            "Iteration 30, loss = 0.61282693\n",
            "Iteration 31, loss = 0.61269890\n",
            "Iteration 32, loss = 0.61255474\n",
            "Iteration 33, loss = 0.61241223\n",
            "Iteration 34, loss = 0.61226940\n",
            "Iteration 35, loss = 0.61213160\n",
            "Iteration 36, loss = 0.61198811\n",
            "Iteration 37, loss = 0.61184581\n",
            "Iteration 38, loss = 0.61170049\n",
            "Iteration 39, loss = 0.61157611\n",
            "Iteration 40, loss = 0.61142730\n",
            "Iteration 41, loss = 0.61127667\n",
            "Iteration 42, loss = 0.61113732\n",
            "Iteration 43, loss = 0.61100411\n",
            "Iteration 44, loss = 0.61086913\n",
            "Iteration 45, loss = 0.61073484\n",
            "Iteration 46, loss = 0.61060145\n",
            "Iteration 47, loss = 0.61046314\n",
            "Iteration 48, loss = 0.61033730\n",
            "Iteration 49, loss = 0.61020822\n",
            "Iteration 50, loss = 0.61007740\n",
            "Iteration 51, loss = 0.60996338\n",
            "Iteration 52, loss = 0.60985178\n",
            "Iteration 53, loss = 0.60973070\n",
            "Iteration 54, loss = 0.60961527\n",
            "Iteration 55, loss = 0.60949856\n",
            "Iteration 56, loss = 0.60938041\n",
            "Iteration 57, loss = 0.60926013\n",
            "Iteration 58, loss = 0.60914580\n",
            "Iteration 59, loss = 0.60902755\n",
            "Iteration 60, loss = 0.60890665\n",
            "Iteration 61, loss = 0.60878651\n",
            "Iteration 62, loss = 0.60867591\n",
            "Iteration 63, loss = 0.60854401\n",
            "Iteration 64, loss = 0.60842057\n",
            "Iteration 65, loss = 0.60829508\n",
            "Iteration 66, loss = 0.60817432\n",
            "Iteration 67, loss = 0.60805562\n",
            "Iteration 68, loss = 0.60794420\n",
            "Iteration 69, loss = 0.60783815\n",
            "Iteration 70, loss = 0.60773163\n",
            "Iteration 71, loss = 0.60763356\n",
            "Iteration 72, loss = 0.60751712\n",
            "Iteration 73, loss = 0.60740617\n",
            "Iteration 74, loss = 0.60728371\n",
            "Iteration 75, loss = 0.60719874\n",
            "Iteration 76, loss = 0.60707233\n",
            "Iteration 77, loss = 0.60697847\n",
            "Iteration 78, loss = 0.60686966\n",
            "Iteration 79, loss = 0.60676477\n",
            "Iteration 80, loss = 0.60666713\n",
            "Iteration 81, loss = 0.60657029\n",
            "Iteration 82, loss = 0.60647206\n",
            "Iteration 83, loss = 0.60638523\n",
            "Iteration 84, loss = 0.60629166\n",
            "Iteration 85, loss = 0.60620042\n",
            "Iteration 86, loss = 0.60612214\n",
            "Iteration 87, loss = 0.60604223\n",
            "Iteration 88, loss = 0.60596838\n",
            "Iteration 89, loss = 0.60589523\n",
            "Iteration 90, loss = 0.60580521\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62088357\n",
            "Iteration 2, loss = 0.62080758\n",
            "Iteration 3, loss = 0.62069495\n",
            "Iteration 4, loss = 0.62054238\n",
            "Iteration 5, loss = 0.62038891\n",
            "Iteration 6, loss = 0.62021289\n",
            "Iteration 7, loss = 0.62003518\n",
            "Iteration 8, loss = 0.61983724\n",
            "Iteration 9, loss = 0.61965125\n",
            "Iteration 10, loss = 0.61946574\n",
            "Iteration 11, loss = 0.61928678\n",
            "Iteration 12, loss = 0.61912370\n",
            "Iteration 13, loss = 0.61894901\n",
            "Iteration 14, loss = 0.61877305\n",
            "Iteration 15, loss = 0.61860692\n",
            "Iteration 16, loss = 0.61844169\n",
            "Iteration 17, loss = 0.61826895\n",
            "Iteration 18, loss = 0.61810540\n",
            "Iteration 19, loss = 0.61794667\n",
            "Iteration 20, loss = 0.61779409\n",
            "Iteration 21, loss = 0.61765462\n",
            "Iteration 22, loss = 0.61749969\n",
            "Iteration 23, loss = 0.61734374\n",
            "Iteration 24, loss = 0.61718970\n",
            "Iteration 25, loss = 0.61702731\n",
            "Iteration 26, loss = 0.61686454\n",
            "Iteration 27, loss = 0.61672331\n",
            "Iteration 28, loss = 0.61655046\n",
            "Iteration 29, loss = 0.61640417\n",
            "Iteration 30, loss = 0.61625657\n",
            "Iteration 31, loss = 0.61612342\n",
            "Iteration 32, loss = 0.61597996\n",
            "Iteration 33, loss = 0.61583342\n",
            "Iteration 34, loss = 0.61569987\n",
            "Iteration 35, loss = 0.61556063\n",
            "Iteration 36, loss = 0.61542958\n",
            "Iteration 37, loss = 0.61530817\n",
            "Iteration 38, loss = 0.61517959\n",
            "Iteration 39, loss = 0.61506558\n",
            "Iteration 40, loss = 0.61494687\n",
            "Iteration 41, loss = 0.61483037\n",
            "Iteration 42, loss = 0.61470883\n",
            "Iteration 43, loss = 0.61460197\n",
            "Iteration 44, loss = 0.61447871\n",
            "Iteration 45, loss = 0.61436047\n",
            "Iteration 46, loss = 0.61425494\n",
            "Iteration 47, loss = 0.61414539\n",
            "Iteration 48, loss = 0.61403406\n",
            "Iteration 49, loss = 0.61392290\n",
            "Iteration 50, loss = 0.61380453\n",
            "Iteration 51, loss = 0.61371073\n",
            "Iteration 52, loss = 0.61360963\n",
            "Iteration 53, loss = 0.61350809\n",
            "Iteration 54, loss = 0.61341021\n",
            "Iteration 55, loss = 0.61331563\n",
            "Iteration 56, loss = 0.61322032\n",
            "Iteration 57, loss = 0.61312362\n",
            "Iteration 58, loss = 0.61303187\n",
            "Iteration 59, loss = 0.61293793\n",
            "Iteration 60, loss = 0.61284660\n",
            "Iteration 61, loss = 0.61275442\n",
            "Iteration 62, loss = 0.61266764\n",
            "Iteration 63, loss = 0.61256719\n",
            "Iteration 64, loss = 0.61246187\n",
            "Iteration 65, loss = 0.61235746\n",
            "Iteration 66, loss = 0.61225596\n",
            "Iteration 67, loss = 0.61215694\n",
            "Iteration 68, loss = 0.61206416\n",
            "Iteration 69, loss = 0.61197032\n",
            "Iteration 70, loss = 0.61187100\n",
            "Iteration 71, loss = 0.61178403\n",
            "Iteration 72, loss = 0.61167796\n",
            "Iteration 73, loss = 0.61157994\n",
            "Iteration 74, loss = 0.61147541\n",
            "Iteration 75, loss = 0.61140259\n",
            "Iteration 76, loss = 0.61129541\n",
            "Iteration 77, loss = 0.61120835\n",
            "Iteration 78, loss = 0.61111527\n",
            "Iteration 79, loss = 0.61103216\n",
            "Iteration 80, loss = 0.61095465\n",
            "Iteration 81, loss = 0.61087089\n",
            "Iteration 82, loss = 0.61079131\n",
            "Iteration 83, loss = 0.61071503\n",
            "Iteration 84, loss = 0.61063483\n",
            "Iteration 85, loss = 0.61055422\n",
            "Iteration 86, loss = 0.61048168\n",
            "Iteration 87, loss = 0.61040607\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 0.62018025\n",
            "Iteration 2, loss = 0.62009819\n",
            "Iteration 3, loss = 0.61998317\n",
            "Iteration 4, loss = 0.61981666\n",
            "Iteration 5, loss = 0.61963710\n",
            "Iteration 6, loss = 0.61944478\n",
            "Iteration 7, loss = 0.61925290\n",
            "Iteration 8, loss = 0.61903845\n",
            "Iteration 9, loss = 0.61885557\n",
            "Iteration 10, loss = 0.61866268\n",
            "Iteration 11, loss = 0.61848604\n",
            "Iteration 12, loss = 0.61833134\n",
            "Iteration 13, loss = 0.61815251\n",
            "Iteration 14, loss = 0.61796856\n",
            "Iteration 15, loss = 0.61780050\n",
            "Iteration 16, loss = 0.61763388\n",
            "Iteration 17, loss = 0.61745805\n",
            "Iteration 18, loss = 0.61727872\n",
            "Iteration 19, loss = 0.61709604\n",
            "Iteration 20, loss = 0.61693584\n",
            "Iteration 21, loss = 0.61678432\n",
            "Iteration 22, loss = 0.61660435\n",
            "Iteration 23, loss = 0.61645117\n",
            "Iteration 24, loss = 0.61629760\n",
            "Iteration 25, loss = 0.61613359\n",
            "Iteration 26, loss = 0.61598174\n",
            "Iteration 27, loss = 0.61583842\n",
            "Iteration 28, loss = 0.61567659\n",
            "Iteration 29, loss = 0.61552571\n",
            "Iteration 30, loss = 0.61538139\n",
            "Iteration 31, loss = 0.61524275\n",
            "Iteration 32, loss = 0.61509965\n",
            "Iteration 33, loss = 0.61494620\n",
            "Iteration 34, loss = 0.61481205\n",
            "Iteration 35, loss = 0.61466606\n",
            "Iteration 36, loss = 0.61454011\n",
            "Iteration 37, loss = 0.61442324\n",
            "Iteration 38, loss = 0.61429471\n",
            "Iteration 39, loss = 0.61417885\n",
            "Iteration 40, loss = 0.61406637\n",
            "Iteration 41, loss = 0.61394071\n",
            "Iteration 42, loss = 0.61382750\n",
            "Iteration 43, loss = 0.61370365\n",
            "Iteration 44, loss = 0.61359746\n",
            "Iteration 45, loss = 0.61346493\n",
            "Iteration 46, loss = 0.61335820\n",
            "Iteration 47, loss = 0.61324493\n",
            "Iteration 48, loss = 0.61312768\n",
            "Iteration 49, loss = 0.61300162\n",
            "Iteration 50, loss = 0.61287863\n",
            "Iteration 51, loss = 0.61278578\n",
            "Iteration 52, loss = 0.61266496\n",
            "Iteration 53, loss = 0.61256068\n",
            "Iteration 54, loss = 0.61244833\n",
            "Iteration 55, loss = 0.61235408\n",
            "Iteration 56, loss = 0.61225595\n",
            "Iteration 57, loss = 0.61215760\n",
            "Iteration 58, loss = 0.61206425\n",
            "Iteration 59, loss = 0.61197433\n",
            "Iteration 60, loss = 0.61188279\n",
            "Iteration 61, loss = 0.61179557\n",
            "Iteration 62, loss = 0.61170404\n",
            "Iteration 63, loss = 0.61161334\n",
            "Iteration 64, loss = 0.61151561\n",
            "Iteration 65, loss = 0.61141306\n",
            "Iteration 66, loss = 0.61130998\n",
            "Iteration 67, loss = 0.61122290\n",
            "Iteration 68, loss = 0.61112557\n",
            "Iteration 69, loss = 0.61103638\n",
            "Iteration 70, loss = 0.61095068\n",
            "Iteration 71, loss = 0.61087186\n",
            "Iteration 72, loss = 0.61077796\n",
            "Iteration 73, loss = 0.61069137\n",
            "Iteration 74, loss = 0.61060678\n",
            "Iteration 75, loss = 0.61052912\n",
            "Iteration 76, loss = 0.61045012\n",
            "Iteration 77, loss = 0.61037871\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 250 and for layer number 2 : 0.7\n",
            "Iteration 1, loss = 0.84801653\n",
            "Iteration 2, loss = 0.84528764\n",
            "Iteration 3, loss = 0.84117560\n",
            "Iteration 4, loss = 0.83582942\n",
            "Iteration 5, loss = 0.82975974\n",
            "Iteration 6, loss = 0.82353252\n",
            "Iteration 7, loss = 0.81664243\n",
            "Iteration 8, loss = 0.81002340\n",
            "Iteration 9, loss = 0.80368440\n",
            "Iteration 10, loss = 0.79741786\n",
            "Iteration 11, loss = 0.79116889\n",
            "Iteration 12, loss = 0.78479499\n",
            "Iteration 13, loss = 0.77897234\n",
            "Iteration 14, loss = 0.77276067\n",
            "Iteration 15, loss = 0.76728918\n",
            "Iteration 16, loss = 0.76206401\n",
            "Iteration 17, loss = 0.75702104\n",
            "Iteration 18, loss = 0.75199301\n",
            "Iteration 19, loss = 0.74737027\n",
            "Iteration 20, loss = 0.74276500\n",
            "Iteration 21, loss = 0.73837289\n",
            "Iteration 22, loss = 0.73407699\n",
            "Iteration 23, loss = 0.73007924\n",
            "Iteration 24, loss = 0.72625222\n",
            "Iteration 25, loss = 0.72244250\n",
            "Iteration 26, loss = 0.71862878\n",
            "Iteration 27, loss = 0.71502063\n",
            "Iteration 28, loss = 0.71165062\n",
            "Iteration 29, loss = 0.70825508\n",
            "Iteration 30, loss = 0.70488572\n",
            "Iteration 31, loss = 0.70184174\n",
            "Iteration 32, loss = 0.69891904\n",
            "Iteration 33, loss = 0.69611030\n",
            "Iteration 34, loss = 0.69321969\n",
            "Iteration 35, loss = 0.69047473\n",
            "Iteration 36, loss = 0.68810891\n",
            "Iteration 37, loss = 0.68540721\n",
            "Iteration 38, loss = 0.68303292\n",
            "Iteration 39, loss = 0.68048045\n",
            "Iteration 40, loss = 0.67779155\n",
            "Iteration 41, loss = 0.67527131\n",
            "Iteration 42, loss = 0.67280099\n",
            "Iteration 43, loss = 0.67046862\n",
            "Iteration 44, loss = 0.66827526\n",
            "Iteration 45, loss = 0.66614495\n",
            "Iteration 46, loss = 0.66422713\n",
            "Iteration 47, loss = 0.66229631\n",
            "Iteration 48, loss = 0.66059117\n",
            "Iteration 49, loss = 0.65875777\n",
            "Iteration 50, loss = 0.65706317\n",
            "Iteration 51, loss = 0.65538023\n",
            "Iteration 52, loss = 0.65371124\n",
            "Iteration 53, loss = 0.65216698\n",
            "Iteration 54, loss = 0.65065006\n",
            "Iteration 55, loss = 0.64920995\n",
            "Iteration 56, loss = 0.64790729\n",
            "Iteration 57, loss = 0.64652529\n",
            "Iteration 58, loss = 0.64516241\n",
            "Iteration 59, loss = 0.64389573\n",
            "Iteration 60, loss = 0.64264924\n",
            "Iteration 61, loss = 0.64145436\n",
            "Iteration 62, loss = 0.64022518\n",
            "Iteration 63, loss = 0.63902072\n",
            "Iteration 64, loss = 0.63780270\n",
            "Iteration 65, loss = 0.63673321\n",
            "Iteration 66, loss = 0.63556199\n",
            "Iteration 67, loss = 0.63443060\n",
            "Iteration 68, loss = 0.63346065\n",
            "Iteration 69, loss = 0.63238086\n",
            "Iteration 70, loss = 0.63142079\n",
            "Iteration 71, loss = 0.63054257\n",
            "Iteration 72, loss = 0.62958069\n",
            "Iteration 73, loss = 0.62866353\n",
            "Iteration 74, loss = 0.62786278\n",
            "Iteration 75, loss = 0.62707678\n",
            "Iteration 76, loss = 0.62631715\n",
            "Iteration 77, loss = 0.62554014\n",
            "Iteration 78, loss = 0.62477088\n",
            "Iteration 79, loss = 0.62400197\n",
            "Iteration 80, loss = 0.62327855\n",
            "Iteration 81, loss = 0.62255956\n",
            "Iteration 82, loss = 0.62183722\n",
            "Iteration 83, loss = 0.62108874\n",
            "Iteration 84, loss = 0.62039759\n",
            "Iteration 85, loss = 0.61967253\n",
            "Iteration 86, loss = 0.61896942\n",
            "Iteration 87, loss = 0.61829945\n",
            "Iteration 88, loss = 0.61761280\n",
            "Iteration 89, loss = 0.61697162\n",
            "Iteration 90, loss = 0.61630797\n",
            "Iteration 91, loss = 0.61566924\n",
            "Iteration 92, loss = 0.61500381\n",
            "Iteration 93, loss = 0.61436935\n",
            "Iteration 94, loss = 0.61373605\n",
            "Iteration 95, loss = 0.61307396\n",
            "Iteration 96, loss = 0.61248474\n",
            "Iteration 97, loss = 0.61183704\n",
            "Iteration 98, loss = 0.61128115\n",
            "Iteration 99, loss = 0.61063105\n",
            "Iteration 100, loss = 0.61004000\n",
            "Iteration 101, loss = 0.60947662\n",
            "Iteration 102, loss = 0.60889804\n",
            "Iteration 103, loss = 0.60843826\n",
            "Iteration 104, loss = 0.60783015\n",
            "Iteration 105, loss = 0.60731472\n",
            "Iteration 106, loss = 0.60679356\n",
            "Iteration 107, loss = 0.60628627\n",
            "Iteration 108, loss = 0.60578700\n",
            "Iteration 109, loss = 0.60530442\n",
            "Iteration 110, loss = 0.60483648\n",
            "Iteration 111, loss = 0.60436338\n",
            "Iteration 112, loss = 0.60393270\n",
            "Iteration 113, loss = 0.60352094\n",
            "Iteration 114, loss = 0.60307740\n",
            "Iteration 115, loss = 0.60268764\n",
            "Iteration 116, loss = 0.60225422\n",
            "Iteration 117, loss = 0.60183950\n",
            "Iteration 118, loss = 0.60144200\n",
            "Iteration 119, loss = 0.60106225\n",
            "Iteration 120, loss = 0.60063420\n",
            "Iteration 121, loss = 0.60023915\n",
            "Iteration 122, loss = 0.59983608\n",
            "Iteration 123, loss = 0.59942553\n",
            "Iteration 124, loss = 0.59901972\n",
            "Iteration 125, loss = 0.59862884\n",
            "Iteration 126, loss = 0.59824233\n",
            "Iteration 127, loss = 0.59785970\n",
            "Iteration 128, loss = 0.59750168\n",
            "Iteration 129, loss = 0.59713842\n",
            "Iteration 130, loss = 0.59680408\n",
            "Iteration 131, loss = 0.59648184\n",
            "Iteration 132, loss = 0.59612641\n",
            "Iteration 133, loss = 0.59579540\n",
            "Iteration 134, loss = 0.59545269\n",
            "Iteration 135, loss = 0.59513843\n",
            "Iteration 136, loss = 0.59481077\n",
            "Iteration 137, loss = 0.59450553\n",
            "Iteration 138, loss = 0.59417295\n",
            "Iteration 139, loss = 0.59386987\n",
            "Iteration 140, loss = 0.59352151\n",
            "Iteration 141, loss = 0.59320636\n",
            "Iteration 142, loss = 0.59288570\n",
            "Iteration 143, loss = 0.59254332\n",
            "Iteration 144, loss = 0.59224173\n",
            "Iteration 145, loss = 0.59192463\n",
            "Iteration 146, loss = 0.59160331\n",
            "Iteration 147, loss = 0.59128449\n",
            "Iteration 148, loss = 0.59096323\n",
            "Iteration 149, loss = 0.59063084\n",
            "Iteration 150, loss = 0.59038802\n",
            "Iteration 151, loss = 0.59008751\n",
            "Iteration 152, loss = 0.58981761\n",
            "Iteration 153, loss = 0.58954686\n",
            "Iteration 154, loss = 0.58929396\n",
            "Iteration 155, loss = 0.58905256\n",
            "Iteration 156, loss = 0.58879207\n",
            "Iteration 157, loss = 0.58854333\n",
            "Iteration 158, loss = 0.58827484\n",
            "Iteration 159, loss = 0.58804633\n",
            "Iteration 160, loss = 0.58781168\n",
            "Iteration 161, loss = 0.58759210\n",
            "Iteration 162, loss = 0.58738600\n",
            "Iteration 163, loss = 0.58716915\n",
            "Iteration 164, loss = 0.58696599\n",
            "Iteration 165, loss = 0.58674771\n",
            "Iteration 166, loss = 0.58651408\n",
            "Iteration 167, loss = 0.58629909\n",
            "Iteration 168, loss = 0.58608911\n",
            "Iteration 169, loss = 0.58586344\n",
            "Iteration 170, loss = 0.58567720\n",
            "Iteration 171, loss = 0.58546245\n",
            "Iteration 172, loss = 0.58521621\n",
            "Iteration 173, loss = 0.58502864\n",
            "Iteration 174, loss = 0.58481075\n",
            "Iteration 175, loss = 0.58460457\n",
            "Iteration 176, loss = 0.58439746\n",
            "Iteration 177, loss = 0.58420359\n",
            "Iteration 178, loss = 0.58401735\n",
            "Iteration 179, loss = 0.58380576\n",
            "Iteration 180, loss = 0.58362774\n",
            "Iteration 181, loss = 0.58342924\n",
            "Iteration 182, loss = 0.58323680\n",
            "Iteration 183, loss = 0.58304079\n",
            "Iteration 184, loss = 0.58286730\n",
            "Iteration 185, loss = 0.58269420\n",
            "Iteration 186, loss = 0.58253006\n",
            "Iteration 187, loss = 0.58235621\n",
            "Iteration 188, loss = 0.58218110\n",
            "Iteration 189, loss = 0.58200841\n",
            "Iteration 190, loss = 0.58182101\n",
            "Iteration 191, loss = 0.58163037\n",
            "Iteration 192, loss = 0.58144884\n",
            "Iteration 193, loss = 0.58127192\n",
            "Iteration 194, loss = 0.58110636\n",
            "Iteration 195, loss = 0.58093690\n",
            "Iteration 196, loss = 0.58077830\n",
            "Iteration 197, loss = 0.58063196\n",
            "Iteration 198, loss = 0.58047848\n",
            "Iteration 199, loss = 0.58031425\n",
            "Iteration 200, loss = 0.58016699\n",
            "Iteration 201, loss = 0.58003329\n",
            "Iteration 202, loss = 0.57989638\n",
            "Iteration 203, loss = 0.57974946\n",
            "Iteration 204, loss = 0.57961334\n",
            "Iteration 205, loss = 0.57947876\n",
            "Iteration 206, loss = 0.57934138\n",
            "Iteration 207, loss = 0.57921788\n",
            "Iteration 208, loss = 0.57905828\n",
            "Iteration 209, loss = 0.57890920\n",
            "Iteration 210, loss = 0.57875888\n",
            "Iteration 211, loss = 0.57862064\n",
            "Iteration 212, loss = 0.57847301\n",
            "Iteration 213, loss = 0.57832406\n",
            "Iteration 214, loss = 0.57817923\n",
            "Iteration 215, loss = 0.57804127\n",
            "Iteration 216, loss = 0.57789830\n",
            "Iteration 217, loss = 0.57776867\n",
            "Iteration 218, loss = 0.57762202\n",
            "Iteration 219, loss = 0.57749283\n",
            "Iteration 220, loss = 0.57733361\n",
            "Iteration 221, loss = 0.57718836\n",
            "Iteration 222, loss = 0.57703088\n",
            "Iteration 223, loss = 0.57690683\n",
            "Iteration 224, loss = 0.57677697\n",
            "Iteration 225, loss = 0.57664511\n",
            "Iteration 226, loss = 0.57651981\n",
            "Iteration 227, loss = 0.57640654\n",
            "Iteration 228, loss = 0.57629020\n",
            "Iteration 229, loss = 0.57616347\n",
            "Iteration 230, loss = 0.57604047\n",
            "Iteration 231, loss = 0.57590951\n",
            "Iteration 232, loss = 0.57577188\n",
            "Iteration 233, loss = 0.57565137\n",
            "Iteration 234, loss = 0.57553818\n",
            "Iteration 235, loss = 0.57540273\n",
            "Iteration 236, loss = 0.57528672\n",
            "Iteration 237, loss = 0.57517759\n",
            "Iteration 238, loss = 0.57506454\n",
            "Iteration 239, loss = 0.57494209\n",
            "Iteration 240, loss = 0.57483235\n",
            "Iteration 241, loss = 0.57470763\n",
            "Iteration 242, loss = 0.57459918\n",
            "Iteration 243, loss = 0.57449307\n",
            "Iteration 244, loss = 0.57438979\n",
            "Iteration 245, loss = 0.57428096\n",
            "Iteration 246, loss = 0.57416797\n",
            "Iteration 247, loss = 0.57406431\n",
            "Iteration 248, loss = 0.57394752\n",
            "Iteration 249, loss = 0.57384500\n",
            "Iteration 250, loss = 0.57375185\n",
            "Iteration 1, loss = 0.84884434\n",
            "Iteration 2, loss = 0.84610604\n",
            "Iteration 3, loss = 0.84230708\n",
            "Iteration 4, loss = 0.83712877\n",
            "Iteration 5, loss = 0.83146006\n",
            "Iteration 6, loss = 0.82550296\n",
            "Iteration 7, loss = 0.81909492\n",
            "Iteration 8, loss = 0.81265443\n",
            "Iteration 9, loss = 0.80669577\n",
            "Iteration 10, loss = 0.80039728\n",
            "Iteration 11, loss = 0.79425360\n",
            "Iteration 12, loss = 0.78824072\n",
            "Iteration 13, loss = 0.78261576\n",
            "Iteration 14, loss = 0.77671008\n",
            "Iteration 15, loss = 0.77114439\n",
            "Iteration 16, loss = 0.76606181\n",
            "Iteration 17, loss = 0.76108403\n",
            "Iteration 18, loss = 0.75613587\n",
            "Iteration 19, loss = 0.75170254\n",
            "Iteration 20, loss = 0.74713289\n",
            "Iteration 21, loss = 0.74293770\n",
            "Iteration 22, loss = 0.73884508\n",
            "Iteration 23, loss = 0.73495863\n",
            "Iteration 24, loss = 0.73114961\n",
            "Iteration 25, loss = 0.72759086\n",
            "Iteration 26, loss = 0.72382245\n",
            "Iteration 27, loss = 0.72028484\n",
            "Iteration 28, loss = 0.71690495\n",
            "Iteration 29, loss = 0.71349934\n",
            "Iteration 30, loss = 0.71009845\n",
            "Iteration 31, loss = 0.70704925\n",
            "Iteration 32, loss = 0.70404036\n",
            "Iteration 33, loss = 0.70113012\n",
            "Iteration 34, loss = 0.69813861\n",
            "Iteration 35, loss = 0.69541121\n",
            "Iteration 36, loss = 0.69283424\n",
            "Iteration 37, loss = 0.69007100\n",
            "Iteration 38, loss = 0.68761821\n",
            "Iteration 39, loss = 0.68511950\n",
            "Iteration 40, loss = 0.68252852\n",
            "Iteration 41, loss = 0.68002110\n",
            "Iteration 42, loss = 0.67761807\n",
            "Iteration 43, loss = 0.67543891\n",
            "Iteration 44, loss = 0.67320696\n",
            "Iteration 45, loss = 0.67119636\n",
            "Iteration 46, loss = 0.66930743\n",
            "Iteration 47, loss = 0.66722632\n",
            "Iteration 48, loss = 0.66538411\n",
            "Iteration 49, loss = 0.66343125\n",
            "Iteration 50, loss = 0.66151966\n",
            "Iteration 51, loss = 0.65973000\n",
            "Iteration 52, loss = 0.65793582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 53, loss = 0.65622869\n",
            "Iteration 54, loss = 0.65468002\n",
            "Iteration 55, loss = 0.65320535\n",
            "Iteration 56, loss = 0.65181447\n",
            "Iteration 57, loss = 0.65035231\n",
            "Iteration 58, loss = 0.64891425\n",
            "Iteration 59, loss = 0.64755073\n",
            "Iteration 60, loss = 0.64626251\n",
            "Iteration 61, loss = 0.64505130\n",
            "Iteration 62, loss = 0.64379440\n",
            "Iteration 63, loss = 0.64255345\n",
            "Iteration 64, loss = 0.64133698\n",
            "Iteration 65, loss = 0.64017137\n",
            "Iteration 66, loss = 0.63893323\n",
            "Iteration 67, loss = 0.63778882\n",
            "Iteration 68, loss = 0.63673076\n",
            "Iteration 69, loss = 0.63560115\n",
            "Iteration 70, loss = 0.63455761\n",
            "Iteration 71, loss = 0.63353220\n",
            "Iteration 72, loss = 0.63246273\n",
            "Iteration 73, loss = 0.63147427\n",
            "Iteration 74, loss = 0.63055777\n",
            "Iteration 75, loss = 0.62967430\n",
            "Iteration 76, loss = 0.62883840\n",
            "Iteration 77, loss = 0.62794709\n",
            "Iteration 78, loss = 0.62718417\n",
            "Iteration 79, loss = 0.62641941\n",
            "Iteration 80, loss = 0.62567713\n",
            "Iteration 81, loss = 0.62496967\n",
            "Iteration 82, loss = 0.62424083\n",
            "Iteration 83, loss = 0.62351271\n",
            "Iteration 84, loss = 0.62281916\n",
            "Iteration 85, loss = 0.62211463\n",
            "Iteration 86, loss = 0.62139155\n",
            "Iteration 87, loss = 0.62068848\n",
            "Iteration 88, loss = 0.61997756\n",
            "Iteration 89, loss = 0.61929985\n",
            "Iteration 90, loss = 0.61858862\n",
            "Iteration 91, loss = 0.61794400\n",
            "Iteration 92, loss = 0.61723905\n",
            "Iteration 93, loss = 0.61656102\n",
            "Iteration 94, loss = 0.61591205\n",
            "Iteration 95, loss = 0.61519728\n",
            "Iteration 96, loss = 0.61459602\n",
            "Iteration 97, loss = 0.61397021\n",
            "Iteration 98, loss = 0.61336479\n",
            "Iteration 99, loss = 0.61277025\n",
            "Iteration 100, loss = 0.61218560\n",
            "Iteration 101, loss = 0.61165672\n",
            "Iteration 102, loss = 0.61110966\n",
            "Iteration 103, loss = 0.61062357\n",
            "Iteration 104, loss = 0.61004453\n",
            "Iteration 105, loss = 0.60951759\n",
            "Iteration 106, loss = 0.60895887\n",
            "Iteration 107, loss = 0.60842317\n",
            "Iteration 108, loss = 0.60790224\n",
            "Iteration 109, loss = 0.60736399\n",
            "Iteration 110, loss = 0.60687911\n",
            "Iteration 111, loss = 0.60639413\n",
            "Iteration 112, loss = 0.60589977\n",
            "Iteration 113, loss = 0.60544970\n",
            "Iteration 114, loss = 0.60499882\n",
            "Iteration 115, loss = 0.60457310\n",
            "Iteration 116, loss = 0.60414893\n",
            "Iteration 117, loss = 0.60368021\n",
            "Iteration 118, loss = 0.60331520\n",
            "Iteration 119, loss = 0.60294032\n",
            "Iteration 120, loss = 0.60252016\n",
            "Iteration 121, loss = 0.60213903\n",
            "Iteration 122, loss = 0.60174523\n",
            "Iteration 123, loss = 0.60133593\n",
            "Iteration 124, loss = 0.60091023\n",
            "Iteration 125, loss = 0.60050976\n",
            "Iteration 126, loss = 0.60010820\n",
            "Iteration 127, loss = 0.59970696\n",
            "Iteration 128, loss = 0.59934052\n",
            "Iteration 129, loss = 0.59898036\n",
            "Iteration 130, loss = 0.59863160\n",
            "Iteration 131, loss = 0.59831106\n",
            "Iteration 132, loss = 0.59796491\n",
            "Iteration 133, loss = 0.59763822\n",
            "Iteration 134, loss = 0.59729262\n",
            "Iteration 135, loss = 0.59696763\n",
            "Iteration 136, loss = 0.59661883\n",
            "Iteration 137, loss = 0.59630678\n",
            "Iteration 138, loss = 0.59598908\n",
            "Iteration 139, loss = 0.59569112\n",
            "Iteration 140, loss = 0.59536288\n",
            "Iteration 141, loss = 0.59505523\n",
            "Iteration 142, loss = 0.59471383\n",
            "Iteration 143, loss = 0.59436415\n",
            "Iteration 144, loss = 0.59405748\n",
            "Iteration 145, loss = 0.59373360\n",
            "Iteration 146, loss = 0.59341229\n",
            "Iteration 147, loss = 0.59310556\n",
            "Iteration 148, loss = 0.59278314\n",
            "Iteration 149, loss = 0.59243203\n",
            "Iteration 150, loss = 0.59216162\n",
            "Iteration 151, loss = 0.59183948\n",
            "Iteration 152, loss = 0.59152583\n",
            "Iteration 153, loss = 0.59122354\n",
            "Iteration 154, loss = 0.59094689\n",
            "Iteration 155, loss = 0.59062364\n",
            "Iteration 156, loss = 0.59034079\n",
            "Iteration 157, loss = 0.59003310\n",
            "Iteration 158, loss = 0.58973518\n",
            "Iteration 159, loss = 0.58946881\n",
            "Iteration 160, loss = 0.58921332\n",
            "Iteration 161, loss = 0.58896173\n",
            "Iteration 162, loss = 0.58873790\n",
            "Iteration 163, loss = 0.58849665\n",
            "Iteration 164, loss = 0.58826871\n",
            "Iteration 165, loss = 0.58801024\n",
            "Iteration 166, loss = 0.58775940\n",
            "Iteration 167, loss = 0.58751073\n",
            "Iteration 168, loss = 0.58728124\n",
            "Iteration 169, loss = 0.58701916\n",
            "Iteration 170, loss = 0.58680194\n",
            "Iteration 171, loss = 0.58655512\n",
            "Iteration 172, loss = 0.58630942\n",
            "Iteration 173, loss = 0.58610079\n",
            "Iteration 174, loss = 0.58588253\n",
            "Iteration 175, loss = 0.58566431\n",
            "Iteration 176, loss = 0.58545213\n",
            "Iteration 177, loss = 0.58524128\n",
            "Iteration 178, loss = 0.58503744\n",
            "Iteration 179, loss = 0.58479710\n",
            "Iteration 180, loss = 0.58458663\n",
            "Iteration 181, loss = 0.58435643\n",
            "Iteration 182, loss = 0.58415343\n",
            "Iteration 183, loss = 0.58392523\n",
            "Iteration 184, loss = 0.58373347\n",
            "Iteration 185, loss = 0.58355026\n",
            "Iteration 186, loss = 0.58335871\n",
            "Iteration 187, loss = 0.58317377\n",
            "Iteration 188, loss = 0.58296934\n",
            "Iteration 189, loss = 0.58278168\n",
            "Iteration 190, loss = 0.58259214\n",
            "Iteration 191, loss = 0.58239186\n",
            "Iteration 192, loss = 0.58221350\n",
            "Iteration 193, loss = 0.58203516\n",
            "Iteration 194, loss = 0.58186028\n",
            "Iteration 195, loss = 0.58168141\n",
            "Iteration 196, loss = 0.58150987\n",
            "Iteration 197, loss = 0.58132432\n",
            "Iteration 198, loss = 0.58114499\n",
            "Iteration 199, loss = 0.58097392\n",
            "Iteration 200, loss = 0.58080477\n",
            "Iteration 201, loss = 0.58064024\n",
            "Iteration 202, loss = 0.58044465\n",
            "Iteration 203, loss = 0.58027359\n",
            "Iteration 204, loss = 0.58010344\n",
            "Iteration 205, loss = 0.57995784\n",
            "Iteration 206, loss = 0.57979727\n",
            "Iteration 207, loss = 0.57966251\n",
            "Iteration 208, loss = 0.57948942\n",
            "Iteration 209, loss = 0.57932728\n",
            "Iteration 210, loss = 0.57914889\n",
            "Iteration 211, loss = 0.57900230\n",
            "Iteration 212, loss = 0.57884169\n",
            "Iteration 213, loss = 0.57868519\n",
            "Iteration 214, loss = 0.57854101\n",
            "Iteration 215, loss = 0.57839435\n",
            "Iteration 216, loss = 0.57825324\n",
            "Iteration 217, loss = 0.57811678\n",
            "Iteration 218, loss = 0.57797700\n",
            "Iteration 219, loss = 0.57784043\n",
            "Iteration 220, loss = 0.57770732\n",
            "Iteration 221, loss = 0.57757894\n",
            "Iteration 222, loss = 0.57742668\n",
            "Iteration 223, loss = 0.57730465\n",
            "Iteration 224, loss = 0.57717560\n",
            "Iteration 225, loss = 0.57705513\n",
            "Iteration 226, loss = 0.57693511\n",
            "Iteration 227, loss = 0.57682526\n",
            "Iteration 228, loss = 0.57670497\n",
            "Iteration 229, loss = 0.57658082\n",
            "Iteration 230, loss = 0.57645771\n",
            "Iteration 231, loss = 0.57632937\n",
            "Iteration 232, loss = 0.57616805\n",
            "Iteration 233, loss = 0.57604242\n",
            "Iteration 234, loss = 0.57591536\n",
            "Iteration 235, loss = 0.57578118\n",
            "Iteration 236, loss = 0.57565220\n",
            "Iteration 237, loss = 0.57552961\n",
            "Iteration 238, loss = 0.57540638\n",
            "Iteration 239, loss = 0.57528452\n",
            "Iteration 240, loss = 0.57516715\n",
            "Iteration 241, loss = 0.57503500\n",
            "Iteration 242, loss = 0.57492954\n",
            "Iteration 243, loss = 0.57481890\n",
            "Iteration 244, loss = 0.57471089\n",
            "Iteration 245, loss = 0.57460015\n",
            "Iteration 246, loss = 0.57449065\n",
            "Iteration 247, loss = 0.57439507\n",
            "Iteration 248, loss = 0.57426052\n",
            "Iteration 249, loss = 0.57415300\n",
            "Iteration 250, loss = 0.57405202\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.86923367\n",
            "Iteration 2, loss = 0.86623006\n",
            "Iteration 3, loss = 0.86201844\n",
            "Iteration 4, loss = 0.85633892\n",
            "Iteration 5, loss = 0.85008599\n",
            "Iteration 6, loss = 0.84356157\n",
            "Iteration 7, loss = 0.83652964\n",
            "Iteration 8, loss = 0.82935828\n",
            "Iteration 9, loss = 0.82278608\n",
            "Iteration 10, loss = 0.81571552\n",
            "Iteration 11, loss = 0.80886293\n",
            "Iteration 12, loss = 0.80218902\n",
            "Iteration 13, loss = 0.79599872\n",
            "Iteration 14, loss = 0.78964699\n",
            "Iteration 15, loss = 0.78337376\n",
            "Iteration 16, loss = 0.77753155\n",
            "Iteration 17, loss = 0.77180928\n",
            "Iteration 18, loss = 0.76609001\n",
            "Iteration 19, loss = 0.76110090\n",
            "Iteration 20, loss = 0.75627648\n",
            "Iteration 21, loss = 0.75155283\n",
            "Iteration 22, loss = 0.74704216\n",
            "Iteration 23, loss = 0.74279740\n",
            "Iteration 24, loss = 0.73856804\n",
            "Iteration 25, loss = 0.73462565\n",
            "Iteration 26, loss = 0.73055780\n",
            "Iteration 27, loss = 0.72667968\n",
            "Iteration 28, loss = 0.72311381\n",
            "Iteration 29, loss = 0.71931481\n",
            "Iteration 30, loss = 0.71559199\n",
            "Iteration 31, loss = 0.71214502\n",
            "Iteration 32, loss = 0.70883599\n",
            "Iteration 33, loss = 0.70545153\n",
            "Iteration 34, loss = 0.70226445\n",
            "Iteration 35, loss = 0.69916085\n",
            "Iteration 36, loss = 0.69616862\n",
            "Iteration 37, loss = 0.69307073\n",
            "Iteration 38, loss = 0.69021815\n",
            "Iteration 39, loss = 0.68742445\n",
            "Iteration 40, loss = 0.68467159\n",
            "Iteration 41, loss = 0.68190363\n",
            "Iteration 42, loss = 0.67946455\n",
            "Iteration 43, loss = 0.67717399\n",
            "Iteration 44, loss = 0.67499244\n",
            "Iteration 45, loss = 0.67284265\n",
            "Iteration 46, loss = 0.67090200\n",
            "Iteration 47, loss = 0.66876285\n",
            "Iteration 48, loss = 0.66681518\n",
            "Iteration 49, loss = 0.66482789\n",
            "Iteration 50, loss = 0.66288284\n",
            "Iteration 51, loss = 0.66109203\n",
            "Iteration 52, loss = 0.65916577\n",
            "Iteration 53, loss = 0.65740321\n",
            "Iteration 54, loss = 0.65573998\n",
            "Iteration 55, loss = 0.65410347\n",
            "Iteration 56, loss = 0.65259831\n",
            "Iteration 57, loss = 0.65109783\n",
            "Iteration 58, loss = 0.64958658\n",
            "Iteration 59, loss = 0.64816078\n",
            "Iteration 60, loss = 0.64678991\n",
            "Iteration 61, loss = 0.64549181\n",
            "Iteration 62, loss = 0.64412795\n",
            "Iteration 63, loss = 0.64282614\n",
            "Iteration 64, loss = 0.64150110\n",
            "Iteration 65, loss = 0.64025473\n",
            "Iteration 66, loss = 0.63892570\n",
            "Iteration 67, loss = 0.63768365\n",
            "Iteration 68, loss = 0.63652642\n",
            "Iteration 69, loss = 0.63536168\n",
            "Iteration 70, loss = 0.63421997\n",
            "Iteration 71, loss = 0.63317325\n",
            "Iteration 72, loss = 0.63205658\n",
            "Iteration 73, loss = 0.63100731\n",
            "Iteration 74, loss = 0.63001822\n",
            "Iteration 75, loss = 0.62901823\n",
            "Iteration 76, loss = 0.62809679\n",
            "Iteration 77, loss = 0.62707914\n",
            "Iteration 78, loss = 0.62623111\n",
            "Iteration 79, loss = 0.62533088\n",
            "Iteration 80, loss = 0.62450127\n",
            "Iteration 81, loss = 0.62368795\n",
            "Iteration 82, loss = 0.62285271\n",
            "Iteration 83, loss = 0.62204441\n",
            "Iteration 84, loss = 0.62127094\n",
            "Iteration 85, loss = 0.62052542\n",
            "Iteration 86, loss = 0.61970997\n",
            "Iteration 87, loss = 0.61893977\n",
            "Iteration 88, loss = 0.61815595\n",
            "Iteration 89, loss = 0.61740030\n",
            "Iteration 90, loss = 0.61662252\n",
            "Iteration 91, loss = 0.61594190\n",
            "Iteration 92, loss = 0.61517044\n",
            "Iteration 93, loss = 0.61448395\n",
            "Iteration 94, loss = 0.61380520\n",
            "Iteration 95, loss = 0.61298957\n",
            "Iteration 96, loss = 0.61235810\n",
            "Iteration 97, loss = 0.61163696\n",
            "Iteration 98, loss = 0.61092207\n",
            "Iteration 99, loss = 0.61023909\n",
            "Iteration 100, loss = 0.60959741\n",
            "Iteration 101, loss = 0.60897924\n",
            "Iteration 102, loss = 0.60830994\n",
            "Iteration 103, loss = 0.60768355\n",
            "Iteration 104, loss = 0.60697099\n",
            "Iteration 105, loss = 0.60632363\n",
            "Iteration 106, loss = 0.60567325\n",
            "Iteration 107, loss = 0.60497776\n",
            "Iteration 108, loss = 0.60439037\n",
            "Iteration 109, loss = 0.60375829\n",
            "Iteration 110, loss = 0.60318738\n",
            "Iteration 111, loss = 0.60258742\n",
            "Iteration 112, loss = 0.60197632\n",
            "Iteration 113, loss = 0.60141942\n",
            "Iteration 114, loss = 0.60086993\n",
            "Iteration 115, loss = 0.60029735\n",
            "Iteration 116, loss = 0.59974641\n",
            "Iteration 117, loss = 0.59909337\n",
            "Iteration 118, loss = 0.59858018\n",
            "Iteration 119, loss = 0.59808021\n",
            "Iteration 120, loss = 0.59759884\n",
            "Iteration 121, loss = 0.59712295\n",
            "Iteration 122, loss = 0.59664494\n",
            "Iteration 123, loss = 0.59613269\n",
            "Iteration 124, loss = 0.59559367\n",
            "Iteration 125, loss = 0.59512475\n",
            "Iteration 126, loss = 0.59460867\n",
            "Iteration 127, loss = 0.59411337\n",
            "Iteration 128, loss = 0.59365786\n",
            "Iteration 129, loss = 0.59322298\n",
            "Iteration 130, loss = 0.59276458\n",
            "Iteration 131, loss = 0.59238339\n",
            "Iteration 132, loss = 0.59196273\n",
            "Iteration 133, loss = 0.59154884\n",
            "Iteration 134, loss = 0.59114234\n",
            "Iteration 135, loss = 0.59075022\n",
            "Iteration 136, loss = 0.59032802\n",
            "Iteration 137, loss = 0.58994490\n",
            "Iteration 138, loss = 0.58955952\n",
            "Iteration 139, loss = 0.58920941\n",
            "Iteration 140, loss = 0.58882673\n",
            "Iteration 141, loss = 0.58847080\n",
            "Iteration 142, loss = 0.58807377\n",
            "Iteration 143, loss = 0.58768820\n",
            "Iteration 144, loss = 0.58730221\n",
            "Iteration 145, loss = 0.58692645\n",
            "Iteration 146, loss = 0.58654280\n",
            "Iteration 147, loss = 0.58619159\n",
            "Iteration 148, loss = 0.58579775\n",
            "Iteration 149, loss = 0.58539863\n",
            "Iteration 150, loss = 0.58505482\n",
            "Iteration 151, loss = 0.58467476\n",
            "Iteration 152, loss = 0.58429695\n",
            "Iteration 153, loss = 0.58394553\n",
            "Iteration 154, loss = 0.58358963\n",
            "Iteration 155, loss = 0.58321133\n",
            "Iteration 156, loss = 0.58288150\n",
            "Iteration 157, loss = 0.58252365\n",
            "Iteration 158, loss = 0.58218811\n",
            "Iteration 159, loss = 0.58189613\n",
            "Iteration 160, loss = 0.58160914\n",
            "Iteration 161, loss = 0.58130793\n",
            "Iteration 162, loss = 0.58103253\n",
            "Iteration 163, loss = 0.58075001\n",
            "Iteration 164, loss = 0.58047517\n",
            "Iteration 165, loss = 0.58018892\n",
            "Iteration 166, loss = 0.57992538\n",
            "Iteration 167, loss = 0.57963637\n",
            "Iteration 168, loss = 0.57938675\n",
            "Iteration 169, loss = 0.57909789\n",
            "Iteration 170, loss = 0.57882554\n",
            "Iteration 171, loss = 0.57855363\n",
            "Iteration 172, loss = 0.57829767\n",
            "Iteration 173, loss = 0.57804112\n",
            "Iteration 174, loss = 0.57780951\n",
            "Iteration 175, loss = 0.57755053\n",
            "Iteration 176, loss = 0.57730370\n",
            "Iteration 177, loss = 0.57705146\n",
            "Iteration 178, loss = 0.57681810\n",
            "Iteration 179, loss = 0.57655681\n",
            "Iteration 180, loss = 0.57632833\n",
            "Iteration 181, loss = 0.57608327\n",
            "Iteration 182, loss = 0.57584442\n",
            "Iteration 183, loss = 0.57559388\n",
            "Iteration 184, loss = 0.57537209\n",
            "Iteration 185, loss = 0.57514602\n",
            "Iteration 186, loss = 0.57491080\n",
            "Iteration 187, loss = 0.57469425\n",
            "Iteration 188, loss = 0.57447699\n",
            "Iteration 189, loss = 0.57425421\n",
            "Iteration 190, loss = 0.57401494\n",
            "Iteration 191, loss = 0.57378281\n",
            "Iteration 192, loss = 0.57356778\n",
            "Iteration 193, loss = 0.57335559\n",
            "Iteration 194, loss = 0.57314874\n",
            "Iteration 195, loss = 0.57293664\n",
            "Iteration 196, loss = 0.57275212\n",
            "Iteration 197, loss = 0.57253941\n",
            "Iteration 198, loss = 0.57234058\n",
            "Iteration 199, loss = 0.57211390\n",
            "Iteration 200, loss = 0.57191341\n",
            "Iteration 201, loss = 0.57169603\n",
            "Iteration 202, loss = 0.57148075\n",
            "Iteration 203, loss = 0.57128223\n",
            "Iteration 204, loss = 0.57106752\n",
            "Iteration 205, loss = 0.57088815\n",
            "Iteration 206, loss = 0.57070236\n",
            "Iteration 207, loss = 0.57051483\n",
            "Iteration 208, loss = 0.57031102\n",
            "Iteration 209, loss = 0.57012345\n",
            "Iteration 210, loss = 0.56992088\n",
            "Iteration 211, loss = 0.56973911\n",
            "Iteration 212, loss = 0.56956778\n",
            "Iteration 213, loss = 0.56939558\n",
            "Iteration 214, loss = 0.56923094\n",
            "Iteration 215, loss = 0.56905277\n",
            "Iteration 216, loss = 0.56886713\n",
            "Iteration 217, loss = 0.56869019\n",
            "Iteration 218, loss = 0.56852462\n",
            "Iteration 219, loss = 0.56833653\n",
            "Iteration 220, loss = 0.56816620\n",
            "Iteration 221, loss = 0.56800548\n",
            "Iteration 222, loss = 0.56782421\n",
            "Iteration 223, loss = 0.56766592\n",
            "Iteration 224, loss = 0.56749913\n",
            "Iteration 225, loss = 0.56734746\n",
            "Iteration 226, loss = 0.56718246\n",
            "Iteration 227, loss = 0.56702643\n",
            "Iteration 228, loss = 0.56687473\n",
            "Iteration 229, loss = 0.56672037\n",
            "Iteration 230, loss = 0.56656884\n",
            "Iteration 231, loss = 0.56642347\n",
            "Iteration 232, loss = 0.56624138\n",
            "Iteration 233, loss = 0.56610797\n",
            "Iteration 234, loss = 0.56594229\n",
            "Iteration 235, loss = 0.56581195\n",
            "Iteration 236, loss = 0.56566528\n",
            "Iteration 237, loss = 0.56552913\n",
            "Iteration 238, loss = 0.56539298\n",
            "Iteration 239, loss = 0.56525786\n",
            "Iteration 240, loss = 0.56512274\n",
            "Iteration 241, loss = 0.56497634\n",
            "Iteration 242, loss = 0.56486457\n",
            "Iteration 243, loss = 0.56473217\n",
            "Iteration 244, loss = 0.56461626\n",
            "Iteration 245, loss = 0.56449188\n",
            "Iteration 246, loss = 0.56436508\n",
            "Iteration 247, loss = 0.56424760\n",
            "Iteration 248, loss = 0.56410116\n",
            "Iteration 249, loss = 0.56397790\n",
            "Iteration 250, loss = 0.56386261\n",
            "Iteration 1, loss = 0.84858612\n",
            "Iteration 2, loss = 0.84586103\n",
            "Iteration 3, loss = 0.84202808\n",
            "Iteration 4, loss = 0.83677269\n",
            "Iteration 5, loss = 0.83102491\n",
            "Iteration 6, loss = 0.82523503\n",
            "Iteration 7, loss = 0.81887373\n",
            "Iteration 8, loss = 0.81242895\n",
            "Iteration 9, loss = 0.80642299\n",
            "Iteration 10, loss = 0.80007903\n",
            "Iteration 11, loss = 0.79384885\n",
            "Iteration 12, loss = 0.78766457\n",
            "Iteration 13, loss = 0.78212698\n",
            "Iteration 14, loss = 0.77616718\n",
            "Iteration 15, loss = 0.77057092\n",
            "Iteration 16, loss = 0.76520465\n",
            "Iteration 17, loss = 0.76001179\n",
            "Iteration 18, loss = 0.75481795\n",
            "Iteration 19, loss = 0.75019268\n",
            "Iteration 20, loss = 0.74570973\n",
            "Iteration 21, loss = 0.74143312\n",
            "Iteration 22, loss = 0.73714002\n",
            "Iteration 23, loss = 0.73317639\n",
            "Iteration 24, loss = 0.72916594\n",
            "Iteration 25, loss = 0.72564340\n",
            "Iteration 26, loss = 0.72187504\n",
            "Iteration 27, loss = 0.71826418\n",
            "Iteration 28, loss = 0.71475157\n",
            "Iteration 29, loss = 0.71126395\n",
            "Iteration 30, loss = 0.70780464\n",
            "Iteration 31, loss = 0.70433933\n",
            "Iteration 32, loss = 0.70129476\n",
            "Iteration 33, loss = 0.69812788\n",
            "Iteration 34, loss = 0.69512355\n",
            "Iteration 35, loss = 0.69245263\n",
            "Iteration 36, loss = 0.68965835\n",
            "Iteration 37, loss = 0.68690679\n",
            "Iteration 38, loss = 0.68418766\n",
            "Iteration 39, loss = 0.68157247\n",
            "Iteration 40, loss = 0.67890392\n",
            "Iteration 41, loss = 0.67638975\n",
            "Iteration 42, loss = 0.67403004\n",
            "Iteration 43, loss = 0.67200248\n",
            "Iteration 44, loss = 0.66999841\n",
            "Iteration 45, loss = 0.66803721\n",
            "Iteration 46, loss = 0.66615995\n",
            "Iteration 47, loss = 0.66413871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 48, loss = 0.66225463\n",
            "Iteration 49, loss = 0.66048563\n",
            "Iteration 50, loss = 0.65875979\n",
            "Iteration 51, loss = 0.65707940\n",
            "Iteration 52, loss = 0.65546792\n",
            "Iteration 53, loss = 0.65384613\n",
            "Iteration 54, loss = 0.65236260\n",
            "Iteration 55, loss = 0.65091725\n",
            "Iteration 56, loss = 0.64959092\n",
            "Iteration 57, loss = 0.64820250\n",
            "Iteration 58, loss = 0.64691513\n",
            "Iteration 59, loss = 0.64568032\n",
            "Iteration 60, loss = 0.64440258\n",
            "Iteration 61, loss = 0.64327986\n",
            "Iteration 62, loss = 0.64202316\n",
            "Iteration 63, loss = 0.64089945\n",
            "Iteration 64, loss = 0.63978166\n",
            "Iteration 65, loss = 0.63867096\n",
            "Iteration 66, loss = 0.63754716\n",
            "Iteration 67, loss = 0.63647957\n",
            "Iteration 68, loss = 0.63545464\n",
            "Iteration 69, loss = 0.63444067\n",
            "Iteration 70, loss = 0.63343899\n",
            "Iteration 71, loss = 0.63246926\n",
            "Iteration 72, loss = 0.63142338\n",
            "Iteration 73, loss = 0.63047341\n",
            "Iteration 74, loss = 0.62955579\n",
            "Iteration 75, loss = 0.62870095\n",
            "Iteration 76, loss = 0.62785827\n",
            "Iteration 77, loss = 0.62697778\n",
            "Iteration 78, loss = 0.62622000\n",
            "Iteration 79, loss = 0.62543734\n",
            "Iteration 80, loss = 0.62467271\n",
            "Iteration 81, loss = 0.62399221\n",
            "Iteration 82, loss = 0.62324994\n",
            "Iteration 83, loss = 0.62256615\n",
            "Iteration 84, loss = 0.62187724\n",
            "Iteration 85, loss = 0.62128307\n",
            "Iteration 86, loss = 0.62055994\n",
            "Iteration 87, loss = 0.61987967\n",
            "Iteration 88, loss = 0.61925896\n",
            "Iteration 89, loss = 0.61861177\n",
            "Iteration 90, loss = 0.61796749\n",
            "Iteration 91, loss = 0.61735920\n",
            "Iteration 92, loss = 0.61671408\n",
            "Iteration 93, loss = 0.61610607\n",
            "Iteration 94, loss = 0.61551042\n",
            "Iteration 95, loss = 0.61475085\n",
            "Iteration 96, loss = 0.61417133\n",
            "Iteration 97, loss = 0.61357502\n",
            "Iteration 98, loss = 0.61292309\n",
            "Iteration 99, loss = 0.61240326\n",
            "Iteration 100, loss = 0.61185817\n",
            "Iteration 101, loss = 0.61132681\n",
            "Iteration 102, loss = 0.61082051\n",
            "Iteration 103, loss = 0.61033679\n",
            "Iteration 104, loss = 0.60983754\n",
            "Iteration 105, loss = 0.60932550\n",
            "Iteration 106, loss = 0.60880611\n",
            "Iteration 107, loss = 0.60827682\n",
            "Iteration 108, loss = 0.60777675\n",
            "Iteration 109, loss = 0.60728584\n",
            "Iteration 110, loss = 0.60683020\n",
            "Iteration 111, loss = 0.60638657\n",
            "Iteration 112, loss = 0.60590872\n",
            "Iteration 113, loss = 0.60549932\n",
            "Iteration 114, loss = 0.60508998\n",
            "Iteration 115, loss = 0.60466578\n",
            "Iteration 116, loss = 0.60427608\n",
            "Iteration 117, loss = 0.60386024\n",
            "Iteration 118, loss = 0.60351764\n",
            "Iteration 119, loss = 0.60317213\n",
            "Iteration 120, loss = 0.60282647\n",
            "Iteration 121, loss = 0.60252279\n",
            "Iteration 122, loss = 0.60215557\n",
            "Iteration 123, loss = 0.60177600\n",
            "Iteration 124, loss = 0.60138698\n",
            "Iteration 125, loss = 0.60104870\n",
            "Iteration 126, loss = 0.60066440\n",
            "Iteration 127, loss = 0.60029622\n",
            "Iteration 128, loss = 0.59995722\n",
            "Iteration 129, loss = 0.59964275\n",
            "Iteration 130, loss = 0.59931902\n",
            "Iteration 131, loss = 0.59905775\n",
            "Iteration 132, loss = 0.59875957\n",
            "Iteration 133, loss = 0.59846534\n",
            "Iteration 134, loss = 0.59818749\n",
            "Iteration 135, loss = 0.59789258\n",
            "Iteration 136, loss = 0.59757564\n",
            "Iteration 137, loss = 0.59730132\n",
            "Iteration 138, loss = 0.59703448\n",
            "Iteration 139, loss = 0.59675296\n",
            "Iteration 140, loss = 0.59646325\n",
            "Iteration 141, loss = 0.59621349\n",
            "Iteration 142, loss = 0.59592745\n",
            "Iteration 143, loss = 0.59564027\n",
            "Iteration 144, loss = 0.59536162\n",
            "Iteration 145, loss = 0.59508834\n",
            "Iteration 146, loss = 0.59481764\n",
            "Iteration 147, loss = 0.59457382\n",
            "Iteration 148, loss = 0.59429337\n",
            "Iteration 149, loss = 0.59403140\n",
            "Iteration 150, loss = 0.59378019\n",
            "Iteration 151, loss = 0.59351883\n",
            "Iteration 152, loss = 0.59324684\n",
            "Iteration 153, loss = 0.59298803\n",
            "Iteration 154, loss = 0.59274647\n",
            "Iteration 155, loss = 0.59246327\n",
            "Iteration 156, loss = 0.59223741\n",
            "Iteration 157, loss = 0.59195932\n",
            "Iteration 158, loss = 0.59173650\n",
            "Iteration 159, loss = 0.59150695\n",
            "Iteration 160, loss = 0.59129473\n",
            "Iteration 161, loss = 0.59105468\n",
            "Iteration 162, loss = 0.59084932\n",
            "Iteration 163, loss = 0.59064474\n",
            "Iteration 164, loss = 0.59043675\n",
            "Iteration 165, loss = 0.59022354\n",
            "Iteration 166, loss = 0.59002231\n",
            "Iteration 167, loss = 0.58980346\n",
            "Iteration 168, loss = 0.58959525\n",
            "Iteration 169, loss = 0.58938385\n",
            "Iteration 170, loss = 0.58915162\n",
            "Iteration 171, loss = 0.58893284\n",
            "Iteration 172, loss = 0.58873316\n",
            "Iteration 173, loss = 0.58851302\n",
            "Iteration 174, loss = 0.58834254\n",
            "Iteration 175, loss = 0.58811887\n",
            "Iteration 176, loss = 0.58792879\n",
            "Iteration 177, loss = 0.58774321\n",
            "Iteration 178, loss = 0.58756283\n",
            "Iteration 179, loss = 0.58734702\n",
            "Iteration 180, loss = 0.58718299\n",
            "Iteration 181, loss = 0.58696382\n",
            "Iteration 182, loss = 0.58678926\n",
            "Iteration 183, loss = 0.58659905\n",
            "Iteration 184, loss = 0.58643580\n",
            "Iteration 185, loss = 0.58625711\n",
            "Iteration 186, loss = 0.58609266\n",
            "Iteration 187, loss = 0.58592706\n",
            "Iteration 188, loss = 0.58575609\n",
            "Iteration 189, loss = 0.58558891\n",
            "Iteration 190, loss = 0.58540802\n",
            "Iteration 191, loss = 0.58523423\n",
            "Iteration 192, loss = 0.58507152\n",
            "Iteration 193, loss = 0.58491719\n",
            "Iteration 194, loss = 0.58477430\n",
            "Iteration 195, loss = 0.58462679\n",
            "Iteration 196, loss = 0.58448415\n",
            "Iteration 197, loss = 0.58432513\n",
            "Iteration 198, loss = 0.58416263\n",
            "Iteration 199, loss = 0.58401465\n",
            "Iteration 200, loss = 0.58385186\n",
            "Iteration 201, loss = 0.58368712\n",
            "Iteration 202, loss = 0.58350935\n",
            "Iteration 203, loss = 0.58334810\n",
            "Iteration 204, loss = 0.58317378\n",
            "Iteration 205, loss = 0.58302035\n",
            "Iteration 206, loss = 0.58286281\n",
            "Iteration 207, loss = 0.58271273\n",
            "Iteration 208, loss = 0.58255888\n",
            "Iteration 209, loss = 0.58241583\n",
            "Iteration 210, loss = 0.58226891\n",
            "Iteration 211, loss = 0.58213010\n",
            "Iteration 212, loss = 0.58200471\n",
            "Iteration 213, loss = 0.58188563\n",
            "Iteration 214, loss = 0.58176456\n",
            "Iteration 215, loss = 0.58163795\n",
            "Iteration 216, loss = 0.58150875\n",
            "Iteration 217, loss = 0.58137178\n",
            "Iteration 218, loss = 0.58125182\n",
            "Iteration 219, loss = 0.58109270\n",
            "Iteration 220, loss = 0.58096308\n",
            "Iteration 221, loss = 0.58083829\n",
            "Iteration 222, loss = 0.58070023\n",
            "Iteration 223, loss = 0.58058274\n",
            "Iteration 224, loss = 0.58044507\n",
            "Iteration 225, loss = 0.58032583\n",
            "Iteration 226, loss = 0.58020788\n",
            "Iteration 227, loss = 0.58007998\n",
            "Iteration 228, loss = 0.57996832\n",
            "Iteration 229, loss = 0.57984631\n",
            "Iteration 230, loss = 0.57973955\n",
            "Iteration 231, loss = 0.57962596\n",
            "Iteration 232, loss = 0.57948968\n",
            "Iteration 233, loss = 0.57938309\n",
            "Iteration 234, loss = 0.57926755\n",
            "Iteration 235, loss = 0.57918857\n",
            "Iteration 236, loss = 0.57907093\n",
            "Iteration 237, loss = 0.57897557\n",
            "Iteration 238, loss = 0.57886148\n",
            "Iteration 239, loss = 0.57876007\n",
            "Iteration 240, loss = 0.57865553\n",
            "Iteration 241, loss = 0.57854502\n",
            "Iteration 242, loss = 0.57845478\n",
            "Iteration 243, loss = 0.57835173\n",
            "Iteration 244, loss = 0.57825415\n",
            "Iteration 245, loss = 0.57815862\n",
            "Iteration 246, loss = 0.57806111\n",
            "Iteration 247, loss = 0.57797290\n",
            "Iteration 248, loss = 0.57785626\n",
            "Iteration 249, loss = 0.57774942\n",
            "Iteration 250, loss = 0.57766348\n",
            "Iteration 1, loss = 0.84407341\n",
            "Iteration 2, loss = 0.84130009\n",
            "Iteration 3, loss = 0.83725245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4, loss = 0.83218094\n",
            "Iteration 5, loss = 0.82697800\n",
            "Iteration 6, loss = 0.82168607\n",
            "Iteration 7, loss = 0.81584661\n",
            "Iteration 8, loss = 0.81000185\n",
            "Iteration 9, loss = 0.80435363\n",
            "Iteration 10, loss = 0.79816107\n",
            "Iteration 11, loss = 0.79237071\n",
            "Iteration 12, loss = 0.78666337\n",
            "Iteration 13, loss = 0.78102998\n",
            "Iteration 14, loss = 0.77550694\n",
            "Iteration 15, loss = 0.77026749\n",
            "Iteration 16, loss = 0.76517323\n",
            "Iteration 17, loss = 0.76032485\n",
            "Iteration 18, loss = 0.75561036\n",
            "Iteration 19, loss = 0.75098454\n",
            "Iteration 20, loss = 0.74658742\n",
            "Iteration 21, loss = 0.74243839\n",
            "Iteration 22, loss = 0.73833552\n",
            "Iteration 23, loss = 0.73436580\n",
            "Iteration 24, loss = 0.73058470\n",
            "Iteration 25, loss = 0.72698704\n",
            "Iteration 26, loss = 0.72315689\n",
            "Iteration 27, loss = 0.71954935\n",
            "Iteration 28, loss = 0.71591667\n",
            "Iteration 29, loss = 0.71260479\n",
            "Iteration 30, loss = 0.70914756\n",
            "Iteration 31, loss = 0.70589731\n",
            "Iteration 32, loss = 0.70265938\n",
            "Iteration 33, loss = 0.69947938\n",
            "Iteration 34, loss = 0.69654095\n",
            "Iteration 35, loss = 0.69348703\n",
            "Iteration 36, loss = 0.69054236\n",
            "Iteration 37, loss = 0.68762425\n",
            "Iteration 38, loss = 0.68489428\n",
            "Iteration 39, loss = 0.68225194\n",
            "Iteration 40, loss = 0.67961680\n",
            "Iteration 41, loss = 0.67710505\n",
            "Iteration 42, loss = 0.67477275\n",
            "Iteration 43, loss = 0.67274744\n",
            "Iteration 44, loss = 0.67056462\n",
            "Iteration 45, loss = 0.66859460\n",
            "Iteration 46, loss = 0.66655832\n",
            "Iteration 47, loss = 0.66454959\n",
            "Iteration 48, loss = 0.66265032\n",
            "Iteration 49, loss = 0.66091053\n",
            "Iteration 50, loss = 0.65917129\n",
            "Iteration 51, loss = 0.65752328\n",
            "Iteration 52, loss = 0.65594358\n",
            "Iteration 53, loss = 0.65432003\n",
            "Iteration 54, loss = 0.65283905\n",
            "Iteration 55, loss = 0.65128565\n",
            "Iteration 56, loss = 0.64996104\n",
            "Iteration 57, loss = 0.64857742\n",
            "Iteration 58, loss = 0.64723652\n",
            "Iteration 59, loss = 0.64599833\n",
            "Iteration 60, loss = 0.64471383\n",
            "Iteration 61, loss = 0.64352032\n",
            "Iteration 62, loss = 0.64224376\n",
            "Iteration 63, loss = 0.64109397\n",
            "Iteration 64, loss = 0.63993373\n",
            "Iteration 65, loss = 0.63886747\n",
            "Iteration 66, loss = 0.63775884\n",
            "Iteration 67, loss = 0.63666581\n",
            "Iteration 68, loss = 0.63561012\n",
            "Iteration 69, loss = 0.63454685\n",
            "Iteration 70, loss = 0.63350548\n",
            "Iteration 71, loss = 0.63253331\n",
            "Iteration 72, loss = 0.63148168\n",
            "Iteration 73, loss = 0.63054464\n",
            "Iteration 74, loss = 0.62962690\n",
            "Iteration 75, loss = 0.62875371\n",
            "Iteration 76, loss = 0.62785837\n",
            "Iteration 77, loss = 0.62703086\n",
            "Iteration 78, loss = 0.62624236\n",
            "Iteration 79, loss = 0.62546140\n",
            "Iteration 80, loss = 0.62474273\n",
            "Iteration 81, loss = 0.62408242\n",
            "Iteration 82, loss = 0.62337742\n",
            "Iteration 83, loss = 0.62269981\n",
            "Iteration 84, loss = 0.62201595\n",
            "Iteration 85, loss = 0.62140697\n",
            "Iteration 86, loss = 0.62071370\n",
            "Iteration 87, loss = 0.62004325\n",
            "Iteration 88, loss = 0.61943810\n",
            "Iteration 89, loss = 0.61880019\n",
            "Iteration 90, loss = 0.61817375\n",
            "Iteration 91, loss = 0.61758277\n",
            "Iteration 92, loss = 0.61693716\n",
            "Iteration 93, loss = 0.61634558\n",
            "Iteration 94, loss = 0.61577297\n",
            "Iteration 95, loss = 0.61512111\n",
            "Iteration 96, loss = 0.61457329\n",
            "Iteration 97, loss = 0.61400662\n",
            "Iteration 98, loss = 0.61334432\n",
            "Iteration 99, loss = 0.61284428\n",
            "Iteration 100, loss = 0.61230045\n",
            "Iteration 101, loss = 0.61178248\n",
            "Iteration 102, loss = 0.61130670\n",
            "Iteration 103, loss = 0.61079670\n",
            "Iteration 104, loss = 0.61029651\n",
            "Iteration 105, loss = 0.60978651\n",
            "Iteration 106, loss = 0.60928542\n",
            "Iteration 107, loss = 0.60876180\n",
            "Iteration 108, loss = 0.60826273\n",
            "Iteration 109, loss = 0.60773135\n",
            "Iteration 110, loss = 0.60722575\n",
            "Iteration 111, loss = 0.60676696\n",
            "Iteration 112, loss = 0.60620469\n",
            "Iteration 113, loss = 0.60571493\n",
            "Iteration 114, loss = 0.60527486\n",
            "Iteration 115, loss = 0.60477620\n",
            "Iteration 116, loss = 0.60433492\n",
            "Iteration 117, loss = 0.60388702\n",
            "Iteration 118, loss = 0.60347394\n",
            "Iteration 119, loss = 0.60307031\n",
            "Iteration 120, loss = 0.60266544\n",
            "Iteration 121, loss = 0.60226998\n",
            "Iteration 122, loss = 0.60185161\n",
            "Iteration 123, loss = 0.60138960\n",
            "Iteration 124, loss = 0.60099822\n",
            "Iteration 125, loss = 0.60059314\n",
            "Iteration 126, loss = 0.60017253\n",
            "Iteration 127, loss = 0.59977016\n",
            "Iteration 128, loss = 0.59938774\n",
            "Iteration 129, loss = 0.59902502\n",
            "Iteration 130, loss = 0.59864652\n",
            "Iteration 131, loss = 0.59832533\n",
            "Iteration 132, loss = 0.59799104\n",
            "Iteration 133, loss = 0.59766587\n",
            "Iteration 134, loss = 0.59736055\n",
            "Iteration 135, loss = 0.59702591\n",
            "Iteration 136, loss = 0.59667199\n",
            "Iteration 137, loss = 0.59635611\n",
            "Iteration 138, loss = 0.59608056\n",
            "Iteration 139, loss = 0.59578235\n",
            "Iteration 140, loss = 0.59548403\n",
            "Iteration 141, loss = 0.59521450\n",
            "Iteration 142, loss = 0.59491029\n",
            "Iteration 143, loss = 0.59461594\n",
            "Iteration 144, loss = 0.59431879\n",
            "Iteration 145, loss = 0.59403609\n",
            "Iteration 146, loss = 0.59375025\n",
            "Iteration 147, loss = 0.59347385\n",
            "Iteration 148, loss = 0.59319147\n",
            "Iteration 149, loss = 0.59291991\n",
            "Iteration 150, loss = 0.59263915\n",
            "Iteration 151, loss = 0.59235449\n",
            "Iteration 152, loss = 0.59205361\n",
            "Iteration 153, loss = 0.59178152\n",
            "Iteration 154, loss = 0.59152328\n",
            "Iteration 155, loss = 0.59123631\n",
            "Iteration 156, loss = 0.59097670\n",
            "Iteration 157, loss = 0.59070864\n",
            "Iteration 158, loss = 0.59047385\n",
            "Iteration 159, loss = 0.59022275\n",
            "Iteration 160, loss = 0.58999782\n",
            "Iteration 161, loss = 0.58973176\n",
            "Iteration 162, loss = 0.58950729\n",
            "Iteration 163, loss = 0.58929229\n",
            "Iteration 164, loss = 0.58904123\n",
            "Iteration 165, loss = 0.58882014\n",
            "Iteration 166, loss = 0.58861401\n",
            "Iteration 167, loss = 0.58839697\n",
            "Iteration 168, loss = 0.58817000\n",
            "Iteration 169, loss = 0.58796628\n",
            "Iteration 170, loss = 0.58773167\n",
            "Iteration 171, loss = 0.58750522\n",
            "Iteration 172, loss = 0.58730681\n",
            "Iteration 173, loss = 0.58707169\n",
            "Iteration 174, loss = 0.58688823\n",
            "Iteration 175, loss = 0.58667755\n",
            "Iteration 176, loss = 0.58650037\n",
            "Iteration 177, loss = 0.58632151\n",
            "Iteration 178, loss = 0.58614904\n",
            "Iteration 179, loss = 0.58594676\n",
            "Iteration 180, loss = 0.58576109\n",
            "Iteration 181, loss = 0.58556065\n",
            "Iteration 182, loss = 0.58536427\n",
            "Iteration 183, loss = 0.58516704\n",
            "Iteration 184, loss = 0.58499852\n",
            "Iteration 185, loss = 0.58480437\n",
            "Iteration 186, loss = 0.58463367\n",
            "Iteration 187, loss = 0.58446392\n",
            "Iteration 188, loss = 0.58429171\n",
            "Iteration 189, loss = 0.58411291\n",
            "Iteration 190, loss = 0.58392177\n",
            "Iteration 191, loss = 0.58376057\n",
            "Iteration 192, loss = 0.58359527\n",
            "Iteration 193, loss = 0.58344437\n",
            "Iteration 194, loss = 0.58328742\n",
            "Iteration 195, loss = 0.58312929\n",
            "Iteration 196, loss = 0.58299545\n",
            "Iteration 197, loss = 0.58282952\n",
            "Iteration 198, loss = 0.58268235\n",
            "Iteration 199, loss = 0.58255022\n",
            "Iteration 200, loss = 0.58240461\n",
            "Iteration 201, loss = 0.58225259\n",
            "Iteration 202, loss = 0.58209694\n",
            "Iteration 203, loss = 0.58194859\n",
            "Iteration 204, loss = 0.58179331\n",
            "Iteration 205, loss = 0.58165482\n",
            "Iteration 206, loss = 0.58150300\n",
            "Iteration 207, loss = 0.58136715\n",
            "Iteration 208, loss = 0.58122821\n",
            "Iteration 209, loss = 0.58109609\n",
            "Iteration 210, loss = 0.58096826\n",
            "Iteration 211, loss = 0.58084505\n",
            "Iteration 212, loss = 0.58072347\n",
            "Iteration 213, loss = 0.58062101\n",
            "Iteration 214, loss = 0.58050200\n",
            "Iteration 215, loss = 0.58039067\n",
            "Iteration 216, loss = 0.58027815\n",
            "Iteration 217, loss = 0.58015330\n",
            "Iteration 218, loss = 0.58005698\n",
            "Iteration 219, loss = 0.57990958\n",
            "Iteration 220, loss = 0.57978412\n",
            "Iteration 221, loss = 0.57967417\n",
            "Iteration 222, loss = 0.57954154\n",
            "Iteration 223, loss = 0.57942960\n",
            "Iteration 224, loss = 0.57929940\n",
            "Iteration 225, loss = 0.57919567\n",
            "Iteration 226, loss = 0.57907179\n",
            "Iteration 227, loss = 0.57895476\n",
            "Iteration 228, loss = 0.57884911\n",
            "Iteration 229, loss = 0.57873998\n",
            "Iteration 230, loss = 0.57863027\n",
            "Iteration 231, loss = 0.57852063\n",
            "Iteration 232, loss = 0.57839477\n",
            "Iteration 233, loss = 0.57828151\n",
            "Iteration 234, loss = 0.57816391\n",
            "Iteration 235, loss = 0.57806889\n",
            "Iteration 236, loss = 0.57794964\n",
            "Iteration 237, loss = 0.57784650\n",
            "Iteration 238, loss = 0.57772169\n",
            "Iteration 239, loss = 0.57762182\n",
            "Iteration 240, loss = 0.57751411\n",
            "Iteration 241, loss = 0.57738948\n",
            "Iteration 242, loss = 0.57727978\n",
            "Iteration 243, loss = 0.57717374\n",
            "Iteration 244, loss = 0.57705313\n",
            "Iteration 245, loss = 0.57694283\n",
            "Iteration 246, loss = 0.57683689\n",
            "Iteration 247, loss = 0.57671677\n",
            "Iteration 248, loss = 0.57659224\n",
            "Iteration 249, loss = 0.57646954\n",
            "Iteration 250, loss = 0.57636323\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 250 and for layer number 3 : 0.6987500000000001\n",
            "Iteration 1, loss = 0.92831019\n",
            "Iteration 2, loss = 0.92628737\n",
            "Iteration 3, loss = 0.92292720\n",
            "Iteration 4, loss = 0.91890896\n",
            "Iteration 5, loss = 0.91406816\n",
            "Iteration 6, loss = 0.90896363\n",
            "Iteration 7, loss = 0.90386015\n",
            "Iteration 8, loss = 0.89884936\n",
            "Iteration 9, loss = 0.89376039\n",
            "Iteration 10, loss = 0.88868258\n",
            "Iteration 11, loss = 0.88381471\n",
            "Iteration 12, loss = 0.87878426\n",
            "Iteration 13, loss = 0.87398219\n",
            "Iteration 14, loss = 0.86928989\n",
            "Iteration 15, loss = 0.86471610\n",
            "Iteration 16, loss = 0.86053140\n",
            "Iteration 17, loss = 0.85612820\n",
            "Iteration 18, loss = 0.85197041\n",
            "Iteration 19, loss = 0.84782638\n",
            "Iteration 20, loss = 0.84387295\n",
            "Iteration 21, loss = 0.83992078\n",
            "Iteration 22, loss = 0.83601275\n",
            "Iteration 23, loss = 0.83220154\n",
            "Iteration 24, loss = 0.82811202\n",
            "Iteration 25, loss = 0.82449396\n",
            "Iteration 26, loss = 0.82082671\n",
            "Iteration 27, loss = 0.81721320\n",
            "Iteration 28, loss = 0.81380905\n",
            "Iteration 29, loss = 0.81040528\n",
            "Iteration 30, loss = 0.80705060\n",
            "Iteration 31, loss = 0.80392526\n",
            "Iteration 32, loss = 0.80063213\n",
            "Iteration 33, loss = 0.79760810\n",
            "Iteration 34, loss = 0.79459235\n",
            "Iteration 35, loss = 0.79159397\n",
            "Iteration 36, loss = 0.78886164\n",
            "Iteration 37, loss = 0.78586148\n",
            "Iteration 38, loss = 0.78317255\n",
            "Iteration 39, loss = 0.78022418\n",
            "Iteration 40, loss = 0.77762076\n",
            "Iteration 41, loss = 0.77495681\n",
            "Iteration 42, loss = 0.77229239\n",
            "Iteration 43, loss = 0.76974491\n",
            "Iteration 44, loss = 0.76720757\n",
            "Iteration 45, loss = 0.76456895\n",
            "Iteration 46, loss = 0.76198026\n",
            "Iteration 47, loss = 0.75960833\n",
            "Iteration 48, loss = 0.75721478\n",
            "Iteration 49, loss = 0.75488200\n",
            "Iteration 50, loss = 0.75248586\n",
            "Iteration 51, loss = 0.75018259\n",
            "Iteration 52, loss = 0.74800238\n",
            "Iteration 53, loss = 0.74586725\n",
            "Iteration 54, loss = 0.74362634\n",
            "Iteration 55, loss = 0.74151143\n",
            "Iteration 56, loss = 0.73950295\n",
            "Iteration 57, loss = 0.73741831\n",
            "Iteration 58, loss = 0.73543089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 59, loss = 0.73347890\n",
            "Iteration 60, loss = 0.73143214\n",
            "Iteration 61, loss = 0.72947052\n",
            "Iteration 62, loss = 0.72761557\n",
            "Iteration 63, loss = 0.72575659\n",
            "Iteration 64, loss = 0.72393461\n",
            "Iteration 65, loss = 0.72217066\n",
            "Iteration 66, loss = 0.72037809\n",
            "Iteration 67, loss = 0.71872244\n",
            "Iteration 68, loss = 0.71699545\n",
            "Iteration 69, loss = 0.71528424\n",
            "Iteration 70, loss = 0.71358358\n",
            "Iteration 71, loss = 0.71201015\n",
            "Iteration 72, loss = 0.71043121\n",
            "Iteration 73, loss = 0.70886096\n",
            "Iteration 74, loss = 0.70734836\n",
            "Iteration 75, loss = 0.70577004\n",
            "Iteration 76, loss = 0.70432856\n",
            "Iteration 77, loss = 0.70282172\n",
            "Iteration 78, loss = 0.70132546\n",
            "Iteration 79, loss = 0.69988901\n",
            "Iteration 80, loss = 0.69838408\n",
            "Iteration 81, loss = 0.69691519\n",
            "Iteration 82, loss = 0.69553924\n",
            "Iteration 83, loss = 0.69407292\n",
            "Iteration 84, loss = 0.69278138\n",
            "Iteration 85, loss = 0.69142090\n",
            "Iteration 86, loss = 0.69015005\n",
            "Iteration 87, loss = 0.68883737\n",
            "Iteration 88, loss = 0.68750556\n",
            "Iteration 89, loss = 0.68619651\n",
            "Iteration 90, loss = 0.68485066\n",
            "Iteration 91, loss = 0.68350643\n",
            "Iteration 92, loss = 0.68209430\n",
            "Iteration 93, loss = 0.68077022\n",
            "Iteration 94, loss = 0.67944943\n",
            "Iteration 95, loss = 0.67819217\n",
            "Iteration 96, loss = 0.67696762\n",
            "Iteration 97, loss = 0.67577008\n",
            "Iteration 98, loss = 0.67458090\n",
            "Iteration 99, loss = 0.67341100\n",
            "Iteration 100, loss = 0.67223428\n",
            "Iteration 101, loss = 0.67104506\n",
            "Iteration 102, loss = 0.66978585\n",
            "Iteration 103, loss = 0.66861227\n",
            "Iteration 104, loss = 0.66745578\n",
            "Iteration 105, loss = 0.66627850\n",
            "Iteration 106, loss = 0.66510940\n",
            "Iteration 107, loss = 0.66393676\n",
            "Iteration 108, loss = 0.66281068\n",
            "Iteration 109, loss = 0.66169601\n",
            "Iteration 110, loss = 0.66061268\n",
            "Iteration 111, loss = 0.65960111\n",
            "Iteration 112, loss = 0.65854483\n",
            "Iteration 113, loss = 0.65754644\n",
            "Iteration 114, loss = 0.65647858\n",
            "Iteration 115, loss = 0.65551663\n",
            "Iteration 116, loss = 0.65447314\n",
            "Iteration 117, loss = 0.65349882\n",
            "Iteration 118, loss = 0.65251628\n",
            "Iteration 119, loss = 0.65153519\n",
            "Iteration 120, loss = 0.65054167\n",
            "Iteration 121, loss = 0.64961018\n",
            "Iteration 122, loss = 0.64876723\n",
            "Iteration 123, loss = 0.64784156\n",
            "Iteration 124, loss = 0.64697288\n",
            "Iteration 125, loss = 0.64616527\n",
            "Iteration 126, loss = 0.64527555\n",
            "Iteration 127, loss = 0.64449520\n",
            "Iteration 128, loss = 0.64361690\n",
            "Iteration 129, loss = 0.64280606\n",
            "Iteration 130, loss = 0.64200862\n",
            "Iteration 131, loss = 0.64122482\n",
            "Iteration 132, loss = 0.64042905\n",
            "Iteration 133, loss = 0.63966933\n",
            "Iteration 134, loss = 0.63888969\n",
            "Iteration 135, loss = 0.63813226\n",
            "Iteration 136, loss = 0.63734726\n",
            "Iteration 137, loss = 0.63663371\n",
            "Iteration 138, loss = 0.63588696\n",
            "Iteration 139, loss = 0.63517280\n",
            "Iteration 140, loss = 0.63446451\n",
            "Iteration 141, loss = 0.63377240\n",
            "Iteration 142, loss = 0.63309239\n",
            "Iteration 143, loss = 0.63242575\n",
            "Iteration 144, loss = 0.63171326\n",
            "Iteration 145, loss = 0.63108449\n",
            "Iteration 146, loss = 0.63045197\n",
            "Iteration 147, loss = 0.62980498\n",
            "Iteration 148, loss = 0.62919895\n",
            "Iteration 149, loss = 0.62856276\n",
            "Iteration 150, loss = 0.62792757\n",
            "Iteration 151, loss = 0.62728295\n",
            "Iteration 152, loss = 0.62668567\n",
            "Iteration 153, loss = 0.62605320\n",
            "Iteration 154, loss = 0.62546216\n",
            "Iteration 155, loss = 0.62489327\n",
            "Iteration 156, loss = 0.62430820\n",
            "Iteration 157, loss = 0.62376765\n",
            "Iteration 158, loss = 0.62323552\n",
            "Iteration 159, loss = 0.62263925\n",
            "Iteration 160, loss = 0.62209814\n",
            "Iteration 161, loss = 0.62156367\n",
            "Iteration 162, loss = 0.62102885\n",
            "Iteration 163, loss = 0.62053098\n",
            "Iteration 164, loss = 0.62002699\n",
            "Iteration 165, loss = 0.61953419\n",
            "Iteration 166, loss = 0.61903164\n",
            "Iteration 167, loss = 0.61857634\n",
            "Iteration 168, loss = 0.61806699\n",
            "Iteration 169, loss = 0.61763124\n",
            "Iteration 170, loss = 0.61720855\n",
            "Iteration 171, loss = 0.61678803\n",
            "Iteration 172, loss = 0.61631638\n",
            "Iteration 173, loss = 0.61592079\n",
            "Iteration 174, loss = 0.61550139\n",
            "Iteration 175, loss = 0.61506420\n",
            "Iteration 176, loss = 0.61463952\n",
            "Iteration 177, loss = 0.61420145\n",
            "Iteration 178, loss = 0.61381566\n",
            "Iteration 179, loss = 0.61337462\n",
            "Iteration 180, loss = 0.61294324\n",
            "Iteration 181, loss = 0.61254902\n",
            "Iteration 182, loss = 0.61213546\n",
            "Iteration 183, loss = 0.61171487\n",
            "Iteration 184, loss = 0.61129479\n",
            "Iteration 185, loss = 0.61090305\n",
            "Iteration 186, loss = 0.61052582\n",
            "Iteration 187, loss = 0.61017439\n",
            "Iteration 188, loss = 0.60981990\n",
            "Iteration 189, loss = 0.60946219\n",
            "Iteration 190, loss = 0.60909915\n",
            "Iteration 191, loss = 0.60873686\n",
            "Iteration 192, loss = 0.60836788\n",
            "Iteration 193, loss = 0.60798240\n",
            "Iteration 194, loss = 0.60760311\n",
            "Iteration 195, loss = 0.60724688\n",
            "Iteration 196, loss = 0.60685989\n",
            "Iteration 197, loss = 0.60648148\n",
            "Iteration 198, loss = 0.60614240\n",
            "Iteration 199, loss = 0.60579278\n",
            "Iteration 200, loss = 0.60546356\n",
            "Iteration 201, loss = 0.60512308\n",
            "Iteration 202, loss = 0.60481478\n",
            "Iteration 203, loss = 0.60446804\n",
            "Iteration 204, loss = 0.60414488\n",
            "Iteration 205, loss = 0.60381406\n",
            "Iteration 206, loss = 0.60350503\n",
            "Iteration 207, loss = 0.60317931\n",
            "Iteration 208, loss = 0.60286872\n",
            "Iteration 209, loss = 0.60249313\n",
            "Iteration 210, loss = 0.60220029\n",
            "Iteration 211, loss = 0.60186909\n",
            "Iteration 212, loss = 0.60153794\n",
            "Iteration 213, loss = 0.60127706\n",
            "Iteration 214, loss = 0.60097145\n",
            "Iteration 215, loss = 0.60069561\n",
            "Iteration 216, loss = 0.60039383\n",
            "Iteration 217, loss = 0.60011349\n",
            "Iteration 218, loss = 0.59983407\n",
            "Iteration 219, loss = 0.59957665\n",
            "Iteration 220, loss = 0.59930464\n",
            "Iteration 221, loss = 0.59902358\n",
            "Iteration 222, loss = 0.59879527\n",
            "Iteration 223, loss = 0.59853669\n",
            "Iteration 224, loss = 0.59830101\n",
            "Iteration 225, loss = 0.59806566\n",
            "Iteration 226, loss = 0.59778795\n",
            "Iteration 227, loss = 0.59754545\n",
            "Iteration 228, loss = 0.59729467\n",
            "Iteration 229, loss = 0.59705610\n",
            "Iteration 230, loss = 0.59676890\n",
            "Iteration 231, loss = 0.59651646\n",
            "Iteration 232, loss = 0.59625137\n",
            "Iteration 233, loss = 0.59597970\n",
            "Iteration 234, loss = 0.59571133\n",
            "Iteration 235, loss = 0.59544198\n",
            "Iteration 236, loss = 0.59518819\n",
            "Iteration 237, loss = 0.59492306\n",
            "Iteration 238, loss = 0.59466901\n",
            "Iteration 239, loss = 0.59442144\n",
            "Iteration 240, loss = 0.59412862\n",
            "Iteration 241, loss = 0.59385211\n",
            "Iteration 242, loss = 0.59357935\n",
            "Iteration 243, loss = 0.59332509\n",
            "Iteration 244, loss = 0.59307999\n",
            "Iteration 245, loss = 0.59282931\n",
            "Iteration 246, loss = 0.59262396\n",
            "Iteration 247, loss = 0.59238381\n",
            "Iteration 248, loss = 0.59212741\n",
            "Iteration 249, loss = 0.59188052\n",
            "Iteration 250, loss = 0.59164886\n",
            "Iteration 1, loss = 0.92515262\n",
            "Iteration 2, loss = 0.92320793\n",
            "Iteration 3, loss = 0.91997489\n",
            "Iteration 4, loss = 0.91591886\n",
            "Iteration 5, loss = 0.91108617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6, loss = 0.90611197\n",
            "Iteration 7, loss = 0.90111358\n",
            "Iteration 8, loss = 0.89611580\n",
            "Iteration 9, loss = 0.89103097\n",
            "Iteration 10, loss = 0.88614356\n",
            "Iteration 11, loss = 0.88144205\n",
            "Iteration 12, loss = 0.87657599\n",
            "Iteration 13, loss = 0.87196161\n",
            "Iteration 14, loss = 0.86739565\n",
            "Iteration 15, loss = 0.86293846\n",
            "Iteration 16, loss = 0.85890079\n",
            "Iteration 17, loss = 0.85483203\n",
            "Iteration 18, loss = 0.85098669\n",
            "Iteration 19, loss = 0.84699066\n",
            "Iteration 20, loss = 0.84324236\n",
            "Iteration 21, loss = 0.83945760\n",
            "Iteration 22, loss = 0.83574260\n",
            "Iteration 23, loss = 0.83208773\n",
            "Iteration 24, loss = 0.82831715\n",
            "Iteration 25, loss = 0.82490285\n",
            "Iteration 26, loss = 0.82145077\n",
            "Iteration 27, loss = 0.81803640\n",
            "Iteration 28, loss = 0.81467422\n",
            "Iteration 29, loss = 0.81143074\n",
            "Iteration 30, loss = 0.80809690\n",
            "Iteration 31, loss = 0.80507070\n",
            "Iteration 32, loss = 0.80183450\n",
            "Iteration 33, loss = 0.79879721\n",
            "Iteration 34, loss = 0.79582125\n",
            "Iteration 35, loss = 0.79278224\n",
            "Iteration 36, loss = 0.78996639\n",
            "Iteration 37, loss = 0.78695614\n",
            "Iteration 38, loss = 0.78424656\n",
            "Iteration 39, loss = 0.78127393\n",
            "Iteration 40, loss = 0.77872840\n",
            "Iteration 41, loss = 0.77606929\n",
            "Iteration 42, loss = 0.77339733\n",
            "Iteration 43, loss = 0.77081091\n",
            "Iteration 44, loss = 0.76835674\n",
            "Iteration 45, loss = 0.76574743\n",
            "Iteration 46, loss = 0.76315724\n",
            "Iteration 47, loss = 0.76082949\n",
            "Iteration 48, loss = 0.75845520\n",
            "Iteration 49, loss = 0.75612422\n",
            "Iteration 50, loss = 0.75378236\n",
            "Iteration 51, loss = 0.75155630\n",
            "Iteration 52, loss = 0.74945850\n",
            "Iteration 53, loss = 0.74738251\n",
            "Iteration 54, loss = 0.74516311\n",
            "Iteration 55, loss = 0.74313444\n",
            "Iteration 56, loss = 0.74120140\n",
            "Iteration 57, loss = 0.73915571\n",
            "Iteration 58, loss = 0.73719056\n",
            "Iteration 59, loss = 0.73528087\n",
            "Iteration 60, loss = 0.73334510\n",
            "Iteration 61, loss = 0.73141029\n",
            "Iteration 62, loss = 0.72959204\n",
            "Iteration 63, loss = 0.72775738\n",
            "Iteration 64, loss = 0.72591830\n",
            "Iteration 65, loss = 0.72420142\n",
            "Iteration 66, loss = 0.72236917\n",
            "Iteration 67, loss = 0.72072695\n",
            "Iteration 68, loss = 0.71904202\n",
            "Iteration 69, loss = 0.71728188\n",
            "Iteration 70, loss = 0.71565896\n",
            "Iteration 71, loss = 0.71408007\n",
            "Iteration 72, loss = 0.71245632\n",
            "Iteration 73, loss = 0.71088491\n",
            "Iteration 74, loss = 0.70937593\n",
            "Iteration 75, loss = 0.70783125\n",
            "Iteration 76, loss = 0.70637332\n",
            "Iteration 77, loss = 0.70488077\n",
            "Iteration 78, loss = 0.70340466\n",
            "Iteration 79, loss = 0.70197001\n",
            "Iteration 80, loss = 0.70049560\n",
            "Iteration 81, loss = 0.69904607\n",
            "Iteration 82, loss = 0.69762145\n",
            "Iteration 83, loss = 0.69614817\n",
            "Iteration 84, loss = 0.69479063\n",
            "Iteration 85, loss = 0.69346291\n",
            "Iteration 86, loss = 0.69214622\n",
            "Iteration 87, loss = 0.69083577\n",
            "Iteration 88, loss = 0.68948877\n",
            "Iteration 89, loss = 0.68815146\n",
            "Iteration 90, loss = 0.68677016\n",
            "Iteration 91, loss = 0.68540869\n",
            "Iteration 92, loss = 0.68391370\n",
            "Iteration 93, loss = 0.68257332\n",
            "Iteration 94, loss = 0.68117831\n",
            "Iteration 95, loss = 0.67987551\n",
            "Iteration 96, loss = 0.67862677\n",
            "Iteration 97, loss = 0.67738780\n",
            "Iteration 98, loss = 0.67615795\n",
            "Iteration 99, loss = 0.67497811\n",
            "Iteration 100, loss = 0.67374746\n",
            "Iteration 101, loss = 0.67250444\n",
            "Iteration 102, loss = 0.67126693\n",
            "Iteration 103, loss = 0.67005472\n",
            "Iteration 104, loss = 0.66890725\n",
            "Iteration 105, loss = 0.66774064\n",
            "Iteration 106, loss = 0.66657045\n",
            "Iteration 107, loss = 0.66541036\n",
            "Iteration 108, loss = 0.66432764\n",
            "Iteration 109, loss = 0.66321036\n",
            "Iteration 110, loss = 0.66213977\n",
            "Iteration 111, loss = 0.66112772\n",
            "Iteration 112, loss = 0.66008692\n",
            "Iteration 113, loss = 0.65914036\n",
            "Iteration 114, loss = 0.65809409\n",
            "Iteration 115, loss = 0.65714780\n",
            "Iteration 116, loss = 0.65615028\n",
            "Iteration 117, loss = 0.65520116\n",
            "Iteration 118, loss = 0.65422912\n",
            "Iteration 119, loss = 0.65325035\n",
            "Iteration 120, loss = 0.65225538\n",
            "Iteration 121, loss = 0.65132736\n",
            "Iteration 122, loss = 0.65045346\n",
            "Iteration 123, loss = 0.64954977\n",
            "Iteration 124, loss = 0.64862332\n",
            "Iteration 125, loss = 0.64775884\n",
            "Iteration 126, loss = 0.64684982\n",
            "Iteration 127, loss = 0.64601374\n",
            "Iteration 128, loss = 0.64505299\n",
            "Iteration 129, loss = 0.64418821\n",
            "Iteration 130, loss = 0.64332865\n",
            "Iteration 131, loss = 0.64247441\n",
            "Iteration 132, loss = 0.64162688\n",
            "Iteration 133, loss = 0.64078769\n",
            "Iteration 134, loss = 0.63992572\n",
            "Iteration 135, loss = 0.63907326\n",
            "Iteration 136, loss = 0.63822019\n",
            "Iteration 137, loss = 0.63748852\n",
            "Iteration 138, loss = 0.63671692\n",
            "Iteration 139, loss = 0.63598547\n",
            "Iteration 140, loss = 0.63523103\n",
            "Iteration 141, loss = 0.63449165\n",
            "Iteration 142, loss = 0.63370574\n",
            "Iteration 143, loss = 0.63296622\n",
            "Iteration 144, loss = 0.63219230\n",
            "Iteration 145, loss = 0.63148231\n",
            "Iteration 146, loss = 0.63076723\n",
            "Iteration 147, loss = 0.63004646\n",
            "Iteration 148, loss = 0.62937075\n",
            "Iteration 149, loss = 0.62869006\n",
            "Iteration 150, loss = 0.62796728\n",
            "Iteration 151, loss = 0.62729451\n",
            "Iteration 152, loss = 0.62660678\n",
            "Iteration 153, loss = 0.62590347\n",
            "Iteration 154, loss = 0.62522243\n",
            "Iteration 155, loss = 0.62458056\n",
            "Iteration 156, loss = 0.62393185\n",
            "Iteration 157, loss = 0.62329807\n",
            "Iteration 158, loss = 0.62267699\n",
            "Iteration 159, loss = 0.62198637\n",
            "Iteration 160, loss = 0.62137726\n",
            "Iteration 161, loss = 0.62076873\n",
            "Iteration 162, loss = 0.62021435\n",
            "Iteration 163, loss = 0.61964814\n",
            "Iteration 164, loss = 0.61909061\n",
            "Iteration 165, loss = 0.61855327\n",
            "Iteration 166, loss = 0.61798790\n",
            "Iteration 167, loss = 0.61747390\n",
            "Iteration 168, loss = 0.61694193\n",
            "Iteration 169, loss = 0.61646207\n",
            "Iteration 170, loss = 0.61597248\n",
            "Iteration 171, loss = 0.61547669\n",
            "Iteration 172, loss = 0.61495934\n",
            "Iteration 173, loss = 0.61449388\n",
            "Iteration 174, loss = 0.61400510\n",
            "Iteration 175, loss = 0.61349986\n",
            "Iteration 176, loss = 0.61300698\n",
            "Iteration 177, loss = 0.61250294\n",
            "Iteration 178, loss = 0.61202343\n",
            "Iteration 179, loss = 0.61154230\n",
            "Iteration 180, loss = 0.61100797\n",
            "Iteration 181, loss = 0.61051376\n",
            "Iteration 182, loss = 0.61004297\n",
            "Iteration 183, loss = 0.60953382\n",
            "Iteration 184, loss = 0.60905911\n",
            "Iteration 185, loss = 0.60858798\n",
            "Iteration 186, loss = 0.60816841\n",
            "Iteration 187, loss = 0.60774487\n",
            "Iteration 188, loss = 0.60733655\n",
            "Iteration 189, loss = 0.60691087\n",
            "Iteration 190, loss = 0.60654567\n",
            "Iteration 191, loss = 0.60611222\n",
            "Iteration 192, loss = 0.60570740\n",
            "Iteration 193, loss = 0.60527708\n",
            "Iteration 194, loss = 0.60482520\n",
            "Iteration 195, loss = 0.60443089\n",
            "Iteration 196, loss = 0.60398392\n",
            "Iteration 197, loss = 0.60355145\n",
            "Iteration 198, loss = 0.60316406\n",
            "Iteration 199, loss = 0.60277230\n",
            "Iteration 200, loss = 0.60238668\n",
            "Iteration 201, loss = 0.60200217\n",
            "Iteration 202, loss = 0.60164905\n",
            "Iteration 203, loss = 0.60124687\n",
            "Iteration 204, loss = 0.60085640\n",
            "Iteration 205, loss = 0.60050969\n",
            "Iteration 206, loss = 0.60012514\n",
            "Iteration 207, loss = 0.59974262\n",
            "Iteration 208, loss = 0.59940881\n",
            "Iteration 209, loss = 0.59899667\n",
            "Iteration 210, loss = 0.59863865\n",
            "Iteration 211, loss = 0.59827191\n",
            "Iteration 212, loss = 0.59788018\n",
            "Iteration 213, loss = 0.59755993\n",
            "Iteration 214, loss = 0.59717499\n",
            "Iteration 215, loss = 0.59683766\n",
            "Iteration 216, loss = 0.59650548\n",
            "Iteration 217, loss = 0.59616901\n",
            "Iteration 218, loss = 0.59583579\n",
            "Iteration 219, loss = 0.59551462\n",
            "Iteration 220, loss = 0.59520169\n",
            "Iteration 221, loss = 0.59487193\n",
            "Iteration 222, loss = 0.59459523\n",
            "Iteration 223, loss = 0.59427384\n",
            "Iteration 224, loss = 0.59397116\n",
            "Iteration 225, loss = 0.59368763\n",
            "Iteration 226, loss = 0.59337822\n",
            "Iteration 227, loss = 0.59312559\n",
            "Iteration 228, loss = 0.59284670\n",
            "Iteration 229, loss = 0.59257281\n",
            "Iteration 230, loss = 0.59225131\n",
            "Iteration 231, loss = 0.59197153\n",
            "Iteration 232, loss = 0.59168469\n",
            "Iteration 233, loss = 0.59139934\n",
            "Iteration 234, loss = 0.59110668\n",
            "Iteration 235, loss = 0.59079372\n",
            "Iteration 236, loss = 0.59051359\n",
            "Iteration 237, loss = 0.59023456\n",
            "Iteration 238, loss = 0.58995929\n",
            "Iteration 239, loss = 0.58971277\n",
            "Iteration 240, loss = 0.58938640\n",
            "Iteration 241, loss = 0.58911708\n",
            "Iteration 242, loss = 0.58885655\n",
            "Iteration 243, loss = 0.58858203\n",
            "Iteration 244, loss = 0.58834648\n",
            "Iteration 245, loss = 0.58810249\n",
            "Iteration 246, loss = 0.58787599\n",
            "Iteration 247, loss = 0.58763223\n",
            "Iteration 248, loss = 0.58737213\n",
            "Iteration 249, loss = 0.58710227\n",
            "Iteration 250, loss = 0.58687766\n",
            "Iteration 1, loss = 0.93462281\n",
            "Iteration 2, loss = 0.93261224\n",
            "Iteration 3, loss = 0.92941834\n",
            "Iteration 4, loss = 0.92525033\n",
            "Iteration 5, loss = 0.92065853\n",
            "Iteration 6, loss = 0.91555772\n",
            "Iteration 7, loss = 0.91059613\n",
            "Iteration 8, loss = 0.90541505\n",
            "Iteration 9, loss = 0.90029749\n",
            "Iteration 10, loss = 0.89529951\n",
            "Iteration 11, loss = 0.89038858\n",
            "Iteration 12, loss = 0.88547267\n",
            "Iteration 13, loss = 0.88066014\n",
            "Iteration 14, loss = 0.87599931\n",
            "Iteration 15, loss = 0.87133466\n",
            "Iteration 16, loss = 0.86701607\n",
            "Iteration 17, loss = 0.86274708\n",
            "Iteration 18, loss = 0.85864838\n",
            "Iteration 19, loss = 0.85439727\n",
            "Iteration 20, loss = 0.85037709\n",
            "Iteration 21, loss = 0.84634247\n",
            "Iteration 22, loss = 0.84239937\n",
            "Iteration 23, loss = 0.83855448\n",
            "Iteration 24, loss = 0.83465813\n",
            "Iteration 25, loss = 0.83096674\n",
            "Iteration 26, loss = 0.82741978\n",
            "Iteration 27, loss = 0.82371122\n",
            "Iteration 28, loss = 0.82025754\n",
            "Iteration 29, loss = 0.81690640\n",
            "Iteration 30, loss = 0.81348565\n",
            "Iteration 31, loss = 0.81033127\n",
            "Iteration 32, loss = 0.80706458\n",
            "Iteration 33, loss = 0.80388785\n",
            "Iteration 34, loss = 0.80070719\n",
            "Iteration 35, loss = 0.79753268\n",
            "Iteration 36, loss = 0.79445505\n",
            "Iteration 37, loss = 0.79131373\n",
            "Iteration 38, loss = 0.78835925\n",
            "Iteration 39, loss = 0.78525734\n",
            "Iteration 40, loss = 0.78258735\n",
            "Iteration 41, loss = 0.77976841\n",
            "Iteration 42, loss = 0.77689181\n",
            "Iteration 43, loss = 0.77404346\n",
            "Iteration 44, loss = 0.77144905\n",
            "Iteration 45, loss = 0.76875241\n",
            "Iteration 46, loss = 0.76603951\n",
            "Iteration 47, loss = 0.76363139\n",
            "Iteration 48, loss = 0.76122007\n",
            "Iteration 49, loss = 0.75874285\n",
            "Iteration 50, loss = 0.75632330\n",
            "Iteration 51, loss = 0.75401154\n",
            "Iteration 52, loss = 0.75172788\n",
            "Iteration 53, loss = 0.74946183\n",
            "Iteration 54, loss = 0.74711655\n",
            "Iteration 55, loss = 0.74496158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 56, loss = 0.74289829\n",
            "Iteration 57, loss = 0.74075807\n",
            "Iteration 58, loss = 0.73864727\n",
            "Iteration 59, loss = 0.73658594\n",
            "Iteration 60, loss = 0.73447008\n",
            "Iteration 61, loss = 0.73240466\n",
            "Iteration 62, loss = 0.73039789\n",
            "Iteration 63, loss = 0.72845788\n",
            "Iteration 64, loss = 0.72653095\n",
            "Iteration 65, loss = 0.72471988\n",
            "Iteration 66, loss = 0.72273809\n",
            "Iteration 67, loss = 0.72088747\n",
            "Iteration 68, loss = 0.71908629\n",
            "Iteration 69, loss = 0.71713136\n",
            "Iteration 70, loss = 0.71527881\n",
            "Iteration 71, loss = 0.71355173\n",
            "Iteration 72, loss = 0.71174876\n",
            "Iteration 73, loss = 0.70999877\n",
            "Iteration 74, loss = 0.70831828\n",
            "Iteration 75, loss = 0.70659337\n",
            "Iteration 76, loss = 0.70499195\n",
            "Iteration 77, loss = 0.70337414\n",
            "Iteration 78, loss = 0.70177357\n",
            "Iteration 79, loss = 0.70025428\n",
            "Iteration 80, loss = 0.69861256\n",
            "Iteration 81, loss = 0.69707178\n",
            "Iteration 82, loss = 0.69552728\n",
            "Iteration 83, loss = 0.69393161\n",
            "Iteration 84, loss = 0.69241273\n",
            "Iteration 85, loss = 0.69095717\n",
            "Iteration 86, loss = 0.68948161\n",
            "Iteration 87, loss = 0.68804856\n",
            "Iteration 88, loss = 0.68658037\n",
            "Iteration 89, loss = 0.68510199\n",
            "Iteration 90, loss = 0.68363864\n",
            "Iteration 91, loss = 0.68220180\n",
            "Iteration 92, loss = 0.68063484\n",
            "Iteration 93, loss = 0.67918267\n",
            "Iteration 94, loss = 0.67773565\n",
            "Iteration 95, loss = 0.67630324\n",
            "Iteration 96, loss = 0.67493092\n",
            "Iteration 97, loss = 0.67360341\n",
            "Iteration 98, loss = 0.67227445\n",
            "Iteration 99, loss = 0.67098636\n",
            "Iteration 100, loss = 0.66966581\n",
            "Iteration 101, loss = 0.66833240\n",
            "Iteration 102, loss = 0.66705544\n",
            "Iteration 103, loss = 0.66575348\n",
            "Iteration 104, loss = 0.66452828\n",
            "Iteration 105, loss = 0.66327437\n",
            "Iteration 106, loss = 0.66199517\n",
            "Iteration 107, loss = 0.66074385\n",
            "Iteration 108, loss = 0.65955795\n",
            "Iteration 109, loss = 0.65835191\n",
            "Iteration 110, loss = 0.65721563\n",
            "Iteration 111, loss = 0.65614481\n",
            "Iteration 112, loss = 0.65496847\n",
            "Iteration 113, loss = 0.65389317\n",
            "Iteration 114, loss = 0.65273817\n",
            "Iteration 115, loss = 0.65168348\n",
            "Iteration 116, loss = 0.65059551\n",
            "Iteration 117, loss = 0.64957036\n",
            "Iteration 118, loss = 0.64856594\n",
            "Iteration 119, loss = 0.64753817\n",
            "Iteration 120, loss = 0.64651576\n",
            "Iteration 121, loss = 0.64553035\n",
            "Iteration 122, loss = 0.64455996\n",
            "Iteration 123, loss = 0.64351206\n",
            "Iteration 124, loss = 0.64252108\n",
            "Iteration 125, loss = 0.64161128\n",
            "Iteration 126, loss = 0.64068012\n",
            "Iteration 127, loss = 0.63985894\n",
            "Iteration 128, loss = 0.63893508\n",
            "Iteration 129, loss = 0.63808524\n",
            "Iteration 130, loss = 0.63725818\n",
            "Iteration 131, loss = 0.63641797\n",
            "Iteration 132, loss = 0.63561298\n",
            "Iteration 133, loss = 0.63477335\n",
            "Iteration 134, loss = 0.63391441\n",
            "Iteration 135, loss = 0.63304245\n",
            "Iteration 136, loss = 0.63216148\n",
            "Iteration 137, loss = 0.63136898\n",
            "Iteration 138, loss = 0.63057856\n",
            "Iteration 139, loss = 0.62978781\n",
            "Iteration 140, loss = 0.62900919\n",
            "Iteration 141, loss = 0.62822730\n",
            "Iteration 142, loss = 0.62742769\n",
            "Iteration 143, loss = 0.62666233\n",
            "Iteration 144, loss = 0.62583832\n",
            "Iteration 145, loss = 0.62506955\n",
            "Iteration 146, loss = 0.62430315\n",
            "Iteration 147, loss = 0.62354551\n",
            "Iteration 148, loss = 0.62284358\n",
            "Iteration 149, loss = 0.62213004\n",
            "Iteration 150, loss = 0.62138943\n",
            "Iteration 151, loss = 0.62070810\n",
            "Iteration 152, loss = 0.62003486\n",
            "Iteration 153, loss = 0.61933758\n",
            "Iteration 154, loss = 0.61866106\n",
            "Iteration 155, loss = 0.61806783\n",
            "Iteration 156, loss = 0.61742271\n",
            "Iteration 157, loss = 0.61680810\n",
            "Iteration 158, loss = 0.61624284\n",
            "Iteration 159, loss = 0.61561695\n",
            "Iteration 160, loss = 0.61504814\n",
            "Iteration 161, loss = 0.61447339\n",
            "Iteration 162, loss = 0.61393348\n",
            "Iteration 163, loss = 0.61338422\n",
            "Iteration 164, loss = 0.61282368\n",
            "Iteration 165, loss = 0.61227801\n",
            "Iteration 166, loss = 0.61174106\n",
            "Iteration 167, loss = 0.61121644\n",
            "Iteration 168, loss = 0.61069513\n",
            "Iteration 169, loss = 0.61022305\n",
            "Iteration 170, loss = 0.60973026\n",
            "Iteration 171, loss = 0.60925344\n",
            "Iteration 172, loss = 0.60876788\n",
            "Iteration 173, loss = 0.60831122\n",
            "Iteration 174, loss = 0.60782154\n",
            "Iteration 175, loss = 0.60732081\n",
            "Iteration 176, loss = 0.60681934\n",
            "Iteration 177, loss = 0.60630945\n",
            "Iteration 178, loss = 0.60582926\n",
            "Iteration 179, loss = 0.60531436\n",
            "Iteration 180, loss = 0.60478088\n",
            "Iteration 181, loss = 0.60425562\n",
            "Iteration 182, loss = 0.60377797\n",
            "Iteration 183, loss = 0.60325576\n",
            "Iteration 184, loss = 0.60278176\n",
            "Iteration 185, loss = 0.60229687\n",
            "Iteration 186, loss = 0.60185511\n",
            "Iteration 187, loss = 0.60139929\n",
            "Iteration 188, loss = 0.60096361\n",
            "Iteration 189, loss = 0.60053984\n",
            "Iteration 190, loss = 0.60015279\n",
            "Iteration 191, loss = 0.59970373\n",
            "Iteration 192, loss = 0.59931360\n",
            "Iteration 193, loss = 0.59887315\n",
            "Iteration 194, loss = 0.59841493\n",
            "Iteration 195, loss = 0.59799575\n",
            "Iteration 196, loss = 0.59756929\n",
            "Iteration 197, loss = 0.59715635\n",
            "Iteration 198, loss = 0.59678665\n",
            "Iteration 199, loss = 0.59639952\n",
            "Iteration 200, loss = 0.59605798\n",
            "Iteration 201, loss = 0.59567445\n",
            "Iteration 202, loss = 0.59529804\n",
            "Iteration 203, loss = 0.59492529\n",
            "Iteration 204, loss = 0.59452481\n",
            "Iteration 205, loss = 0.59415295\n",
            "Iteration 206, loss = 0.59374794\n",
            "Iteration 207, loss = 0.59336058\n",
            "Iteration 208, loss = 0.59303213\n",
            "Iteration 209, loss = 0.59263896\n",
            "Iteration 210, loss = 0.59229245\n",
            "Iteration 211, loss = 0.59192970\n",
            "Iteration 212, loss = 0.59155037\n",
            "Iteration 213, loss = 0.59121449\n",
            "Iteration 214, loss = 0.59083264\n",
            "Iteration 215, loss = 0.59048992\n",
            "Iteration 216, loss = 0.59014744\n",
            "Iteration 217, loss = 0.58980054\n",
            "Iteration 218, loss = 0.58946185\n",
            "Iteration 219, loss = 0.58913646\n",
            "Iteration 220, loss = 0.58878273\n",
            "Iteration 221, loss = 0.58844726\n",
            "Iteration 222, loss = 0.58813050\n",
            "Iteration 223, loss = 0.58777449\n",
            "Iteration 224, loss = 0.58745350\n",
            "Iteration 225, loss = 0.58713377\n",
            "Iteration 226, loss = 0.58680092\n",
            "Iteration 227, loss = 0.58649837\n",
            "Iteration 228, loss = 0.58620669\n",
            "Iteration 229, loss = 0.58592622\n",
            "Iteration 230, loss = 0.58557289\n",
            "Iteration 231, loss = 0.58526589\n",
            "Iteration 232, loss = 0.58496729\n",
            "Iteration 233, loss = 0.58464114\n",
            "Iteration 234, loss = 0.58433769\n",
            "Iteration 235, loss = 0.58402419\n",
            "Iteration 236, loss = 0.58374427\n",
            "Iteration 237, loss = 0.58344246\n",
            "Iteration 238, loss = 0.58317028\n",
            "Iteration 239, loss = 0.58289752\n",
            "Iteration 240, loss = 0.58257876\n",
            "Iteration 241, loss = 0.58229895\n",
            "Iteration 242, loss = 0.58204002\n",
            "Iteration 243, loss = 0.58175113\n",
            "Iteration 244, loss = 0.58151711\n",
            "Iteration 245, loss = 0.58126643\n",
            "Iteration 246, loss = 0.58102384\n",
            "Iteration 247, loss = 0.58077152\n",
            "Iteration 248, loss = 0.58050719\n",
            "Iteration 249, loss = 0.58025206\n",
            "Iteration 250, loss = 0.58001426\n",
            "Iteration 1, loss = 0.93310249\n",
            "Iteration 2, loss = 0.93101092\n",
            "Iteration 3, loss = 0.92770703\n",
            "Iteration 4, loss = 0.92348094\n",
            "Iteration 5, loss = 0.91870634\n",
            "Iteration 6, loss = 0.91350854\n",
            "Iteration 7, loss = 0.90855626\n",
            "Iteration 8, loss = 0.90333343\n",
            "Iteration 9, loss = 0.89816244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10, loss = 0.89306903\n",
            "Iteration 11, loss = 0.88797547\n",
            "Iteration 12, loss = 0.88302867\n",
            "Iteration 13, loss = 0.87830591\n",
            "Iteration 14, loss = 0.87363722\n",
            "Iteration 15, loss = 0.86901748\n",
            "Iteration 16, loss = 0.86479539\n",
            "Iteration 17, loss = 0.86038833\n",
            "Iteration 18, loss = 0.85639915\n",
            "Iteration 19, loss = 0.85227054\n",
            "Iteration 20, loss = 0.84834947\n",
            "Iteration 21, loss = 0.84451866\n",
            "Iteration 22, loss = 0.84076112\n",
            "Iteration 23, loss = 0.83717459\n",
            "Iteration 24, loss = 0.83341012\n",
            "Iteration 25, loss = 0.82979470\n",
            "Iteration 26, loss = 0.82628064\n",
            "Iteration 27, loss = 0.82271049\n",
            "Iteration 28, loss = 0.81929932\n",
            "Iteration 29, loss = 0.81605612\n",
            "Iteration 30, loss = 0.81276286\n",
            "Iteration 31, loss = 0.80960758\n",
            "Iteration 32, loss = 0.80651049\n",
            "Iteration 33, loss = 0.80331972\n",
            "Iteration 34, loss = 0.80023102\n",
            "Iteration 35, loss = 0.79699869\n",
            "Iteration 36, loss = 0.79400137\n",
            "Iteration 37, loss = 0.79097394\n",
            "Iteration 38, loss = 0.78805094\n",
            "Iteration 39, loss = 0.78511982\n",
            "Iteration 40, loss = 0.78247517\n",
            "Iteration 41, loss = 0.77979332\n",
            "Iteration 42, loss = 0.77699418\n",
            "Iteration 43, loss = 0.77429562\n",
            "Iteration 44, loss = 0.77172579\n",
            "Iteration 45, loss = 0.76916050\n",
            "Iteration 46, loss = 0.76647297\n",
            "Iteration 47, loss = 0.76409478\n",
            "Iteration 48, loss = 0.76166267\n",
            "Iteration 49, loss = 0.75916081\n",
            "Iteration 50, loss = 0.75678157\n",
            "Iteration 51, loss = 0.75448138\n",
            "Iteration 52, loss = 0.75215647\n",
            "Iteration 53, loss = 0.74984933\n",
            "Iteration 54, loss = 0.74751943\n",
            "Iteration 55, loss = 0.74539031\n",
            "Iteration 56, loss = 0.74335546\n",
            "Iteration 57, loss = 0.74121902\n",
            "Iteration 58, loss = 0.73912598\n",
            "Iteration 59, loss = 0.73715404\n",
            "Iteration 60, loss = 0.73507830\n",
            "Iteration 61, loss = 0.73318207\n",
            "Iteration 62, loss = 0.73126676\n",
            "Iteration 63, loss = 0.72939938\n",
            "Iteration 64, loss = 0.72756473\n",
            "Iteration 65, loss = 0.72579634\n",
            "Iteration 66, loss = 0.72394008\n",
            "Iteration 67, loss = 0.72217471\n",
            "Iteration 68, loss = 0.72043589\n",
            "Iteration 69, loss = 0.71850239\n",
            "Iteration 70, loss = 0.71671073\n",
            "Iteration 71, loss = 0.71501354\n",
            "Iteration 72, loss = 0.71325594\n",
            "Iteration 73, loss = 0.71157480\n",
            "Iteration 74, loss = 0.70997376\n",
            "Iteration 75, loss = 0.70834349\n",
            "Iteration 76, loss = 0.70678517\n",
            "Iteration 77, loss = 0.70527284\n",
            "Iteration 78, loss = 0.70373065\n",
            "Iteration 79, loss = 0.70226150\n",
            "Iteration 80, loss = 0.70070864\n",
            "Iteration 81, loss = 0.69912475\n",
            "Iteration 82, loss = 0.69757835\n",
            "Iteration 83, loss = 0.69597960\n",
            "Iteration 84, loss = 0.69442451\n",
            "Iteration 85, loss = 0.69299803\n",
            "Iteration 86, loss = 0.69155555\n",
            "Iteration 87, loss = 0.69017218\n",
            "Iteration 88, loss = 0.68882177\n",
            "Iteration 89, loss = 0.68743527\n",
            "Iteration 90, loss = 0.68603636\n",
            "Iteration 91, loss = 0.68469807\n",
            "Iteration 92, loss = 0.68322054\n",
            "Iteration 93, loss = 0.68188267\n",
            "Iteration 94, loss = 0.68056752\n",
            "Iteration 95, loss = 0.67922529\n",
            "Iteration 96, loss = 0.67794626\n",
            "Iteration 97, loss = 0.67673420\n",
            "Iteration 98, loss = 0.67547574\n",
            "Iteration 99, loss = 0.67423622\n",
            "Iteration 100, loss = 0.67305365\n",
            "Iteration 101, loss = 0.67184034\n",
            "Iteration 102, loss = 0.67071311\n",
            "Iteration 103, loss = 0.66952001\n",
            "Iteration 104, loss = 0.66837398\n",
            "Iteration 105, loss = 0.66719879\n",
            "Iteration 106, loss = 0.66603831\n",
            "Iteration 107, loss = 0.66481475\n",
            "Iteration 108, loss = 0.66370231\n",
            "Iteration 109, loss = 0.66253770\n",
            "Iteration 110, loss = 0.66148177\n",
            "Iteration 111, loss = 0.66037129\n",
            "Iteration 112, loss = 0.65928578\n",
            "Iteration 113, loss = 0.65824376\n",
            "Iteration 114, loss = 0.65714020\n",
            "Iteration 115, loss = 0.65610681\n",
            "Iteration 116, loss = 0.65510969\n",
            "Iteration 117, loss = 0.65415203\n",
            "Iteration 118, loss = 0.65322297\n",
            "Iteration 119, loss = 0.65223789\n",
            "Iteration 120, loss = 0.65126999\n",
            "Iteration 121, loss = 0.65039327\n",
            "Iteration 122, loss = 0.64948728\n",
            "Iteration 123, loss = 0.64856928\n",
            "Iteration 124, loss = 0.64763909\n",
            "Iteration 125, loss = 0.64676217\n",
            "Iteration 126, loss = 0.64587163\n",
            "Iteration 127, loss = 0.64508020\n",
            "Iteration 128, loss = 0.64418209\n",
            "Iteration 129, loss = 0.64334651\n",
            "Iteration 130, loss = 0.64251319\n",
            "Iteration 131, loss = 0.64166131\n",
            "Iteration 132, loss = 0.64084183\n",
            "Iteration 133, loss = 0.64000666\n",
            "Iteration 134, loss = 0.63915158\n",
            "Iteration 135, loss = 0.63828495\n",
            "Iteration 136, loss = 0.63743946\n",
            "Iteration 137, loss = 0.63664379\n",
            "Iteration 138, loss = 0.63587903\n",
            "Iteration 139, loss = 0.63510284\n",
            "Iteration 140, loss = 0.63436477\n",
            "Iteration 141, loss = 0.63362194\n",
            "Iteration 142, loss = 0.63288549\n",
            "Iteration 143, loss = 0.63218612\n",
            "Iteration 144, loss = 0.63138964\n",
            "Iteration 145, loss = 0.63070869\n",
            "Iteration 146, loss = 0.62998211\n",
            "Iteration 147, loss = 0.62929853\n",
            "Iteration 148, loss = 0.62865177\n",
            "Iteration 149, loss = 0.62799051\n",
            "Iteration 150, loss = 0.62730519\n",
            "Iteration 151, loss = 0.62669492\n",
            "Iteration 152, loss = 0.62603760\n",
            "Iteration 153, loss = 0.62537564\n",
            "Iteration 154, loss = 0.62473405\n",
            "Iteration 155, loss = 0.62414264\n",
            "Iteration 156, loss = 0.62350340\n",
            "Iteration 157, loss = 0.62289037\n",
            "Iteration 158, loss = 0.62231641\n",
            "Iteration 159, loss = 0.62174108\n",
            "Iteration 160, loss = 0.62120407\n",
            "Iteration 161, loss = 0.62066035\n",
            "Iteration 162, loss = 0.62013935\n",
            "Iteration 163, loss = 0.61959465\n",
            "Iteration 164, loss = 0.61906516\n",
            "Iteration 165, loss = 0.61850232\n",
            "Iteration 166, loss = 0.61796057\n",
            "Iteration 167, loss = 0.61744452\n",
            "Iteration 168, loss = 0.61693136\n",
            "Iteration 169, loss = 0.61646205\n",
            "Iteration 170, loss = 0.61597786\n",
            "Iteration 171, loss = 0.61553054\n",
            "Iteration 172, loss = 0.61508273\n",
            "Iteration 173, loss = 0.61465326\n",
            "Iteration 174, loss = 0.61419772\n",
            "Iteration 175, loss = 0.61374751\n",
            "Iteration 176, loss = 0.61327769\n",
            "Iteration 177, loss = 0.61283197\n",
            "Iteration 178, loss = 0.61237918\n",
            "Iteration 179, loss = 0.61192631\n",
            "Iteration 180, loss = 0.61142976\n",
            "Iteration 181, loss = 0.61096015\n",
            "Iteration 182, loss = 0.61051523\n",
            "Iteration 183, loss = 0.61005702\n",
            "Iteration 184, loss = 0.60960340\n",
            "Iteration 185, loss = 0.60918261\n",
            "Iteration 186, loss = 0.60877521\n",
            "Iteration 187, loss = 0.60837667\n",
            "Iteration 188, loss = 0.60799180\n",
            "Iteration 189, loss = 0.60762399\n",
            "Iteration 190, loss = 0.60728135\n",
            "Iteration 191, loss = 0.60689327\n",
            "Iteration 192, loss = 0.60654312\n",
            "Iteration 193, loss = 0.60616760\n",
            "Iteration 194, loss = 0.60578300\n",
            "Iteration 195, loss = 0.60541803\n",
            "Iteration 196, loss = 0.60505822\n",
            "Iteration 197, loss = 0.60472181\n",
            "Iteration 198, loss = 0.60435601\n",
            "Iteration 199, loss = 0.60402445\n",
            "Iteration 200, loss = 0.60371389\n",
            "Iteration 201, loss = 0.60335020\n",
            "Iteration 202, loss = 0.60300005\n",
            "Iteration 203, loss = 0.60264644\n",
            "Iteration 204, loss = 0.60226073\n",
            "Iteration 205, loss = 0.60190212\n",
            "Iteration 206, loss = 0.60152442\n",
            "Iteration 207, loss = 0.60114677\n",
            "Iteration 208, loss = 0.60081300\n",
            "Iteration 209, loss = 0.60043416\n",
            "Iteration 210, loss = 0.60009942\n",
            "Iteration 211, loss = 0.59975300\n",
            "Iteration 212, loss = 0.59940765\n",
            "Iteration 213, loss = 0.59905708\n",
            "Iteration 214, loss = 0.59870200\n",
            "Iteration 215, loss = 0.59836752\n",
            "Iteration 216, loss = 0.59806000\n",
            "Iteration 217, loss = 0.59773913\n",
            "Iteration 218, loss = 0.59741148\n",
            "Iteration 219, loss = 0.59710224\n",
            "Iteration 220, loss = 0.59678618\n",
            "Iteration 221, loss = 0.59645936\n",
            "Iteration 222, loss = 0.59613489\n",
            "Iteration 223, loss = 0.59579875\n",
            "Iteration 224, loss = 0.59548192\n",
            "Iteration 225, loss = 0.59518877\n",
            "Iteration 226, loss = 0.59489289\n",
            "Iteration 227, loss = 0.59459119\n",
            "Iteration 228, loss = 0.59431780\n",
            "Iteration 229, loss = 0.59404019\n",
            "Iteration 230, loss = 0.59370961\n",
            "Iteration 231, loss = 0.59341933\n",
            "Iteration 232, loss = 0.59313706\n",
            "Iteration 233, loss = 0.59280537\n",
            "Iteration 234, loss = 0.59250600\n",
            "Iteration 235, loss = 0.59219187\n",
            "Iteration 236, loss = 0.59192268\n",
            "Iteration 237, loss = 0.59160218\n",
            "Iteration 238, loss = 0.59131715\n",
            "Iteration 239, loss = 0.59104956\n",
            "Iteration 240, loss = 0.59073027\n",
            "Iteration 241, loss = 0.59047873\n",
            "Iteration 242, loss = 0.59018584\n",
            "Iteration 243, loss = 0.58989161\n",
            "Iteration 244, loss = 0.58963684\n",
            "Iteration 245, loss = 0.58936643\n",
            "Iteration 246, loss = 0.58910464\n",
            "Iteration 247, loss = 0.58884971\n",
            "Iteration 248, loss = 0.58858402\n",
            "Iteration 249, loss = 0.58831647\n",
            "Iteration 250, loss = 0.58809075\n",
            "Iteration 1, loss = 0.93946865\n",
            "Iteration 2, loss = 0.93734548\n",
            "Iteration 3, loss = 0.93396022\n",
            "Iteration 4, loss = 0.92974495\n",
            "Iteration 5, loss = 0.92508441\n",
            "Iteration 6, loss = 0.92006070\n",
            "Iteration 7, loss = 0.91518496\n",
            "Iteration 8, loss = 0.91020915\n",
            "Iteration 9, loss = 0.90529195\n",
            "Iteration 10, loss = 0.90050938\n",
            "Iteration 11, loss = 0.89552786\n",
            "Iteration 12, loss = 0.89092624\n",
            "Iteration 13, loss = 0.88627671\n",
            "Iteration 14, loss = 0.88176332\n",
            "Iteration 15, loss = 0.87748747\n",
            "Iteration 16, loss = 0.87338356\n",
            "Iteration 17, loss = 0.86932619\n",
            "Iteration 18, loss = 0.86557100\n",
            "Iteration 19, loss = 0.86175953\n",
            "Iteration 20, loss = 0.85810708\n",
            "Iteration 21, loss = 0.85457487\n",
            "Iteration 22, loss = 0.85096897\n",
            "Iteration 23, loss = 0.84761148\n",
            "Iteration 24, loss = 0.84404867\n",
            "Iteration 25, loss = 0.84066128\n",
            "Iteration 26, loss = 0.83730958\n",
            "Iteration 27, loss = 0.83388757\n",
            "Iteration 28, loss = 0.83057501\n",
            "Iteration 29, loss = 0.82738517\n",
            "Iteration 30, loss = 0.82415322\n",
            "Iteration 31, loss = 0.82106702\n",
            "Iteration 32, loss = 0.81815273\n",
            "Iteration 33, loss = 0.81509467\n",
            "Iteration 34, loss = 0.81215013\n",
            "Iteration 35, loss = 0.80906566\n",
            "Iteration 36, loss = 0.80619125\n",
            "Iteration 37, loss = 0.80336983\n",
            "Iteration 38, loss = 0.80049635\n",
            "Iteration 39, loss = 0.79772338\n",
            "Iteration 40, loss = 0.79507950\n",
            "Iteration 41, loss = 0.79258288\n",
            "Iteration 42, loss = 0.78995614\n",
            "Iteration 43, loss = 0.78747175\n",
            "Iteration 44, loss = 0.78509049\n",
            "Iteration 45, loss = 0.78279095\n",
            "Iteration 46, loss = 0.78030914\n",
            "Iteration 47, loss = 0.77804290\n",
            "Iteration 48, loss = 0.77571093\n",
            "Iteration 49, loss = 0.77337670\n",
            "Iteration 50, loss = 0.77119120\n",
            "Iteration 51, loss = 0.76906050\n",
            "Iteration 52, loss = 0.76683251\n",
            "Iteration 53, loss = 0.76474393\n",
            "Iteration 54, loss = 0.76252897\n",
            "Iteration 55, loss = 0.76055234\n",
            "Iteration 56, loss = 0.75863057\n",
            "Iteration 57, loss = 0.75661412\n",
            "Iteration 58, loss = 0.75460319\n",
            "Iteration 59, loss = 0.75272004\n",
            "Iteration 60, loss = 0.75074771\n",
            "Iteration 61, loss = 0.74888193\n",
            "Iteration 62, loss = 0.74706429\n",
            "Iteration 63, loss = 0.74522519\n",
            "Iteration 64, loss = 0.74342654\n",
            "Iteration 65, loss = 0.74166896\n",
            "Iteration 66, loss = 0.73980177\n",
            "Iteration 67, loss = 0.73797811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 68, loss = 0.73626208\n",
            "Iteration 69, loss = 0.73436834\n",
            "Iteration 70, loss = 0.73252826\n",
            "Iteration 71, loss = 0.73089009\n",
            "Iteration 72, loss = 0.72915676\n",
            "Iteration 73, loss = 0.72752511\n",
            "Iteration 74, loss = 0.72591320\n",
            "Iteration 75, loss = 0.72434095\n",
            "Iteration 76, loss = 0.72284166\n",
            "Iteration 77, loss = 0.72136264\n",
            "Iteration 78, loss = 0.71987881\n",
            "Iteration 79, loss = 0.71844868\n",
            "Iteration 80, loss = 0.71699466\n",
            "Iteration 81, loss = 0.71551285\n",
            "Iteration 82, loss = 0.71406031\n",
            "Iteration 83, loss = 0.71258023\n",
            "Iteration 84, loss = 0.71109177\n",
            "Iteration 85, loss = 0.70973876\n",
            "Iteration 86, loss = 0.70839930\n",
            "Iteration 87, loss = 0.70708242\n",
            "Iteration 88, loss = 0.70577438\n",
            "Iteration 89, loss = 0.70451113\n",
            "Iteration 90, loss = 0.70322118\n",
            "Iteration 91, loss = 0.70202655\n",
            "Iteration 92, loss = 0.70078451\n",
            "Iteration 93, loss = 0.69961631\n",
            "Iteration 94, loss = 0.69844678\n",
            "Iteration 95, loss = 0.69724830\n",
            "Iteration 96, loss = 0.69613427\n",
            "Iteration 97, loss = 0.69501374\n",
            "Iteration 98, loss = 0.69384144\n",
            "Iteration 99, loss = 0.69264942\n",
            "Iteration 100, loss = 0.69152324\n",
            "Iteration 101, loss = 0.69032139\n",
            "Iteration 102, loss = 0.68921963\n",
            "Iteration 103, loss = 0.68806356\n",
            "Iteration 104, loss = 0.68694547\n",
            "Iteration 105, loss = 0.68580049\n",
            "Iteration 106, loss = 0.68467709\n",
            "Iteration 107, loss = 0.68351202\n",
            "Iteration 108, loss = 0.68244297\n",
            "Iteration 109, loss = 0.68129337\n",
            "Iteration 110, loss = 0.68018769\n",
            "Iteration 111, loss = 0.67911323\n",
            "Iteration 112, loss = 0.67800563\n",
            "Iteration 113, loss = 0.67688688\n",
            "Iteration 114, loss = 0.67577377\n",
            "Iteration 115, loss = 0.67469987\n",
            "Iteration 116, loss = 0.67365680\n",
            "Iteration 117, loss = 0.67266171\n",
            "Iteration 118, loss = 0.67166519\n",
            "Iteration 119, loss = 0.67065051\n",
            "Iteration 120, loss = 0.66964606\n",
            "Iteration 121, loss = 0.66875745\n",
            "Iteration 122, loss = 0.66785397\n",
            "Iteration 123, loss = 0.66691974\n",
            "Iteration 124, loss = 0.66597774\n",
            "Iteration 125, loss = 0.66508293\n",
            "Iteration 126, loss = 0.66416539\n",
            "Iteration 127, loss = 0.66333100\n",
            "Iteration 128, loss = 0.66245092\n",
            "Iteration 129, loss = 0.66161204\n",
            "Iteration 130, loss = 0.66077100\n",
            "Iteration 131, loss = 0.65992557\n",
            "Iteration 132, loss = 0.65910868\n",
            "Iteration 133, loss = 0.65826914\n",
            "Iteration 134, loss = 0.65743825\n",
            "Iteration 135, loss = 0.65660546\n",
            "Iteration 136, loss = 0.65580924\n",
            "Iteration 137, loss = 0.65503717\n",
            "Iteration 138, loss = 0.65433598\n",
            "Iteration 139, loss = 0.65356897\n",
            "Iteration 140, loss = 0.65286764\n",
            "Iteration 141, loss = 0.65214869\n",
            "Iteration 142, loss = 0.65143177\n",
            "Iteration 143, loss = 0.65075158\n",
            "Iteration 144, loss = 0.64999049\n",
            "Iteration 145, loss = 0.64932034\n",
            "Iteration 146, loss = 0.64861187\n",
            "Iteration 147, loss = 0.64793553\n",
            "Iteration 148, loss = 0.64725881\n",
            "Iteration 149, loss = 0.64656754\n",
            "Iteration 150, loss = 0.64585344\n",
            "Iteration 151, loss = 0.64521098\n",
            "Iteration 152, loss = 0.64453091\n",
            "Iteration 153, loss = 0.64387464\n",
            "Iteration 154, loss = 0.64322345\n",
            "Iteration 155, loss = 0.64257659\n",
            "Iteration 156, loss = 0.64193601\n",
            "Iteration 157, loss = 0.64126137\n",
            "Iteration 158, loss = 0.64067131\n",
            "Iteration 159, loss = 0.64006415\n",
            "Iteration 160, loss = 0.63951919\n",
            "Iteration 161, loss = 0.63896076\n",
            "Iteration 162, loss = 0.63843980\n",
            "Iteration 163, loss = 0.63790762\n",
            "Iteration 164, loss = 0.63737248\n",
            "Iteration 165, loss = 0.63682077\n",
            "Iteration 166, loss = 0.63629540\n",
            "Iteration 167, loss = 0.63578067\n",
            "Iteration 168, loss = 0.63528664\n",
            "Iteration 169, loss = 0.63478826\n",
            "Iteration 170, loss = 0.63429989\n",
            "Iteration 171, loss = 0.63383149\n",
            "Iteration 172, loss = 0.63337751\n",
            "Iteration 173, loss = 0.63292391\n",
            "Iteration 174, loss = 0.63245834\n",
            "Iteration 175, loss = 0.63198994\n",
            "Iteration 176, loss = 0.63152790\n",
            "Iteration 177, loss = 0.63106858\n",
            "Iteration 178, loss = 0.63058891\n",
            "Iteration 179, loss = 0.63013966\n",
            "Iteration 180, loss = 0.62966603\n",
            "Iteration 181, loss = 0.62919883\n",
            "Iteration 182, loss = 0.62875810\n",
            "Iteration 183, loss = 0.62828319\n",
            "Iteration 184, loss = 0.62782626\n",
            "Iteration 185, loss = 0.62740296\n",
            "Iteration 186, loss = 0.62697229\n",
            "Iteration 187, loss = 0.62652175\n",
            "Iteration 188, loss = 0.62612073\n",
            "Iteration 189, loss = 0.62573812\n",
            "Iteration 190, loss = 0.62537260\n",
            "Iteration 191, loss = 0.62496575\n",
            "Iteration 192, loss = 0.62457609\n",
            "Iteration 193, loss = 0.62417561\n",
            "Iteration 194, loss = 0.62377016\n",
            "Iteration 195, loss = 0.62335560\n",
            "Iteration 196, loss = 0.62296276\n",
            "Iteration 197, loss = 0.62257833\n",
            "Iteration 198, loss = 0.62217987\n",
            "Iteration 199, loss = 0.62179790\n",
            "Iteration 200, loss = 0.62146252\n",
            "Iteration 201, loss = 0.62107398\n",
            "Iteration 202, loss = 0.62067985\n",
            "Iteration 203, loss = 0.62031356\n",
            "Iteration 204, loss = 0.61989937\n",
            "Iteration 205, loss = 0.61951087\n",
            "Iteration 206, loss = 0.61909107\n",
            "Iteration 207, loss = 0.61870415\n",
            "Iteration 208, loss = 0.61835966\n",
            "Iteration 209, loss = 0.61799803\n",
            "Iteration 210, loss = 0.61765772\n",
            "Iteration 211, loss = 0.61731273\n",
            "Iteration 212, loss = 0.61699533\n",
            "Iteration 213, loss = 0.61667297\n",
            "Iteration 214, loss = 0.61635334\n",
            "Iteration 215, loss = 0.61603638\n",
            "Iteration 216, loss = 0.61574314\n",
            "Iteration 217, loss = 0.61543664\n",
            "Iteration 218, loss = 0.61515517\n",
            "Iteration 219, loss = 0.61483175\n",
            "Iteration 220, loss = 0.61453806\n",
            "Iteration 221, loss = 0.61424154\n",
            "Iteration 222, loss = 0.61394180\n",
            "Iteration 223, loss = 0.61365148\n",
            "Iteration 224, loss = 0.61336238\n",
            "Iteration 225, loss = 0.61309941\n",
            "Iteration 226, loss = 0.61283699\n",
            "Iteration 227, loss = 0.61255641\n",
            "Iteration 228, loss = 0.61231490\n",
            "Iteration 229, loss = 0.61204464\n",
            "Iteration 230, loss = 0.61175129\n",
            "Iteration 231, loss = 0.61146645\n",
            "Iteration 232, loss = 0.61120710\n",
            "Iteration 233, loss = 0.61092975\n",
            "Iteration 234, loss = 0.61064674\n",
            "Iteration 235, loss = 0.61035656\n",
            "Iteration 236, loss = 0.61008881\n",
            "Iteration 237, loss = 0.60980277\n",
            "Iteration 238, loss = 0.60952111\n",
            "Iteration 239, loss = 0.60926111\n",
            "Iteration 240, loss = 0.60896687\n",
            "Iteration 241, loss = 0.60868671\n",
            "Iteration 242, loss = 0.60839248\n",
            "Iteration 243, loss = 0.60808778\n",
            "Iteration 244, loss = 0.60781169\n",
            "Iteration 245, loss = 0.60751737\n",
            "Iteration 246, loss = 0.60724026\n",
            "Iteration 247, loss = 0.60696029\n",
            "Iteration 248, loss = 0.60669595\n",
            "Iteration 249, loss = 0.60641909\n",
            "Iteration 250, loss = 0.60616891\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 250 and for layer number 4 : 0.68125\n",
            "Iteration 1, loss = 0.88791930\n",
            "Iteration 2, loss = 0.88478701\n",
            "Iteration 3, loss = 0.87977954\n",
            "Iteration 4, loss = 0.87382746\n",
            "Iteration 5, loss = 0.86693408\n",
            "Iteration 6, loss = 0.85991963\n",
            "Iteration 7, loss = 0.85349317\n",
            "Iteration 8, loss = 0.84609686\n",
            "Iteration 9, loss = 0.83903239\n",
            "Iteration 10, loss = 0.83199676\n",
            "Iteration 11, loss = 0.82476252\n",
            "Iteration 12, loss = 0.81799272\n",
            "Iteration 13, loss = 0.81153730\n",
            "Iteration 14, loss = 0.80501491\n",
            "Iteration 15, loss = 0.79881692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 16, loss = 0.79283969\n",
            "Iteration 17, loss = 0.78704952\n",
            "Iteration 18, loss = 0.78142027\n",
            "Iteration 19, loss = 0.77612815\n",
            "Iteration 20, loss = 0.77058668\n",
            "Iteration 21, loss = 0.76563879\n",
            "Iteration 22, loss = 0.76077411\n",
            "Iteration 23, loss = 0.75622954\n",
            "Iteration 24, loss = 0.75181798\n",
            "Iteration 25, loss = 0.74780850\n",
            "Iteration 26, loss = 0.74367194\n",
            "Iteration 27, loss = 0.73976022\n",
            "Iteration 28, loss = 0.73608417\n",
            "Iteration 29, loss = 0.73214581\n",
            "Iteration 30, loss = 0.72863038\n",
            "Iteration 31, loss = 0.72522531\n",
            "Iteration 32, loss = 0.72208870\n",
            "Iteration 33, loss = 0.71905862\n",
            "Iteration 34, loss = 0.71605055\n",
            "Iteration 35, loss = 0.71338575\n",
            "Iteration 36, loss = 0.71054771\n",
            "Iteration 37, loss = 0.70780651\n",
            "Iteration 38, loss = 0.70507846\n",
            "Iteration 39, loss = 0.70241483\n",
            "Iteration 40, loss = 0.69974696\n",
            "Iteration 41, loss = 0.69719897\n",
            "Iteration 42, loss = 0.69474538\n",
            "Iteration 43, loss = 0.69229605\n",
            "Iteration 44, loss = 0.68998519\n",
            "Iteration 45, loss = 0.68769249\n",
            "Iteration 46, loss = 0.68541216\n",
            "Iteration 47, loss = 0.68334998\n",
            "Iteration 48, loss = 0.68137544\n",
            "Iteration 49, loss = 0.67942804\n",
            "Iteration 50, loss = 0.67769897\n",
            "Iteration 51, loss = 0.67587683\n",
            "Iteration 52, loss = 0.67441020\n",
            "Iteration 53, loss = 0.67273943\n",
            "Iteration 54, loss = 0.67136315\n",
            "Iteration 55, loss = 0.66969470\n",
            "Iteration 56, loss = 0.66819849\n",
            "Iteration 57, loss = 0.66678144\n",
            "Iteration 58, loss = 0.66532375\n",
            "Iteration 59, loss = 0.66394656\n",
            "Iteration 60, loss = 0.66254487\n",
            "Iteration 61, loss = 0.66119003\n",
            "Iteration 62, loss = 0.65990917\n",
            "Iteration 63, loss = 0.65860006\n",
            "Iteration 64, loss = 0.65745658\n",
            "Iteration 65, loss = 0.65639810\n",
            "Iteration 66, loss = 0.65515649\n",
            "Iteration 67, loss = 0.65406199\n",
            "Iteration 68, loss = 0.65300071\n",
            "Iteration 69, loss = 0.65201781\n",
            "Iteration 70, loss = 0.65097950\n",
            "Iteration 71, loss = 0.65009911\n",
            "Iteration 72, loss = 0.64908267\n",
            "Iteration 73, loss = 0.64825233\n",
            "Iteration 74, loss = 0.64739105\n",
            "Iteration 75, loss = 0.64651078\n",
            "Iteration 76, loss = 0.64567306\n",
            "Iteration 77, loss = 0.64489576\n",
            "Iteration 78, loss = 0.64421106\n",
            "Iteration 79, loss = 0.64343015\n",
            "Iteration 80, loss = 0.64269074\n",
            "Iteration 81, loss = 0.64200003\n",
            "Iteration 82, loss = 0.64132052\n",
            "Iteration 83, loss = 0.64064109\n",
            "Iteration 84, loss = 0.63989578\n",
            "Iteration 85, loss = 0.63924996\n",
            "Iteration 86, loss = 0.63849229\n",
            "Iteration 87, loss = 0.63783327\n",
            "Iteration 88, loss = 0.63715252\n",
            "Iteration 89, loss = 0.63649693\n",
            "Iteration 90, loss = 0.63589299\n",
            "Iteration 91, loss = 0.63518930\n",
            "Iteration 92, loss = 0.63457609\n",
            "Iteration 93, loss = 0.63395665\n",
            "Iteration 94, loss = 0.63331641\n",
            "Iteration 95, loss = 0.63264966\n",
            "Iteration 96, loss = 0.63204170\n",
            "Iteration 97, loss = 0.63145729\n",
            "Iteration 98, loss = 0.63083574\n",
            "Iteration 99, loss = 0.63029201\n",
            "Iteration 100, loss = 0.62972358\n",
            "Iteration 101, loss = 0.62916841\n",
            "Iteration 102, loss = 0.62864743\n",
            "Iteration 103, loss = 0.62811693\n",
            "Iteration 104, loss = 0.62753695\n",
            "Iteration 105, loss = 0.62700635\n",
            "Iteration 106, loss = 0.62649924\n",
            "Iteration 107, loss = 0.62599200\n",
            "Iteration 108, loss = 0.62549756\n",
            "Iteration 109, loss = 0.62495530\n",
            "Iteration 110, loss = 0.62447073\n",
            "Iteration 111, loss = 0.62393226\n",
            "Iteration 112, loss = 0.62347574\n",
            "Iteration 113, loss = 0.62301065\n",
            "Iteration 114, loss = 0.62254573\n",
            "Iteration 115, loss = 0.62215107\n",
            "Iteration 116, loss = 0.62173290\n",
            "Iteration 117, loss = 0.62131221\n",
            "Iteration 118, loss = 0.62083700\n",
            "Iteration 119, loss = 0.62046053\n",
            "Iteration 120, loss = 0.62002285\n",
            "Iteration 121, loss = 0.61963050\n",
            "Iteration 122, loss = 0.61921993\n",
            "Iteration 123, loss = 0.61881579\n",
            "Iteration 124, loss = 0.61840680\n",
            "Iteration 125, loss = 0.61801465\n",
            "Iteration 126, loss = 0.61765406\n",
            "Iteration 127, loss = 0.61726355\n",
            "Iteration 128, loss = 0.61687388\n",
            "Iteration 129, loss = 0.61654076\n",
            "Iteration 130, loss = 0.61616167\n",
            "Iteration 131, loss = 0.61579524\n",
            "Iteration 132, loss = 0.61543350\n",
            "Iteration 133, loss = 0.61508540\n",
            "Iteration 134, loss = 0.61473026\n",
            "Iteration 135, loss = 0.61439139\n",
            "Iteration 136, loss = 0.61404298\n",
            "Iteration 137, loss = 0.61372779\n",
            "Iteration 138, loss = 0.61341953\n",
            "Iteration 139, loss = 0.61310868\n",
            "Iteration 140, loss = 0.61276988\n",
            "Iteration 141, loss = 0.61242081\n",
            "Iteration 142, loss = 0.61206685\n",
            "Iteration 143, loss = 0.61169393\n",
            "Iteration 144, loss = 0.61138846\n",
            "Iteration 145, loss = 0.61104682\n",
            "Iteration 146, loss = 0.61072881\n",
            "Iteration 147, loss = 0.61042243\n",
            "Iteration 148, loss = 0.61011410\n",
            "Iteration 149, loss = 0.60980392\n",
            "Iteration 150, loss = 0.60945383\n",
            "Iteration 151, loss = 0.60914143\n",
            "Iteration 152, loss = 0.60883958\n",
            "Iteration 153, loss = 0.60851290\n",
            "Iteration 154, loss = 0.60821514\n",
            "Iteration 155, loss = 0.60787110\n",
            "Iteration 156, loss = 0.60756271\n",
            "Iteration 157, loss = 0.60726763\n",
            "Iteration 158, loss = 0.60696357\n",
            "Iteration 159, loss = 0.60667989\n",
            "Iteration 160, loss = 0.60637682\n",
            "Iteration 161, loss = 0.60607746\n",
            "Iteration 162, loss = 0.60580819\n",
            "Iteration 163, loss = 0.60549342\n",
            "Iteration 164, loss = 0.60522086\n",
            "Iteration 165, loss = 0.60494172\n",
            "Iteration 166, loss = 0.60468410\n",
            "Iteration 167, loss = 0.60442890\n",
            "Iteration 168, loss = 0.60419657\n",
            "Iteration 169, loss = 0.60394767\n",
            "Iteration 170, loss = 0.60370146\n",
            "Iteration 171, loss = 0.60345721\n",
            "Iteration 172, loss = 0.60322870\n",
            "Iteration 173, loss = 0.60298054\n",
            "Iteration 174, loss = 0.60275245\n",
            "Iteration 175, loss = 0.60254950\n",
            "Iteration 176, loss = 0.60230616\n",
            "Iteration 177, loss = 0.60209045\n",
            "Iteration 178, loss = 0.60186878\n",
            "Iteration 179, loss = 0.60164130\n",
            "Iteration 180, loss = 0.60140268\n",
            "Iteration 181, loss = 0.60117165\n",
            "Iteration 182, loss = 0.60095455\n",
            "Iteration 183, loss = 0.60070854\n",
            "Iteration 184, loss = 0.60050614\n",
            "Iteration 185, loss = 0.60024558\n",
            "Iteration 186, loss = 0.60004448\n",
            "Iteration 187, loss = 0.59984445\n",
            "Iteration 188, loss = 0.59962281\n",
            "Iteration 189, loss = 0.59942377\n",
            "Iteration 190, loss = 0.59921879\n",
            "Iteration 191, loss = 0.59901873\n",
            "Iteration 192, loss = 0.59881232\n",
            "Iteration 193, loss = 0.59862066\n",
            "Iteration 194, loss = 0.59841907\n",
            "Iteration 195, loss = 0.59823296\n",
            "Iteration 196, loss = 0.59805454\n",
            "Iteration 197, loss = 0.59785025\n",
            "Iteration 198, loss = 0.59764457\n",
            "Iteration 199, loss = 0.59743038\n",
            "Iteration 200, loss = 0.59721128\n",
            "Iteration 201, loss = 0.59698414\n",
            "Iteration 202, loss = 0.59678873\n",
            "Iteration 203, loss = 0.59659009\n",
            "Iteration 204, loss = 0.59638561\n",
            "Iteration 205, loss = 0.59616588\n",
            "Iteration 206, loss = 0.59597447\n",
            "Iteration 207, loss = 0.59577418\n",
            "Iteration 208, loss = 0.59555702\n",
            "Iteration 209, loss = 0.59534084\n",
            "Iteration 210, loss = 0.59513453\n",
            "Iteration 211, loss = 0.59491649\n",
            "Iteration 212, loss = 0.59470192\n",
            "Iteration 213, loss = 0.59448730\n",
            "Iteration 214, loss = 0.59427774\n",
            "Iteration 215, loss = 0.59404781\n",
            "Iteration 216, loss = 0.59384479\n",
            "Iteration 217, loss = 0.59365182\n",
            "Iteration 218, loss = 0.59348498\n",
            "Iteration 219, loss = 0.59328571\n",
            "Iteration 220, loss = 0.59307599\n",
            "Iteration 221, loss = 0.59286986\n",
            "Iteration 222, loss = 0.59266476\n",
            "Iteration 223, loss = 0.59245189\n",
            "Iteration 224, loss = 0.59224262\n",
            "Iteration 225, loss = 0.59202878\n",
            "Iteration 226, loss = 0.59182129\n",
            "Iteration 227, loss = 0.59160723\n",
            "Iteration 228, loss = 0.59138019\n",
            "Iteration 229, loss = 0.59116082\n",
            "Iteration 230, loss = 0.59096305\n",
            "Iteration 231, loss = 0.59073413\n",
            "Iteration 232, loss = 0.59051361\n",
            "Iteration 233, loss = 0.59029291\n",
            "Iteration 234, loss = 0.59009264\n",
            "Iteration 235, loss = 0.58986925\n",
            "Iteration 236, loss = 0.58965616\n",
            "Iteration 237, loss = 0.58946376\n",
            "Iteration 238, loss = 0.58923720\n",
            "Iteration 239, loss = 0.58902716\n",
            "Iteration 240, loss = 0.58881292\n",
            "Iteration 241, loss = 0.58861118\n",
            "Iteration 242, loss = 0.58843135\n",
            "Iteration 243, loss = 0.58824825\n",
            "Iteration 244, loss = 0.58805780\n",
            "Iteration 245, loss = 0.58787857\n",
            "Iteration 246, loss = 0.58771041\n",
            "Iteration 247, loss = 0.58752425\n",
            "Iteration 248, loss = 0.58734257\n",
            "Iteration 249, loss = 0.58716866\n",
            "Iteration 250, loss = 0.58698725\n",
            "Iteration 1, loss = 0.88667506\n",
            "Iteration 2, loss = 0.88351311\n",
            "Iteration 3, loss = 0.87864988\n",
            "Iteration 4, loss = 0.87260271\n",
            "Iteration 5, loss = 0.86575091\n",
            "Iteration 6, loss = 0.85865495\n",
            "Iteration 7, loss = 0.85211290\n",
            "Iteration 8, loss = 0.84485323\n",
            "Iteration 9, loss = 0.83785117\n",
            "Iteration 10, loss = 0.83103528\n",
            "Iteration 11, loss = 0.82391039\n",
            "Iteration 12, loss = 0.81728381\n",
            "Iteration 13, loss = 0.81056237\n",
            "Iteration 14, loss = 0.80417780\n",
            "Iteration 15, loss = 0.79788403\n",
            "Iteration 16, loss = 0.79172048\n",
            "Iteration 17, loss = 0.78589084\n",
            "Iteration 18, loss = 0.78024639\n",
            "Iteration 19, loss = 0.77527683\n",
            "Iteration 20, loss = 0.76983525\n",
            "Iteration 21, loss = 0.76512748\n",
            "Iteration 22, loss = 0.76031041\n",
            "Iteration 23, loss = 0.75566756\n",
            "Iteration 24, loss = 0.75123287\n",
            "Iteration 25, loss = 0.74717290\n",
            "Iteration 26, loss = 0.74309808\n",
            "Iteration 27, loss = 0.73909650\n",
            "Iteration 28, loss = 0.73534184\n",
            "Iteration 29, loss = 0.73146632\n",
            "Iteration 30, loss = 0.72783360\n",
            "Iteration 31, loss = 0.72415824\n",
            "Iteration 32, loss = 0.72081620\n",
            "Iteration 33, loss = 0.71757346\n",
            "Iteration 34, loss = 0.71430564\n",
            "Iteration 35, loss = 0.71143757\n",
            "Iteration 36, loss = 0.70844375\n",
            "Iteration 37, loss = 0.70547686\n",
            "Iteration 38, loss = 0.70270393\n",
            "Iteration 39, loss = 0.70016842\n",
            "Iteration 40, loss = 0.69738412\n",
            "Iteration 41, loss = 0.69481862\n",
            "Iteration 42, loss = 0.69239524\n",
            "Iteration 43, loss = 0.68976645\n",
            "Iteration 44, loss = 0.68741275\n",
            "Iteration 45, loss = 0.68502015\n",
            "Iteration 46, loss = 0.68269702\n",
            "Iteration 47, loss = 0.68062921\n",
            "Iteration 48, loss = 0.67855526\n",
            "Iteration 49, loss = 0.67663369\n",
            "Iteration 50, loss = 0.67475577\n",
            "Iteration 51, loss = 0.67285993\n",
            "Iteration 52, loss = 0.67128721\n",
            "Iteration 53, loss = 0.66942488\n",
            "Iteration 54, loss = 0.66796022\n",
            "Iteration 55, loss = 0.66608528\n",
            "Iteration 56, loss = 0.66438449\n",
            "Iteration 57, loss = 0.66280374\n",
            "Iteration 58, loss = 0.66126445\n",
            "Iteration 59, loss = 0.65981469\n",
            "Iteration 60, loss = 0.65833838\n",
            "Iteration 61, loss = 0.65696295\n",
            "Iteration 62, loss = 0.65566478\n",
            "Iteration 63, loss = 0.65425246\n",
            "Iteration 64, loss = 0.65304333\n",
            "Iteration 65, loss = 0.65190526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 66, loss = 0.65073936\n",
            "Iteration 67, loss = 0.64955138\n",
            "Iteration 68, loss = 0.64845330\n",
            "Iteration 69, loss = 0.64739906\n",
            "Iteration 70, loss = 0.64639450\n",
            "Iteration 71, loss = 0.64546332\n",
            "Iteration 72, loss = 0.64444218\n",
            "Iteration 73, loss = 0.64356265\n",
            "Iteration 74, loss = 0.64269640\n",
            "Iteration 75, loss = 0.64173035\n",
            "Iteration 76, loss = 0.64084432\n",
            "Iteration 77, loss = 0.63998132\n",
            "Iteration 78, loss = 0.63918837\n",
            "Iteration 79, loss = 0.63833688\n",
            "Iteration 80, loss = 0.63748454\n",
            "Iteration 81, loss = 0.63668468\n",
            "Iteration 82, loss = 0.63590312\n",
            "Iteration 83, loss = 0.63515432\n",
            "Iteration 84, loss = 0.63435972\n",
            "Iteration 85, loss = 0.63367797\n",
            "Iteration 86, loss = 0.63287418\n",
            "Iteration 87, loss = 0.63216616\n",
            "Iteration 88, loss = 0.63143260\n",
            "Iteration 89, loss = 0.63074916\n",
            "Iteration 90, loss = 0.63012356\n",
            "Iteration 91, loss = 0.62939877\n",
            "Iteration 92, loss = 0.62878563\n",
            "Iteration 93, loss = 0.62818137\n",
            "Iteration 94, loss = 0.62753163\n",
            "Iteration 95, loss = 0.62687453\n",
            "Iteration 96, loss = 0.62626042\n",
            "Iteration 97, loss = 0.62562589\n",
            "Iteration 98, loss = 0.62499300\n",
            "Iteration 99, loss = 0.62440384\n",
            "Iteration 100, loss = 0.62381887\n",
            "Iteration 101, loss = 0.62324946\n",
            "Iteration 102, loss = 0.62268885\n",
            "Iteration 103, loss = 0.62213448\n",
            "Iteration 104, loss = 0.62156196\n",
            "Iteration 105, loss = 0.62102050\n",
            "Iteration 106, loss = 0.62050980\n",
            "Iteration 107, loss = 0.61999940\n",
            "Iteration 108, loss = 0.61947200\n",
            "Iteration 109, loss = 0.61889228\n",
            "Iteration 110, loss = 0.61837360\n",
            "Iteration 111, loss = 0.61780234\n",
            "Iteration 112, loss = 0.61731808\n",
            "Iteration 113, loss = 0.61682786\n",
            "Iteration 114, loss = 0.61634198\n",
            "Iteration 115, loss = 0.61588132\n",
            "Iteration 116, loss = 0.61539914\n",
            "Iteration 117, loss = 0.61496000\n",
            "Iteration 118, loss = 0.61445999\n",
            "Iteration 119, loss = 0.61405786\n",
            "Iteration 120, loss = 0.61359280\n",
            "Iteration 121, loss = 0.61315311\n",
            "Iteration 122, loss = 0.61272132\n",
            "Iteration 123, loss = 0.61229425\n",
            "Iteration 124, loss = 0.61186651\n",
            "Iteration 125, loss = 0.61149580\n",
            "Iteration 126, loss = 0.61113313\n",
            "Iteration 127, loss = 0.61075871\n",
            "Iteration 128, loss = 0.61038748\n",
            "Iteration 129, loss = 0.61006564\n",
            "Iteration 130, loss = 0.60973529\n",
            "Iteration 131, loss = 0.60940572\n",
            "Iteration 132, loss = 0.60907905\n",
            "Iteration 133, loss = 0.60876726\n",
            "Iteration 134, loss = 0.60845105\n",
            "Iteration 135, loss = 0.60813815\n",
            "Iteration 136, loss = 0.60783352\n",
            "Iteration 137, loss = 0.60753356\n",
            "Iteration 138, loss = 0.60723539\n",
            "Iteration 139, loss = 0.60693029\n",
            "Iteration 140, loss = 0.60661511\n",
            "Iteration 141, loss = 0.60632463\n",
            "Iteration 142, loss = 0.60599017\n",
            "Iteration 143, loss = 0.60568620\n",
            "Iteration 144, loss = 0.60541104\n",
            "Iteration 145, loss = 0.60510121\n",
            "Iteration 146, loss = 0.60480565\n",
            "Iteration 147, loss = 0.60450824\n",
            "Iteration 148, loss = 0.60423121\n",
            "Iteration 149, loss = 0.60393618\n",
            "Iteration 150, loss = 0.60361900\n",
            "Iteration 151, loss = 0.60333554\n",
            "Iteration 152, loss = 0.60305321\n",
            "Iteration 153, loss = 0.60274115\n",
            "Iteration 154, loss = 0.60245413\n",
            "Iteration 155, loss = 0.60212651\n",
            "Iteration 156, loss = 0.60184832\n",
            "Iteration 157, loss = 0.60155171\n",
            "Iteration 158, loss = 0.60126582\n",
            "Iteration 159, loss = 0.60100262\n",
            "Iteration 160, loss = 0.60068187\n",
            "Iteration 161, loss = 0.60039488\n",
            "Iteration 162, loss = 0.60013446\n",
            "Iteration 163, loss = 0.59983366\n",
            "Iteration 164, loss = 0.59954878\n",
            "Iteration 165, loss = 0.59927854\n",
            "Iteration 166, loss = 0.59901146\n",
            "Iteration 167, loss = 0.59876075\n",
            "Iteration 168, loss = 0.59851650\n",
            "Iteration 169, loss = 0.59825617\n",
            "Iteration 170, loss = 0.59801253\n",
            "Iteration 171, loss = 0.59774458\n",
            "Iteration 172, loss = 0.59748128\n",
            "Iteration 173, loss = 0.59722409\n",
            "Iteration 174, loss = 0.59697695\n",
            "Iteration 175, loss = 0.59673937\n",
            "Iteration 176, loss = 0.59645825\n",
            "Iteration 177, loss = 0.59619640\n",
            "Iteration 178, loss = 0.59594427\n",
            "Iteration 179, loss = 0.59567512\n",
            "Iteration 180, loss = 0.59541360\n",
            "Iteration 181, loss = 0.59514124\n",
            "Iteration 182, loss = 0.59487859\n",
            "Iteration 183, loss = 0.59461068\n",
            "Iteration 184, loss = 0.59437536\n",
            "Iteration 185, loss = 0.59409224\n",
            "Iteration 186, loss = 0.59387115\n",
            "Iteration 187, loss = 0.59364717\n",
            "Iteration 188, loss = 0.59340668\n",
            "Iteration 189, loss = 0.59319492\n",
            "Iteration 190, loss = 0.59297039\n",
            "Iteration 191, loss = 0.59275976\n",
            "Iteration 192, loss = 0.59253535\n",
            "Iteration 193, loss = 0.59232778\n",
            "Iteration 194, loss = 0.59210413\n",
            "Iteration 195, loss = 0.59189420\n",
            "Iteration 196, loss = 0.59170085\n",
            "Iteration 197, loss = 0.59147506\n",
            "Iteration 198, loss = 0.59125193\n",
            "Iteration 199, loss = 0.59102154\n",
            "Iteration 200, loss = 0.59078368\n",
            "Iteration 201, loss = 0.59054039\n",
            "Iteration 202, loss = 0.59032809\n",
            "Iteration 203, loss = 0.59010026\n",
            "Iteration 204, loss = 0.58986991\n",
            "Iteration 205, loss = 0.58963112\n",
            "Iteration 206, loss = 0.58941335\n",
            "Iteration 207, loss = 0.58918573\n",
            "Iteration 208, loss = 0.58896337\n",
            "Iteration 209, loss = 0.58873598\n",
            "Iteration 210, loss = 0.58851840\n",
            "Iteration 211, loss = 0.58828996\n",
            "Iteration 212, loss = 0.58806361\n",
            "Iteration 213, loss = 0.58782486\n",
            "Iteration 214, loss = 0.58759651\n",
            "Iteration 215, loss = 0.58735872\n",
            "Iteration 216, loss = 0.58713121\n",
            "Iteration 217, loss = 0.58692872\n",
            "Iteration 218, loss = 0.58672360\n",
            "Iteration 219, loss = 0.58650681\n",
            "Iteration 220, loss = 0.58629423\n",
            "Iteration 221, loss = 0.58608350\n",
            "Iteration 222, loss = 0.58585830\n",
            "Iteration 223, loss = 0.58564246\n",
            "Iteration 224, loss = 0.58543376\n",
            "Iteration 225, loss = 0.58519631\n",
            "Iteration 226, loss = 0.58498812\n",
            "Iteration 227, loss = 0.58476707\n",
            "Iteration 228, loss = 0.58454570\n",
            "Iteration 229, loss = 0.58432371\n",
            "Iteration 230, loss = 0.58411162\n",
            "Iteration 231, loss = 0.58388436\n",
            "Iteration 232, loss = 0.58365447\n",
            "Iteration 233, loss = 0.58342326\n",
            "Iteration 234, loss = 0.58322329\n",
            "Iteration 235, loss = 0.58299458\n",
            "Iteration 236, loss = 0.58275471\n",
            "Iteration 237, loss = 0.58255214\n",
            "Iteration 238, loss = 0.58231984\n",
            "Iteration 239, loss = 0.58209341\n",
            "Iteration 240, loss = 0.58187159\n",
            "Iteration 241, loss = 0.58166580\n",
            "Iteration 242, loss = 0.58147505\n",
            "Iteration 243, loss = 0.58127237\n",
            "Iteration 244, loss = 0.58105595\n",
            "Iteration 245, loss = 0.58085123\n",
            "Iteration 246, loss = 0.58065880\n",
            "Iteration 247, loss = 0.58044354\n",
            "Iteration 248, loss = 0.58023983\n",
            "Iteration 249, loss = 0.58002529\n",
            "Iteration 250, loss = 0.57981581\n",
            "Iteration 1, loss = 0.88994796\n",
            "Iteration 2, loss = 0.88647879\n",
            "Iteration 3, loss = 0.88112416\n",
            "Iteration 4, loss = 0.87477056\n",
            "Iteration 5, loss = 0.86759332\n",
            "Iteration 6, loss = 0.86026211\n",
            "Iteration 7, loss = 0.85335631\n",
            "Iteration 8, loss = 0.84586796\n",
            "Iteration 9, loss = 0.83874205\n",
            "Iteration 10, loss = 0.83175349\n",
            "Iteration 11, loss = 0.82448515\n",
            "Iteration 12, loss = 0.81778325\n",
            "Iteration 13, loss = 0.81084241\n",
            "Iteration 14, loss = 0.80421718\n",
            "Iteration 15, loss = 0.79782291\n",
            "Iteration 16, loss = 0.79153897\n",
            "Iteration 17, loss = 0.78578672\n",
            "Iteration 18, loss = 0.78028990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 19, loss = 0.77514713\n",
            "Iteration 20, loss = 0.76965946\n",
            "Iteration 21, loss = 0.76458983\n",
            "Iteration 22, loss = 0.75975845\n",
            "Iteration 23, loss = 0.75512558\n",
            "Iteration 24, loss = 0.75073017\n",
            "Iteration 25, loss = 0.74659412\n",
            "Iteration 26, loss = 0.74241300\n",
            "Iteration 27, loss = 0.73858643\n",
            "Iteration 28, loss = 0.73478476\n",
            "Iteration 29, loss = 0.73086329\n",
            "Iteration 30, loss = 0.72730311\n",
            "Iteration 31, loss = 0.72350860\n",
            "Iteration 32, loss = 0.72016727\n",
            "Iteration 33, loss = 0.71697302\n",
            "Iteration 34, loss = 0.71360923\n",
            "Iteration 35, loss = 0.71068663\n",
            "Iteration 36, loss = 0.70783843\n",
            "Iteration 37, loss = 0.70491116\n",
            "Iteration 38, loss = 0.70215099\n",
            "Iteration 39, loss = 0.69968857\n",
            "Iteration 40, loss = 0.69682383\n",
            "Iteration 41, loss = 0.69427119\n",
            "Iteration 42, loss = 0.69198212\n",
            "Iteration 43, loss = 0.68919723\n",
            "Iteration 44, loss = 0.68691056\n",
            "Iteration 45, loss = 0.68454722\n",
            "Iteration 46, loss = 0.68224448\n",
            "Iteration 47, loss = 0.68018345\n",
            "Iteration 48, loss = 0.67814086\n",
            "Iteration 49, loss = 0.67618343\n",
            "Iteration 50, loss = 0.67429781\n",
            "Iteration 51, loss = 0.67240548\n",
            "Iteration 52, loss = 0.67073257\n",
            "Iteration 53, loss = 0.66887623\n",
            "Iteration 54, loss = 0.66727539\n",
            "Iteration 55, loss = 0.66548070\n",
            "Iteration 56, loss = 0.66375165\n",
            "Iteration 57, loss = 0.66206313\n",
            "Iteration 58, loss = 0.66052993\n",
            "Iteration 59, loss = 0.65902693\n",
            "Iteration 60, loss = 0.65749027\n",
            "Iteration 61, loss = 0.65599758\n",
            "Iteration 62, loss = 0.65470773\n",
            "Iteration 63, loss = 0.65327373\n",
            "Iteration 64, loss = 0.65200876\n",
            "Iteration 65, loss = 0.65082649\n",
            "Iteration 66, loss = 0.64977568\n",
            "Iteration 67, loss = 0.64850480\n",
            "Iteration 68, loss = 0.64743751\n",
            "Iteration 69, loss = 0.64638809\n",
            "Iteration 70, loss = 0.64542021\n",
            "Iteration 71, loss = 0.64443926\n",
            "Iteration 72, loss = 0.64338729\n",
            "Iteration 73, loss = 0.64241557\n",
            "Iteration 74, loss = 0.64150518\n",
            "Iteration 75, loss = 0.64053993\n",
            "Iteration 76, loss = 0.63963000\n",
            "Iteration 77, loss = 0.63876754\n",
            "Iteration 78, loss = 0.63794665\n",
            "Iteration 79, loss = 0.63710295\n",
            "Iteration 80, loss = 0.63618590\n",
            "Iteration 81, loss = 0.63534354\n",
            "Iteration 82, loss = 0.63453306\n",
            "Iteration 83, loss = 0.63371928\n",
            "Iteration 84, loss = 0.63288966\n",
            "Iteration 85, loss = 0.63216956\n",
            "Iteration 86, loss = 0.63129485\n",
            "Iteration 87, loss = 0.63059605\n",
            "Iteration 88, loss = 0.62984039\n",
            "Iteration 89, loss = 0.62914261\n",
            "Iteration 90, loss = 0.62847791\n",
            "Iteration 91, loss = 0.62776941\n",
            "Iteration 92, loss = 0.62715342\n",
            "Iteration 93, loss = 0.62653126\n",
            "Iteration 94, loss = 0.62586031\n",
            "Iteration 95, loss = 0.62520916\n",
            "Iteration 96, loss = 0.62453176\n",
            "Iteration 97, loss = 0.62392353\n",
            "Iteration 98, loss = 0.62329005\n",
            "Iteration 99, loss = 0.62268263\n",
            "Iteration 100, loss = 0.62210977\n",
            "Iteration 101, loss = 0.62153879\n",
            "Iteration 102, loss = 0.62096210\n",
            "Iteration 103, loss = 0.62039536\n",
            "Iteration 104, loss = 0.61979729\n",
            "Iteration 105, loss = 0.61921874\n",
            "Iteration 106, loss = 0.61872081\n",
            "Iteration 107, loss = 0.61819904\n",
            "Iteration 108, loss = 0.61769996\n",
            "Iteration 109, loss = 0.61714989\n",
            "Iteration 110, loss = 0.61665917\n",
            "Iteration 111, loss = 0.61610002\n",
            "Iteration 112, loss = 0.61560955\n",
            "Iteration 113, loss = 0.61511586\n",
            "Iteration 114, loss = 0.61462980\n",
            "Iteration 115, loss = 0.61415450\n",
            "Iteration 116, loss = 0.61366960\n",
            "Iteration 117, loss = 0.61321745\n",
            "Iteration 118, loss = 0.61273741\n",
            "Iteration 119, loss = 0.61233518\n",
            "Iteration 120, loss = 0.61192375\n",
            "Iteration 121, loss = 0.61147456\n",
            "Iteration 122, loss = 0.61104419\n",
            "Iteration 123, loss = 0.61065688\n",
            "Iteration 124, loss = 0.61023305\n",
            "Iteration 125, loss = 0.60986801\n",
            "Iteration 126, loss = 0.60947917\n",
            "Iteration 127, loss = 0.60912309\n",
            "Iteration 128, loss = 0.60875497\n",
            "Iteration 129, loss = 0.60843382\n",
            "Iteration 130, loss = 0.60809005\n",
            "Iteration 131, loss = 0.60775346\n",
            "Iteration 132, loss = 0.60741015\n",
            "Iteration 133, loss = 0.60707433\n",
            "Iteration 134, loss = 0.60673199\n",
            "Iteration 135, loss = 0.60638836\n",
            "Iteration 136, loss = 0.60605999\n",
            "Iteration 137, loss = 0.60574818\n",
            "Iteration 138, loss = 0.60542733\n",
            "Iteration 139, loss = 0.60511922\n",
            "Iteration 140, loss = 0.60481512\n",
            "Iteration 141, loss = 0.60451908\n",
            "Iteration 142, loss = 0.60418285\n",
            "Iteration 143, loss = 0.60388202\n",
            "Iteration 144, loss = 0.60357479\n",
            "Iteration 145, loss = 0.60325144\n",
            "Iteration 146, loss = 0.60294371\n",
            "Iteration 147, loss = 0.60261059\n",
            "Iteration 148, loss = 0.60232324\n",
            "Iteration 149, loss = 0.60202313\n",
            "Iteration 150, loss = 0.60171767\n",
            "Iteration 151, loss = 0.60142944\n",
            "Iteration 152, loss = 0.60115186\n",
            "Iteration 153, loss = 0.60083333\n",
            "Iteration 154, loss = 0.60052540\n",
            "Iteration 155, loss = 0.60019635\n",
            "Iteration 156, loss = 0.59990429\n",
            "Iteration 157, loss = 0.59958906\n",
            "Iteration 158, loss = 0.59926592\n",
            "Iteration 159, loss = 0.59898832\n",
            "Iteration 160, loss = 0.59865708\n",
            "Iteration 161, loss = 0.59835231\n",
            "Iteration 162, loss = 0.59808585\n",
            "Iteration 163, loss = 0.59778898\n",
            "Iteration 164, loss = 0.59750615\n",
            "Iteration 165, loss = 0.59722219\n",
            "Iteration 166, loss = 0.59696026\n",
            "Iteration 167, loss = 0.59669025\n",
            "Iteration 168, loss = 0.59641560\n",
            "Iteration 169, loss = 0.59615613\n",
            "Iteration 170, loss = 0.59588431\n",
            "Iteration 171, loss = 0.59560134\n",
            "Iteration 172, loss = 0.59531639\n",
            "Iteration 173, loss = 0.59504823\n",
            "Iteration 174, loss = 0.59477155\n",
            "Iteration 175, loss = 0.59449986\n",
            "Iteration 176, loss = 0.59421497\n",
            "Iteration 177, loss = 0.59394613\n",
            "Iteration 178, loss = 0.59368823\n",
            "Iteration 179, loss = 0.59343269\n",
            "Iteration 180, loss = 0.59317814\n",
            "Iteration 181, loss = 0.59291144\n",
            "Iteration 182, loss = 0.59265383\n",
            "Iteration 183, loss = 0.59237597\n",
            "Iteration 184, loss = 0.59212673\n",
            "Iteration 185, loss = 0.59184754\n",
            "Iteration 186, loss = 0.59159807\n",
            "Iteration 187, loss = 0.59135200\n",
            "Iteration 188, loss = 0.59110612\n",
            "Iteration 189, loss = 0.59086219\n",
            "Iteration 190, loss = 0.59062950\n",
            "Iteration 191, loss = 0.59039656\n",
            "Iteration 192, loss = 0.59014839\n",
            "Iteration 193, loss = 0.58993496\n",
            "Iteration 194, loss = 0.58969934\n",
            "Iteration 195, loss = 0.58946922\n",
            "Iteration 196, loss = 0.58925289\n",
            "Iteration 197, loss = 0.58902386\n",
            "Iteration 198, loss = 0.58879472\n",
            "Iteration 199, loss = 0.58858062\n",
            "Iteration 200, loss = 0.58834958\n",
            "Iteration 201, loss = 0.58812385\n",
            "Iteration 202, loss = 0.58790268\n",
            "Iteration 203, loss = 0.58767348\n",
            "Iteration 204, loss = 0.58744325\n",
            "Iteration 205, loss = 0.58720039\n",
            "Iteration 206, loss = 0.58697571\n",
            "Iteration 207, loss = 0.58673688\n",
            "Iteration 208, loss = 0.58651547\n",
            "Iteration 209, loss = 0.58628855\n",
            "Iteration 210, loss = 0.58606096\n",
            "Iteration 211, loss = 0.58582558\n",
            "Iteration 212, loss = 0.58558949\n",
            "Iteration 213, loss = 0.58534818\n",
            "Iteration 214, loss = 0.58511390\n",
            "Iteration 215, loss = 0.58487052\n",
            "Iteration 216, loss = 0.58464455\n",
            "Iteration 217, loss = 0.58443558\n",
            "Iteration 218, loss = 0.58420323\n",
            "Iteration 219, loss = 0.58398025\n",
            "Iteration 220, loss = 0.58375575\n",
            "Iteration 221, loss = 0.58350657\n",
            "Iteration 222, loss = 0.58327223\n",
            "Iteration 223, loss = 0.58303647\n",
            "Iteration 224, loss = 0.58282041\n",
            "Iteration 225, loss = 0.58256581\n",
            "Iteration 226, loss = 0.58234604\n",
            "Iteration 227, loss = 0.58211992\n",
            "Iteration 228, loss = 0.58191425\n",
            "Iteration 229, loss = 0.58168379\n",
            "Iteration 230, loss = 0.58146818\n",
            "Iteration 231, loss = 0.58125194\n",
            "Iteration 232, loss = 0.58101450\n",
            "Iteration 233, loss = 0.58079745\n",
            "Iteration 234, loss = 0.58059710\n",
            "Iteration 235, loss = 0.58039216\n",
            "Iteration 236, loss = 0.58016487\n",
            "Iteration 237, loss = 0.57996787\n",
            "Iteration 238, loss = 0.57975313\n",
            "Iteration 239, loss = 0.57953951\n",
            "Iteration 240, loss = 0.57933454\n",
            "Iteration 241, loss = 0.57912296\n",
            "Iteration 242, loss = 0.57892243\n",
            "Iteration 243, loss = 0.57872395\n",
            "Iteration 244, loss = 0.57851888\n",
            "Iteration 245, loss = 0.57831769\n",
            "Iteration 246, loss = 0.57812164\n",
            "Iteration 247, loss = 0.57792617\n",
            "Iteration 248, loss = 0.57772006\n",
            "Iteration 249, loss = 0.57751830\n",
            "Iteration 250, loss = 0.57732206\n",
            "Iteration 1, loss = 0.88349736\n",
            "Iteration 2, loss = 0.87995489\n",
            "Iteration 3, loss = 0.87484423\n",
            "Iteration 4, loss = 0.86857561\n",
            "Iteration 5, loss = 0.86173197\n",
            "Iteration 6, loss = 0.85454050\n",
            "Iteration 7, loss = 0.84774058\n",
            "Iteration 8, loss = 0.84088394\n",
            "Iteration 9, loss = 0.83393267\n",
            "Iteration 10, loss = 0.82742553\n",
            "Iteration 11, loss = 0.82086785\n",
            "Iteration 12, loss = 0.81452708\n",
            "Iteration 13, loss = 0.80823130\n",
            "Iteration 14, loss = 0.80203476\n",
            "Iteration 15, loss = 0.79614496\n",
            "Iteration 16, loss = 0.79049779\n",
            "Iteration 17, loss = 0.78504210\n",
            "Iteration 18, loss = 0.77952845\n",
            "Iteration 19, loss = 0.77469909\n",
            "Iteration 20, loss = 0.76936348\n",
            "Iteration 21, loss = 0.76442730\n",
            "Iteration 22, loss = 0.75965800\n",
            "Iteration 23, loss = 0.75500764\n",
            "Iteration 24, loss = 0.75071470\n",
            "Iteration 25, loss = 0.74647049\n",
            "Iteration 26, loss = 0.74261509\n",
            "Iteration 27, loss = 0.73863169\n",
            "Iteration 28, loss = 0.73493685\n",
            "Iteration 29, loss = 0.73128501\n",
            "Iteration 30, loss = 0.72789941\n",
            "Iteration 31, loss = 0.72426266\n",
            "Iteration 32, loss = 0.72101993\n",
            "Iteration 33, loss = 0.71785043\n",
            "Iteration 34, loss = 0.71458345\n",
            "Iteration 35, loss = 0.71165677\n",
            "Iteration 36, loss = 0.70878583\n",
            "Iteration 37, loss = 0.70588880\n",
            "Iteration 38, loss = 0.70314079\n",
            "Iteration 39, loss = 0.70061319\n",
            "Iteration 40, loss = 0.69798247\n",
            "Iteration 41, loss = 0.69549327\n",
            "Iteration 42, loss = 0.69327842\n",
            "Iteration 43, loss = 0.69073254\n",
            "Iteration 44, loss = 0.68867848\n",
            "Iteration 45, loss = 0.68642668\n",
            "Iteration 46, loss = 0.68434546\n",
            "Iteration 47, loss = 0.68236535\n",
            "Iteration 48, loss = 0.68048017\n",
            "Iteration 49, loss = 0.67864669\n",
            "Iteration 50, loss = 0.67680541\n",
            "Iteration 51, loss = 0.67496606\n",
            "Iteration 52, loss = 0.67331816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 53, loss = 0.67162095\n",
            "Iteration 54, loss = 0.67012659\n",
            "Iteration 55, loss = 0.66851262\n",
            "Iteration 56, loss = 0.66698837\n",
            "Iteration 57, loss = 0.66532003\n",
            "Iteration 58, loss = 0.66385469\n",
            "Iteration 59, loss = 0.66236615\n",
            "Iteration 60, loss = 0.66095100\n",
            "Iteration 61, loss = 0.65952010\n",
            "Iteration 62, loss = 0.65829264\n",
            "Iteration 63, loss = 0.65702747\n",
            "Iteration 64, loss = 0.65574388\n",
            "Iteration 65, loss = 0.65461995\n",
            "Iteration 66, loss = 0.65353183\n",
            "Iteration 67, loss = 0.65225074\n",
            "Iteration 68, loss = 0.65117719\n",
            "Iteration 69, loss = 0.64998474\n",
            "Iteration 70, loss = 0.64900805\n",
            "Iteration 71, loss = 0.64805924\n",
            "Iteration 72, loss = 0.64707263\n",
            "Iteration 73, loss = 0.64619598\n",
            "Iteration 74, loss = 0.64539438\n",
            "Iteration 75, loss = 0.64456929\n",
            "Iteration 76, loss = 0.64368399\n",
            "Iteration 77, loss = 0.64289466\n",
            "Iteration 78, loss = 0.64211176\n",
            "Iteration 79, loss = 0.64132455\n",
            "Iteration 80, loss = 0.64048025\n",
            "Iteration 81, loss = 0.63968838\n",
            "Iteration 82, loss = 0.63893461\n",
            "Iteration 83, loss = 0.63821109\n",
            "Iteration 84, loss = 0.63747211\n",
            "Iteration 85, loss = 0.63685252\n",
            "Iteration 86, loss = 0.63609556\n",
            "Iteration 87, loss = 0.63543753\n",
            "Iteration 88, loss = 0.63471118\n",
            "Iteration 89, loss = 0.63404085\n",
            "Iteration 90, loss = 0.63340349\n",
            "Iteration 91, loss = 0.63278193\n",
            "Iteration 92, loss = 0.63214743\n",
            "Iteration 93, loss = 0.63157648\n",
            "Iteration 94, loss = 0.63095984\n",
            "Iteration 95, loss = 0.63041787\n",
            "Iteration 96, loss = 0.62977208\n",
            "Iteration 97, loss = 0.62918399\n",
            "Iteration 98, loss = 0.62857863\n",
            "Iteration 99, loss = 0.62795666\n",
            "Iteration 100, loss = 0.62743278\n",
            "Iteration 101, loss = 0.62685021\n",
            "Iteration 102, loss = 0.62633634\n",
            "Iteration 103, loss = 0.62579168\n",
            "Iteration 104, loss = 0.62524932\n",
            "Iteration 105, loss = 0.62471175\n",
            "Iteration 106, loss = 0.62418637\n",
            "Iteration 107, loss = 0.62365253\n",
            "Iteration 108, loss = 0.62314381\n",
            "Iteration 109, loss = 0.62264512\n",
            "Iteration 110, loss = 0.62216813\n",
            "Iteration 111, loss = 0.62165089\n",
            "Iteration 112, loss = 0.62116648\n",
            "Iteration 113, loss = 0.62068983\n",
            "Iteration 114, loss = 0.62022157\n",
            "Iteration 115, loss = 0.61975804\n",
            "Iteration 116, loss = 0.61928792\n",
            "Iteration 117, loss = 0.61889902\n",
            "Iteration 118, loss = 0.61843594\n",
            "Iteration 119, loss = 0.61802865\n",
            "Iteration 120, loss = 0.61762445\n",
            "Iteration 121, loss = 0.61721078\n",
            "Iteration 122, loss = 0.61681830\n",
            "Iteration 123, loss = 0.61644115\n",
            "Iteration 124, loss = 0.61604421\n",
            "Iteration 125, loss = 0.61570259\n",
            "Iteration 126, loss = 0.61535021\n",
            "Iteration 127, loss = 0.61506127\n",
            "Iteration 128, loss = 0.61473832\n",
            "Iteration 129, loss = 0.61444763\n",
            "Iteration 130, loss = 0.61415855\n",
            "Iteration 131, loss = 0.61385193\n",
            "Iteration 132, loss = 0.61355253\n",
            "Iteration 133, loss = 0.61322731\n",
            "Iteration 134, loss = 0.61292373\n",
            "Iteration 135, loss = 0.61261517\n",
            "Iteration 136, loss = 0.61231101\n",
            "Iteration 137, loss = 0.61202998\n",
            "Iteration 138, loss = 0.61175024\n",
            "Iteration 139, loss = 0.61147013\n",
            "Iteration 140, loss = 0.61120172\n",
            "Iteration 141, loss = 0.61090642\n",
            "Iteration 142, loss = 0.61061183\n",
            "Iteration 143, loss = 0.61032082\n",
            "Iteration 144, loss = 0.61000956\n",
            "Iteration 145, loss = 0.60972792\n",
            "Iteration 146, loss = 0.60945950\n",
            "Iteration 147, loss = 0.60917434\n",
            "Iteration 148, loss = 0.60890758\n",
            "Iteration 149, loss = 0.60864347\n",
            "Iteration 150, loss = 0.60839202\n",
            "Iteration 151, loss = 0.60811992\n",
            "Iteration 152, loss = 0.60786268\n",
            "Iteration 153, loss = 0.60760296\n",
            "Iteration 154, loss = 0.60733963\n",
            "Iteration 155, loss = 0.60707550\n",
            "Iteration 156, loss = 0.60684228\n",
            "Iteration 157, loss = 0.60657545\n",
            "Iteration 158, loss = 0.60631319\n",
            "Iteration 159, loss = 0.60608399\n",
            "Iteration 160, loss = 0.60581160\n",
            "Iteration 161, loss = 0.60556069\n",
            "Iteration 162, loss = 0.60529163\n",
            "Iteration 163, loss = 0.60502466\n",
            "Iteration 164, loss = 0.60478753\n",
            "Iteration 165, loss = 0.60454314\n",
            "Iteration 166, loss = 0.60431867\n",
            "Iteration 167, loss = 0.60407499\n",
            "Iteration 168, loss = 0.60383628\n",
            "Iteration 169, loss = 0.60361231\n",
            "Iteration 170, loss = 0.60336176\n",
            "Iteration 171, loss = 0.60313222\n",
            "Iteration 172, loss = 0.60290132\n",
            "Iteration 173, loss = 0.60268011\n",
            "Iteration 174, loss = 0.60245866\n",
            "Iteration 175, loss = 0.60223474\n",
            "Iteration 176, loss = 0.60201685\n",
            "Iteration 177, loss = 0.60180890\n",
            "Iteration 178, loss = 0.60159975\n",
            "Iteration 179, loss = 0.60139617\n",
            "Iteration 180, loss = 0.60120760\n",
            "Iteration 181, loss = 0.60100129\n",
            "Iteration 182, loss = 0.60081331\n",
            "Iteration 183, loss = 0.60060786\n",
            "Iteration 184, loss = 0.60040209\n",
            "Iteration 185, loss = 0.60021868\n",
            "Iteration 186, loss = 0.60002204\n",
            "Iteration 187, loss = 0.59984107\n",
            "Iteration 188, loss = 0.59966458\n",
            "Iteration 189, loss = 0.59947868\n",
            "Iteration 190, loss = 0.59929886\n",
            "Iteration 191, loss = 0.59911707\n",
            "Iteration 192, loss = 0.59892208\n",
            "Iteration 193, loss = 0.59874415\n",
            "Iteration 194, loss = 0.59855266\n",
            "Iteration 195, loss = 0.59837945\n",
            "Iteration 196, loss = 0.59819202\n",
            "Iteration 197, loss = 0.59801147\n",
            "Iteration 198, loss = 0.59783531\n",
            "Iteration 199, loss = 0.59765422\n",
            "Iteration 200, loss = 0.59747330\n",
            "Iteration 201, loss = 0.59730403\n",
            "Iteration 202, loss = 0.59713448\n",
            "Iteration 203, loss = 0.59695801\n",
            "Iteration 204, loss = 0.59677898\n",
            "Iteration 205, loss = 0.59659486\n",
            "Iteration 206, loss = 0.59642201\n",
            "Iteration 207, loss = 0.59622851\n",
            "Iteration 208, loss = 0.59605956\n",
            "Iteration 209, loss = 0.59588003\n",
            "Iteration 210, loss = 0.59570127\n",
            "Iteration 211, loss = 0.59550779\n",
            "Iteration 212, loss = 0.59532714\n",
            "Iteration 213, loss = 0.59514535\n",
            "Iteration 214, loss = 0.59497030\n",
            "Iteration 215, loss = 0.59478398\n",
            "Iteration 216, loss = 0.59460306\n",
            "Iteration 217, loss = 0.59442528\n",
            "Iteration 218, loss = 0.59424685\n",
            "Iteration 219, loss = 0.59406483\n",
            "Iteration 220, loss = 0.59389901\n",
            "Iteration 221, loss = 0.59370465\n",
            "Iteration 222, loss = 0.59352481\n",
            "Iteration 223, loss = 0.59333402\n",
            "Iteration 224, loss = 0.59316767\n",
            "Iteration 225, loss = 0.59300983\n",
            "Iteration 226, loss = 0.59283503\n",
            "Iteration 227, loss = 0.59266744\n",
            "Iteration 228, loss = 0.59250843\n",
            "Iteration 229, loss = 0.59233584\n",
            "Iteration 230, loss = 0.59217176\n",
            "Iteration 231, loss = 0.59200872\n",
            "Iteration 232, loss = 0.59181593\n",
            "Iteration 233, loss = 0.59164936\n",
            "Iteration 234, loss = 0.59150172\n",
            "Iteration 235, loss = 0.59134491\n",
            "Iteration 236, loss = 0.59117384\n",
            "Iteration 237, loss = 0.59101296\n",
            "Iteration 238, loss = 0.59084409\n",
            "Iteration 239, loss = 0.59067806\n",
            "Iteration 240, loss = 0.59051354\n",
            "Iteration 241, loss = 0.59035552\n",
            "Iteration 242, loss = 0.59019760\n",
            "Iteration 243, loss = 0.59003226\n",
            "Iteration 244, loss = 0.58988360\n",
            "Iteration 245, loss = 0.58973452\n",
            "Iteration 246, loss = 0.58958588\n",
            "Iteration 247, loss = 0.58942750\n",
            "Iteration 248, loss = 0.58929252\n",
            "Iteration 249, loss = 0.58914071\n",
            "Iteration 250, loss = 0.58900995\n",
            "Iteration 1, loss = 0.88921381\n",
            "Iteration 2, loss = 0.88561120\n",
            "Iteration 3, loss = 0.88039745\n",
            "Iteration 4, loss = 0.87411510\n",
            "Iteration 5, loss = 0.86695385\n",
            "Iteration 6, loss = 0.85950718\n",
            "Iteration 7, loss = 0.85233922\n",
            "Iteration 8, loss = 0.84527473\n",
            "Iteration 9, loss = 0.83809709\n",
            "Iteration 10, loss = 0.83164349\n",
            "Iteration 11, loss = 0.82493838\n",
            "Iteration 12, loss = 0.81870921\n",
            "Iteration 13, loss = 0.81251318\n",
            "Iteration 14, loss = 0.80607251\n",
            "Iteration 15, loss = 0.79981985\n",
            "Iteration 16, loss = 0.79393281\n",
            "Iteration 17, loss = 0.78828581\n",
            "Iteration 18, loss = 0.78245479\n",
            "Iteration 19, loss = 0.77739177\n",
            "Iteration 20, loss = 0.77198009\n",
            "Iteration 21, loss = 0.76686429\n",
            "Iteration 22, loss = 0.76204430\n",
            "Iteration 23, loss = 0.75723984\n",
            "Iteration 24, loss = 0.75306036\n",
            "Iteration 25, loss = 0.74865567\n",
            "Iteration 26, loss = 0.74459266\n",
            "Iteration 27, loss = 0.74053604\n",
            "Iteration 28, loss = 0.73681378\n",
            "Iteration 29, loss = 0.73320126\n",
            "Iteration 30, loss = 0.72947076\n",
            "Iteration 31, loss = 0.72576794\n",
            "Iteration 32, loss = 0.72239030\n",
            "Iteration 33, loss = 0.71920770\n",
            "Iteration 34, loss = 0.71580616\n",
            "Iteration 35, loss = 0.71274085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 36, loss = 0.70975790\n",
            "Iteration 37, loss = 0.70664366\n",
            "Iteration 38, loss = 0.70366441\n",
            "Iteration 39, loss = 0.70088208\n",
            "Iteration 40, loss = 0.69810738\n",
            "Iteration 41, loss = 0.69560192\n",
            "Iteration 42, loss = 0.69316336\n",
            "Iteration 43, loss = 0.69061309\n",
            "Iteration 44, loss = 0.68855524\n",
            "Iteration 45, loss = 0.68625829\n",
            "Iteration 46, loss = 0.68415410\n",
            "Iteration 47, loss = 0.68216332\n",
            "Iteration 48, loss = 0.68023366\n",
            "Iteration 49, loss = 0.67837345\n",
            "Iteration 50, loss = 0.67652492\n",
            "Iteration 51, loss = 0.67465587\n",
            "Iteration 52, loss = 0.67291367\n",
            "Iteration 53, loss = 0.67120569\n",
            "Iteration 54, loss = 0.66960302\n",
            "Iteration 55, loss = 0.66800315\n",
            "Iteration 56, loss = 0.66652548\n",
            "Iteration 57, loss = 0.66485587\n",
            "Iteration 58, loss = 0.66349741\n",
            "Iteration 59, loss = 0.66208478\n",
            "Iteration 60, loss = 0.66070181\n",
            "Iteration 61, loss = 0.65933908\n",
            "Iteration 62, loss = 0.65815998\n",
            "Iteration 63, loss = 0.65691715\n",
            "Iteration 64, loss = 0.65568373\n",
            "Iteration 65, loss = 0.65459061\n",
            "Iteration 66, loss = 0.65352896\n",
            "Iteration 67, loss = 0.65233664\n",
            "Iteration 68, loss = 0.65132622\n",
            "Iteration 69, loss = 0.65019996\n",
            "Iteration 70, loss = 0.64924598\n",
            "Iteration 71, loss = 0.64821908\n",
            "Iteration 72, loss = 0.64725881\n",
            "Iteration 73, loss = 0.64638887\n",
            "Iteration 74, loss = 0.64557963\n",
            "Iteration 75, loss = 0.64472783\n",
            "Iteration 76, loss = 0.64381309\n",
            "Iteration 77, loss = 0.64297595\n",
            "Iteration 78, loss = 0.64215750\n",
            "Iteration 79, loss = 0.64136949\n",
            "Iteration 80, loss = 0.64060358\n",
            "Iteration 81, loss = 0.63981219\n",
            "Iteration 82, loss = 0.63909431\n",
            "Iteration 83, loss = 0.63840286\n",
            "Iteration 84, loss = 0.63766622\n",
            "Iteration 85, loss = 0.63702776\n",
            "Iteration 86, loss = 0.63626099\n",
            "Iteration 87, loss = 0.63554534\n",
            "Iteration 88, loss = 0.63482509\n",
            "Iteration 89, loss = 0.63410681\n",
            "Iteration 90, loss = 0.63339896\n",
            "Iteration 91, loss = 0.63272694\n",
            "Iteration 92, loss = 0.63204511\n",
            "Iteration 93, loss = 0.63144044\n",
            "Iteration 94, loss = 0.63084003\n",
            "Iteration 95, loss = 0.63028250\n",
            "Iteration 96, loss = 0.62965499\n",
            "Iteration 97, loss = 0.62905022\n",
            "Iteration 98, loss = 0.62844429\n",
            "Iteration 99, loss = 0.62780816\n",
            "Iteration 100, loss = 0.62726404\n",
            "Iteration 101, loss = 0.62667447\n",
            "Iteration 102, loss = 0.62614736\n",
            "Iteration 103, loss = 0.62561211\n",
            "Iteration 104, loss = 0.62507882\n",
            "Iteration 105, loss = 0.62452230\n",
            "Iteration 106, loss = 0.62401096\n",
            "Iteration 107, loss = 0.62343172\n",
            "Iteration 108, loss = 0.62293591\n",
            "Iteration 109, loss = 0.62242508\n",
            "Iteration 110, loss = 0.62197929\n",
            "Iteration 111, loss = 0.62148484\n",
            "Iteration 112, loss = 0.62100910\n",
            "Iteration 113, loss = 0.62055945\n",
            "Iteration 114, loss = 0.62011406\n",
            "Iteration 115, loss = 0.61961416\n",
            "Iteration 116, loss = 0.61915852\n",
            "Iteration 117, loss = 0.61873552\n",
            "Iteration 118, loss = 0.61828072\n",
            "Iteration 119, loss = 0.61786798\n",
            "Iteration 120, loss = 0.61746353\n",
            "Iteration 121, loss = 0.61703375\n",
            "Iteration 122, loss = 0.61663113\n",
            "Iteration 123, loss = 0.61623362\n",
            "Iteration 124, loss = 0.61582066\n",
            "Iteration 125, loss = 0.61543730\n",
            "Iteration 126, loss = 0.61503144\n",
            "Iteration 127, loss = 0.61465035\n",
            "Iteration 128, loss = 0.61429004\n",
            "Iteration 129, loss = 0.61393473\n",
            "Iteration 130, loss = 0.61360773\n",
            "Iteration 131, loss = 0.61325552\n",
            "Iteration 132, loss = 0.61291127\n",
            "Iteration 133, loss = 0.61256136\n",
            "Iteration 134, loss = 0.61222469\n",
            "Iteration 135, loss = 0.61188378\n",
            "Iteration 136, loss = 0.61154140\n",
            "Iteration 137, loss = 0.61123058\n",
            "Iteration 138, loss = 0.61090626\n",
            "Iteration 139, loss = 0.61058499\n",
            "Iteration 140, loss = 0.61029187\n",
            "Iteration 141, loss = 0.60998706\n",
            "Iteration 142, loss = 0.60965266\n",
            "Iteration 143, loss = 0.60933431\n",
            "Iteration 144, loss = 0.60899844\n",
            "Iteration 145, loss = 0.60866336\n",
            "Iteration 146, loss = 0.60834776\n",
            "Iteration 147, loss = 0.60804879\n",
            "Iteration 148, loss = 0.60774415\n",
            "Iteration 149, loss = 0.60745530\n",
            "Iteration 150, loss = 0.60720143\n",
            "Iteration 151, loss = 0.60690900\n",
            "Iteration 152, loss = 0.60661669\n",
            "Iteration 153, loss = 0.60635691\n",
            "Iteration 154, loss = 0.60606572\n",
            "Iteration 155, loss = 0.60579646\n",
            "Iteration 156, loss = 0.60554041\n",
            "Iteration 157, loss = 0.60524318\n",
            "Iteration 158, loss = 0.60496307\n",
            "Iteration 159, loss = 0.60471253\n",
            "Iteration 160, loss = 0.60443430\n",
            "Iteration 161, loss = 0.60417556\n",
            "Iteration 162, loss = 0.60389384\n",
            "Iteration 163, loss = 0.60362849\n",
            "Iteration 164, loss = 0.60336430\n",
            "Iteration 165, loss = 0.60310983\n",
            "Iteration 166, loss = 0.60286222\n",
            "Iteration 167, loss = 0.60260034\n",
            "Iteration 168, loss = 0.60235280\n",
            "Iteration 169, loss = 0.60210638\n",
            "Iteration 170, loss = 0.60184359\n",
            "Iteration 171, loss = 0.60160474\n",
            "Iteration 172, loss = 0.60135206\n",
            "Iteration 173, loss = 0.60111741\n",
            "Iteration 174, loss = 0.60086754\n",
            "Iteration 175, loss = 0.60062921\n",
            "Iteration 176, loss = 0.60042049\n",
            "Iteration 177, loss = 0.60019989\n",
            "Iteration 178, loss = 0.59996135\n",
            "Iteration 179, loss = 0.59972880\n",
            "Iteration 180, loss = 0.59951067\n",
            "Iteration 181, loss = 0.59928351\n",
            "Iteration 182, loss = 0.59908205\n",
            "Iteration 183, loss = 0.59886021\n",
            "Iteration 184, loss = 0.59861248\n",
            "Iteration 185, loss = 0.59839858\n",
            "Iteration 186, loss = 0.59817611\n",
            "Iteration 187, loss = 0.59796455\n",
            "Iteration 188, loss = 0.59776394\n",
            "Iteration 189, loss = 0.59755618\n",
            "Iteration 190, loss = 0.59735454\n",
            "Iteration 191, loss = 0.59716118\n",
            "Iteration 192, loss = 0.59694838\n",
            "Iteration 193, loss = 0.59676518\n",
            "Iteration 194, loss = 0.59657703\n",
            "Iteration 195, loss = 0.59640539\n",
            "Iteration 196, loss = 0.59619902\n",
            "Iteration 197, loss = 0.59602469\n",
            "Iteration 198, loss = 0.59583830\n",
            "Iteration 199, loss = 0.59564561\n",
            "Iteration 200, loss = 0.59545688\n",
            "Iteration 201, loss = 0.59526075\n",
            "Iteration 202, loss = 0.59507266\n",
            "Iteration 203, loss = 0.59487867\n",
            "Iteration 204, loss = 0.59470277\n",
            "Iteration 205, loss = 0.59450646\n",
            "Iteration 206, loss = 0.59432173\n",
            "Iteration 207, loss = 0.59411471\n",
            "Iteration 208, loss = 0.59392723\n",
            "Iteration 209, loss = 0.59373691\n",
            "Iteration 210, loss = 0.59353840\n",
            "Iteration 211, loss = 0.59333321\n",
            "Iteration 212, loss = 0.59314301\n",
            "Iteration 213, loss = 0.59293332\n",
            "Iteration 214, loss = 0.59274070\n",
            "Iteration 215, loss = 0.59253958\n",
            "Iteration 216, loss = 0.59233819\n",
            "Iteration 217, loss = 0.59214975\n",
            "Iteration 218, loss = 0.59195239\n",
            "Iteration 219, loss = 0.59175318\n",
            "Iteration 220, loss = 0.59157408\n",
            "Iteration 221, loss = 0.59137316\n",
            "Iteration 222, loss = 0.59117547\n",
            "Iteration 223, loss = 0.59098070\n",
            "Iteration 224, loss = 0.59080087\n",
            "Iteration 225, loss = 0.59062302\n",
            "Iteration 226, loss = 0.59043518\n",
            "Iteration 227, loss = 0.59024470\n",
            "Iteration 228, loss = 0.59006601\n",
            "Iteration 229, loss = 0.58988277\n",
            "Iteration 230, loss = 0.58970526\n",
            "Iteration 231, loss = 0.58953555\n",
            "Iteration 232, loss = 0.58933528\n",
            "Iteration 233, loss = 0.58917466\n",
            "Iteration 234, loss = 0.58900684\n",
            "Iteration 235, loss = 0.58884936\n",
            "Iteration 236, loss = 0.58866729\n",
            "Iteration 237, loss = 0.58848644\n",
            "Iteration 238, loss = 0.58830599\n",
            "Iteration 239, loss = 0.58812407\n",
            "Iteration 240, loss = 0.58794979\n",
            "Iteration 241, loss = 0.58776471\n",
            "Iteration 242, loss = 0.58758615\n",
            "Iteration 243, loss = 0.58739869\n",
            "Iteration 244, loss = 0.58722562\n",
            "Iteration 245, loss = 0.58705720\n",
            "Iteration 246, loss = 0.58686944\n",
            "Iteration 247, loss = 0.58668417\n",
            "Iteration 248, loss = 0.58650944\n",
            "Iteration 249, loss = 0.58633852\n",
            "Iteration 250, loss = 0.58617686\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 250 and for layer number 5 : 0.6987500000000001\n",
            "Iteration 1, loss = 1.38926569\n",
            "Iteration 2, loss = 1.37280501\n",
            "Iteration 3, loss = 1.34746797\n",
            "Iteration 4, loss = 1.31619589\n",
            "Iteration 5, loss = 1.28284600\n",
            "Iteration 6, loss = 1.24835476\n",
            "Iteration 7, loss = 1.21472357\n",
            "Iteration 8, loss = 1.18128774\n",
            "Iteration 9, loss = 1.14886509\n",
            "Iteration 10, loss = 1.11879494\n",
            "Iteration 11, loss = 1.08953649\n",
            "Iteration 12, loss = 1.06187751\n",
            "Iteration 13, loss = 1.03601333\n",
            "Iteration 14, loss = 1.01129749\n",
            "Iteration 15, loss = 0.98904579\n",
            "Iteration 16, loss = 0.96813749\n",
            "Iteration 17, loss = 0.94777197\n",
            "Iteration 18, loss = 0.92847460\n",
            "Iteration 19, loss = 0.91099975\n",
            "Iteration 20, loss = 0.89440660\n",
            "Iteration 21, loss = 0.87873317\n",
            "Iteration 22, loss = 0.86430695\n",
            "Iteration 23, loss = 0.85086938\n",
            "Iteration 24, loss = 0.83820984\n",
            "Iteration 25, loss = 0.82677373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 26, loss = 0.81533009\n",
            "Iteration 27, loss = 0.80526332\n",
            "Iteration 28, loss = 0.79582014\n",
            "Iteration 29, loss = 0.78704031\n",
            "Iteration 30, loss = 0.77874972\n",
            "Iteration 31, loss = 0.77095374\n",
            "Iteration 32, loss = 0.76369540\n",
            "Iteration 33, loss = 0.75713497\n",
            "Iteration 34, loss = 0.75066766\n",
            "Iteration 35, loss = 0.74477483\n",
            "Iteration 36, loss = 0.73962234\n",
            "Iteration 37, loss = 0.73447076\n",
            "Iteration 38, loss = 0.72976653\n",
            "Iteration 39, loss = 0.72535374\n",
            "Iteration 40, loss = 0.72111863\n",
            "Iteration 41, loss = 0.71707931\n",
            "Iteration 42, loss = 0.71336464\n",
            "Iteration 43, loss = 0.70972019\n",
            "Iteration 44, loss = 0.70623239\n",
            "Iteration 45, loss = 0.70314381\n",
            "Iteration 46, loss = 0.69989642\n",
            "Iteration 47, loss = 0.69679974\n",
            "Iteration 48, loss = 0.69401116\n",
            "Iteration 49, loss = 0.69139935\n",
            "Iteration 50, loss = 0.68879462\n",
            "Iteration 51, loss = 0.68651470\n",
            "Iteration 52, loss = 0.68421757\n",
            "Iteration 53, loss = 0.68200484\n",
            "Iteration 54, loss = 0.68007585\n",
            "Iteration 55, loss = 0.67808367\n",
            "Iteration 56, loss = 0.67618600\n",
            "Iteration 57, loss = 0.67429057\n",
            "Iteration 58, loss = 0.67248101\n",
            "Iteration 59, loss = 0.67080894\n",
            "Iteration 60, loss = 0.66927500\n",
            "Iteration 61, loss = 0.66761747\n",
            "Iteration 62, loss = 0.66614964\n",
            "Iteration 63, loss = 0.66454833\n",
            "Iteration 64, loss = 0.66305573\n",
            "Iteration 65, loss = 0.66161184\n",
            "Iteration 66, loss = 0.66013871\n",
            "Iteration 67, loss = 0.65874027\n",
            "Iteration 68, loss = 0.65728994\n",
            "Iteration 69, loss = 0.65605861\n",
            "Iteration 70, loss = 0.65477739\n",
            "Iteration 71, loss = 0.65354960\n",
            "Iteration 72, loss = 0.65235350\n",
            "Iteration 73, loss = 0.65124673\n",
            "Iteration 74, loss = 0.65008456\n",
            "Iteration 75, loss = 0.64907371\n",
            "Iteration 76, loss = 0.64797402\n",
            "Iteration 77, loss = 0.64701224\n",
            "Iteration 78, loss = 0.64597245\n",
            "Iteration 79, loss = 0.64505579\n",
            "Iteration 80, loss = 0.64412907\n",
            "Iteration 81, loss = 0.64320081\n",
            "Iteration 82, loss = 0.64230711\n",
            "Iteration 83, loss = 0.64139953\n",
            "Iteration 84, loss = 0.64054124\n",
            "Iteration 85, loss = 0.63968788\n",
            "Iteration 86, loss = 0.63885672\n",
            "Iteration 87, loss = 0.63805459\n",
            "Iteration 88, loss = 0.63726151\n",
            "Iteration 89, loss = 0.63648889\n",
            "Iteration 90, loss = 0.63578801\n",
            "Iteration 91, loss = 0.63499720\n",
            "Iteration 92, loss = 0.63423864\n",
            "Iteration 93, loss = 0.63348850\n",
            "Iteration 94, loss = 0.63277767\n",
            "Iteration 95, loss = 0.63198463\n",
            "Iteration 96, loss = 0.63126116\n",
            "Iteration 97, loss = 0.63054150\n",
            "Iteration 98, loss = 0.62982044\n",
            "Iteration 99, loss = 0.62910262\n",
            "Iteration 100, loss = 0.62846924\n",
            "Iteration 101, loss = 0.62771756\n",
            "Iteration 102, loss = 0.62705986\n",
            "Iteration 103, loss = 0.62638727\n",
            "Iteration 104, loss = 0.62572004\n",
            "Iteration 105, loss = 0.62509265\n",
            "Iteration 106, loss = 0.62441787\n",
            "Iteration 107, loss = 0.62381414\n",
            "Iteration 108, loss = 0.62318615\n",
            "Iteration 109, loss = 0.62257460\n",
            "Iteration 110, loss = 0.62195155\n",
            "Iteration 111, loss = 0.62131250\n",
            "Iteration 112, loss = 0.62072269\n",
            "Iteration 113, loss = 0.62017960\n",
            "Iteration 114, loss = 0.61958378\n",
            "Iteration 115, loss = 0.61903643\n",
            "Iteration 116, loss = 0.61843817\n",
            "Iteration 117, loss = 0.61785837\n",
            "Iteration 118, loss = 0.61732369\n",
            "Iteration 119, loss = 0.61674102\n",
            "Iteration 120, loss = 0.61622506\n",
            "Iteration 121, loss = 0.61576215\n",
            "Iteration 122, loss = 0.61529013\n",
            "Iteration 123, loss = 0.61483731\n",
            "Iteration 124, loss = 0.61438639\n",
            "Iteration 125, loss = 0.61390245\n",
            "Iteration 126, loss = 0.61345612\n",
            "Iteration 127, loss = 0.61299340\n",
            "Iteration 128, loss = 0.61254224\n",
            "Iteration 129, loss = 0.61210542\n",
            "Iteration 130, loss = 0.61164362\n",
            "Iteration 131, loss = 0.61120969\n",
            "Iteration 132, loss = 0.61073231\n",
            "Iteration 133, loss = 0.61026151\n",
            "Iteration 134, loss = 0.60980717\n",
            "Iteration 135, loss = 0.60934013\n",
            "Iteration 136, loss = 0.60886240\n",
            "Iteration 137, loss = 0.60837324\n",
            "Iteration 138, loss = 0.60790973\n",
            "Iteration 139, loss = 0.60744643\n",
            "Iteration 140, loss = 0.60697494\n",
            "Iteration 141, loss = 0.60651293\n",
            "Iteration 142, loss = 0.60607800\n",
            "Iteration 143, loss = 0.60558981\n",
            "Iteration 144, loss = 0.60514142\n",
            "Iteration 145, loss = 0.60471048\n",
            "Iteration 146, loss = 0.60428408\n",
            "Iteration 147, loss = 0.60388988\n",
            "Iteration 148, loss = 0.60350121\n",
            "Iteration 149, loss = 0.60305622\n",
            "Iteration 150, loss = 0.60264952\n",
            "Iteration 151, loss = 0.60226329\n",
            "Iteration 152, loss = 0.60184344\n",
            "Iteration 153, loss = 0.60141317\n",
            "Iteration 154, loss = 0.60101958\n",
            "Iteration 155, loss = 0.60059526\n",
            "Iteration 156, loss = 0.60021206\n",
            "Iteration 157, loss = 0.59982640\n",
            "Iteration 158, loss = 0.59942785\n",
            "Iteration 159, loss = 0.59905748\n",
            "Iteration 160, loss = 0.59869435\n",
            "Iteration 161, loss = 0.59835766\n",
            "Iteration 162, loss = 0.59803055\n",
            "Iteration 163, loss = 0.59768243\n",
            "Iteration 164, loss = 0.59736333\n",
            "Iteration 165, loss = 0.59704623\n",
            "Iteration 166, loss = 0.59667919\n",
            "Iteration 167, loss = 0.59634940\n",
            "Iteration 168, loss = 0.59600875\n",
            "Iteration 169, loss = 0.59569654\n",
            "Iteration 170, loss = 0.59535122\n",
            "Iteration 171, loss = 0.59503962\n",
            "Iteration 172, loss = 0.59471082\n",
            "Iteration 173, loss = 0.59437875\n",
            "Iteration 174, loss = 0.59404223\n",
            "Iteration 175, loss = 0.59369966\n",
            "Iteration 176, loss = 0.59338444\n",
            "Iteration 177, loss = 0.59306672\n",
            "Iteration 178, loss = 0.59275374\n",
            "Iteration 179, loss = 0.59246291\n",
            "Iteration 180, loss = 0.59214536\n",
            "Iteration 181, loss = 0.59185390\n",
            "Iteration 182, loss = 0.59156543\n",
            "Iteration 183, loss = 0.59128214\n",
            "Iteration 184, loss = 0.59095869\n",
            "Iteration 185, loss = 0.59068455\n",
            "Iteration 186, loss = 0.59037852\n",
            "Iteration 187, loss = 0.59006571\n",
            "Iteration 188, loss = 0.58982558\n",
            "Iteration 189, loss = 0.58956873\n",
            "Iteration 190, loss = 0.58932398\n",
            "Iteration 191, loss = 0.58908908\n",
            "Iteration 192, loss = 0.58886339\n",
            "Iteration 193, loss = 0.58861219\n",
            "Iteration 194, loss = 0.58835646\n",
            "Iteration 195, loss = 0.58808979\n",
            "Iteration 196, loss = 0.58780606\n",
            "Iteration 197, loss = 0.58753925\n",
            "Iteration 198, loss = 0.58730038\n",
            "Iteration 199, loss = 0.58705267\n",
            "Iteration 200, loss = 0.58679714\n",
            "Iteration 201, loss = 0.58656454\n",
            "Iteration 202, loss = 0.58630640\n",
            "Iteration 203, loss = 0.58605002\n",
            "Iteration 204, loss = 0.58579086\n",
            "Iteration 205, loss = 0.58555934\n",
            "Iteration 206, loss = 0.58528070\n",
            "Iteration 207, loss = 0.58505382\n",
            "Iteration 208, loss = 0.58481503\n",
            "Iteration 209, loss = 0.58459892\n",
            "Iteration 210, loss = 0.58437705\n",
            "Iteration 211, loss = 0.58415463\n",
            "Iteration 212, loss = 0.58394018\n",
            "Iteration 213, loss = 0.58371019\n",
            "Iteration 214, loss = 0.58348715\n",
            "Iteration 215, loss = 0.58327968\n",
            "Iteration 216, loss = 0.58306125\n",
            "Iteration 217, loss = 0.58284810\n",
            "Iteration 218, loss = 0.58261976\n",
            "Iteration 219, loss = 0.58241544\n",
            "Iteration 220, loss = 0.58221518\n",
            "Iteration 221, loss = 0.58198467\n",
            "Iteration 222, loss = 0.58176731\n",
            "Iteration 223, loss = 0.58155314\n",
            "Iteration 224, loss = 0.58133165\n",
            "Iteration 225, loss = 0.58112667\n",
            "Iteration 226, loss = 0.58093494\n",
            "Iteration 227, loss = 0.58071496\n",
            "Iteration 228, loss = 0.58050460\n",
            "Iteration 229, loss = 0.58031311\n",
            "Iteration 230, loss = 0.58009412\n",
            "Iteration 231, loss = 0.57988999\n",
            "Iteration 232, loss = 0.57967848\n",
            "Iteration 233, loss = 0.57945290\n",
            "Iteration 234, loss = 0.57922833\n",
            "Iteration 235, loss = 0.57903670\n",
            "Iteration 236, loss = 0.57880603\n",
            "Iteration 237, loss = 0.57863330\n",
            "Iteration 238, loss = 0.57844443\n",
            "Iteration 239, loss = 0.57825405\n",
            "Iteration 240, loss = 0.57806232\n",
            "Iteration 241, loss = 0.57787387\n",
            "Iteration 242, loss = 0.57768607\n",
            "Iteration 243, loss = 0.57750017\n",
            "Iteration 244, loss = 0.57728540\n",
            "Iteration 245, loss = 0.57711327\n",
            "Iteration 246, loss = 0.57690883\n",
            "Iteration 247, loss = 0.57673115\n",
            "Iteration 248, loss = 0.57651713\n",
            "Iteration 249, loss = 0.57632951\n",
            "Iteration 250, loss = 0.57613477\n",
            "Iteration 1, loss = 1.37877399\n",
            "Iteration 2, loss = 1.36256087\n",
            "Iteration 3, loss = 1.33739238\n",
            "Iteration 4, loss = 1.30659378\n",
            "Iteration 5, loss = 1.27335856\n",
            "Iteration 6, loss = 1.23889435\n",
            "Iteration 7, loss = 1.20499233\n",
            "Iteration 8, loss = 1.17181650\n",
            "Iteration 9, loss = 1.13971284\n",
            "Iteration 10, loss = 1.10956324\n",
            "Iteration 11, loss = 1.08045416\n",
            "Iteration 12, loss = 1.05294176\n",
            "Iteration 13, loss = 1.02710830\n",
            "Iteration 14, loss = 1.00261165\n",
            "Iteration 15, loss = 0.98070591\n",
            "Iteration 16, loss = 0.95987920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 17, loss = 0.94020854\n",
            "Iteration 18, loss = 0.92122763\n",
            "Iteration 19, loss = 0.90383330\n",
            "Iteration 20, loss = 0.88748408\n",
            "Iteration 21, loss = 0.87191253\n",
            "Iteration 22, loss = 0.85761649\n",
            "Iteration 23, loss = 0.84438059\n",
            "Iteration 24, loss = 0.83166341\n",
            "Iteration 25, loss = 0.82034630\n",
            "Iteration 26, loss = 0.80900028\n",
            "Iteration 27, loss = 0.79924699\n",
            "Iteration 28, loss = 0.78975345\n",
            "Iteration 29, loss = 0.78110653\n",
            "Iteration 30, loss = 0.77320177\n",
            "Iteration 31, loss = 0.76575073\n",
            "Iteration 32, loss = 0.75849142\n",
            "Iteration 33, loss = 0.75189914\n",
            "Iteration 34, loss = 0.74559673\n",
            "Iteration 35, loss = 0.73983119\n",
            "Iteration 36, loss = 0.73485730\n",
            "Iteration 37, loss = 0.72990421\n",
            "Iteration 38, loss = 0.72544570\n",
            "Iteration 39, loss = 0.72117603\n",
            "Iteration 40, loss = 0.71703850\n",
            "Iteration 41, loss = 0.71322451\n",
            "Iteration 42, loss = 0.70966686\n",
            "Iteration 43, loss = 0.70626584\n",
            "Iteration 44, loss = 0.70282098\n",
            "Iteration 45, loss = 0.69979339\n",
            "Iteration 46, loss = 0.69664159\n",
            "Iteration 47, loss = 0.69359879\n",
            "Iteration 48, loss = 0.69084866\n",
            "Iteration 49, loss = 0.68833276\n",
            "Iteration 50, loss = 0.68587860\n",
            "Iteration 51, loss = 0.68370393\n",
            "Iteration 52, loss = 0.68146700\n",
            "Iteration 53, loss = 0.67934255\n",
            "Iteration 54, loss = 0.67741969\n",
            "Iteration 55, loss = 0.67540396\n",
            "Iteration 56, loss = 0.67352010\n",
            "Iteration 57, loss = 0.67177485\n",
            "Iteration 58, loss = 0.67002014\n",
            "Iteration 59, loss = 0.66846780\n",
            "Iteration 60, loss = 0.66695573\n",
            "Iteration 61, loss = 0.66542439\n",
            "Iteration 62, loss = 0.66394678\n",
            "Iteration 63, loss = 0.66235577\n",
            "Iteration 64, loss = 0.66088106\n",
            "Iteration 65, loss = 0.65935602\n",
            "Iteration 66, loss = 0.65793334\n",
            "Iteration 67, loss = 0.65652492\n",
            "Iteration 68, loss = 0.65508253\n",
            "Iteration 69, loss = 0.65383545\n",
            "Iteration 70, loss = 0.65253073\n",
            "Iteration 71, loss = 0.65132221\n",
            "Iteration 72, loss = 0.65005109\n",
            "Iteration 73, loss = 0.64889076\n",
            "Iteration 74, loss = 0.64770514\n",
            "Iteration 75, loss = 0.64660515\n",
            "Iteration 76, loss = 0.64544655\n",
            "Iteration 77, loss = 0.64443220\n",
            "Iteration 78, loss = 0.64331724\n",
            "Iteration 79, loss = 0.64235456\n",
            "Iteration 80, loss = 0.64132897\n",
            "Iteration 81, loss = 0.64038882\n",
            "Iteration 82, loss = 0.63947120\n",
            "Iteration 83, loss = 0.63852183\n",
            "Iteration 84, loss = 0.63767949\n",
            "Iteration 85, loss = 0.63682970\n",
            "Iteration 86, loss = 0.63597486\n",
            "Iteration 87, loss = 0.63514300\n",
            "Iteration 88, loss = 0.63435968\n",
            "Iteration 89, loss = 0.63354415\n",
            "Iteration 90, loss = 0.63282646\n",
            "Iteration 91, loss = 0.63199980\n",
            "Iteration 92, loss = 0.63122688\n",
            "Iteration 93, loss = 0.63045721\n",
            "Iteration 94, loss = 0.62970582\n",
            "Iteration 95, loss = 0.62887494\n",
            "Iteration 96, loss = 0.62811898\n",
            "Iteration 97, loss = 0.62738428\n",
            "Iteration 98, loss = 0.62665124\n",
            "Iteration 99, loss = 0.62594012\n",
            "Iteration 100, loss = 0.62527980\n",
            "Iteration 101, loss = 0.62456794\n",
            "Iteration 102, loss = 0.62386710\n",
            "Iteration 103, loss = 0.62320385\n",
            "Iteration 104, loss = 0.62253996\n",
            "Iteration 105, loss = 0.62189415\n",
            "Iteration 106, loss = 0.62120134\n",
            "Iteration 107, loss = 0.62057750\n",
            "Iteration 108, loss = 0.61992975\n",
            "Iteration 109, loss = 0.61930433\n",
            "Iteration 110, loss = 0.61866132\n",
            "Iteration 111, loss = 0.61802694\n",
            "Iteration 112, loss = 0.61740918\n",
            "Iteration 113, loss = 0.61684939\n",
            "Iteration 114, loss = 0.61623249\n",
            "Iteration 115, loss = 0.61565796\n",
            "Iteration 116, loss = 0.61504501\n",
            "Iteration 117, loss = 0.61446406\n",
            "Iteration 118, loss = 0.61390667\n",
            "Iteration 119, loss = 0.61329184\n",
            "Iteration 120, loss = 0.61276745\n",
            "Iteration 121, loss = 0.61226539\n",
            "Iteration 122, loss = 0.61178911\n",
            "Iteration 123, loss = 0.61131873\n",
            "Iteration 124, loss = 0.61082428\n",
            "Iteration 125, loss = 0.61033579\n",
            "Iteration 126, loss = 0.60984979\n",
            "Iteration 127, loss = 0.60935164\n",
            "Iteration 128, loss = 0.60887902\n",
            "Iteration 129, loss = 0.60840988\n",
            "Iteration 130, loss = 0.60792078\n",
            "Iteration 131, loss = 0.60746030\n",
            "Iteration 132, loss = 0.60695595\n",
            "Iteration 133, loss = 0.60645273\n",
            "Iteration 134, loss = 0.60595229\n",
            "Iteration 135, loss = 0.60546307\n",
            "Iteration 136, loss = 0.60495070\n",
            "Iteration 137, loss = 0.60449123\n",
            "Iteration 138, loss = 0.60401363\n",
            "Iteration 139, loss = 0.60356238\n",
            "Iteration 140, loss = 0.60309091\n",
            "Iteration 141, loss = 0.60264393\n",
            "Iteration 142, loss = 0.60224576\n",
            "Iteration 143, loss = 0.60177122\n",
            "Iteration 144, loss = 0.60137298\n",
            "Iteration 145, loss = 0.60096009\n",
            "Iteration 146, loss = 0.60056936\n",
            "Iteration 147, loss = 0.60019633\n",
            "Iteration 148, loss = 0.59982456\n",
            "Iteration 149, loss = 0.59941932\n",
            "Iteration 150, loss = 0.59899122\n",
            "Iteration 151, loss = 0.59859589\n",
            "Iteration 152, loss = 0.59818445\n",
            "Iteration 153, loss = 0.59777471\n",
            "Iteration 154, loss = 0.59741094\n",
            "Iteration 155, loss = 0.59701005\n",
            "Iteration 156, loss = 0.59663178\n",
            "Iteration 157, loss = 0.59625552\n",
            "Iteration 158, loss = 0.59587864\n",
            "Iteration 159, loss = 0.59553984\n",
            "Iteration 160, loss = 0.59521080\n",
            "Iteration 161, loss = 0.59488221\n",
            "Iteration 162, loss = 0.59458009\n",
            "Iteration 163, loss = 0.59425744\n",
            "Iteration 164, loss = 0.59394011\n",
            "Iteration 165, loss = 0.59363717\n",
            "Iteration 166, loss = 0.59326735\n",
            "Iteration 167, loss = 0.59293728\n",
            "Iteration 168, loss = 0.59261492\n",
            "Iteration 169, loss = 0.59231779\n",
            "Iteration 170, loss = 0.59198436\n",
            "Iteration 171, loss = 0.59166940\n",
            "Iteration 172, loss = 0.59134855\n",
            "Iteration 173, loss = 0.59100645\n",
            "Iteration 174, loss = 0.59067415\n",
            "Iteration 175, loss = 0.59034669\n",
            "Iteration 176, loss = 0.59000864\n",
            "Iteration 177, loss = 0.58967364\n",
            "Iteration 178, loss = 0.58932761\n",
            "Iteration 179, loss = 0.58901187\n",
            "Iteration 180, loss = 0.58867165\n",
            "Iteration 181, loss = 0.58836517\n",
            "Iteration 182, loss = 0.58805065\n",
            "Iteration 183, loss = 0.58773935\n",
            "Iteration 184, loss = 0.58741684\n",
            "Iteration 185, loss = 0.58711122\n",
            "Iteration 186, loss = 0.58679375\n",
            "Iteration 187, loss = 0.58646867\n",
            "Iteration 188, loss = 0.58618550\n",
            "Iteration 189, loss = 0.58589695\n",
            "Iteration 190, loss = 0.58561923\n",
            "Iteration 191, loss = 0.58535488\n",
            "Iteration 192, loss = 0.58509026\n",
            "Iteration 193, loss = 0.58483283\n",
            "Iteration 194, loss = 0.58454814\n",
            "Iteration 195, loss = 0.58426541\n",
            "Iteration 196, loss = 0.58397641\n",
            "Iteration 197, loss = 0.58370621\n",
            "Iteration 198, loss = 0.58345101\n",
            "Iteration 199, loss = 0.58317530\n",
            "Iteration 200, loss = 0.58290522\n",
            "Iteration 201, loss = 0.58264491\n",
            "Iteration 202, loss = 0.58234540\n",
            "Iteration 203, loss = 0.58208092\n",
            "Iteration 204, loss = 0.58178226\n",
            "Iteration 205, loss = 0.58151838\n",
            "Iteration 206, loss = 0.58123337\n",
            "Iteration 207, loss = 0.58097135\n",
            "Iteration 208, loss = 0.58070119\n",
            "Iteration 209, loss = 0.58045120\n",
            "Iteration 210, loss = 0.58019343\n",
            "Iteration 211, loss = 0.57995364\n",
            "Iteration 212, loss = 0.57971897\n",
            "Iteration 213, loss = 0.57948584\n",
            "Iteration 214, loss = 0.57925127\n",
            "Iteration 215, loss = 0.57904260\n",
            "Iteration 216, loss = 0.57881732\n",
            "Iteration 217, loss = 0.57858262\n",
            "Iteration 218, loss = 0.57837091\n",
            "Iteration 219, loss = 0.57815395\n",
            "Iteration 220, loss = 0.57794554\n",
            "Iteration 221, loss = 0.57769686\n",
            "Iteration 222, loss = 0.57746942\n",
            "Iteration 223, loss = 0.57725230\n",
            "Iteration 224, loss = 0.57701518\n",
            "Iteration 225, loss = 0.57679813\n",
            "Iteration 226, loss = 0.57657989\n",
            "Iteration 227, loss = 0.57635495\n",
            "Iteration 228, loss = 0.57611002\n",
            "Iteration 229, loss = 0.57589625\n",
            "Iteration 230, loss = 0.57566125\n",
            "Iteration 231, loss = 0.57543141\n",
            "Iteration 232, loss = 0.57522295\n",
            "Iteration 233, loss = 0.57500159\n",
            "Iteration 234, loss = 0.57476565\n",
            "Iteration 235, loss = 0.57455963\n",
            "Iteration 236, loss = 0.57432499\n",
            "Iteration 237, loss = 0.57412489\n",
            "Iteration 238, loss = 0.57394816\n",
            "Iteration 239, loss = 0.57374229\n",
            "Iteration 240, loss = 0.57353986\n",
            "Iteration 241, loss = 0.57332992\n",
            "Iteration 242, loss = 0.57311899\n",
            "Iteration 243, loss = 0.57293407\n",
            "Iteration 244, loss = 0.57268776\n",
            "Iteration 245, loss = 0.57249636\n",
            "Iteration 246, loss = 0.57227904\n",
            "Iteration 247, loss = 0.57208689\n",
            "Iteration 248, loss = 0.57185663\n",
            "Iteration 249, loss = 0.57168133\n",
            "Iteration 250, loss = 0.57145727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.37951553\n",
            "Iteration 2, loss = 1.36345276\n",
            "Iteration 3, loss = 1.33844177\n",
            "Iteration 4, loss = 1.30796298\n",
            "Iteration 5, loss = 1.27459126\n",
            "Iteration 6, loss = 1.24040492\n",
            "Iteration 7, loss = 1.20671426\n",
            "Iteration 8, loss = 1.17282486\n",
            "Iteration 9, loss = 1.14067304\n",
            "Iteration 10, loss = 1.10992688\n",
            "Iteration 11, loss = 1.08024299\n",
            "Iteration 12, loss = 1.05249805\n",
            "Iteration 13, loss = 1.02632044\n",
            "Iteration 14, loss = 1.00219240\n",
            "Iteration 15, loss = 0.97949832\n",
            "Iteration 16, loss = 0.95866806\n",
            "Iteration 17, loss = 0.93831233\n",
            "Iteration 18, loss = 0.91955176\n",
            "Iteration 19, loss = 0.90227991\n",
            "Iteration 20, loss = 0.88594294\n",
            "Iteration 21, loss = 0.87069089\n",
            "Iteration 22, loss = 0.85667586\n",
            "Iteration 23, loss = 0.84359414\n",
            "Iteration 24, loss = 0.83093219\n",
            "Iteration 25, loss = 0.81954024\n",
            "Iteration 26, loss = 0.80797193\n",
            "Iteration 27, loss = 0.79813972\n",
            "Iteration 28, loss = 0.78827489\n",
            "Iteration 29, loss = 0.77945534\n",
            "Iteration 30, loss = 0.77167073\n",
            "Iteration 31, loss = 0.76379821\n",
            "Iteration 32, loss = 0.75668072\n",
            "Iteration 33, loss = 0.74977028\n",
            "Iteration 34, loss = 0.74333202\n",
            "Iteration 35, loss = 0.73744851\n",
            "Iteration 36, loss = 0.73224584\n",
            "Iteration 37, loss = 0.72713655\n",
            "Iteration 38, loss = 0.72251036\n",
            "Iteration 39, loss = 0.71809401\n",
            "Iteration 40, loss = 0.71381178\n",
            "Iteration 41, loss = 0.71006817\n",
            "Iteration 42, loss = 0.70642663\n",
            "Iteration 43, loss = 0.70297709\n",
            "Iteration 44, loss = 0.69956808\n",
            "Iteration 45, loss = 0.69665075\n",
            "Iteration 46, loss = 0.69345907\n",
            "Iteration 47, loss = 0.69050376\n",
            "Iteration 48, loss = 0.68787513\n",
            "Iteration 49, loss = 0.68529710\n",
            "Iteration 50, loss = 0.68283550\n",
            "Iteration 51, loss = 0.68057320\n",
            "Iteration 52, loss = 0.67831132\n",
            "Iteration 53, loss = 0.67622728\n",
            "Iteration 54, loss = 0.67421572\n",
            "Iteration 55, loss = 0.67206718\n",
            "Iteration 56, loss = 0.67019947\n",
            "Iteration 57, loss = 0.66839904\n",
            "Iteration 58, loss = 0.66666072\n",
            "Iteration 59, loss = 0.66504789\n",
            "Iteration 60, loss = 0.66353676\n",
            "Iteration 61, loss = 0.66204168\n",
            "Iteration 62, loss = 0.66055577\n",
            "Iteration 63, loss = 0.65901870\n",
            "Iteration 64, loss = 0.65756080\n",
            "Iteration 65, loss = 0.65607636\n",
            "Iteration 66, loss = 0.65469643\n",
            "Iteration 67, loss = 0.65336830\n",
            "Iteration 68, loss = 0.65205637\n",
            "Iteration 69, loss = 0.65084949\n",
            "Iteration 70, loss = 0.64960277\n",
            "Iteration 71, loss = 0.64837394\n",
            "Iteration 72, loss = 0.64713407\n",
            "Iteration 73, loss = 0.64596524\n",
            "Iteration 74, loss = 0.64480252\n",
            "Iteration 75, loss = 0.64368401\n",
            "Iteration 76, loss = 0.64254765\n",
            "Iteration 77, loss = 0.64150617\n",
            "Iteration 78, loss = 0.64039261\n",
            "Iteration 79, loss = 0.63941413\n",
            "Iteration 80, loss = 0.63838386\n",
            "Iteration 81, loss = 0.63743753\n",
            "Iteration 82, loss = 0.63650474\n",
            "Iteration 83, loss = 0.63557633\n",
            "Iteration 84, loss = 0.63469972\n",
            "Iteration 85, loss = 0.63385219\n",
            "Iteration 86, loss = 0.63298910\n",
            "Iteration 87, loss = 0.63211509\n",
            "Iteration 88, loss = 0.63130517\n",
            "Iteration 89, loss = 0.63045727\n",
            "Iteration 90, loss = 0.62971541\n",
            "Iteration 91, loss = 0.62884744\n",
            "Iteration 92, loss = 0.62806282\n",
            "Iteration 93, loss = 0.62730010\n",
            "Iteration 94, loss = 0.62658916\n",
            "Iteration 95, loss = 0.62581543\n",
            "Iteration 96, loss = 0.62512103\n",
            "Iteration 97, loss = 0.62443816\n",
            "Iteration 98, loss = 0.62373486\n",
            "Iteration 99, loss = 0.62304011\n",
            "Iteration 100, loss = 0.62238348\n",
            "Iteration 101, loss = 0.62169762\n",
            "Iteration 102, loss = 0.62100535\n",
            "Iteration 103, loss = 0.62032091\n",
            "Iteration 104, loss = 0.61961880\n",
            "Iteration 105, loss = 0.61893901\n",
            "Iteration 106, loss = 0.61820553\n",
            "Iteration 107, loss = 0.61757926\n",
            "Iteration 108, loss = 0.61690315\n",
            "Iteration 109, loss = 0.61627656\n",
            "Iteration 110, loss = 0.61564412\n",
            "Iteration 111, loss = 0.61502192\n",
            "Iteration 112, loss = 0.61441879\n",
            "Iteration 113, loss = 0.61384866\n",
            "Iteration 114, loss = 0.61324066\n",
            "Iteration 115, loss = 0.61265048\n",
            "Iteration 116, loss = 0.61203885\n",
            "Iteration 117, loss = 0.61143757\n",
            "Iteration 118, loss = 0.61083038\n",
            "Iteration 119, loss = 0.61019121\n",
            "Iteration 120, loss = 0.60962254\n",
            "Iteration 121, loss = 0.60904441\n",
            "Iteration 122, loss = 0.60850742\n",
            "Iteration 123, loss = 0.60796362\n",
            "Iteration 124, loss = 0.60739810\n",
            "Iteration 125, loss = 0.60686236\n",
            "Iteration 126, loss = 0.60632434\n",
            "Iteration 127, loss = 0.60582041\n",
            "Iteration 128, loss = 0.60527198\n",
            "Iteration 129, loss = 0.60476835\n",
            "Iteration 130, loss = 0.60423221\n",
            "Iteration 131, loss = 0.60373134\n",
            "Iteration 132, loss = 0.60320781\n",
            "Iteration 133, loss = 0.60265126\n",
            "Iteration 134, loss = 0.60212403\n",
            "Iteration 135, loss = 0.60161982\n",
            "Iteration 136, loss = 0.60110169\n",
            "Iteration 137, loss = 0.60061041\n",
            "Iteration 138, loss = 0.60013873\n",
            "Iteration 139, loss = 0.59963681\n",
            "Iteration 140, loss = 0.59914936\n",
            "Iteration 141, loss = 0.59866027\n",
            "Iteration 142, loss = 0.59821810\n",
            "Iteration 143, loss = 0.59770969\n",
            "Iteration 144, loss = 0.59729074\n",
            "Iteration 145, loss = 0.59684055\n",
            "Iteration 146, loss = 0.59639239\n",
            "Iteration 147, loss = 0.59598622\n",
            "Iteration 148, loss = 0.59557002\n",
            "Iteration 149, loss = 0.59513057\n",
            "Iteration 150, loss = 0.59465560\n",
            "Iteration 151, loss = 0.59421569\n",
            "Iteration 152, loss = 0.59376454\n",
            "Iteration 153, loss = 0.59332523\n",
            "Iteration 154, loss = 0.59290748\n",
            "Iteration 155, loss = 0.59246859\n",
            "Iteration 156, loss = 0.59205294\n",
            "Iteration 157, loss = 0.59162856\n",
            "Iteration 158, loss = 0.59118886\n",
            "Iteration 159, loss = 0.59077922\n",
            "Iteration 160, loss = 0.59039004\n",
            "Iteration 161, loss = 0.59000902\n",
            "Iteration 162, loss = 0.58962546\n",
            "Iteration 163, loss = 0.58926966\n",
            "Iteration 164, loss = 0.58888854\n",
            "Iteration 165, loss = 0.58853074\n",
            "Iteration 166, loss = 0.58813874\n",
            "Iteration 167, loss = 0.58776302\n",
            "Iteration 168, loss = 0.58737658\n",
            "Iteration 169, loss = 0.58703757\n",
            "Iteration 170, loss = 0.58664457\n",
            "Iteration 171, loss = 0.58627862\n",
            "Iteration 172, loss = 0.58590590\n",
            "Iteration 173, loss = 0.58553044\n",
            "Iteration 174, loss = 0.58516114\n",
            "Iteration 175, loss = 0.58480732\n",
            "Iteration 176, loss = 0.58444066\n",
            "Iteration 177, loss = 0.58408786\n",
            "Iteration 178, loss = 0.58372734\n",
            "Iteration 179, loss = 0.58336305\n",
            "Iteration 180, loss = 0.58298927\n",
            "Iteration 181, loss = 0.58265563\n",
            "Iteration 182, loss = 0.58229694\n",
            "Iteration 183, loss = 0.58194131\n",
            "Iteration 184, loss = 0.58157458\n",
            "Iteration 185, loss = 0.58122149\n",
            "Iteration 186, loss = 0.58086295\n",
            "Iteration 187, loss = 0.58050201\n",
            "Iteration 188, loss = 0.58016811\n",
            "Iteration 189, loss = 0.57980168\n",
            "Iteration 190, loss = 0.57949534\n",
            "Iteration 191, loss = 0.57916765\n",
            "Iteration 192, loss = 0.57886601\n",
            "Iteration 193, loss = 0.57858799\n",
            "Iteration 194, loss = 0.57826395\n",
            "Iteration 195, loss = 0.57794561\n",
            "Iteration 196, loss = 0.57763663\n",
            "Iteration 197, loss = 0.57731809\n",
            "Iteration 198, loss = 0.57701732\n",
            "Iteration 199, loss = 0.57669483\n",
            "Iteration 200, loss = 0.57637690\n",
            "Iteration 201, loss = 0.57606730\n",
            "Iteration 202, loss = 0.57572577\n",
            "Iteration 203, loss = 0.57542216\n",
            "Iteration 204, loss = 0.57509896\n",
            "Iteration 205, loss = 0.57478811\n",
            "Iteration 206, loss = 0.57449896\n",
            "Iteration 207, loss = 0.57417554\n",
            "Iteration 208, loss = 0.57387770\n",
            "Iteration 209, loss = 0.57355966\n",
            "Iteration 210, loss = 0.57325565\n",
            "Iteration 211, loss = 0.57297349\n",
            "Iteration 212, loss = 0.57267414\n",
            "Iteration 213, loss = 0.57241156\n",
            "Iteration 214, loss = 0.57214107\n",
            "Iteration 215, loss = 0.57187998\n",
            "Iteration 216, loss = 0.57163291\n",
            "Iteration 217, loss = 0.57136389\n",
            "Iteration 218, loss = 0.57112880\n",
            "Iteration 219, loss = 0.57089271\n",
            "Iteration 220, loss = 0.57066080\n",
            "Iteration 221, loss = 0.57037154\n",
            "Iteration 222, loss = 0.57010533\n",
            "Iteration 223, loss = 0.56985591\n",
            "Iteration 224, loss = 0.56956149\n",
            "Iteration 225, loss = 0.56930886\n",
            "Iteration 226, loss = 0.56902540\n",
            "Iteration 227, loss = 0.56875709\n",
            "Iteration 228, loss = 0.56845512\n",
            "Iteration 229, loss = 0.56820236\n",
            "Iteration 230, loss = 0.56792577\n",
            "Iteration 231, loss = 0.56766054\n",
            "Iteration 232, loss = 0.56741154\n",
            "Iteration 233, loss = 0.56714850\n",
            "Iteration 234, loss = 0.56689615\n",
            "Iteration 235, loss = 0.56662443\n",
            "Iteration 236, loss = 0.56636005\n",
            "Iteration 237, loss = 0.56612566\n",
            "Iteration 238, loss = 0.56588901\n",
            "Iteration 239, loss = 0.56563611\n",
            "Iteration 240, loss = 0.56540662\n",
            "Iteration 241, loss = 0.56514512\n",
            "Iteration 242, loss = 0.56489741\n",
            "Iteration 243, loss = 0.56466330\n",
            "Iteration 244, loss = 0.56438036\n",
            "Iteration 245, loss = 0.56414925\n",
            "Iteration 246, loss = 0.56390232\n",
            "Iteration 247, loss = 0.56365120\n",
            "Iteration 248, loss = 0.56338482\n",
            "Iteration 249, loss = 0.56315838\n",
            "Iteration 250, loss = 0.56291662\n",
            "Iteration 1, loss = 1.39138454\n",
            "Iteration 2, loss = 1.37512562\n",
            "Iteration 3, loss = 1.34984865\n",
            "Iteration 4, loss = 1.31828804\n",
            "Iteration 5, loss = 1.28379813\n",
            "Iteration 6, loss = 1.24919730\n",
            "Iteration 7, loss = 1.21383002\n",
            "Iteration 8, loss = 1.17922096\n",
            "Iteration 9, loss = 1.14576061\n",
            "Iteration 10, loss = 1.11442987\n",
            "Iteration 11, loss = 1.08433757\n",
            "Iteration 12, loss = 1.05600586\n",
            "Iteration 13, loss = 1.02936580\n",
            "Iteration 14, loss = 1.00449374\n",
            "Iteration 15, loss = 0.98126814\n",
            "Iteration 16, loss = 0.96045322\n",
            "Iteration 17, loss = 0.93985076\n",
            "Iteration 18, loss = 0.92131534\n",
            "Iteration 19, loss = 0.90427757\n",
            "Iteration 20, loss = 0.88794149\n",
            "Iteration 21, loss = 0.87257105\n",
            "Iteration 22, loss = 0.85865619\n",
            "Iteration 23, loss = 0.84553420\n",
            "Iteration 24, loss = 0.83288102\n",
            "Iteration 25, loss = 0.82164075\n",
            "Iteration 26, loss = 0.81031884\n",
            "Iteration 27, loss = 0.80044622\n",
            "Iteration 28, loss = 0.79078083\n",
            "Iteration 29, loss = 0.78199497\n",
            "Iteration 30, loss = 0.77411266\n",
            "Iteration 31, loss = 0.76624354\n",
            "Iteration 32, loss = 0.75938182\n",
            "Iteration 33, loss = 0.75241517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 34, loss = 0.74614686\n",
            "Iteration 35, loss = 0.74020298\n",
            "Iteration 36, loss = 0.73504213\n",
            "Iteration 37, loss = 0.72986011\n",
            "Iteration 38, loss = 0.72514082\n",
            "Iteration 39, loss = 0.72075279\n",
            "Iteration 40, loss = 0.71636099\n",
            "Iteration 41, loss = 0.71262350\n",
            "Iteration 42, loss = 0.70892549\n",
            "Iteration 43, loss = 0.70543635\n",
            "Iteration 44, loss = 0.70195493\n",
            "Iteration 45, loss = 0.69894456\n",
            "Iteration 46, loss = 0.69579898\n",
            "Iteration 47, loss = 0.69295315\n",
            "Iteration 48, loss = 0.69035026\n",
            "Iteration 49, loss = 0.68781951\n",
            "Iteration 50, loss = 0.68550089\n",
            "Iteration 51, loss = 0.68324006\n",
            "Iteration 52, loss = 0.68114943\n",
            "Iteration 53, loss = 0.67907425\n",
            "Iteration 54, loss = 0.67705865\n",
            "Iteration 55, loss = 0.67497031\n",
            "Iteration 56, loss = 0.67311956\n",
            "Iteration 57, loss = 0.67123379\n",
            "Iteration 58, loss = 0.66945703\n",
            "Iteration 59, loss = 0.66776204\n",
            "Iteration 60, loss = 0.66623540\n",
            "Iteration 61, loss = 0.66472561\n",
            "Iteration 62, loss = 0.66332128\n",
            "Iteration 63, loss = 0.66181759\n",
            "Iteration 64, loss = 0.66047596\n",
            "Iteration 65, loss = 0.65908815\n",
            "Iteration 66, loss = 0.65776076\n",
            "Iteration 67, loss = 0.65648902\n",
            "Iteration 68, loss = 0.65521748\n",
            "Iteration 69, loss = 0.65400813\n",
            "Iteration 70, loss = 0.65278035\n",
            "Iteration 71, loss = 0.65149094\n",
            "Iteration 72, loss = 0.65028077\n",
            "Iteration 73, loss = 0.64905842\n",
            "Iteration 74, loss = 0.64790424\n",
            "Iteration 75, loss = 0.64677162\n",
            "Iteration 76, loss = 0.64562035\n",
            "Iteration 77, loss = 0.64452933\n",
            "Iteration 78, loss = 0.64345644\n",
            "Iteration 79, loss = 0.64241040\n",
            "Iteration 80, loss = 0.64140986\n",
            "Iteration 81, loss = 0.64043956\n",
            "Iteration 82, loss = 0.63949342\n",
            "Iteration 83, loss = 0.63859004\n",
            "Iteration 84, loss = 0.63771080\n",
            "Iteration 85, loss = 0.63683352\n",
            "Iteration 86, loss = 0.63594807\n",
            "Iteration 87, loss = 0.63506056\n",
            "Iteration 88, loss = 0.63421892\n",
            "Iteration 89, loss = 0.63337974\n",
            "Iteration 90, loss = 0.63261508\n",
            "Iteration 91, loss = 0.63177660\n",
            "Iteration 92, loss = 0.63099911\n",
            "Iteration 93, loss = 0.63024576\n",
            "Iteration 94, loss = 0.62950227\n",
            "Iteration 95, loss = 0.62874205\n",
            "Iteration 96, loss = 0.62802933\n",
            "Iteration 97, loss = 0.62731275\n",
            "Iteration 98, loss = 0.62657765\n",
            "Iteration 99, loss = 0.62582972\n",
            "Iteration 100, loss = 0.62512388\n",
            "Iteration 101, loss = 0.62440886\n",
            "Iteration 102, loss = 0.62369126\n",
            "Iteration 103, loss = 0.62301138\n",
            "Iteration 104, loss = 0.62231346\n",
            "Iteration 105, loss = 0.62164193\n",
            "Iteration 106, loss = 0.62091321\n",
            "Iteration 107, loss = 0.62027969\n",
            "Iteration 108, loss = 0.61963396\n",
            "Iteration 109, loss = 0.61900211\n",
            "Iteration 110, loss = 0.61838819\n",
            "Iteration 111, loss = 0.61778817\n",
            "Iteration 112, loss = 0.61718123\n",
            "Iteration 113, loss = 0.61662347\n",
            "Iteration 114, loss = 0.61606190\n",
            "Iteration 115, loss = 0.61547712\n",
            "Iteration 116, loss = 0.61488928\n",
            "Iteration 117, loss = 0.61433814\n",
            "Iteration 118, loss = 0.61377771\n",
            "Iteration 119, loss = 0.61320796\n",
            "Iteration 120, loss = 0.61267437\n",
            "Iteration 121, loss = 0.61214127\n",
            "Iteration 122, loss = 0.61166081\n",
            "Iteration 123, loss = 0.61117147\n",
            "Iteration 124, loss = 0.61067739\n",
            "Iteration 125, loss = 0.61019738\n",
            "Iteration 126, loss = 0.60970918\n",
            "Iteration 127, loss = 0.60927424\n",
            "Iteration 128, loss = 0.60878360\n",
            "Iteration 129, loss = 0.60829878\n",
            "Iteration 130, loss = 0.60781020\n",
            "Iteration 131, loss = 0.60733642\n",
            "Iteration 132, loss = 0.60684527\n",
            "Iteration 133, loss = 0.60634604\n",
            "Iteration 134, loss = 0.60588950\n",
            "Iteration 135, loss = 0.60540424\n",
            "Iteration 136, loss = 0.60493219\n",
            "Iteration 137, loss = 0.60446340\n",
            "Iteration 138, loss = 0.60405119\n",
            "Iteration 139, loss = 0.60358573\n",
            "Iteration 140, loss = 0.60316252\n",
            "Iteration 141, loss = 0.60271924\n",
            "Iteration 142, loss = 0.60230785\n",
            "Iteration 143, loss = 0.60185146\n",
            "Iteration 144, loss = 0.60144689\n",
            "Iteration 145, loss = 0.60102438\n",
            "Iteration 146, loss = 0.60057588\n",
            "Iteration 147, loss = 0.60019066\n",
            "Iteration 148, loss = 0.59977412\n",
            "Iteration 149, loss = 0.59935245\n",
            "Iteration 150, loss = 0.59893304\n",
            "Iteration 151, loss = 0.59850764\n",
            "Iteration 152, loss = 0.59810748\n",
            "Iteration 153, loss = 0.59771272\n",
            "Iteration 154, loss = 0.59732168\n",
            "Iteration 155, loss = 0.59690309\n",
            "Iteration 156, loss = 0.59651319\n",
            "Iteration 157, loss = 0.59609727\n",
            "Iteration 158, loss = 0.59568627\n",
            "Iteration 159, loss = 0.59527740\n",
            "Iteration 160, loss = 0.59489084\n",
            "Iteration 161, loss = 0.59451146\n",
            "Iteration 162, loss = 0.59414111\n",
            "Iteration 163, loss = 0.59382351\n",
            "Iteration 164, loss = 0.59347342\n",
            "Iteration 165, loss = 0.59316499\n",
            "Iteration 166, loss = 0.59280595\n",
            "Iteration 167, loss = 0.59246808\n",
            "Iteration 168, loss = 0.59209276\n",
            "Iteration 169, loss = 0.59177597\n",
            "Iteration 170, loss = 0.59138530\n",
            "Iteration 171, loss = 0.59102396\n",
            "Iteration 172, loss = 0.59067631\n",
            "Iteration 173, loss = 0.59032790\n",
            "Iteration 174, loss = 0.58998561\n",
            "Iteration 175, loss = 0.58965719\n",
            "Iteration 176, loss = 0.58931282\n",
            "Iteration 177, loss = 0.58897694\n",
            "Iteration 178, loss = 0.58862537\n",
            "Iteration 179, loss = 0.58827694\n",
            "Iteration 180, loss = 0.58793072\n",
            "Iteration 181, loss = 0.58761554\n",
            "Iteration 182, loss = 0.58729607\n",
            "Iteration 183, loss = 0.58696715\n",
            "Iteration 184, loss = 0.58665574\n",
            "Iteration 185, loss = 0.58629769\n",
            "Iteration 186, loss = 0.58597816\n",
            "Iteration 187, loss = 0.58565617\n",
            "Iteration 188, loss = 0.58535530\n",
            "Iteration 189, loss = 0.58501471\n",
            "Iteration 190, loss = 0.58473598\n",
            "Iteration 191, loss = 0.58442454\n",
            "Iteration 192, loss = 0.58413628\n",
            "Iteration 193, loss = 0.58387189\n",
            "Iteration 194, loss = 0.58357290\n",
            "Iteration 195, loss = 0.58328531\n",
            "Iteration 196, loss = 0.58298410\n",
            "Iteration 197, loss = 0.58267924\n",
            "Iteration 198, loss = 0.58238710\n",
            "Iteration 199, loss = 0.58207347\n",
            "Iteration 200, loss = 0.58177106\n",
            "Iteration 201, loss = 0.58146240\n",
            "Iteration 202, loss = 0.58116008\n",
            "Iteration 203, loss = 0.58085017\n",
            "Iteration 204, loss = 0.58056408\n",
            "Iteration 205, loss = 0.58025481\n",
            "Iteration 206, loss = 0.57997554\n",
            "Iteration 207, loss = 0.57968125\n",
            "Iteration 208, loss = 0.57941064\n",
            "Iteration 209, loss = 0.57911621\n",
            "Iteration 210, loss = 0.57884453\n",
            "Iteration 211, loss = 0.57859288\n",
            "Iteration 212, loss = 0.57832532\n",
            "Iteration 213, loss = 0.57808370\n",
            "Iteration 214, loss = 0.57783174\n",
            "Iteration 215, loss = 0.57758466\n",
            "Iteration 216, loss = 0.57735393\n",
            "Iteration 217, loss = 0.57712740\n",
            "Iteration 218, loss = 0.57692416\n",
            "Iteration 219, loss = 0.57670919\n",
            "Iteration 220, loss = 0.57649164\n",
            "Iteration 221, loss = 0.57622780\n",
            "Iteration 222, loss = 0.57596441\n",
            "Iteration 223, loss = 0.57574434\n",
            "Iteration 224, loss = 0.57548325\n",
            "Iteration 225, loss = 0.57525555\n",
            "Iteration 226, loss = 0.57500224\n",
            "Iteration 227, loss = 0.57474899\n",
            "Iteration 228, loss = 0.57447979\n",
            "Iteration 229, loss = 0.57424557\n",
            "Iteration 230, loss = 0.57399091\n",
            "Iteration 231, loss = 0.57375074\n",
            "Iteration 232, loss = 0.57352263\n",
            "Iteration 233, loss = 0.57329776\n",
            "Iteration 234, loss = 0.57308623\n",
            "Iteration 235, loss = 0.57284047\n",
            "Iteration 236, loss = 0.57261530\n",
            "Iteration 237, loss = 0.57241622\n",
            "Iteration 238, loss = 0.57220205\n",
            "Iteration 239, loss = 0.57198798\n",
            "Iteration 240, loss = 0.57178895\n",
            "Iteration 241, loss = 0.57157482\n",
            "Iteration 242, loss = 0.57137134\n",
            "Iteration 243, loss = 0.57118413\n",
            "Iteration 244, loss = 0.57096255\n",
            "Iteration 245, loss = 0.57077432\n",
            "Iteration 246, loss = 0.57059080\n",
            "Iteration 247, loss = 0.57036207\n",
            "Iteration 248, loss = 0.57014745\n",
            "Iteration 249, loss = 0.56993385\n",
            "Iteration 250, loss = 0.56971827\n",
            "Iteration 1, loss = 1.36378915\n",
            "Iteration 2, loss = 1.34879364\n",
            "Iteration 3, loss = 1.32549217\n",
            "Iteration 4, loss = 1.29708034\n",
            "Iteration 5, loss = 1.26554033\n",
            "Iteration 6, loss = 1.23383872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7, loss = 1.20200469\n",
            "Iteration 8, loss = 1.17053414\n",
            "Iteration 9, loss = 1.14064775\n",
            "Iteration 10, loss = 1.11151975\n",
            "Iteration 11, loss = 1.08406253\n",
            "Iteration 12, loss = 1.05814511\n",
            "Iteration 13, loss = 1.03354346\n",
            "Iteration 14, loss = 1.01070814\n",
            "Iteration 15, loss = 0.98890098\n",
            "Iteration 16, loss = 0.96886449\n",
            "Iteration 17, loss = 0.94923104\n",
            "Iteration 18, loss = 0.93151074\n",
            "Iteration 19, loss = 0.91502500\n",
            "Iteration 20, loss = 0.89945880\n",
            "Iteration 21, loss = 0.88472298\n",
            "Iteration 22, loss = 0.87121456\n",
            "Iteration 23, loss = 0.85846639\n",
            "Iteration 24, loss = 0.84606854\n",
            "Iteration 25, loss = 0.83470170\n",
            "Iteration 26, loss = 0.82388112\n",
            "Iteration 27, loss = 0.81424012\n",
            "Iteration 28, loss = 0.80487305\n",
            "Iteration 29, loss = 0.79605592\n",
            "Iteration 30, loss = 0.78816723\n",
            "Iteration 31, loss = 0.78027235\n",
            "Iteration 32, loss = 0.77329705\n",
            "Iteration 33, loss = 0.76630727\n",
            "Iteration 34, loss = 0.75998863\n",
            "Iteration 35, loss = 0.75396852\n",
            "Iteration 36, loss = 0.74833678\n",
            "Iteration 37, loss = 0.74285837\n",
            "Iteration 38, loss = 0.73803442\n",
            "Iteration 39, loss = 0.73341573\n",
            "Iteration 40, loss = 0.72891424\n",
            "Iteration 41, loss = 0.72504769\n",
            "Iteration 42, loss = 0.72109686\n",
            "Iteration 43, loss = 0.71757315\n",
            "Iteration 44, loss = 0.71400332\n",
            "Iteration 45, loss = 0.71081284\n",
            "Iteration 46, loss = 0.70768563\n",
            "Iteration 47, loss = 0.70471192\n",
            "Iteration 48, loss = 0.70194281\n",
            "Iteration 49, loss = 0.69925315\n",
            "Iteration 50, loss = 0.69669744\n",
            "Iteration 51, loss = 0.69424636\n",
            "Iteration 52, loss = 0.69189830\n",
            "Iteration 53, loss = 0.68973931\n",
            "Iteration 54, loss = 0.68760345\n",
            "Iteration 55, loss = 0.68543024\n",
            "Iteration 56, loss = 0.68342348\n",
            "Iteration 57, loss = 0.68147262\n",
            "Iteration 58, loss = 0.67964124\n",
            "Iteration 59, loss = 0.67787307\n",
            "Iteration 60, loss = 0.67621673\n",
            "Iteration 61, loss = 0.67462365\n",
            "Iteration 62, loss = 0.67309221\n",
            "Iteration 63, loss = 0.67150391\n",
            "Iteration 64, loss = 0.67007641\n",
            "Iteration 65, loss = 0.66858230\n",
            "Iteration 66, loss = 0.66714332\n",
            "Iteration 67, loss = 0.66581116\n",
            "Iteration 68, loss = 0.66446415\n",
            "Iteration 69, loss = 0.66313131\n",
            "Iteration 70, loss = 0.66182390\n",
            "Iteration 71, loss = 0.66046432\n",
            "Iteration 72, loss = 0.65919492\n",
            "Iteration 73, loss = 0.65790677\n",
            "Iteration 74, loss = 0.65671474\n",
            "Iteration 75, loss = 0.65551452\n",
            "Iteration 76, loss = 0.65432494\n",
            "Iteration 77, loss = 0.65319837\n",
            "Iteration 78, loss = 0.65212463\n",
            "Iteration 79, loss = 0.65105388\n",
            "Iteration 80, loss = 0.65003300\n",
            "Iteration 81, loss = 0.64903759\n",
            "Iteration 82, loss = 0.64809585\n",
            "Iteration 83, loss = 0.64717154\n",
            "Iteration 84, loss = 0.64626768\n",
            "Iteration 85, loss = 0.64537314\n",
            "Iteration 86, loss = 0.64449899\n",
            "Iteration 87, loss = 0.64359621\n",
            "Iteration 88, loss = 0.64275708\n",
            "Iteration 89, loss = 0.64191812\n",
            "Iteration 90, loss = 0.64113195\n",
            "Iteration 91, loss = 0.64030779\n",
            "Iteration 92, loss = 0.63949773\n",
            "Iteration 93, loss = 0.63875682\n",
            "Iteration 94, loss = 0.63795612\n",
            "Iteration 95, loss = 0.63717242\n",
            "Iteration 96, loss = 0.63645109\n",
            "Iteration 97, loss = 0.63570073\n",
            "Iteration 98, loss = 0.63498115\n",
            "Iteration 99, loss = 0.63421861\n",
            "Iteration 100, loss = 0.63348999\n",
            "Iteration 101, loss = 0.63275874\n",
            "Iteration 102, loss = 0.63202608\n",
            "Iteration 103, loss = 0.63134886\n",
            "Iteration 104, loss = 0.63066372\n",
            "Iteration 105, loss = 0.63001472\n",
            "Iteration 106, loss = 0.62930887\n",
            "Iteration 107, loss = 0.62867911\n",
            "Iteration 108, loss = 0.62807512\n",
            "Iteration 109, loss = 0.62747870\n",
            "Iteration 110, loss = 0.62685895\n",
            "Iteration 111, loss = 0.62630162\n",
            "Iteration 112, loss = 0.62570594\n",
            "Iteration 113, loss = 0.62516088\n",
            "Iteration 114, loss = 0.62457410\n",
            "Iteration 115, loss = 0.62400773\n",
            "Iteration 116, loss = 0.62342274\n",
            "Iteration 117, loss = 0.62285866\n",
            "Iteration 118, loss = 0.62229865\n",
            "Iteration 119, loss = 0.62175565\n",
            "Iteration 120, loss = 0.62119369\n",
            "Iteration 121, loss = 0.62065287\n",
            "Iteration 122, loss = 0.62014015\n",
            "Iteration 123, loss = 0.61962987\n",
            "Iteration 124, loss = 0.61911487\n",
            "Iteration 125, loss = 0.61863745\n",
            "Iteration 126, loss = 0.61813522\n",
            "Iteration 127, loss = 0.61768179\n",
            "Iteration 128, loss = 0.61719308\n",
            "Iteration 129, loss = 0.61668202\n",
            "Iteration 130, loss = 0.61622575\n",
            "Iteration 131, loss = 0.61574837\n",
            "Iteration 132, loss = 0.61529571\n",
            "Iteration 133, loss = 0.61482482\n",
            "Iteration 134, loss = 0.61439165\n",
            "Iteration 135, loss = 0.61393492\n",
            "Iteration 136, loss = 0.61349984\n",
            "Iteration 137, loss = 0.61306190\n",
            "Iteration 138, loss = 0.61265032\n",
            "Iteration 139, loss = 0.61218768\n",
            "Iteration 140, loss = 0.61176824\n",
            "Iteration 141, loss = 0.61134064\n",
            "Iteration 142, loss = 0.61094127\n",
            "Iteration 143, loss = 0.61051579\n",
            "Iteration 144, loss = 0.61011162\n",
            "Iteration 145, loss = 0.60970606\n",
            "Iteration 146, loss = 0.60928174\n",
            "Iteration 147, loss = 0.60888100\n",
            "Iteration 148, loss = 0.60846789\n",
            "Iteration 149, loss = 0.60803492\n",
            "Iteration 150, loss = 0.60762284\n",
            "Iteration 151, loss = 0.60719954\n",
            "Iteration 152, loss = 0.60679411\n",
            "Iteration 153, loss = 0.60641267\n",
            "Iteration 154, loss = 0.60602227\n",
            "Iteration 155, loss = 0.60563330\n",
            "Iteration 156, loss = 0.60528969\n",
            "Iteration 157, loss = 0.60490894\n",
            "Iteration 158, loss = 0.60454348\n",
            "Iteration 159, loss = 0.60415489\n",
            "Iteration 160, loss = 0.60378039\n",
            "Iteration 161, loss = 0.60341914\n",
            "Iteration 162, loss = 0.60303635\n",
            "Iteration 163, loss = 0.60269236\n",
            "Iteration 164, loss = 0.60233687\n",
            "Iteration 165, loss = 0.60200957\n",
            "Iteration 166, loss = 0.60165254\n",
            "Iteration 167, loss = 0.60130815\n",
            "Iteration 168, loss = 0.60094719\n",
            "Iteration 169, loss = 0.60061875\n",
            "Iteration 170, loss = 0.60026172\n",
            "Iteration 171, loss = 0.59990238\n",
            "Iteration 172, loss = 0.59953859\n",
            "Iteration 173, loss = 0.59919669\n",
            "Iteration 174, loss = 0.59885553\n",
            "Iteration 175, loss = 0.59853789\n",
            "Iteration 176, loss = 0.59818744\n",
            "Iteration 177, loss = 0.59784538\n",
            "Iteration 178, loss = 0.59752370\n",
            "Iteration 179, loss = 0.59716771\n",
            "Iteration 180, loss = 0.59684887\n",
            "Iteration 181, loss = 0.59653215\n",
            "Iteration 182, loss = 0.59620998\n",
            "Iteration 183, loss = 0.59588931\n",
            "Iteration 184, loss = 0.59559595\n",
            "Iteration 185, loss = 0.59524715\n",
            "Iteration 186, loss = 0.59491951\n",
            "Iteration 187, loss = 0.59461223\n",
            "Iteration 188, loss = 0.59429729\n",
            "Iteration 189, loss = 0.59399093\n",
            "Iteration 190, loss = 0.59370681\n",
            "Iteration 191, loss = 0.59340244\n",
            "Iteration 192, loss = 0.59311298\n",
            "Iteration 193, loss = 0.59284117\n",
            "Iteration 194, loss = 0.59253337\n",
            "Iteration 195, loss = 0.59224873\n",
            "Iteration 196, loss = 0.59194932\n",
            "Iteration 197, loss = 0.59164598\n",
            "Iteration 198, loss = 0.59134717\n",
            "Iteration 199, loss = 0.59103068\n",
            "Iteration 200, loss = 0.59075643\n",
            "Iteration 201, loss = 0.59045046\n",
            "Iteration 202, loss = 0.59016545\n",
            "Iteration 203, loss = 0.58985720\n",
            "Iteration 204, loss = 0.58959806\n",
            "Iteration 205, loss = 0.58930059\n",
            "Iteration 206, loss = 0.58903866\n",
            "Iteration 207, loss = 0.58877067\n",
            "Iteration 208, loss = 0.58851434\n",
            "Iteration 209, loss = 0.58823934\n",
            "Iteration 210, loss = 0.58797763\n",
            "Iteration 211, loss = 0.58771696\n",
            "Iteration 212, loss = 0.58746251\n",
            "Iteration 213, loss = 0.58721679\n",
            "Iteration 214, loss = 0.58696384\n",
            "Iteration 215, loss = 0.58671036\n",
            "Iteration 216, loss = 0.58649428\n",
            "Iteration 217, loss = 0.58627956\n",
            "Iteration 218, loss = 0.58606850\n",
            "Iteration 219, loss = 0.58587649\n",
            "Iteration 220, loss = 0.58567332\n",
            "Iteration 221, loss = 0.58543206\n",
            "Iteration 222, loss = 0.58519632\n",
            "Iteration 223, loss = 0.58497061\n",
            "Iteration 224, loss = 0.58473354\n",
            "Iteration 225, loss = 0.58450728\n",
            "Iteration 226, loss = 0.58426040\n",
            "Iteration 227, loss = 0.58400694\n",
            "Iteration 228, loss = 0.58375297\n",
            "Iteration 229, loss = 0.58350181\n",
            "Iteration 230, loss = 0.58326165\n",
            "Iteration 231, loss = 0.58301169\n",
            "Iteration 232, loss = 0.58277938\n",
            "Iteration 233, loss = 0.58254214\n",
            "Iteration 234, loss = 0.58232653\n",
            "Iteration 235, loss = 0.58207785\n",
            "Iteration 236, loss = 0.58183789\n",
            "Iteration 237, loss = 0.58161401\n",
            "Iteration 238, loss = 0.58139173\n",
            "Iteration 239, loss = 0.58114049\n",
            "Iteration 240, loss = 0.58090472\n",
            "Iteration 241, loss = 0.58065321\n",
            "Iteration 242, loss = 0.58041202\n",
            "Iteration 243, loss = 0.58021539\n",
            "Iteration 244, loss = 0.57995992\n",
            "Iteration 245, loss = 0.57973590\n",
            "Iteration 246, loss = 0.57952446\n",
            "Iteration 247, loss = 0.57926950\n",
            "Iteration 248, loss = 0.57902200\n",
            "Iteration 249, loss = 0.57878474\n",
            "Iteration 250, loss = 0.57854433\n",
            "Validation accuracy for learning rate= 0.001  and epoaches= 250 and for layer number 6 : 0.7125\n",
            "Best mean validation accuracy ( 0.73 ) is achieved with learning rate= 0.01  and epoaches= 100 and for layer number 6\n",
            "Iteration 1, loss = 1.34583332\n",
            "Iteration 2, loss = 1.19228963\n",
            "Iteration 3, loss = 1.00693240\n",
            "Iteration 4, loss = 0.85226040\n",
            "Iteration 5, loss = 0.75442605\n",
            "Iteration 6, loss = 0.69851996\n",
            "Iteration 7, loss = 0.67018279\n",
            "Iteration 8, loss = 0.65785531\n",
            "Iteration 9, loss = 0.64931653\n",
            "Iteration 10, loss = 0.64192734\n",
            "Iteration 11, loss = 0.63541374\n",
            "Iteration 12, loss = 0.62772848\n",
            "Iteration 13, loss = 0.62095788\n",
            "Iteration 14, loss = 0.61510400\n",
            "Iteration 15, loss = 0.60955249\n",
            "Iteration 16, loss = 0.60504832\n",
            "Iteration 17, loss = 0.60068649\n",
            "Iteration 18, loss = 0.59706942\n",
            "Iteration 19, loss = 0.59363662\n",
            "Iteration 20, loss = 0.59059470\n",
            "Iteration 21, loss = 0.58768067\n",
            "Iteration 22, loss = 0.58509056\n",
            "Iteration 23, loss = 0.58291901\n",
            "Iteration 24, loss = 0.58068858\n",
            "Iteration 25, loss = 0.57839501\n",
            "Iteration 26, loss = 0.57652719\n",
            "Iteration 27, loss = 0.57459081\n",
            "Iteration 28, loss = 0.57278865\n",
            "Iteration 29, loss = 0.57109502\n",
            "Iteration 30, loss = 0.56941703\n",
            "Iteration 31, loss = 0.56786194\n",
            "Iteration 32, loss = 0.56626404\n",
            "Iteration 33, loss = 0.56474033\n",
            "Iteration 34, loss = 0.56335623\n",
            "Iteration 35, loss = 0.56198379\n",
            "Iteration 36, loss = 0.56064987\n",
            "Iteration 37, loss = 0.55930722\n",
            "Iteration 38, loss = 0.55797971\n",
            "Iteration 39, loss = 0.55688857\n",
            "Iteration 40, loss = 0.55585362\n",
            "Iteration 41, loss = 0.55460104\n",
            "Iteration 42, loss = 0.55350280\n",
            "Iteration 43, loss = 0.55254342\n",
            "Iteration 44, loss = 0.55148231\n",
            "Iteration 45, loss = 0.55054650\n",
            "Iteration 46, loss = 0.54968451\n",
            "Iteration 47, loss = 0.54878067\n",
            "Iteration 48, loss = 0.54795099\n",
            "Iteration 49, loss = 0.54724950\n",
            "Iteration 50, loss = 0.54650500\n",
            "Iteration 51, loss = 0.54582815\n",
            "Iteration 52, loss = 0.54513513\n",
            "Iteration 53, loss = 0.54446326\n",
            "Iteration 54, loss = 0.54388911\n",
            "Iteration 55, loss = 0.54329583\n",
            "Iteration 56, loss = 0.54272157\n",
            "Iteration 57, loss = 0.54214323\n",
            "Iteration 58, loss = 0.54161020\n",
            "Iteration 59, loss = 0.54106504\n",
            "Iteration 60, loss = 0.54045762\n",
            "Iteration 61, loss = 0.53996234\n",
            "Iteration 62, loss = 0.53947021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 63, loss = 0.53891887\n",
            "Iteration 64, loss = 0.53848063\n",
            "Iteration 65, loss = 0.53795218\n",
            "Iteration 66, loss = 0.53746444\n",
            "Iteration 67, loss = 0.53703847\n",
            "Iteration 68, loss = 0.53665493\n",
            "Iteration 69, loss = 0.53615533\n",
            "Iteration 70, loss = 0.53574124\n",
            "Iteration 71, loss = 0.53537037\n",
            "Iteration 72, loss = 0.53493488\n",
            "Iteration 73, loss = 0.53461346\n",
            "Iteration 74, loss = 0.53421632\n",
            "Iteration 75, loss = 0.53381969\n",
            "Iteration 76, loss = 0.53361070\n",
            "Iteration 77, loss = 0.53310850\n",
            "Iteration 78, loss = 0.53275768\n",
            "Iteration 79, loss = 0.53235864\n",
            "Iteration 80, loss = 0.53201836\n",
            "Iteration 81, loss = 0.53166741\n",
            "Iteration 82, loss = 0.53130179\n",
            "Iteration 83, loss = 0.53099631\n",
            "Iteration 84, loss = 0.53064131\n",
            "Iteration 85, loss = 0.53030533\n",
            "Iteration 86, loss = 0.53001820\n",
            "Iteration 87, loss = 0.52975476\n",
            "Iteration 88, loss = 0.52931934\n",
            "Iteration 89, loss = 0.52906048\n",
            "Iteration 90, loss = 0.52872691\n",
            "Iteration 91, loss = 0.52840069\n",
            "Iteration 92, loss = 0.52812607\n",
            "Iteration 93, loss = 0.52786876\n",
            "Iteration 94, loss = 0.52758447\n",
            "Iteration 95, loss = 0.52724741\n",
            "Iteration 96, loss = 0.52701274\n",
            "Iteration 97, loss = 0.52673875\n",
            "Iteration 98, loss = 0.52646010\n",
            "Iteration 99, loss = 0.52622235\n",
            "Iteration 100, loss = 0.52595421\n",
            "Test accuracy:  0.7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing additional libraries for plotting and tabulating\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Creating DataFrame for KNN results\n",
        "df_knn = pd.DataFrame(results_knn, columns=['K', 'Accuracy'])\n",
        "\n",
        "# Plotting the accuracy score for each K value\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_knn, x='K', y='Accuracy', marker='o')\n",
        "plt.title('KNN Accuracy for each K')\n",
        "plt.show()\n",
        "\n",
        "#Creating DataFrame for Decision Tree results\n",
        "df_tree = pd.DataFrame(results_tree, columns=['Max Depth', 'Min Samples Leaf', 'Accuracy'])\n",
        "\n",
        "# Since we have two hyperparameters for Decision Tree, we plot two different graphs\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_tree, x='Max Depth', y='Accuracy', marker='o')\n",
        "plt.title('Decision Tree Accuracy for each Max Depth value')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_tree, x='Min Samples Leaf', y='Accuracy', marker='o')\n",
        "plt.title('Decision Tree Accuracy for each Min Samples Leaf value')\n",
        "plt.show()\n",
        "\n",
        "# Creating DataFrame for MLP results\n",
        "df_mlp = pd.DataFrame(results_mlp, columns=['Learning Rate', 'Epochs', 'Hidden Layers', 'Accuracy'])\n",
        "\n",
        "# Again, since we have three hyperparameters for MLP, we plot three different graphs\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_mlp, x='Learning Rate', y='Accuracy', marker='o')\n",
        "plt.title('MLP Accuracy for each Learning Rate value')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_mlp, x='Epochs', y='Accuracy', marker='o')\n",
        "plt.title('MLP Accuracy for each Epoch value')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=df_mlp, x='Hidden Layers', y='Accuracy', marker='o')\n",
        "plt.title('MLP Accuracy for each Hidden Layer value')\n",
        "plt.show()\n",
        "\n",
        "#Creating a loss curve for neural network algorithm\n",
        "plt.plot(best_mlp.loss_curve_)\n",
        "plt.title('Training Loss Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J5R6LiGFyUwW",
        "outputId": "7ad151fc-9d67-46d4-fc8f-da9123e0973c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA18AAAIjCAYAAAD80aFnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAC6ZklEQVR4nOzdeXxU1f0//tfdZ0kySchKWBKCCiqioiKoRSuIO1oXsP0KUheKVEGrrbQftVWLH9fi9pHqrwJFtCgupZVKLYhWwX1BkX3fErJNZl/u8vtjMkOGmYQEspG8no9HlNx7586ZyUxyX3POeR/BsiwLRERERERE1K7Ezm4AERERERFRT8DwRURERERE1AEYvoiIiIiIiDoAwxcREREREVEHYPgiIiIiIiLqAAxfREREREREHYDhi4iIiIiIqAMwfBEREREREXUAhi8iIiIiIqIOwPBFRF3O9u3bIQgC5s2b16rbnXvuuTj33HPbpU1ElZWVuPrqq9GrVy8IgoDZs2d3dpPa3A033ICMjIzObkaPFP+99/jjj3d2UwAAK1euhCAIWLlyZWc3hahbYfgiohTz5s2DIAiJL5vNht69e2Ps2LF4+umn4fV6O7uJXUZpaWnSc9XUV2uDZHtZunQpBEFA7969YZpmZzfnqHLHHXdg2bJlmDlzJhYsWIALL7yws5vU5Zx77rkQBAHHHHNM2v3vvfde4j2xePHiDm1bPNzEvxRFQV5eHkaOHInf/va32LlzZ4e0Y+nSpfj973/fIfdFRF2P3NkNIKKu64EHHkBZWRmi0SgqKiqwcuVKzJgxA08++SSWLFmCk046qV3ut3///ggGg1AUpVW3+/e//90u7WnO7Nmz4fP5Et8vXboUr776Kv70pz8hLy8vsX3kyJEd3rZ0Fi5ciNLSUmzfvh0rVqzA6NGjO7tJR40VK1Zg3LhxuOuuuzq7KV2azWbD5s2b8dlnn+GMM85I2rdw4ULYbDaEQqFOah1w3XXX4eKLL4Zpmqirq8Pnn3+O2bNn46mnnsJf/vIXTJgwoV3vf+nSpXjuuecYwIh6KIYvImrSRRddhNNOOy3x/cyZM7FixQpceumluPzyy7Fu3TrY7fY2v994b1trqara5m05lCuuuCLp+4qKCrz66qu44oorUFpa2uTt/H4/nE5n+zYuzX3+/e9/x8MPP4y5c+di4cKFXTZ8dcbzcyj79+9HdnZ2m50vFApBVVWIYvcahFJeXg5d1/Hqq68mha9QKIS33noLl1xyCd54441Oa9+pp56K//f//l/Sth07duCCCy7ApEmTMHjwYAwdOrSTWkdE3V33+o1PRO3uxz/+Me69917s2LEDL7/8ctK+9evX4+qrr0Zubi5sNhtOO+00LFmyJOUcbrcbd9xxB0pLS6FpGvr06YOJEyeiuroaQPo5XxUVFZg8eTL69OkDTdNQXFyMcePGYfv27Ylj0s352r9/P2688UYUFhbCZrNh6NChmD9/ftIxjedavPDCCygvL4emaTj99NPx+eefH9kThgPzaLZs2YKLL74YmZmZ+NnPfgYAME0Ts2fPxgknnACbzYbCwkJMmTIFdXV1Kef517/+hXPOOQdOpxOZmZm45JJLsHbt2ha346233kIwGMQ111yDCRMm4M0330zbAxEKhfD73/8exx57LGw2G4qLi/GTn/wEW7ZsSRxjmiaeeuopDBkyBDabDfn5+bjwwgvxxRdfAGh+3p4gCEmf+v/+97+HIAj44Ycf8NOf/hQ5OTk4++yzAQBr1qzBDTfcgAEDBsBms6GoqAg///nPUVNTk3LePXv24MYbb0Tv3r2haRrKysowdepURCIRbN26FYIg4E9/+lPK7VatWgVBEPDqq6+mfd7iw3Aty8Jzzz2XGLYWt3XrVlxzzTXIzc2Fw+HAmWeeiXfeeSfpHPH5M3/729/wP//zPygpKYHD4YDH40l7n/HnuCWvjb///e+45JJLEo+7vLwcDz74IAzDSDnnp59+iosvvhg5OTlwOp046aST8NRTT6V9Lq+44gpkZGQgPz8fd911V9rzNeW6667DokWLkoa2/uMf/0AgEMC1116bcvyOHTtw66234rjjjoPdbkevXr1wzTXXJL2/LcvCeeedh/z8fOzfvz+xPRKJYMiQISgvL4ff729xGxvr378/5s2bh0gkgkcffTRpn9vtxowZM9C3b19omoaBAwfikUceSXpsjX+H/OlPf0L//v1ht9sxatQofP/994njbrjhBjz33HMAkDQE8mCt/T30xRdfQBCElN9tALBs2TIIgoB//vOfAFr2XDeltLQUN9xwQ8r2dL97w+Ew7r//fgwcOBCapqFv37749a9/jXA4fMj7IerO2PNFRK12/fXX47e//S3+/e9/4+abbwYArF27FmeddRZKSkpwzz33wOl04rXXXsMVV1yBN954A1deeSUAwOfz4ZxzzsG6devw85//HKeeeiqqq6uxZMkS7N69O2moXmNXXXUV1q5di9tuuw2lpaXYv38/3nvvPezcubPJHqZgMIhzzz0Xmzdvxi9/+UuUlZXh9ddfxw033AC3243p06cnHf/KK6/A6/ViypQpEAQBjz76KH7yk59g69atrR4CeTBd1zF27FicffbZePzxx+FwOAAAU6ZMwbx58zB58mTcfvvt2LZtG5599ll8/fXX+PjjjxP3u2DBAkyaNAljx47FI488gkAggOeffx5nn302vv7662Z72eIWLlyI8847D0VFRZgwYQLuuece/OMf/8A111yTOMYwDFx66aVYvnw5JkyYgOnTp8Pr9eK9997D999/j/LycgDAjTfeiHnz5uGiiy7CTTfdBF3X8d///heffPJJUm9pa1xzzTU45phjMGvWLFiWBSA2R2jr1q2YPHkyioqKsHbtWrzwwgtYu3YtPvnkk8SF6969e3HGGWfA7XbjlltuwaBBg7Bnzx4sXrwYgUAAAwYMwFlnnYWFCxfijjvuSHleMjMzMW7cuLTt+tGPfoQFCxbg+uuvx5gxYzBx4sTEvsrKSowcORKBQAC33347evXqhfnz5+Pyyy/H4sWLE6/7uAcffBCqquKuu+5COBxutre2pa+NefPmISMjA3feeScyMjKwYsUK3HffffB4PHjssccS53vvvfdw6aWXori4GNOnT0dRURHWrVuHf/7zn0nvBcMwMHbsWAwfPhyPP/44/vOf/+CJJ55AeXk5pk6d2pIfJX7605/i97//PVauXIkf//jHAGLvr/PPPx8FBQUpx3/++edYtWoVJkyYgD59+mD79u14/vnnce655+KHH36Aw+GAIAh46aWXcNJJJ+EXv/gF3nzzTQDA/fffj7Vr12LlypVH1Fs6YsQIlJeX47333ktsCwQCGDVqFPbs2YMpU6agX79+WLVqFWbOnIl9+/alFF3561//Cq/Xi2nTpiEUCuGpp57Cj3/8Y3z33XeJ8Lx371689957WLBgQdp2HM7vodNOOw0DBgzAa6+9hkmTJiXtW7RoEXJycjB27FgALXuuj5Rpmrj88svx0Ucf4ZZbbsHgwYPx3Xff4U9/+hM2btyIt99++4jvg+ioZRERHWTu3LkWAOvzzz9v8hiXy2Wdcsopie/PP/98a8iQIVYoFEpsM03TGjlypHXMMccktt13330WAOvNN99MOadpmpZlWda2bdssANbcuXMty7Ksuro6C4D12GOPNdvuUaNGWaNGjUp8P3v2bAuA9fLLLye2RSIRa8SIEVZGRobl8XiS7q9Xr15WbW1t4ti///3vFgDrH//4R7P329hjjz1mAbC2bduW2DZp0iQLgHXPPfckHfvf//7XAmAtXLgwafu7776btN3r9VrZ2dnWzTffnHRcRUWF5XK5UranU1lZacmybL344ouJbSNHjrTGjRuXdNxLL71kAbCefPLJlHPEfz4rVqywAFi33357k8cc/DNsDIB1//33J76///77LQDWddddl3JsIBBI2fbqq69aAKwPP/wwsW3ixImWKIppX7PxNv35z3+2AFjr1q1L7ItEIlZeXp41adKklNula/e0adOSts2YMcMCYP33v/9NbPN6vVZZWZlVWlpqGYZhWZZlvf/++xYAa8CAAWkf08Fa+tqwrPTP0ZQpUyyHw5F4P+q6bpWVlVn9+/e36urqko6NPz+WdeC1+sADDyQdc8opp1jDhg07ZLtHjRplnXDCCZZlWdZpp51m3XjjjZZlxd7Dqqpa8+fPTzwXr7/+erOPYfXq1RYA669//WvS9vjP8eWXX7Y++eQTS5Ika8aMGYdsW/w12dzvkXHjxlkArPr6esuyLOvBBx+0nE6ntXHjxqTj7rnnHkuSJGvnzp1J57bb7dbu3bsTx3366acWAOuOO+5IbJs2bZqV7vLrSH8PzZw501IUJem24XDYys7Otn7+858ntrX0uY7/nN5///3Etv79+6d9rxz8u3fBggWWKIpJ7wvLsqw5c+ZYAKyPP/642cdC1J1x2CERHZaMjIxE1cPa2lqsWLEC1157LbxeL6qrq1FdXY2amhqMHTsWmzZtwp49ewAAb7zxBoYOHZrSIwAg7fAbALDb7VBVFStXrkw7HK8pS5cuRVFREa677rrENkVRcPvtt8Pn8+GDDz5IOn78+PHIyclJfH/OOecAiA0rawsH9xq8/vrrcLlcGDNmTOI5q66uxrBhw5CRkYH3338fQKzHwu1247rrrks6TpIkDB8+PHFcc/72t79BFEVcddVViW3XXXcd/vWvfyU9p2+88Qby8vJw2223pZwj/vN54403IAgC7r///iaPORy/+MUvUrY1nlMYCoVQXV2NM888EwDw1VdfAYh9yv7222/jsssuS9vrFm/TtddeC5vNhoULFyb2LVu2DNXV1SlzgFpq6dKlOOOMMxLDJIHYe+OWW27B9u3b8cMPPyQdP2nSpBbNk2zpawNIfo7i779zzjkHgUAA69evBwB8/fXX2LZtG2bMmJEyby3dz+zgn8U555zT6vfBT3/6U7z55puIRCJYvHgxJElK+74/+DFEo1HU1NRg4MCByM7OTvyc42655RaMHTsWt912G66//nqUl5dj1qxZrWpbU+Jl9uO/215//XWcc845yMnJSfo5jB49GoZh4MMPP0y6/RVXXIGSkpLE92eccQaGDx+OpUuXtrgNh/t7aPz48YhGo4keQSBWhMjtdmP8+PGJba15rg/X66+/jsGDB2PQoEFJz1u8F7Qlv7OIuiuGLyI6LD6fD5mZmQCAzZs3w7Is3HvvvcjPz0/6il+gx+dobNmyBSeeeGKr7kvTNDzyyCP417/+hcLCQvzoRz/Co48+ioqKimZvt2PHDhxzzDEpBQ0GDx6c2N9Yv379kr6PXwC1JvA1RZZl9OnTJ2nbpk2bUF9fj4KCgpTnzefzJZ6zTZs2AYjNtzv4uH//+99J81+a8vLLL+OMM85ATU0NNm/ejM2bN+OUU05BJBLB66+/njhuy5YtOO644yDLTY9K37JlC3r37o3c3NzDeSqaVFZWlrKttrYW06dPR2FhIex2O/Lz8xPH1dfXAwCqqqrg8XgO+brKzs7GZZddhldeeSWxbeHChSgpKUlcFLbWjh07cNxxx6Vsb+o1lu4xptPS1wYQG/J75ZVXwuVyISsrC/n5+YkwGX+O4vP1WvLei8/haywnJ6fV74MJEyagvr4e//rXv7Bw4UJceumlid8ZBwsGg7jvvvsS86ry8vKQn58Pt9udeAyN/eUvf0EgEMCmTZswb968Niv8E69cGm/npk2b8O6776b8DOKFag5+76UrsX/ssce2aD5V3OH+Hho6dCgGDRqERYsWJbYtWrQIeXl5Sa/v1j7Xh2PTpk1Yu3ZtyvN27LHHAkh93oh6Es75IqJW2717N+rr6zFw4EAASEw8v+uuuxLzCg4WP/ZwzZgxA5dddhnefvttLFu2DPfeey8efvhhrFixAqeccsoRnTtOkqS0262G+UdHQtO0lBBomiYKCgqSemIai18Ax5/fBQsWoKioKOW45oISELsQik/YT3dxuHDhQtxyyy2HfhCt0FQPWHNFG9JdQF977bVYtWoV7r77bpx88snIyMiAaZq48MILD2udsokTJ+L111/HqlWrMGTIECxZsgS33nprh1UcbGlIaOlrw+12Y9SoUcjKysIDDzyA8vJy2Gw2fPXVV/jNb35zWM9RU++D1iouLsa5556LJ554Ah9//HGzFQ5vu+02zJ07FzNmzMCIESPgcrkgCAImTJiQ9jGsXLkyUbjhu+++w4gRI9qkzd9//z0KCgqQlZUFIPZzGDNmDH7961+nPT4eJtrSkfweGj9+PP74xz+iuroamZmZWLJkCa677rqk3xGtfa4ba+593bjdpmliyJAhePLJJ9Me37dv30M+FqLuiuGLiFotPlE8HrQGDBgAIDak71Cly8vLy5Oqf7VGeXk5fvWrX+FXv/oVNm3ahJNPPhlPPPFEStXFuP79+2PNmjUwTTPp4jo+FKt///6H1Y62Ul5ejv/85z8466yzmr0ojxe5KCgoOKzS8AsXLoSiKFiwYEHKhd1HH32Ep59+Gjt37kS/fv1QXl6OTz/9FNFotMnJ/eXl5Vi2bBlqa2ub7P2Kf1rvdruTth/cE9Scuro6LF++HH/4wx9w3333JbbHewLj8vPzkZWV1aLX1YUXXoj8/HwsXLgQw4cPRyAQwPXXX9/iNh2sf//+2LBhQ8r2I32NtfS1sXLlStTU1ODNN9/Ej370o8T2bdu2pZwPiIWLjlxe4Kc//SluuukmZGdn4+KLL27yuMWLF2PSpEl44oknEttCoVDK6wcA9u3bh9tuuw0XXHBBonjJ2LFjj/j9vHr1amzZsiVpCGp5eTl8Pl+Ln7ODX5sAsHHjxqSCOEcyNPdQxo8fjz/84Q944403UFhYCI/Hk7JuWWue64Pl5OSkPW7Hjh2JvwNA7Hn79ttvcf7557fr4yU6GnHYIRG1yooVK/Dggw+irKwsUS69oKAA5557Lv785z9j3759KbepqqpK/Puqq67Ct99+i7feeivluKY+2Q0EAikl0cvLy5GZmdls2eKLL74YFRUVScNwdF3HM888g4yMDIwaNar5B9vOrr32WhiGgQcffDBln67riYucsWPHIisrC7NmzUI0Gk05tvHzm87ChQtxzjnnYPz48bj66quTvu6++24ASJRZv+qqq1BdXY1nn3025Tzxn89VV10Fy7Lwhz/8ocljsrKykJeXlzIn5v/+7/+abWtj8aB48Ovi4ApzoijiiiuuwD/+8Y9Eqft0bQJivYTXXXcdXnvtNcybNw9Dhgw5osXCL774Ynz22WdYvXp1Ypvf78cLL7yA0tJSHH/88Yd13pa+NtI9R5FIJOV5PvXUU1FWVobZs2enXDy3Rc9uU66++mrcf//9+L//+79mKztKkpTSjmeeeSZtT+nNN98M0zTxl7/8BS+88AJkWcaNN954RI9jx44duOGGG6CqauI9AcR+DqtXr8ayZctSbuN2u6HretK2t99+OzG/FQA+++wzfPrpp7jooosS2+IVGVsSdlpr8ODBGDJkCBYtWoRFixahuLg4KZQDrXuuD1ZeXo5PPvkEkUgkse2f//wndu3alXTctddeiz179uDFF19MOUcwGDzsJQGIugP2fBFRk/71r39h/fr10HUdlZWVWLFiBd577z30798fS5YsSVoI+bnnnsPZZ5+NIUOG4Oabb8aAAQNQWVmJ1atXY/fu3fj2228BAHfffTcWL16Ma665Bj//+c8xbNgw1NbWYsmSJZgzZ07axU03btyI888/H9deey2OP/54yLKMt956C5WVlSmf6jZ2yy234M9//jNuuOEGfPnllygtLcXixYvx8ccfY/bs2U3OP+koo0aNwpQpU/Dwww/jm2++wQUXXABFUbBp0ya8/vrreOqpp3D11VcjKysLzz//PK6//nqceuqpmDBhAvLz87Fz50688847OOuss9KGJSC2rlO81H46JSUlOPXUU7Fw4UL85je/wcSJE/HXv/4Vd955Jz777DOcc8458Pv9+M9//oNbb70V48aNw3nnnYfrr78eTz/9NDZt2pQYAvjf//4X5513XuK+brrpJvzv//4vbrrpJpx22mn48MMPsXHjxhY/P1lZWYn5fdFoFCUlJfj3v/+d0qsDALNmzcK///1vjBo1KlHaet++fXj99dfx0UcfJRWZmDhxIp5++mm8//77eOSRR1rcnnTuuecevPrqq7joootw++23Izc3F/Pnz8e2bdvwxhtvHPZwxpa+NkaOHImcnBxMmjQJt99+OwRBwIIFC1IurkVRxPPPP4/LLrsMJ598MiZPnozi4mKsX78ea9euTRsu2oLL5Upa060pl156KRYsWACXy4Xjjz8eq1evxn/+8x/06tUr6bi5c+finXfewbx58xJzKJ955hn8v//3//D888/j1ltvPeR9ffXVV3j55Zdhmibcbjc+//zzRBGZBQsWJIXxu+++G0uWLMGll16KG264AcOGDYPf78d3332HxYsXY/v27UnLYwwcOBBnn302pk6dinA4jNmzZ6NXr15JwxaHDRsGALj99tsxduxYSJLU7O+x1ho/fjzuu+8+2Gw23HjjjSmvwZY+1+ncdNNNWLx4MS688EJce+212LJlC15++eVEz2rc9ddfj9deew2/+MUv8P777+Oss86CYRhYv349XnvtNSxbtuywl6QgOup1QoVFIuri4qXm41+qqlpFRUXWmDFjrKeeeipRov1gW7ZssSZOnGgVFRVZiqJYJSUl1qWXXmotXrw46biamhrrl7/8pVVSUmKpqmr16dPHmjRpklVdXW1ZVmqZ8urqamvatGnWoEGDLKfTablcLmv48OHWa6+9lnTeg8sdW1asxPrkyZOtvLw8S1VVa8iQISnlz5srQY2DyqIfSlOl5p1OZ5O3eeGFF6xhw4ZZdrvdyszMtIYMGWL9+te/tvbu3Zt03Pvvv2+NHTvWcrlcls1ms8rLy60bbrjB+uKLL5o892233WYBsLZs2dLkMb///e8tANa3335rWVasFPXvfvc7q6yszFIUxSoqKrKuvvrqpHPoum499thj1qBBgyxVVa38/Hzroosusr788svEMYFAwLrxxhstl8tlZWZmWtdee621f//+JkvNV1VVpbRt9+7d1pVXXmllZ2dbLpfLuuaaa6y9e/em/bns2LHDmjhxopWfn29pmmYNGDDAmjZtmhUOh1POe8IJJ1iiKCaVBT8UpCk1b1mx1/3VV19tZWdnWzabzTrjjDOsf/7zn0nHpCuv3hIteW18/PHH1plnnmnZ7Xard+/e1q9//Wtr2bJlKWXCLcuyPvroI2vMmDFWZmam5XQ6rZNOOsl65plnEvubeq3Gf0aH0rjUfFPSPRd1dXWJ92lGRoY1duxYa/369UmlzXft2mW5XC7rsssuSznnlVdeaTmdTmvr1q1N3m/8fR7/kmXZys3NtYYPH27NnDnT2rFjR9rbeb1ea+bMmdbAgQMtVVWtvLw8a+TIkdbjjz9uRSKRpHM/9thj1hNPPGH17dvX0jTNOueccxLvqzhd163bbrvNys/PtwRBSDyvbfV7aNOmTYnH+NFHH6Xsb8lzbVnpS81blmU98cQTVklJiaVpmnXWWWdZX3zxRdrfvZFIxHrkkUesE044wdI0zcrJybGGDRtm/eEPf0iU8ifqiQTLasfxBkRERF3QKaecgtzcXCxfvryzm0LdwPbt21FWVobHHnsMd911V2c3h4i6MM75IiKiHuWLL77AN998g4kTJ3Z2U4iIqIfhnC8iIuoRvv/+e3z55Zd44oknUFxcnLTwLBERUUdgzxcREfUIixcvxuTJkxGNRvHqq68mFYwhIiLqCJzzRURERERE1AHY80VERERERNQBGL6IiIiIiIg6AAtuHCbTNLF3715kZmZCEITObg4REREREXUSy7Lg9XrRu3fvlMXNG2P4Okx79+5F3759O7sZRERERETURezatQt9+vRpcj/D12HKzMwEEHuCs7KyOrk1RERERETUWTweD/r27ZvICE1h+DpM8aGGWVlZDF9ERERERHTI6UgsuEFERERERNQBGL6IiIiIiIg6AMMXERERERFRB2D4IiIiIiIi6gAMX0RERERERB2A4YuIiIiIiKgDMHwRERERERF1AIYvIiIiIiKiDsDwRURERERE1AEYvoiIiIiIiDoAwxcREREREVEHYPgiIiIiIiLqAAxfREREREREHYDhi4iIiIiIqAMwfBEREREREXUAhi8iIiIiIqIOwPBFRERERETUARi+uon6QBSmaXV2M4iIiIiIqAkMX92AYVrYVuOHOxjt7KYQEREREVETGL66ibBuIBDRO7sZRERERETUBIavbkI3LHhC7PkiIiIiIuqqGL66CYcqIRA2OO+LiIiIiKiLkju7AXRkghEdkiiiJNsOl0OBP6Ij06Z0drOIiIiIiOggDF9HsXDUwJwPtmLuqm3wBHVk2WVMHlmGW88th6ZInd08IiIiIiJqhOHrKBWM6JjzwVY8tXxTYpsnqCe+nzJqABwqf7xERERERF0F53wdpSRRxNxV29Lum7tqG2SRP1oiIiIioq6EV+hHKW8oCk8wfWl5T1CHl5UPiYiIiIi6FIavo1SmTUGWPf2wwiy7zKIbRERERERdDMPXUcowTUweWZZ23+SRZdBNs4NbREREREREzWFFhqOUXZVx67nlAJBU7fCGkaX4xahy2FVWOyQiIiIi6koEy7K4Ku9h8Hg8cLlcqK+vR1ZWVqe1IxDRIYsi9ntDyHWqqPVF4A3pGNy789pERERERNSTtDQbsOfrKOdQZYSjBqYs+BL76kP4n0sGoyDLhohuQpU5qpSIiIiIqKvg1Xk3IEsiQlEDtf4I9rqDCEUNBKNGZzeLiIiIiIgaYfjqJvr3cgIAdtQEYBgmghGGLyIiIiKirqTTw9dzzz2H0tJS2Gw2DB8+HJ999lmzx7vdbkybNg3FxcXQNA3HHnssli5dmtjv9XoxY8YM9O/fH3a7HSNHjsTnn3+edI4bbrgBgiAkfV144YXt8vg6SmkvBwBgS5UfoijAF+Y6X0REREREXUmnzvlatGgR7rzzTsyZMwfDhw/H7NmzMXbsWGzYsAEFBQUpx0ciEYwZMwYFBQVYvHgxSkpKsGPHDmRnZyeOuemmm/D9999jwYIF6N27N15++WWMHj0aP/zwA0pKShLHXXjhhZg7d27ie03T2vWxtidJFDCwIAMAsK3aB02S4A4wfBERERERdSWdGr6efPJJ3HzzzZg8eTIAYM6cOXjnnXfw0ksv4Z577kk5/qWXXkJtbS1WrVoFRYktIlxaWprYHwwG8cYbb+Dvf/87fvSjHwEAfv/73+Mf//gHnn/+eTz00EOJYzVNQ1FRUTs+uo41pMQFQQDqAlEEozogAKGoAZvCkvNERERERF1Bpw07jEQi+PLLLzF69OgDjRFFjB49GqtXr057myVLlmDEiBGYNm0aCgsLceKJJ2LWrFkwjNj8Jl3XYRgGbDZb0u3sdjs++uijpG0rV65EQUEBjjvuOEydOhU1NTXNtjccDsPj8SR9dSW5GRqKsmKPe3ddCBGd876IiIiIiLqSTgtf1dXVMAwDhYWFSdsLCwtRUVGR9jZbt27F4sWLYRgGli5dinvvvRdPPPFEokcrMzMTI0aMwIMPPoi9e/fCMAy8/PLLWL16Nfbt25c4z4UXXoi//vWvWL58OR555BF88MEHuOiiixIhLp2HH34YLpcr8dW3b982eBbajiaL6JsTm/e1o8YPw7RY8ZCIiIiIqAvp9IIbrWGaJgoKCvDCCy9g2LBhGD9+PH73u99hzpw5iWMWLFgAy7JQUlICTdPw9NNP47rrroMoHnioEyZMwOWXX44hQ4bgiiuuwD//+U98/vnnWLlyZZP3PXPmTNTX1ye+du3a1Z4PtdVsioTSvIaiG9WxohveEOd9ERERERF1FZ0WvvLy8iBJEiorK5O2V1ZWNjkXq7i4GMceeywk6cA8psGDB6OiogKRSAQAUF5ejg8++AA+nw+7du3CZ599hmg0igEDBjTZlgEDBiAvLw+bN29u8hhN05CVlZX01ZXYZBHl+bGiG1urfLDJItyBKCzL6uSWERERERER0InhS1VVDBs2DMuXL09sM00Ty5cvx4gRI9Le5qyzzsLmzZthmmZi28aNG1FcXAxVVZOOdTqdKC4uRl1dHZYtW4Zx48Y12Zbdu3ejpqYGxcXFR/ioOo8siRhcFAuE++pDsKxYwY1Q1DzELYmIiIiIqCN06rDDO++8Ey+++CLmz5+PdevWYerUqfD7/YnqhxMnTsTMmTMTx0+dOhW1tbWYPn06Nm7ciHfeeQezZs3CtGnTEscsW7YM7777LrZt24b33nsP5513HgYNGpQ4p8/nw913341PPvkE27dvx/LlyzFu3DgMHDgQY8eO7dgnoI31zrEhxxGrArnHHURYNxGI6J3cKiIiIiIiAjq51Pz48eNRVVWF++67DxUVFTj55JPx7rvvJopw7Ny5M2muVt++fbFs2TLccccdOOmkk1BSUoLp06fjN7/5TeKY+vp6zJw5E7t370Zubi6uuuoq/PGPf0yUppckCWvWrMH8+fPhdrvRu3dvXHDBBXjwwQeP6rW+AMChyuib60BdoB7bqv3IzVBZdIOIiIiIqIsQLE4KOiwejwculwv19fVdZv5XrT+C+/7+Pf65Zh/GDC7EhNP7ojjbhuN7uzq7aURERERE3VZLs8FRVe2QmmdTRJTlOQEAW6p90BQJ9UEdpsl8TURERETU2Ri+uhFNljCwoeLhzpoAJBEIRw0OPSQiIiIi6gIYvroRSRTQv5cDDlWCblrY7wkjrBsIRBi+iIiIiIg6G8NXN+OyK+iTYwcAbK32AwIQZPgiIiIiIup0DF/djL2h4iEAbKv2Q5UkuIORTm4VERERERExfHUzmiyif25D0Y0qHzRZhDekQze42DIRERERUWdi+OpmbIqEAXmNer5kMTbvi0U3iIiIiIg6FcNXN6PJIvrnOaFIAgIRA3X+KKKGhRDnfRERERERdSqGr25GFAXkOBT0zo4V3dhS5QMA+MN6ZzaLiIiIiKjHY/jqhjJtCvrmxIYebq32Q5NEuIPRTm4VEREREVHPxvDVDdkUCf1yG8rNV/lgUyT4wzoiOotuEBERERF1FoavbsgmS+jXUPFwa7UfmiwiFDW53hcRERERUSdi+OqGNEVEWb4DAoBafwS+sA7TshBkxUMiIiIiok7D8NUNabIIl11FkcsGINb7BQC+MOd9ERERERF1FoavbkgQBGTZZPTJic/78sMmS3AHorAsq5NbR0RERETUMzF8dVOZNiURvrZV+6ApIgIRA2EW3SAiIiIi6hQMX92UpojomxMrurGlyg9NlhBm0Q0iIiIiok7D8NVNabKE0rzYWl973UFEdBMmLARYdIOIiIiIqFMwfHVTNkVEfqaGHIcCC8COGj8kQYAvxKIbRERERESdgeGrm9JkqWGx5Vjv15aG9b7qWHSDiIiIiKhTMHx1YxmajD45sfC1tcoHmyIhHDW43hcRERERUSdg+OrGYuGrodx8tR+qLCKss+gGEREREVFnYPjqxmyKhL4N4WtHjR+macG0gADDFxERERFRh2P46sZsioiibBscqoSoYWF3XRCKJMATZNENIiIiIqKOxvDVjdkUCTZZQv+Gohtbq/2wyRI8QR2GyaIbREREREQdieGrG1MksWHo4YGiG5oiIqTrLLpBRERERNTBGL66OZddQe/GRTckERHdRCCid3LLiIiIiIh6Foavbs6hyeiTYwMAbK32xTYKQChidmKriIiIiIh6Hoavbk6TRRS77JBFAf6wgf3eMFRJQl0g0tlNIyIiIiLqURi+ujmbIsGhSuib22ixZVmCL6xDN9j7RURERETUURi+ujmbLEKVJfRrVPEwVnTDQIBFN4iIiIiIOgzDVzcnSyKcqow+8aIbVX4okgjdMBHkYstERERERB2G4asHyLBJKMk+UPEQAAQICIRZ8ZCIiIiIqKMwfPUADlVG74bwVe0LwxOMQpVE1AVZdIOIiIiIqKMwfPUA8aIbxa54yXk/bIqEQNhAWOfQQyIiIiKijsDw1QPYFBGqLKK0lxNAQ8VDRUIoanK9LyIiIiKiDsLw1QNosgRVEtE398C8L0kUYFgWAlHO+yIiIiIi6ggMXz2AJApwajL6xItuVPkAACIE+EIMX0REREREHYHhq4fIsskobghfe9xBhKIGNEWEOxiFZVmd3DoiIiIiou6P4auHsKsysuwKsh0KTAvYUROAJosIRHSEdc77IiIiIiJqbwxfPYQmixAhYEBeQ9GNah80WUIkaiHAxZaJiIiIiNodw1cPYVMkqLKAfrkOAMDWqljRDRMmglGGLyIiIiKi9sbw1UNosghVltAnpyF8VceKbkiCCE8w2plNIyIiIiLqERi+eghRFJBhk1DSUHRje3UAhmnBpkioD0Zhmiy6QURERETUnhi+epAsm4JspwK7IiFimNjjDkKTRYSiBkI6hx4SEREREbUnhq8exKZIkASgNF50o8oHTRYR1k0W3SAiIiIiamcMXz2ITZYgCiLKEhUP/RAEAbCAIMMXEREREVG7YvjqQTRFhCIL6JsTm/e1tSpWdEOWBNQHI53ZNCIiIiKibo/hqwfRZBFa44qHVX5YlgWbLMEbNGCw6AYRERERUbth+OpBBEFAlk1GfqYKSRTgDeuo9kVgUySEdJ3rfRERERERtSOGrx4m06ZAEHBg6GG1D6osImJYCET0Tm4dEREREVH3xfDVw2iyCMsSMCA/A0Bs6CEAwAICYYYvIiIiIqL2wvDVw2iKBFkSUNqrYd5XdazohiaLqA8yfBERERERtReGrx7GpojQZBF9GxXdAABNluAN64gaZmc2j4iIiIio22L46mE0WYJNkVCcbQMA7PeG4Q1FoSkiwrrBxZaJiIiIiNoJw1cPlKHJkEURhVkaAGBbtR+KJMIwTIRY8ZCIiIiIqF0wfPVAGZoMw7QwIO+gohsQ4Atx3hcRERERUXtg+OqBbIoEwEJ5vhMAsCVRdEOCOxjpxJYREREREXVfDF89kE0Rocgi+uXGim5sa+j5sikiAhEDYZ1DD4mIiIiI2hrDVw9kUySokog+DRUPd9UFENYNaLKEcNREkEU3iIiIiIjaHMNXD6RIImyKBKcqwWVXYFrAzpoAJFGAYVmseEhERERE1A4Yvnool11BxLRQlheb97W1Ojb0UIQAf5hFN4iIiIiI2hrDVw/l0GQYpnmg6EZVQ9ENRYQ7GIVlWZ3ZPCIiIiKibofhq4fSZBGCICR6vrY19HzZZAmBiI6wbnZm84iIiIiIuh2Grx4qVnRDQN+Gohvbqv0wTAuaIiIcNTnvi4iIiIiojTF89VA2WYQqS8hzatBkEWHdxN76IERBgGlZCEQ474uIiIiIqC0xfPVQsiTCqcqImiZKezUMPWxY70sRRXhDDF9ERERERG2J4asHy7BJiBoWBuTHKx7Gi25IqA9GYZosukFERERE1FYYvnowhyrDhIXy/AwAwJaGni9NFhGKGghGOe+LiIiIiKitMHz1YDZFggCgtNeBohuWZSXmgDF8ERERERG1HYavHsymiFBlEcUuO0QBqA9GUeuPQBAEWJaFICseEhERERG1mU4PX8899xxKS0ths9kwfPhwfPbZZ80e73a7MW3aNBQXF0PTNBx77LFYunRpYr/X68WMGTPQv39/2O12jBw5Ep9//nnSOSzLwn333Yfi4mLY7XaMHj0amzZtapfH15VpsgRVEiEISJScjw89VCUJ7mCkM5tHRERERNStdGr4WrRoEe68807cf//9+OqrrzB06FCMHTsW+/fvT3t8JBLBmDFjsH37dixevBgbNmzAiy++iJKSksQxN910E9577z0sWLAA3333HS644AKMHj0ae/bsSRzz6KOP4umnn8acOXPw6aefwul0YuzYsQiFQu3+mLsSSRTg1GREdBNl+fHFlmNFN2yKCG9Qh8GiG0REREREbUKwLKvTrq6HDx+O008/Hc8++ywAwDRN9O3bF7fddhvuueeelOPnzJmDxx57DOvXr4eiKCn7g8EgMjMz8fe//x2XXHJJYvuwYcNw0UUX4aGHHoJlWejduzd+9atf4a677gIA1NfXo7CwEPPmzcOECRNa1HaPxwOXy4X6+npkZWUdzsPvErZV+bBpvw+fbq3FXz7ehhEDeuG3Fw9GRDdRH4rg9NJcZNpSn2siIiIiIoppaTbotJ6vSCSCL7/8EqNHjz7QGFHE6NGjsXr16rS3WbJkCUaMGIFp06ahsLAQJ554ImbNmgXDiM1N0nUdhmHAZrMl3c5ut+Ojjz4CAGzbtg0VFRVJ9+tyuTB8+PAm7xcAwuEwPB5P0ld3YFdlWECi3Py26oZhh7KIqMF5X0REREREbaXTwld1dTUMw0BhYWHS9sLCQlRUVKS9zdatW7F48WIYhoGlS5fi3nvvxRNPPIGHHnoIAJCZmYkRI0bgwQcfxN69e2EYBl5++WWsXr0a+/btA4DEuVtzvwDw8MMPw+VyJb769u172I+9K9FkESIE9MuNzfmq8ITgDx9YYDkQ4WLLRERERERtodMLbrSGaZooKCjACy+8gGHDhmH8+PH43e9+hzlz5iSOWbBgASzLQklJCTRNw9NPP43rrrsOonhkD3XmzJmor69PfO3atetIH06XYFMkqLIAmyKhIFMDAGxt6P3SJBHuAMMXEREREVFb6LTwlZeXB0mSUFlZmbS9srISRUVFaW9TXFyMY489FpIkJbYNHjwYFRUViERilfnKy8vxwQcfwOfzYdeuXfjss88QjUYxYMAAAEicuzX3CwCapiErKyvpqzvQZBGqLCGsmyjLSy66oSkSfBEdUcPszCYSEREREXULnRa+VFXFsGHDsHz58sQ20zSxfPlyjBgxIu1tzjrrLGzevBmmeSAMbNy4EcXFxVBVNelYp9OJ4uJi1NXVYdmyZRg3bhwAoKysDEVFRUn36/F48OmnnzZ5v92ZKArIsEmI6CbK8zMAHCg3b5NFhKIGApz3RURERER0xDp12OGdd96JF198EfPnz8e6deswdepU+P1+TJ48GQAwceJEzJw5M3H81KlTUVtbi+nTp2Pjxo145513MGvWLEybNi1xzLJly/Duu+9i27ZteO+993Deeedh0KBBiXMKgoAZM2bgoYcewpIlS/Ddd99h4sSJ6N27N6644ooOffxdRZZNQdQ0E0U3tlbFer5kSYRhmCy6QURERETUBuTOvPPx48ejqqoK9913HyoqKnDyySfj3XffTRTD2LlzZ9Jcrb59+2LZsmW44447cNJJJ6GkpATTp0/Hb37zm8Qx9fX1mDlzJnbv3o3c3FxcddVV+OMf/5hUmv7Xv/41/H4/brnlFrjdbpx99tl49913U6ok9hQ2RYIAKzHscFddEFHDhCKJEEUBvnAUQM98boiIiIiI2kqnrvN1NOsu63wBQH0gis+316KXU8XElz6DN6zjT9eejIEFGaj1R+DUJJxWmtvZzSQiIiIi6pK6/Dpf1HVoighFFhA1rANDDxuKbtgUEcGogVCUQw+JiIiIiI4EwxdBk0VosoSIYaIsL1Z0Y1tD0Q1NlhCOmgxfRERERERHiOGLIAgCsmwywrqB8oaery0Na31JogDDtFjxkIiIiIjoCDF8EQAg06ZAN00MaCg3v73aD7NhOqAoCvCFuNgyEREREdGRYPgiALGhh5YloCTbDlWKzfOqqA8BiK33VReIgLVZiIiIiIgOH8MXAQA0RYIsCbAsC6V5DgDAlob1vjRZQihqIBQ1mzsFERERERE1g+GLAMSqGmqyiIhhYkC86EbDvC9NiW0PRDj0kIiIiIjocHXqIsvUdWiyBJsiIRgxEuXmtzRUPBQFAaYFBNuh4qFlWQjrJsK6iYhuIqwbyLQpcNmVQ9+YiIiIiOgowvBFCRmaDE8gmuj5iq/1BQCyIMATjAI5h3fueLCKNAStcNSAN6zDHzYQNUxEDBOGYUI3LZTlOeGyu9riIRERERERdRkMX5SQocnQTQv9ezkgCoA7EEWdP4IcpwpNkVAf1GGaFkRRSHt73WjcgxULW/6wDl9YR0SPBayo0VBBUQBkUYQixYY7ZmoyZEmEL6yj1h9BKGrApkgd+fCJiIiIiNoVwxclxMKOBZsioSTbjl11QWyp9uE0Zy5sighfSIcvokMRxURPVliPzQXzhnSEdRPRhpAFABYAWRSgSrGQ5VBlyKIAQUgf3gDAoUqoqI/AE4oyfBERERFRt8LwRQk2RYQii4gasfW+dtUFsbXKj9P658bKz+sG1u72IGIYiBoWDMsCYEESxETAyrDJUCQRYjMBqzmiIEASRdT6IijItLXtAyQiIiIi6kQMX5SgyRJUKdarNSDPiQ82VmFrQ8VDQRCQY1dhmBYcaixgSU0MPzxSDlVCjT+CiG5ClVmQk4iIiIi6B17ZUoIqi7ApUix85TcU3ag6UHTDqcnIsiuwKVK7BS8AcKgy/GEdnlC03e6DiIiIiKijMXxREpddQdgwUZYXKze/rz7U4et7xYOdOxDp0PslIiIiImpPDF+UxKHJMEwTLruCvAwVwIHFlju0HaqMKk8EekPxDiIiIiKiox3DFyXRZBGCIMCyrAPrfVV1fPjK0GT4wjo8oY7tdSMiIiIiai8MX5TEpkhQJQFRw8KA/NjQw8aLLXcUSRRgwUI9hx4SERERUTfB8EVJbLIIVZYSFQ8BJCoedjS7IqHSG4ZhWp1y/0REREREbYnhi5LIkginKiOsG4mKhztrAoh2wtwrpybDF9LhZdVDIiIiIuoGGL4oRYZNQtSwUJCpwalJ0E0Lu+sCHd4ORRKhmxbqgwxfRERERHT0Y/iiFA5VhgkLgiAkim5s6YSiGwDgUCTs94ZhcughERERER3lGL4ohU2RIAAwLevAvK+q9i+6IYkCNFlMWsDZqcnwhqLwhln1kIiIiIiObnJnN4C6HpsiQpXEWNGN/PYvuqHJIrKdCrLtCuqDUbjsCtyBKNyB2HDDiGHC07CdiIiIiOhoxfBFKTRZgiqLiBpmYtjhtmo/TMuCKAiHuHVr70tEn1w75nywBfNWbYcnqCPLLmPyyDJMGTUAu2uDsEkS9ntD6JNjh9DG909ERERE1FE47JBSSKIApyYjopvok2OHIgkIRAxUekJtfl/ZTgVzPtiCp5dvhicYG1roCep4avkm/PmDrch2KLGhh0EdPg49JCIiIqKjGMMXpZVlkxExTMiSiP658XlfbTv0UBIFuOwK5q3annb/3FXbEuErpJvwhBi+iIiIiOjoxWGHlJZdlRGvLzgg34naQASBiAFJFA5r0WPLslDlC2N7tR/bqv3YVhOAKACzrhyS6PE6mCeooz6oQ24oxFHtDaMk234Ej4qIiIiIqPMwfFFamixChABZFDBj9LEozXOgzh9FfqaaKIYR1tMvvByKGthZG8C2an8sbNX4sb3GD3/YSDou16miV4aKLLucNoBl2WW47DKqvGE4VRnuQAT+sA6nxpctERERER19eBVLadkUCdkOGf3zHE0Ww9hVE8BudxDbqwPYVuNPhK199UGk6xyTRAF9c+wo7eVEWZ4Tpb2ccPujmDyyDE8t35Ry/OSRZXAHojBMCzZFRF3AgCcUZfgiIiIioqMSr2IpLU0W0SfHkSiGERcvhmFaFob2zcZN879Ie/ssm4yyPGfiq7SXE31zHVCk5GmGvrCOKaMGAIjN8YoHvBtGliaqHQKAIAiQJRHVvjCKXRx6SERERERHH4YvSksUBfTKUJsshjF/9XZ8cu75yM/U4FAklMaDVi8nSvOcyHEoLSoLH9ZN7K4N4mfD+2HaeeWo9UeQZVewZnc9dtcGk4Y2Zmgy6vxRhKIGbIrUVg+ViIiIiKhDMHxRk+qDerPFMHwhHfMnnw4j/dSvFgvrJio9YVT7IthTF8Bv3vwO4aiBhTedCUk8EODsioS6QBT1wSjDFxEREREddVhqnprksivIsqfP51l2GdkOBUDbLXpsmBbyM22I6Cb8EQObKr1J+wVBgCQKqPGH2+w+iYiIiIg6CsMXNSkUNTB5ZFnafY2LYbQlSRQwtG82AODrXe6U/RmajFpfBKGokbKPiIiIiKgrY/iiJmmyiFt+NADTzz8m0QOWZZcx/fxjMGXUALgD0Xa531OaCV8OVUIgEqt6SERERER0NOGcL2qSKotYt7se40/vi2nnlaM+qMNll+EORFOKYbSlkxvC14YKT8q6XqIgQBAAtz+Kgkxbu9w/EREREVF7YM8XNUkQBKiSgHX7PNi83486fwSb9/tR6Qm3W/ACgMIsG0qy7TAtYM2e+pT9TlVGlS+MSDu2gYiIiIiorTF8UbMybQp004RhWgjrZpvP8WpKYujhzrqUfQ5VRiCic+ghERERER1VGL6oWZoswrLarqJhS53SLxsA8E2aeV+SKMC0AHcg0rGNIiIiIiI6Agxf1CxNkSBLAvQjXcyrlU4scUESBeyrD6GiPpSy36nKqPJEOrxdRERERESHi+GLmmVTRGiyiEgHhxyHKmNQUSYA4OtdqUMPnaoEf0SHJ5R+EWgiIiIioq6G4YuapckSbIrUrgU2mnJg3pc7ZZ8siTAsE/UcekhERERERwmGLzqkDE1GtDPCV78cAMCa3e60hT4cioxKb7jDioAQERERER0Jhi86pAxNht4JAac8PwMZmgx/xMCmSm/KfqcmwxfW4ePQQyIiIiI6CjB80SHZFAlAx4cvSRQwND70ME3VQ0USoRsW3EEOPSQiIiKiro/hiw7JpohQZBHRTqgs2Nx6XwBgVyTs94ZhcughEREREXVxDF90SJosQZVERDph3tfJDeFrQ6UX/nDq8MIMTYY3FIUvwqGHRERERNS1MXzRIamyCJsidUj4ihpmUgGNwiwbSrLtMC1gzZ76tG2LGCbqA9F2bxsRERER0ZFg+KIWcdkVhNt42GFEN+ENRVHtC2OvO4i99UG4gxFU+8JJxx1q6KFNklDlC8GyOPSQiIiIiLouubMbQEcHhybDMA8vfFmWhahhIawbCOsmwroBQRCgSAJssoRcp4ochwqbKiIcNfHDvuQerlP6ZeOf3+3DN2mKbgCxqoeegA5/xECGxpc0EREREXVNvFKlFtFkEYIgwLIsCILQ5HGWZSFimAhHTYR1E1HTAKzY8EBNltArQ0W2PRa07IoEuyJBlg50wHpDUSgN88tUObb9xBIXJFHAvvoQ9tUHUeyyJ92nTZFQG4igPhhl+CIiIiKiLotXqtQiNkWCKgmIGhZUORa+DhW08rNUuGwq7KoU+1IkSGLTwQ0AHKoMuywjGDUS4cuhyhhUlIm1ez34Zpc7JXwBgCKKqPaGUZKduo+IiIiIqCtg+KIWsckiVFlKrKkVNUwIiFVC1GQJBVkqXHYVNqXlQSsdSRSQ41Swuy4Il11JbD+lbzbW7vXg651uXHRiccrtMjQZ7kAEgYgOh8qXNRERERF1PbxKpRaRJRFZNgUiBGTapUTQcqgSbIcZtJqSZVdg1AaStp3SLwcvf7oTa3a7YZhWyv3ZFBF1QQP1wSjDFxERERF1SbxKpRY7vncWALRp0ErHqcmQRAFRw4TSMB+sPD8DGZoMX1jHpkovBhVnJd1GEATIgogaXyTtsEQiIiIios7GUvPUYpIotHvwAgBnw7DFUNRIuu+h8ZLzTVQ9zLDJqPVHkm5HRERERNRVMHxRlyNLIrLtCoIHhahDrfdlVyQEowY8QS64TERERERdD8MXdUkuh4LoQeuKxcPXhkov/GE95TaCIEAUBFT7wyn7iIiIiIg6G8MXdUkZmgxZEGGYVmJbQZYNJdl2mBawZk99+tupMmp9EYR1Dj0kIiIioq6F4Yu6JIcqw6aIrR566NAkBCIGPMHUnjEiIiIios7E8EVdkiqLyLQpKcUzTumXDQD4pomiG6IgQBCAOn+knVtIRERERNQ6DF/UZeU6VUT05HlfJ5a4IIkC9tWHsK8+mPZ2DkVGlS+MqGGm3U9ERERE1BkYvqjLcqgSRBEwLavRNhmDijIBNN375dRkBCI6qx4SERERUZfC8EVdllOToclS6tDDxLwvd9rbSaIA0wLqAhx6SERERERdB8MXdVk2RUKGTUIwcvC8rxwAwJrd7qRqiI05VRnV3gh0Dj0kIiIioi6C4Yu6tFyHivBB877K8zOQocnwRwxsqvSmvZ1TleAL6/CGWPWQiIiIiLoGhi/q0pyaAkEArEbzviRRwND40MMm5n3JkgjDMuHm0EMiIiIi6iIYvqhLy9BkaLKIUDS59+tQ630BgF2Rsd8bhtnE0EQiIiIioo7E8EVdmk0R4dDkJotubKj0whdOP7QwQ5Ph5dBDIiIiIuoiGL6oSxMEAbkOFSE9OXwVZNlQkm2HaQHf7Xanva0iidANC54QS84TERERUedj+KIuL9Mmw0LyvC+g0dDDJuZ9AYBdkVDpCXHoIRERERF1ulaHr9LSUjzwwAPYuXNnmzTgueeeQ2lpKWw2G4YPH47PPvus2ePdbjemTZuG4uJiaJqGY489FkuXLk3sNwwD9957L8rKymC321FeXo4HH3ww6cL9hhtugCAISV8XXnhhmzweantOTYYmiSlVD0/plw2g6cWWY7eV4AlF4Ytw6CERERERda5Wh68ZM2bgzTffxIABAzBmzBj87W9/QzgcPqw7X7RoEe68807cf//9+OqrrzB06FCMHTsW+/fvT3t8JBLBmDFjsH37dixevBgbNmzAiy++iJKSksQxjzzyCJ5//nk8++yzWLduHR555BE8+uijeOaZZ5LOdeGFF2Lfvn2Jr1dfffWwHgO1P7siwaamLrZ8YokLkihgX30I++qDaW+ryRIihglPkEMPiYiIiKhzHVb4+uabb/DZZ59h8ODBuO2221BcXIxf/vKX+Oqrr1p1rieffBI333wzJk+ejOOPPx5z5syBw+HASy+9lPb4l156CbW1tXj77bdx1llnobS0FKNGjcLQoUMTx6xatQrjxo3DJZdcgtLSUlx99dW44IILUnrUNE1DUVFR4isnJ6e1TwV1EFEU0MupplQ8dKgyBhVlAmi+98smSajyhlOGLRIRERERdaTDnvN16qmn4umnn8bevXtx//334//7//4/nH766Tj55JPx0ksvHfJCNxKJ4Msvv8To0aMPNEYUMXr0aKxevTrtbZYsWYIRI0Zg2rRpKCwsxIknnohZs2bBMA70iIwcORLLly/Hxo0bAQDffvstPvroI1x00UVJ51q5ciUKCgpw3HHHYerUqaipqWm2veFwGB6PJ+mLOk6mTYEFM2X7gZLz7iZv69Rk1Aei8EeMJo8hIiIiImpv8uHeMBqN4q233sLcuXPx3nvv4cwzz8SNN96I3bt347e//S3+85//4JVXXmny9tXV1TAMA4WFhUnbCwsLsX79+rS32bp1K1asWIGf/exnWLp0KTZv3oxbb70V0WgU999/PwDgnnvugcfjwaBBgyBJEgzDwB//+Ef87Gc/S5znwgsvxE9+8hOUlZVhy5Yt+O1vf4uLLroIq1evhiRJae/74Ycfxh/+8IfWPk3URhyaBFkSEdFNqPKBzwxO6ZeDlz/diW93u2GYFiRRSLmtTZFQG4jAE4wiQzvslzwRERER0RFp9ZXoV199hblz5+LVV1+FKIqYOHEi/vSnP2HQoEGJY6688kqcfvrpbdpQADBNEwUFBXjhhRcgSRKGDRuGPXv24LHHHkuEr9deew0LFy7EK6+8ghNOOAHffPMNZsyYgd69e2PSpEkAgAkTJiTOOWTIEJx00kkoLy/HypUrcf7556e975kzZ+LOO+9MfO/xeNC3b982f4yUnlOVYZdlBKNGUvgqz89AhibDF9axqdKLQcVZaW+viCKqvGH0zrZ3VJOJiIiIiJK0OnydfvrpGDNmDJ5//nlcccUVUBQl5ZiysrKkgJNOXl4eJElCZWVl0vbKykoUFRWlvU1xcTEURUnqnRo8eDAqKioQiUSgqiruvvtu3HPPPYn7HzJkCHbs2IGHH344Eb4ONmDAAOTl5WHz5s1Nhi9N06BpWrOPidqPJArIdirYUxeEy64kbR/aNxsfb67G17vcTYYvpybBHYggENHhUNn7RUREREQdr9VzvrZu3Yp3330X11xzTdrgBQBOpxNz585t9jyqqmLYsGFYvnx5Yptpmli+fDlGjBiR9jZnnXUWNm/eDNM8MPdn48aNKC4uhqqqAIBAIABRTH5YkiQl3eZgu3fvRk1NDYqLi5ttM3Uul12BkWYu4YF5X3VN3tauSAjpBjxBlpwnIiIios7R6vC1f/9+fPrppynbP/30U3zxxRetOtedd96JF198EfPnz8e6deswdepU+P1+TJ48GQAwceJEzJw5M3H81KlTUVtbi+nTp2Pjxo145513MGvWLEybNi1xzGWXXYY//vGPeOedd7B9+3a89dZbePLJJ3HllVcCAHw+H+6++2588skn2L59O5YvX45x48Zh4MCBGDt2bGufDupATlWGJArQjYPW+2oIXxsqvfCF04crQRAgCyKqfYe3LAIRERER0ZFqdfiaNm0adu3albJ9z549SSGoJcaPH4/HH38c9913H04++WR88803ePfddxNFOHbu3Il9+/Ylju/bty+WLVuGzz//HCeddBJuv/12TJ8+Hffcc0/imGeeeQZXX301br31VgwePBh33XUXpkyZggcffBBArBdszZo1uPzyy3HsscfixhtvxLBhw/Df//6Xwwq7OIcmwa5ICB603ldBlg0l2XaYFvDdbneTt3dqMuoCkZT1woiIiIiIOoJgtXLxo4yMDKxZswYDBgxI2r5t2zacdNJJ8Hq9bdrArsrj8cDlcqG+vh5ZWennGVHb+36PG5WeMAoybUnb//zBFvzzu3246MQi3HruwLS3tSwL+zwhnNI3GwVZtrTHEBERERG1VkuzQat7vjRNSymSAQD79u2DLLOQAbWvbIcKPc38vVP6ZQNofrFlQRAgCgJq/ZF2ah0RERERUdNaHb4uuOACzJw5E/X19Yltbrcbv/3tbzFmzJg2bRzRwZyqDEkQYZjJHbYnlrggiQL21Yewrz7Y5O0zVBnV/gjCOoceEhEREVHHanX4evzxx7Fr1y70798f5513Hs477zyUlZWhoqICTzzxRHu0kSjBqcmwKWLKvC+HKmNQUSaA5nu/HJoEf1hn1UMiIiIi6nCtDl8lJSVYs2YNHn30URx//PEYNmwYnnrqKXz33XdcdJjanSqLyLQpaYtmnNIvBwDw9U53k7cXBQGiALgDHHpIRERERB3rsCZpOZ1O3HLLLW3dFqIWyXWqqKgPpWw/pW82Xv5kB77d7YZhWpBEIe3t7YqMam8EA/KbPoaIiIiIqK0ddoWMH374ATt37kQkktyDcPnllx9xo4ia41AliCJgWhZE4UB4Ks/PQIYmwxfWsbHSi8HF6SvNOFQJtYEIfCEdLkf6hcKJiIiIiNpaq8PX1q1bceWVV+K7776DIAiIV6oXGi6CDYOFDKh9OTUZmiwhFDXgUA+8hCVRwNC+2fh4czW+2eVuMnwpkgjdMOEJRRm+iIiIiKjDtHrO1/Tp01FWVob9+/fD4XBg7dq1+PDDD3Haaadh5cqV7dBEomQ2RUKGTUIwkmbeV99sAMDXO+uaPYcmSyw5T0REREQdqtXha/Xq1XjggQeQl5cHURQhiiLOPvtsPPzww7j99tvbo41EKXIdKsJ6mvW+GsLXhkovfOGmKxo6VAnuYCRt4Q4iIiIiovbQ6vBlGAYyM2MlvfPy8rB3714AQP/+/bFhw4a2bR1RE5yaAkFAYthrXEGWDSXZdpgW8N1ud5O3tysSQhEDnlC0nVtKRERERBTT6vB14okn4ttvvwUADB8+HI8++ig+/vhjPPDAAxgwYECbN5AoHacmQZPFZnu/vm5mvS9BECAIAjxBhi8iIiIi6hitDl//8z//A9OMXfA+8MAD2LZtG8455xwsXboUTz/9dJs3kCgduyLBocnp5331ywbQ/HpfQGxh5mpfBIZpNXscEREREVFbaHW1w7Fjxyb+PXDgQKxfvx61tbXIyclJVDwkam+CICDXoWKL35ey78QSFyRRQIUnhH31QRS77GnP4VAl1AUj8IV1uOysekhERERE7atVPV/RaBSyLOP7779P2p6bm8vgRR0u0ybDtFLnfTlUGYOKYvMSv2lm6KEiiYgaJryc90VEREREHaBV4UtRFPTr149reVGX4NBk2GQRESPNvK9+OQAOPfRQFSXU+FhynoiIiIjaX6vnfP3ud7/Db3/7W9TW1rZHe4hazKFIsKnNr/f17W53s3O6nJqE+mCUJeeJiIiIqN21es7Xs88+i82bN6N3797o378/nE5n0v6vvvqqzRpH1BxRFNDLqWJ7dSBlX3l+BjI0Gb6wjo2VXgwuzkp7DpsSC1/ekA6bIrV3k4mIiIioB2t1+LriiivaoRlEhyfTpsC0UocdSqKAoX2z8fHmany9s67J8CU2zFX0BKPIz9Tata1ERERE1LO1Onzdf//97dEOosPi0CQocqxwhiIlj6I9pSF8fbPLjZ8O79/kOeyKhCpfGGV5TogiC8cQERERUfto9Zwvoq7EqcqwyzICzcz72lDphS+sN30OTUYgosMXafoYIiIiIqIj1erwJYoiJElq8ouoI0miAJdTTlswoyDLhpJsO0wL+G63u8lzKJKIqG7BG2L4IiIiIqL20+phh2+99VbS99FoFF9//TXmz5+PP/zhD23WMKKWyrar2FUTTLvvlL7Z2OMO4utdbowoz2vyHLIkoNYfRkl2+gWZiYiIiIiOVKvD17hx41K2XX311TjhhBOwaNEi3HjjjW3SMKKWcqoyJEmAbpiQD5731S8b//xu3yHX+3KqMtyBWMl5Vj0kIiIiovbQZnO+zjzzTCxfvrytTkfUYg5Ngl2REIqmVj08scQFSRRQ4QlhX3363jEAsDesF9bc3DAiIiIioiPRJuErGAzi6aefRklJSVucjqhVFElEll1GIJoanByqjEFFmQCAb3a5mzyHKAiwAHiD0XZqJRERERH1dK0edpiTkwNBOFCO27IseL1eOBwOvPzyy23aOKKWynGo2FPXxLyvfjlYu9eDr3e6cdGJxU2eI15yvn8vlpwnIiIiorbX6vD1pz/9KSl8iaKI/Px8DB8+HDk5OW3aOKKWcqoyJEGEYVqQDgpOp/TNxsuf7MC3u91p9zc+hycUgT+iI9OmdESziYiIiKgHaXX4uuGGG9qhGURHxqFJsKsiQlEDTi35ZV2en4EMTYYvrGNjpReDi7PSnkOVRUSMWMl5hi8iIiIiamutnvM1d+5cvP766ynbX3/9dcyfP79NGkXUWposIdOmIJhmvS9JFDC0YcHlr3fWNXseWRBQ54+0RxOJiIiIqIdrdfh6+OGHkZeXul5SQUEBZs2a1SaNIjocuU4VYT01fAGxoYcA8HUzRTcAwKHJqAtEmjwPEREREdHhanX42rlzJ8rKylK29+/fHzt37myTRhEdDocqQRQEmJaVsi8evjZWepstJ29XJASjBnwhlpwnIiIiorbV6vBVUFCANWvWpGz/9ttv0atXrzZpFNHhcGoybIqEUJqhhwVZNpRk22FawHe73U2eQxIFmCbgYcl5IiIiImpjrQ5f1113HW6//Xa8//77MAwDhmFgxYoVmD59OiZMmNAebSRqEZsiwanFFktO55R+2QCATft90GSxyaqHdkVCtT8CK00PGhERERHR4Wp1tcMHH3wQ27dvx/nnnw9Zjt3cNE1MnDiRc76o0+U6VFR70xfMOPfYfIw7uTfOGpgHf1iHy67AHYjCHYgirJuJ4xyaBG8oCn/EQIbW6rcIEREREVFagnWYH+9v2rQJ33zzDex2O4YMGYL+/fu3ddu6NI/HA5fLhfr6emRlpS9dTh2vyhvGVztqUeyyJ61Hp8ki+uTa8fzKLZi/ejs8QR1ZdhmTR5ZhyqgB2F0bTApge90BnNQ3G8Uue2c8DCIiIiI6irQ0Gxz2x/rHHHMMjjnmmMO9OVG7cGoSbIqEsG7CpkiJ7dlOBXM+2IJnVmxObPMEdTy1fBMA4GfD+6HSE07sk0QRdYEIwxcRERERtZlWz/m66qqr8Mgjj6Rsf/TRR3HNNde0SaOIDpddkeDQ5KT1viRRQLZdwbxV29PeZu6qbch2KElzwJyqjFp/FJFGvWFEREREREei1eHrww8/xMUXX5yy/aKLLsKHH37YJo0iOlyCICDHoSRVPJRFAfXBKDzB9OXjPUEd9UEdcqPwZVclBMNGs2XpiYiIiIhao9Xhy+fzQVXVlO2KosDj8bRJo4iORJZNgdloJqNuWnDZFWTZ04+yzbLLcNll6I1uJIkCTJjwBNMX7yAiIiIiaq1Wh68hQ4Zg0aJFKdv/9re/4fjjj2+TRhEdCYcmQ5MFhPVY75dhWnAHo5g8MnVxcACYPLIM7kAUhplce8Ymy6j2seQ8EREREbWNVhfcuPfee/GTn/wEW7ZswY9//GMAwPLly/HKK69g8eLFbd5AotZyKBLsqoxQ1IQmx4puuP1RTBk1AEBsjle6aocp51El+EI6AhEDTpacJyIiIqIj1Ooryssuuwxvv/02Zs2ahcWLF8Nut2Po0KFYsWIFcnNz26ONRK0iigJyHSp21ATgsisAgLBuYndtED8b3g9Tzy1HtS+MXk4V3pCeUmY+zqZIqA2E4Q3pDF9EREREdMQO64rykksuwSWXXAIgVtP+1VdfxV133YUvv/wShmEc4tZE7S/LrsC0kgNVWDdR6Qlj834f/vCPH1DtC+P/fnYqHGrTbwNRiJWcL3LZ2rvJRERERNTNtXrOV9yHH36ISZMmoXfv3njiiSfw4x//GJ988klbto3osDk0CbIsImqk9mhl2hS4gxHU+CNYX+Ft9jxOVUKtP5L2PERERERErdGqnq+KigrMmzcPf/nLX+DxeHDttdciHA7j7bffZrEN6lKcqgyHLCMYMaDYUz9jGFyUhUpPFdbv8+DUfjlNnsehyqj2heEL6chxplb5JCIiIiJqqRb3fF122WU47rjjsGbNGsyePRt79+7FM888055tIzpskijA5UxebLmxwcVZAIB1h+j5kkQBhmnBG+J6X0RERER0ZFrc8/Wvf/0Lt99+O6ZOnYpjjjmmPdtE1CZcdhW7alKrGALA4OJMAMCGCi8M04LUaIHlg2myiCpfCH1z7RCEpo8jIiIiImpOi3u+PvroI3i9XgwbNgzDhw/Hs88+i+rq6vZsG9ERyVBlSKIAPc18rX65TtgVCcGogR01/mbP49RkeEN6k71oREREREQt0eLwdeaZZ+LFF1/Evn37MGXKFPztb39D7969YZom3nvvPXi9zQ/fIupoDk2CXZUQiqaGL0kUcFxRrPfrUEMPNVlEKGpy6CERERERHZFWVzt0Op34+c9/jo8++gjfffcdfvWrX+F///d/UVBQgMsvv7w92kh0WBRJRJa96Xlfx8fnfe3zNHseQRAgCQLcgUibt5GIiIiIeo7DLjUPAMcddxweffRR7N69G6+++mpbtYmozWTbVUSaWHtuULzn6xDhCwAcqoQaXyTtEEYiIiIiopY4ovAVJ0kSrrjiCixZsqQtTkfUZjI0GZIgwjCtlH3HFWVCFID93jBqfOFmz+NQJQQiBnxhDj0kIiIiosPTJuGLqKtyaBJsqohQmqGHDlVG/15OADjkYsuyJEJnyXkiIiIiOgIMX9StabKELJuSNnwBB4Ye/tCCoYc2WUT1IXrIiIiIiIiawvBF3V6OQ0VIb77oxvqKlsz7kuEJRhGMsOQ8EREREbUewxd1e05NgigIMK3UeV+DGsLXlip/k71jcTZFREg34Q1F26WdRERERNS9MXxRt+fUZGhK+nlfhZkach0qDNPC5v2+Zs8jCAJEAagPMnwRERERUesxfFG3Z1MkZGhy2sWWBUHAoOL4YsuHHnpoV2RUeyNpqycSERERETWH4Yt6hFyH2uSwwsEtXGwZiJWc90d1+Fj1kIiIiIhaieGLegSnTQZgwUoz72twUUPRjX3etPsbUyQRumHCw3lfRERERNRKDF/UI2RoMjRFQlhPHXo4IN8JVRLhDevY7Q4e8lyaLKHWH2mPZhIRERFRN8bwRT2CXZHg1GQE0ww9VCQRxxRmAADWt3DooTsYOWR1RCIiIiKixhi+qEcQBAE5DgXhJtb7ig89XLfPe8hz2RUJoYjBoYdERERE1CoMX9RjZNoUGKmjDgEAg1tR8VAQBAiCAA9LzhMRERFRKzB8UY/h1GRospC29+u4hp6v3XXBFoUqhyqj2seS80RERETUcgxf1GM4FAl2Nf16Xy67gpJsOwBgfcWhhx46VAm+sA5fmCXniYiIiKhlGL6oxxDF2LyvdEU3AOD4Vqz3pUgidNOEl/O+iIiIiKiFGL6oR8myKzDN9BO/BrVi3hcAqKKEGh9LzhMRERFRyzB8UY/i1GTIsohomsobgxt6vjZV+tLuTz2XhPpglCXniYiIiKhFGL6oR3EoEuyyhGAkNTCVZNuRqcmIGCa2VfsPeS6bIiEUNeANcd4XERERER0awxf1KLIkItuppO2tEgUhMfTwhxbM+xIFAQBYcp6IiIiIWqTTw9dzzz2H0tJS2Gw2DB8+HJ999lmzx7vdbkybNg3FxcXQNA3HHnssli5dmthvGAbuvfdelJWVwW63o7y8HA8++CAs60BJcMuycN9996G4uBh2ux2jR4/Gpk2b2u0xUtfisquIWumHFcYXW17fgvAFxBZcrvKFYbLkPBEREREdQqeGr0WLFuHOO+/E/fffj6+++gpDhw7F2LFjsX///rTHRyIRjBkzBtu3b8fixYuxYcMGvPjiiygpKUkc88gjj+D555/Hs88+i3Xr1uGRRx7Bo48+imeeeSZxzKOPPoqnn34ac+bMwaeffgqn04mxY8ciFAq1+2OmzmdXJEgQ067RNThR8dCbFNib4lBl+CM6fBEOPSQiIiKi5smdeedPPvkkbr75ZkyePBkAMGfOHLzzzjt46aWXcM8996Qc/9JLL6G2tharVq2CoigAgNLS0qRjVq1ahXHjxuGSSy5J7H/11VcTPWqWZWH27Nn4n//5H4wbNw4A8Ne//hWFhYV4++23MWHChPZ6uNRFOFQJqiIgopuwq1LSvoEFGZBEAbWBCPZ7wyjMsjV7LlUWoesWvCEdWTalPZtNREREREe5Tuv5ikQi+PLLLzF69OgDjRFFjB49GqtXr057myVLlmDEiBGYNm0aCgsLceKJJ2LWrFkwjAPzd0aOHInly5dj48aNAIBvv/0WH330ES666CIAwLZt21BRUZF0vy6XC8OHD2/yfgEgHA7D4/EkfdHRSZNF2GQJYT113pdNkVCe7wTQsvW+AECWBNT6w23aRiIiIiLqfjotfFVXV8MwDBQWFiZtLywsREVFRdrbbN26FYsXL4ZhGFi6dCnuvfdePPHEE3jooYcSx9xzzz2YMGECBg0aBEVRcMopp2DGjBn42c9+BgCJc7fmfgHg4YcfhsvlSnz17dv3sB43dT5BEJDtUBDWm1jvq2He17oKb4vO51RluAMsOU9EREREzev0ghutYZomCgoK8MILL2DYsGEYP348fve732HOnDmJY1577TUsXLgQr7zyCr766ivMnz8fjz/+OObPn39E9z1z5kzU19cnvnbt2nWkD4c6UYZNgd5E0Y3jE/O+Wlh0Q42VrveFOe+LiIiIiJrWaXO+8vLyIEkSKisrk7ZXVlaiqKgo7W2Ki4uhKAok6cA8ncGDB6OiogKRSASqquLuu+9O9H4BwJAhQ7Bjxw48/PDDmDRpUuLclZWVKC4uTrrfk08+ucn2apoGTdMO9+FSF2NXJIgQYFpWomR83KCiWLn5HTV+BCI6HGrzbxNREGAhVnI+L+Poeo1YlgXdtGDEvywLphnbFv+/BSDHoRzyeSAiIiKi5nXa1ZSqqhg2bBiWL1+OK664AkCsZ2v58uX45S9/mfY2Z511Fl555RWYpglRjHXabdy4EcXFxVBVFQAQCAQS++IkSYJpxno5ysrKUFRUhOXLlyfClsfjwaeffoqpU6e2wyOlrsiuSNBkEeFoatGNXhkaCjI17PeGsaHCi1P65bTofNW+MEp7OSGKwiGPb0tmQ2hqHKAMIzlIxfdFDRNRw0REtxA1Tei6BQOx40yz4d8GYMKCAMACYFoWMm0KSns5UJRlgywdVR3mRERERF1Gp36Ufeedd2LSpEk47bTTcMYZZ2D27Nnw+/2J6ocTJ05ESUkJHn74YQDA1KlT8eyzz2L69Om47bbbsGnTJsyaNQu333574pyXXXYZ/vjHP6Jfv3444YQT8PXXX+PJJ5/Ez3/+cwCx+T4zZszAQw89hGOOOQZlZWW499570bt370QIpO7PpoiwKbGiGweHLyBWcn6/twrrWxi+nKqM+lAE/oiOzA6oehg1TFR5w9hXH0QkGgtNhmnCMnFQgLJgQUA8DgqCAEkQIAqAJAoQBQGSKECVRYgiYvsatsdZlgVPSMf3ezyo9IRQ2suJXKcKQejYkElERER0tOvU8DV+/HhUVVXhvvvuQ0VFBU4++WS8++67iWIYO3fuTOrF6tu3L5YtW4Y77rgDJ510EkpKSjB9+nT85je/SRzzzDPP4N5778Wtt96K/fv3o3fv3pgyZQruu+++xDG//vWv4ff7ccstt8DtduPss8/Gu+++C5ut+bLi1H0IggCXXcHuumDa/YOLs/DBxir80MJ5X6osImrESs63Z/jSDRNVvjB21QZQ64/CJotQJBGyKECV5SYD1JGIP1cZmowaXxhfB9woybahX64TTo1DEYmIiIhaSrBaspIspfB4PHC5XKivr0dWVlZnN4cOw+66ANburUdvlyNl39YqH6Yv+gZ2RcKrN58JqQVDCfd7Qihy2XBCiavN22qYFqobQleNLwxNlpDtUFvUrrYWihqo9oeRqcno38uJIpcNCociEhERUQ/W0mzAj62px7IrEkQhfdGN/r2csCsSglEDO2sDKMtzHvJ8Dk1GXSCCsG5Ak1OHMh4Ow7RQ4wtjV10ANb4IFElEYZa9U0JXnE2R0CfbgfpgFD/sq48NRcxzoheHIhIRERE1ix9XU4/lUGWokohImvW+JFHAcQ1VD1tccr4hrPlCR15y3jQt7PeGsGa3G9/scqM+oCM/Q0Nehtapwasxl11BYaYd9cEovtnpxroKD8vtExERETWD4Yt6LJsiQlOkJhdbHhwPXxUtC1+SKMA0YyXnD5fZMLxwzW43vt3lRq0/grwMDfmZWpesMiiJAgoybXDZFeysCeDrnXXYWeNH1Ej/nBIRERH1ZBx2SD1WvJDE3rogYE8tkjGolYstAw0l5/0RlOZZrRqCZ1kWav0R7K4LosobBgD0cmpHzVwqmyKhJNsBTzCKH/Z5sN8bRv9eTuRlcCgiERERURzDF/VomTYZUTN9L82gokwIACo9YdT6I8h1qoc8n0OT4A1F4Y8YyGhBJUDLslAXiGJ3XQD7PbHQleNQocpHR+g6WJZdgVOTUeuP4NtdbvTOtqFvrqNDyu8TERERdXVH5xUeURtpXHTjYA5VRv9esUqILe390mQJEd2EN9T80EPLsuAORLB2rwdf7azDfk8Y2Q4FhVm2ozZ4xUmigPxMDdkOBbvqAvh6pxs7avxp59YRERER9SRH91Ue0RGyqxJUOX3RDSC23hcArG/hvC8AkEQRdf5Ik/vrA1Gs2xcLXXvdQbhssdDVVhUSuwpNltDb5YAsCli3z4Nvd7ux3xMCV7cgIiKinorhi3o0myzB1lzRjcS8L2+Lz+lUZdQGoimBrj4Yxbp99fhqZy121QWRqSkodtlhU7pX6DpYpk1BUZYdvpCOb3fXY+1eDzyH6BkkIiIi6o4454t6NFEUkGWXsa8ulLboxuCiWPjaUuVr8fpddlVClTcMX1hHrqzCE4pinzuEfe4gwrqJHIeKPLV7B66DSaKAvAwNEd3Enrogavxh9M91oDjb3u16/IiIiIiawp4v6vEyNQV6E0PhCrM05DgU6KaFzft9LTqfJAowYaLWF8amSi++2lGH7TU+2FUJvbPtsPew4NWYKovonW2HKkpYX+HDt7tiQxFNk0MRiYiIqPtj+KIez6FKEIC0c5EEQcCgotYPPbTLMrbXBrClyg+7Epv75FDZ0RyXYZNR7LIhEDbw7W43ftjnQZU3jFDU6OymEREREbUbXg1SjxcvuhHWzbTzr44vzsLqrTWtWu/L5VAQaeJ8FCMKAno1DEXcVx/EnroAbKoEl01FrwwVGZqMDJt81Kx1RkRERHQoDF/U49lkCZosNRm+BhVnAgDWVXhgWS1bPFkUBAavFlJlEUVZdpiWhVDUgDsQQaU3BEkQYFcl5DgV5DhiYcypyhBFLtpMRERERyeGL+rxRFGAyy5jX30IQGrRjfL8DCiSAG9Ixx53EH1yHB3fyB5AFAQ4VDkxPNMwLQQiOvbWhbCrJgBVFuFQZeRlaMiyK8i0yT0m4EZ0E2HdQCga+79hWlBlEZosQZNFaLIImT2EdAQM00KNPwx/SIeqSFAlseE1JkKRREj80IN6MMuyEIgY8Id1hKIm7KqELLvMglF0WBi+iBArh76zNph2nyKJOKYgEz/s82D9Pi/DVweRRAGZNgWZtlggDusGAhEDm6t8EABoitjthihGdBMh3UA4aiIUNRCI6PCGdISiBiKGiahuAo16XgUAqiRCkUXYVRGZmgK7GuvJ1ZTYhbMqiS3qraWeSTdM1Pgj2F0XQI0vgtjMVwuwAEkSoYgCFEmETRXhbPhwRG14XSkN/z/aF4YnOljjsOUJRVHrjyAQNhA2zMQIGKcmI9epoJdTQ5ZN6dHFtKh1GL6IEJv3JQhocljh4OIs/LDPgx8qPBh9fGEntJC0huGhOcBRP0QxrBsI67GAFY6a8Id1eMM6wtHYH3ddNwEIkEQhcXHrUFPDpWlZiBomoroFb9BAjS8K07IAy4IcvzCWRGTaZGRoyoFA1tBrxt6MnitqmKj2hbG7LohaXxiqLCE/Q0vqQdUNE7oZe415gwZqfVEYlglYsWJEihQLZookwqlJcKpySq+ZKolt8j40TQumZcGwLJgmEv+2TMS2WVbDMQ3fmxbkePtEEYosQBZFKJLADyMaCesG6gNRRE0LqiRCUw783uiqvz/bg2laCEQNBA4KWyHdhCgANkWKhS059mGWYVoIRgzsqQthZ00QDlVCtkNBXoYGl12JFfLqwa+zUNSAN6TDE4wirBuQxNjfM1kUIQoCBCE26kgUcOB7QYAoCJAEAYIY/z51vyjgqH9uGb6IANgVCZrUdNGNwQ3zvta3ougGtZ+WDFF0anLsE8lOGqJoWRYihpkYKhiOmvCFdfjCB3qydKOhh0EUEr0J6UJWU0RBaAilqfuihomoYTYUNAlBN4IQBECAAEUWEveVYZNgV+XE8EVNllrck2FZFqzYQ4CZ+Hfs/0ja1nAsgNi1+4Fj7KrUY4aPdgUR3USVL4xdtQG4A1HYZBGFWfa0QVyWRMgS0v58TMuCbliJ15g/bMCwQg3hv6HXTBKgiKm9ZrIowLRir4lYcDoQrqINgU83TBgmEt9bJmAgFqqseNhC7PUUe53FPjhL1Ky1gIYytpDE2H3KkgBJFGFXJNjU2P8VSYQsxd4PcqM2d+fgYZgW3IEIav0RVHrD8IeiDc9V7OemNgRqhyrBqcXen/HfC20ZqDtTPGz5wzq8oSiqfRGEIrGwJYmxD/tiPVvpRw5IooAMW2zEhWVZCEYNVHnD2OsOwqZIcNkV5GfG/v5kdOEPAtuKYVrwNTyXNb4I6oNRBBuqFyuiGHvP4sDfAQGx/1gWIMCCFdsCAbG/axAQC16IBS+hUeiKB7BEmBOBXKd6VI1KYvgiQkP4UpoputFQbn5XXRDeUDQxFI66hkMNUYz/MXQ0DAuJ/y0V4r/YceCTtMb74tJ9yJY47qBhgI3/CIV1E2HdhNFwwRj7BD52AeNsRcg6HEriAip5u9HQkxE1TLgDEVR5LZiI9bQpDRehmiLBrsTaFrtIbuhpMGN/OOM9DIiHLTQKWA3bcHDoQuw8aDif1XCZHP/Z5DrVhh46mfPX2kGo4eJwT10Q7mAEDkVGUZbtsHs/RUGAKgtNBnXdMBE1LOhmcq9ZLA/FrvQFAYmLrtgLQ4h9Go7Yp+IC4p+ON1xsCbEPDg7+BPxQn4QbZqwdumFBbwgehs9qWN8xdikoQoAsAZJ4oKfM0fDBgNow7y0pqInCUfU6tSwLnpCOOn8YFZ4wvEEdJixkajKKXPbYBS8O/Nwihok6fwT7PRZMxH5/xcOpKkqwqxIybAeeH006EM66YtBoHLY8wShq/BEEIzoihhXr2ZIlZNhk9DqMOVzCQR8GhqKx3sRKb6hh5IGCgsxYj1imTek2Iw6CEQPecBT1gdjz6Qvr0E0TqhgL7S67knhdtZTV8GFM/O9G/AO8xAd5DR/UGADCugnLMuCP6LAsMHwRHW1EUUCWTUaFJ33RDZddQUm2HXvcQayv8OL00tyObyS1WLohinX+WNBI+mQ88c+GmS4N22IXhWh8QMOO2D8ECIljITS6TcNGWYgNu1JlERlq1woTseEfqb1NlhW7MI3oZuyPalBPBFMgdoERC6mNgmrD92J8nxjfHv+08sC/AST+EMfPayF2oVLrj2BffRCyGPu0PdepItuhIsMmw9nDh+8cqWDEwH5vCLsbPjhyqjKKG11st5d4rxnQ+b2a8dd8uh7iuPiHC1Ej9mFJIGzCE4xdTMbEQpokCbHgJcbe39l2BVl2BU6ta75WAxEddYEoKj1BuANRRHQLDlVCXoaa9vdS/OdmT/NziwezxAc3vlgPJNAQjBuGeTpUuVGP2YHedFkSDvyuaOehY6ZpwR/REYgYB4UtM1aNWJaQaVPapWCGTTnw+zXWK6xjvc+b6C3Lz9CQ7Yi9bo6mecq6YTZ8sKij2heGJxRFMBIbUmhXZOQ61CN+PIIgQIp3i7VQujVauzqGL6IGWXYFu+vSF90AYkMP97iDWLfPw/B1FDl4iCKl13gOj7Oj7hNI+tnoholAxMCuuiC21/hjPYRa7GIl064gQ+t6FS6NhsAqNgxV6ioCER2V9SHscQfhC+vI0BT0dtm7XDjoKkRBgNjw+m9K/AMKw4wNuQxGYvNODROwySIcWuyDgyybggybDLvSOWEsPo9rvzeMGl8YId2EJonIOsKw0VwwS+lR98WGiEI4EMwkSYz1ZiL26YsQH1omCBBFQBLEg7Y1zP9p+F4SheQPdhoFuXioMy3AE4yiNtAobEGEXW2/sNWc2IdwKnIQ+/3mbxiRIQpoqN6rIqfhNdPVfrfFi474wjrq/LFhqoGIAcOyoEmx5zTHrvJ3ymHg1QhRA7sqNYxBTl90Y1BRFv6zbn+rFlsmopaTJRFZdhFZ9uThoxv3+wDEhgd3xhBF04wNwwo3lPxP9A6GowhFTEQME5IowKnFLkactligdChShw/B8oV1VNQHsdcdQiCiI5Ohq80c+IACKQMkQlEDwaiBrVV+WEDig4M8p4oMW/yDg/arPKobJuqD0cQ8Ll8oClmMFdvJcbT/BXJTPerAgWCmm/HhyFbD3M8Dw8tgAaalo+GfibmDBw9pRrzXPDFxKPYPAY3/HSvAlGVTu1QlTlkS4bKLcNmVxDzlXbVBbK/2w6HFeo56ZWjIsnfeh4UR3UwMm6/yhuEL6wjrBkRBhEOR0MuZvseUWofhi6hBvOhGxDDTfjp2fHFs3tfG/T7ohslfQETtLN3w0fYaohgvkBJpmKcXaahG6Q3Fhi5FDANRI9brASs2VFltmAPjVGWYlgVPQEeVNxJruyTCpsbCYpZdgVOVEssAtAdPKIoKdwj76oMIRk24bApKso+eORBHu8ZDzSzLQlg3EQwb2Oj3AZYFm9IwpygpjB3ZayFlHldIh2VZyNA6ZmhpS8WDGR3QeJ6yacV6USvqQ9jtDsAmS3DZVdhVMalCoCimVvxL9BI2qhDYuLewcS9iOvHhmYnerUAUwbABwzJhV2JVg3M7ILz3NAxfRA3iFZ3C0fThqyTHjgxNhi+sY2u1H8cWZnZCK4l6poOHjzYeorij1g9NkuC0xXoamhuiGD0oYIV1A95QFP5ww1pqDfNaYqUgDlTBs8syXLbmFxuOt61xpcs9dUHsrA1AEgTYlFhvSI5DgaNhOQT7EfSOWZYFT1DH3voAKuvDCBsGXDYVuU7tsM5HbUMQhEQYy0Hs5xSKmvCFDFR7fRAaSpdn2mIVWZ1arLJeS8OYP6zDHYyioj6I+mBsHpdTlZDHXomjktiwZphTkxOvFXcgghpfrEKgaTaa0yTEe/cOVAhMVAQUARENwzETJdqFxLZYtc/Y8E5JjBWXif3u0xHRTUhirBBUfqbWbYqCdFUMX0QNJFFApl3Cfk8k7X5REDCoKBNf7KjDun0ehi+iTnTwEMVQ1Ij1NPhiF7exYUexIYpA7ILVF9YR0mPrqEUMM1FHRWlY/0ltqEIpi0e2FpSQWAIg1vMFxIZexaug7feEAVjQZAk2VUoUbXCoUqIce3Msy4I7EMW++iAqPCFEDQvZdgW9VIaurkhoWIcwvghvvBe3PhBFpScESYyFtfjrNXYhntxLGtYNuAOxoWBtOY+LupaDXyst0bhCoGklL/uRWNKhYfkGq2F4Z3y70lCNka+hjsXwRdSIy64eouhGVix8VXgxrgPbRUTNa9zT0HiIYqyCqQVZaCgXLouw25RE5bWOEpsTFvt0G0juHdtdF4RRE2i4CBeRoSnIdshwarFAFu8dM00LdYEI9riD2O8Nw7IsZNvVLjdRn5qXbp3CA0Nq44vGxy6Kc50qQlEjZR4Xezcp7nAqBFLnYvgiasSmSA2TedMX3RjcMO9r3T5Pk8cQUec6GipcNtc75g5EUNkQGuO9YzkOBaGoiSpvGACQ7eCn1d3FwcHcMGOL9tb4Yh8eCECXm8dFRIev6/5lIuoEdjW2HklTRTeOKciAJAqo9UdQ5Q2jIMvWCa0kou4oXe9YfKHunbVBiAKQ6zzytXSoa5NEARlarJonEXU//A1O1IhdkWCTZYSjZtr9NkXCgLzYKkjrKrwd2TQi6mHihRtcdgVFWTYUZNoYvIiIjnL8LU7UiCQKyLBLCOvpwxeQPPSQiIiIiKilGL6IDuKyqYiaRpP7E+GrguGLiIiIiFqO4YvoIHZVAhrKsKYzuChWYn57tR+BiN6RTSMiIiKioxjDF9FB4kU3YgutpuqVoaEgU4NpAZsqfR3cOiIiIiI6WjF8ER3ErsTKP4f1poceDiqKDT38gfO+iIiIiKiFGL6IDiKJAjJsMkJNVDwEgOOLY0MP13PeFxERERG1EMMXURrZ9uaLbgxqKLqxvsILw0w/PJGIiIiIqDGGL6I0bKoIQGhyf2kvJ+yKhEDEwK7aQMc1jIiIiIiOWgxfRGnYFQmqJCDSxHpfkijg2MIMACw5T0REREQtw/BFlEZLim5wsWUiIiIiag2GL6I0ZEmEU5MRbqLnCwAGF8XDl7ejmtXhJFGAJouQxKaHYBIRERFRy8id3QCirirboaCiPtTk/uOKMiEAqPCEUOePIMepdlzj2pkmi8h2Ksi2K6gPRuGyK3AHonAHos0GUiIiIiJqGsMXURPsqtRczQ04NRn9ezmwvSaAdRUejCzP67jGtSNNFtEn1445H2zBvFXb4QnqyLLLmDyyDFNGDcDu2iADGBEREdFh4LBDoibYFQmKJCBqNB00BnXDoYfZTgVzPtiCp5dvhieoAwA8QR1PLd+EP3+wFdkOpZNbSERERHR0YvgiakKi6EYziy13l6IbYd3AD/s8WLZ2HzI1BfNWbU973NxV25DtUDgHjIiIiOgwcNghURNkSUSGJqPWH0FGE2+VwcWZAIAtVT5EdBOq3HGfZ0iiAFkUoJtWqxZ6Ni0Le9xBbKzwYkOlFxsrvdheE4BhWjiuMBPjT++X6PE6mCeooz6oQxYFLi5NRERE1EoMX0TNcNnlZotuFGXZkO2IFaPYtN+LE3q72r1NrS2GUR+MYkNFLGRtqPRi034v/OHUEvo5DgV9c+3Iz9SQZZfTBrAsu4wsu4wqb7hdHhsRERFRd8bwRdQMhyqjuf4dQRAwuCgLq7fWYH1F+4evQxXD2Fblxw/7PLGgVeHDxkovKjyp4VGVRJQXZOC4wkwcV5SJYwszkJ+hQRAEeEJRTB5ZhqeWb0q53aQRpfjvpmrsrQvilH457fpYiYiIiLobhi+iZthUCaocK7qhSOmHFA4uzsTqrTUdMu+rcTGMuHgxDNOycFKJC3cvXpNyuz45dhxbmInjCjNxbGEmSns5IDfxeNz+KKaMGgAgNserccC74axSXP38amyr9uGmswfg0pOKIQic/0VERETUEgxfRM1oXHSjyfBVdKDohmVZ7RZGJFFAtr3pYhjzV2/HJzPPR2kvB/IztUTQOqYwExlay9/qYd3E7togfja8H6adV476oA6XXYY7EMW+uiD693JgS5UPL/x3K3bU+DFlVHmTzw0RERERHcDwRdQMpaHoRl0zRTfKCzKgSAI8IR173SGU5NjbpS2yKMAdjDZbDMMX1vHC9cMQ1o+sGEZYN1HpCaPaF4EsCqjyhhMFNmacfwz65zowb9V2LPuhErvdQcy8aDBcdpagJyIiImoOP64mOoQsm4xwM2t9KZKIgQWxqofrKtpv6OE3u9zI0GIFL9LJssvItitoy/WPDdNCWDeTKhsKgoCfnNoH9156POyKhLV7PfjV699gR42/7e6YiIiIqBti+CI6BGcLhuwd31Byvj3mffnDOp5dsQm/fmMNPt5cjUkjStMeN3lkGdyBaIeVgD+9NBePXzMURVk2VHrCuHvxGny2raZD7puIiIjoaMTwRXQINlWCLMWKbjRlUHzeV4W3Te/7yx11+OWrX2HZD5UAgE+21mLqueWYfv4xiR6wLLuM6ecfgymjBsAdiLbp/R9Kv1wHnrhmKIaUuBCMGnjonXV446vdsCyuAUZERER0MM75IjqERNENvbmKh7Hwtas2AF9IR4btyN5avrCOv3y0Ff9Ztx8AUOyy4fYfH4MTS1xNFsPYXRtMu85Xe8uyK3jg8hPwwn+34l/fV2Dequ3YUePHL887pkMXnSYiIiLq6hi+iA5BkURkqDLcgUiTVQNddgW9XTbsrQ9hfYUHp5XmHvb9fb69Fs++vxm1/ggEAJcN7Y3rz+wPmyIBaL4YRmeRJRG3njsQ/Xs58cKHW/D+hirsdYfwu4sHI8epdmrbiIiIiLoKfixN1AIue/NFN4ADvV+HO/TQG4riyfc24IF//oBafwS9XTb871Un4eZzBiSCV2PpimF0tkuGFOMPl58IpyZhQ6UXd77+DTbv93V2s4iIiIi6BIYvohZwaDKA5kNOInwdRtGNT7bWYNorX+H9DVUQBeDKU0rw9HWn4PiGcx5NTu6bjSevORkl2XZU+yL4zZtr8NHm6s5uFhEREVGnY/giagGbIkGWxGaLbsTD18ZKL/RD9JLFeYJRPP7vDfjj0nWoC0TRJ8eOR646CT8/qwyanNrbdbTonW3H49cMxan9shHRTTzy7nq8+tlOmCzEQURERD0YwxdRCzQuutGUPjl2OLXYMduqD73m1aot1Zj2ylf4YGOst+uqU/vgqfGnJConHu0yNBn3XXoCxg3tDQB45bOdePTd9QhFjU5uGREREVHnYPgiagFVFuFUJYSbCQ6iIGBwC0rO1wejeOTd9Xj4X+vhDkbRN9eBx64eihtGlna76oCSKOCmcwbg9h8PhCwK+HhLDX7z5hpUecNJx2iyCEkUOrGlRERERO2ve13pEbWjbLuCyCGGEw5qZt6XZVn476Yq3LrwS3y0uRqiAFx7Wl88Nf5kHFuY2S5t7irGHF+Eh644EVk2GVur/Ljz9W+wxx1AoUvDwAIncpwKBhY4UZilQetmAZSIiIgojqXmiVqoJUU3ji+KhagKTxCaLEI3LRimhbpABHM+2IJVW2oAAKW9HJh+/rEYWJDR3s3uMk7o7cKT156Mh975AZIo4vTSXMxfvR3zVm2HJ6gjyy5j8sgyTBk1oNPWLCMiIiJqTwxfRC1kUySIogjdMCE3sdjykD4uvDhxGM4amAdfSEe2Q8G2aj8e//cGrNldD0kUcM2wPrj2tL5NLtjcnRVm2fDoVUNhwcK8VdvxzIrNiX2eoI6nlm8CAPxseD9UesJNnYaIiIjoqMTwRdRCdkWCTRYR1tOHL00W0SfXjn//UIlfvf5tojdn0ohSzL3hdNz52rf4ySklGJDfc3q70smwyRhY4MTNC75Iu3/uqm2Ydl45qn2RLrWGGREREdGRYvgiaiFVFuHQJHgCOpxa6lsn26lgzgdbUnpznlmxGYIAPHrVSdjvZW+OLAqoD0bhCepp93uCOuqDOmRRYPgiIiKibqXnjXsiOgLZdjVt0Q1JFJBtVzBv1fa0t5u3ajtynAor+gHQTQsuu4Ise/rPfrLsMpyahBf/uxVbqnwd3DoiIiKi9sPwRdQKDlWGlWah4Nb05vR0hmnBHYxi8siytPsnjSjFR5uq8fqXuzFj0Te4Y9E3ePf7CgQi6Z9bIiIioqMFwxdRK9gVCZIUK7rRWEt6c1x2GTqH0QEA3P4opowagOnnH5N4zrLsMqaffwymnluOUNTAj47JgywK2Fzlw3MrN2PS3M/w7IpN2FjpTRuAiYiIiLo6weJVzGHxeDxwuVyor69HVlZWZzeHOkhYN/D5tlrIopgy76vQpWHhJzsTFfsam37+MazgdxBNFpHtUJDtUFAf1OGyy3AHonAHooky8/XBKFasr8SytZXY4w4mbjsgz4mxJxRh1LH5aeff0ZGRRAGyKCSWSqCegT93Ijra1PjCyHGqGNo3u7Ob0uJswPB1mBi+eq4vd9TCFzKQ61STtserHf75g62Yu2ob165qoZZc8FmWhbV7PVi2tgIfb6lG1Igdp8kizjkmD2NPKMJxhZkQhPTDOnlR2TKaLCLbqSDbrqA+GIXLrqQEYup++HMnahn+Lel6GL56EIavnmvLfh+2VvtQlGVP2deS3hw6Mt5QFO9v2I9311ZiV20gsb1/rgNjTyjCeccVIMMW6w3rjheV7fXHP/7hwZwPtnDh6x6ko3/uHXHxygtkamvd8W9JR+iI9yLDVw/C8NVzVdSHsGa3G8Wu1PAVxz/+7c+yLKyv8OLdtRX4aFN1ogqlKok4a2AvXDOsD84dVNBtwkR7//EvdGl4+ZMdeHr55pR9HDbbfXXUz70jLl55gUztgR9MtV5HvhePxvDFyRJErWRXJIhCbA2qpkrHGwxd7U4QBAwuzsLg4izcfPYArNy4H8vWVmB7TQDvb6jCT4f3w/MrU9ddi8/Ja48w0ZV7paKGifpg7I9fXSACdyCS+LdhWXjimpObXCqBC193T5IowNXMEhlzV23DL0aV49n3N0ORRGTbFeQ41IbefRWZNhliE0N9G+uIi9fu2IPXnRzNz1d8Dc/GH1C099+SoxnD6qF1ifD13HPP4bHHHkNFRQWGDh2KZ555BmeccUaTx7vdbvzud7/Dm2++idraWvTv3x+zZ8/GxRdfDAAoLS3Fjh07Um5366234rnnngMAnHvuufjggw+S9k+ZMgVz5sxpw0dG3ZFNFaEpIsK6AYfaJd5CPV6GTcalJ/XGJUOKsbHSh483V+OsgXn41evfpj1+7qptuPW8clTUh5qcJ9Ya7f0p36H++F95Sgk+3lx9IFgFo3AHIqgLRBMhyxtuulT/cYWZqPaFm10qYb83jJc/2Y5cp4YhJS4Uu2xt8ty1p6P5gq89RQ0TX+90Y2u1D78YVd7sz73GH8anW2uxodKbsj8e3rIdCrLtsVCWCGd2BTlOFdl2Baf2z2n3i9eOukDujr1r7fk+6cjnq10eh2Ud8gMKfjCVrKPDqiIJsClHV/H2Tr9yXLRoEe68807MmTMHw4cPx+zZszF27Fhs2LABBQUFKcdHIhGMGTMGBQUFWLx4MUpKSrBjxw5kZ2cnjvn8889hGEbi+++//x5jxozBNddck3Sum2++GQ888EDie4fD0fYPkLodTZZgVyT4wwYc6qGPp44jCAKOK8rESX1c8IX1Zi8qq7xh/HrxGuz3hpGXqeL/b+/O4+SoC7zxf+ruu+e+MpMbkmAOMJIQUEATDcfyk8NdVnAJCT+5AmLyQzGLGDwewsrDyqq4HHL5QETlByriSYSwSriiMYLk5EhCMjOZzEzf3dVV9X3+qO7OTDJXMjPdc3zer1en0901Xd/u6uquT32v6oCB6qCBqtx1dcBAVdBAmVfrM2QM5Vk+2xGIZyxEUllEkiYiaQum7eCaM6f2+eN/zVlT8b0XdqE9Yfb5/EceLJfnajAayjyoDhoIedUe37OQV0WFX8dv32oprKMqYGBuYxhzJ4Qxt7EM1UFjQK+xa1nGwgHfaGE7Am/uj+B/dhzEn3cfQjxjocKv49bzZ/W53asCBuZPKkdNyMi9h26oj2cs2I5Ae8LMfSYSPa63wq/jT7d8vN/atWv+z2Z0JE04wm1SnL8WABwhIAS6PJa/7V6HfSp+v+qsvtdx9jT84IVdUBQZYa+GsEdDKLcvhL0aNKX/g7exdkZ/uPeTYr1fg3kdQghE0xZaomm0RNNojqTRHM1dImlU+nU8cMVH+vwtaYub2PB2M0IeHdNrAgh5tUG/puE2XN+/lu30G1av//g0HIxlMNjV5rf7iXUBRFNZmJYDy3FGxUnxkpfwP//zP/H5z38ey5cvBwDcd999eO655/Dwww/jK1/5ylHLP/zww2hvb8fLL78MTXM/4JMnT+62THV1dbfbd955J6ZNm4azzjqr2/0+nw91dXVD+GpovCjzaehI9n2gS6VjOQJluXnX+goTrfEM2pMm2pMmdrTEe3wuTZHcQJYLY/lglg9rH55U1udZvktPbcL29zoQSWcRTWXdYJW7RFNZdKYO3x9LWzjy92hGbRAXnzKhzx//9oSJUyaWoSOR7bX2ob9mYtG0O/F1T1MlLD99CpojaSw9qRZbP4hge3MMbfEM/ritFX/c1goAqA97CkFsTmMY5b2cmRgrB3x5I7l2TQiBHS1xvLTzIP60sw3tXb6zyn0aPjq9Cq3RTJ/bPZrK4rMLJh71WF/NWDu73F8dMHAobvZbu5Y0bbx3KNnjMv1pKPP2v454Bpt6qcED3ObkYa/W7RLyagh71cL/PzGjZsw0PyvGflKMGpCBvI54Jh+uMoVQ1dIlYKWydq/PbzkClQG9z9+SMp+GR15+v3Biqi7kwQm1AZxQE8AJNUFMqw7AqysDfk0j+cSUIwQ6EmbhvcsH1ZZoBi2RNKqDBn64rO+wejCWwS3//1YcjGdQ4TdQ4dNQ4ddR7tdR4dNR4T98CRhqjyc/+9ru1589DYY28Pe7FEoavkzTxObNm7FmzZrCfbIsY8mSJdi0aVOPf/PLX/4SixYtwsqVK/GLX/wC1dXVuOyyy3DLLbdAUY5+s03TxOOPP47Vq1cftQGfeOIJPP7446irq8MFF1yA2267rdfar0wmg0zm8JdENBo9npdMY4TfUAd91oaGj+0IdKb6DhPxtIX/uvRktMVNHIylcTCewcGYiYPxDNpiGRyMZ9CRMJG1BQ5E0jgQSR/1PAM5q3/NWVOx9tm3+q2V6ipoqLkDPw2N5V5U9VMrVRM08P99csagfqjzE1/ny93TQcxlCyfhMgDprI23D0SxdV8Ef/8ggp2tscJ79Lt/tAAAmip8mDchjLmNYcyeEEbQo42ZAz5gZNeuvX8ogY07DuJ/drahOXr4cxswVJw+rRJnnliN2Q1hKLIE03L63e490RQZVQG3prgviiyhJtT357cqYGD56ZNh2g5kSYIsubXYkgTIXa/h3i8fcb+hygNaxykTy1AbMrqd8Iim3Rq8VNZGKmt3e7+6qvDruPTUpjHT/Ky//eSyhROx51ASGctB1nZgWrlLT/+33WUyloOs5SBjO9AVCf9+/kn91kb+79/vQMq0oMgSFFl2ryX3cyPnQoh7X/fbsuRef/Kk2l5fhyMEPjyxHCseff2ok1pHqvTrqAt7UBvyoC7kQV3Yva4NeRBPW73+llx5+mS8ezCBORPC2NkSw/4ugeR/drYBACQAjRU+nFATwIk1AZxQG8SUKv9Rta0j5cRU0rQKQbWla1DN1Q7mp3rpkYR+w2qFX8f+SBrtCbPf72JNkVCeC2TlPh2VuZB2+cKJfX5+rzlr6oiuAStpydra2mDbNmpra7vdX1tbi23btvX4N++88w7++Mc/4vLLL8evf/1r7Nq1C9dffz2y2SzWrl171PI///nP0dnZiSuvvLLb/ZdddhkmTZqEhoYGbN26Fbfccgu2b9+Op59+usf1rlu3Dl//+teP74XSmOPVFCj9DLpBpTWQMFHm01Hmc5uK9CRrO2hPmDiYC2P5UHYwlkFbPIMyr9bvGff2hInJlT74daUQqPJn0su6nmH3uP8PelSoR/wox/qplepMZgd9wJexHOxrT+HyhROx8uPTuk2VcGQo8mgKTplYjlMmlgMAEhkLb+2P4u8fdGLrvgjeaUtgb3sSe9uT+NXfD0ACMKXaj//853nHHIyEEMjaAumsjYzlIG3ZyGQdZCwb6SOuFUnCyk9M7/OA77qzp+H5f7RAVWQEDNW9eNQBNTvLK0Xzs/7OhjdH0nhp50G8tOMg3u8yBYOhylg4pRJnnViFUyaWH/U6j2W7Hw/bEYj0cyIkmsriQxPCg1rPQNZx+cJJRz0mhEAiY7u1eCkzF8qsQk11ZzKLaDqLCr/e775+MGbiN38/gNqQB7PqQ9DVkdsPJezpu2nYNWdNxTWPbz6mk0ZdzagNoi3Wdz/SQ/EM/ra3s9fayP5U+HVcecbkXl/HY5vew3VnT0O5X0fKtFEbMrqFqrqQB7VhD2qDnj63VWey/9+Smz81AwAQT1vYdTCOnS0x7GyNY2drDG1xs/B9mG8poMoSJlf6CzVkpzSVY+G0iiH/Tsl/f5q2g0q/r9fvXyEEFkypxMr1f0Ekle3zOWUJqA4abkjNvYd1XUJrX2E1vy/+x8Vz0Z400ZEwcSjhXrcn3FYo7bnbsYyFrC3QGsugNXb4d6HCr2PNeTP7OREy/Zjfq2IaubGwF47joKamBg888AAURcH8+fPxwQcf4K677uoxfD300EM499xz0dDQ0O3+q6++uvD/OXPmoL6+HosXL8bu3bsxbdq0o55nzZo1WL16deF2NBpFU1PTEL4yGk08msJBN0a4oTio1BQZtbkf6p4M5Kx+TdDAty6cM+y1UkMhYzloiWbQFjehyhIOxjIDKrffULFgSgUWTKkAAERTWby5P4Kt+yLYuq8TeztS6ExmcWJdEFc88lqPz5Hv+/P/PvYGWmJpZCynELIG+tbNqA3iX05t6qd/RgaPbXr/qAM+Xe0Sxrpc/IZSCGj5+z4xs3jNz/o6G34gksafdh3ESzvaur0eVZYwf1I5zjyhGgumVMDTTxOc493uA1WMz+/xrkOSJHfbelRMKO97+pDafvb1cr+GJ17bg/aECV2RcVJDCPMay3ByUxmmVPlLdqIukspie3MU25pjePtAFJIE/ODy+f2eNKoOGGhPuJ8JXZWhKzJ0VYamyDC6Xnd5LL9cuV/rtx9pVcDAx6ZXYl5TWW6EYMe9Fuh+2wFsIeA4ApbjwHbc5m+N5V50JPoOxPG0hR9e8RGosnTcAwQdy29JwKPi5CZ3m+d1JEzsbI1hR2scO1vcQBbLhbRdB+P4DYAHr5jf6+i8AgKfmFmDh/7n3R5qHEW3GsdsD7WSQP+tNB7d9B6uPXta4TMaNNSjQlU+aFUHjD4/ywMJq3Vh9zn7YloOOroGtFww82oKOhLZPrd7LJ1FZT+18qVU0qPGqqoqKIqClpaWbve3tLT02hervr4emqZ1a2I4a9YsNDc3wzRN6Prhfgbvv/8+nn/++V5rs7pauHAhAGDXrl09hi/DMGAYI3dDUnF5NHfQjSQH3RjRhvugciBn9YtdKzUUBjtVQsir4fRpVTh9WhUAoD1hYn+nG8D66/uT32Y9UWUJhibDoyowVNk9CaLKMDQFHi3XDK6fA75Kv4GAoaIu5EE8YyGRcfvZmZaDdsvs90x/hV/HZxdO7HcAiW/+6h8QQJcQd0Swy4U5Q5V7PSjsrYbtytMnY/kZU7By/V+ws9XtqyhLwJwJYZx5YjVOn1pVmGj8WAzXFBnF+PwWowavv6bMe9qTmNcYxt/2RtCeNLFlbye27O3EY5vcz8HcxjBObirDvMayAY0Wejx9f2xHYE97Am8fiGFbLnAd2WS6wq/32zSsOmjgzovnQMo17zse/fUjjaay+PQpjcf13ID7/vQX8Mp82pA0BR3Mb0m5X8eCKZVYMKUSgFsb1RLLFGrHWqLpPkfnffTl93DtWdPw592HjrsmciB9L2NpC//7M3Ph1d3vpuM1VPuirvZ88tPd7n1/foOekT3oSUnDl67rmD9/PjZs2IALL7wQgFuztWHDBtxwww09/s0ZZ5yB9evXw3EcyLJbTbxjxw7U19d3C14A8Mgjj6Cmpgbnn39+v2XZsmULADfcEQ1EmU9DR6Lv6nkaGYZz3rWRXis1ElT49dxIkn3/YFYFDFz10SmQABiaDENVDgcsVT6qOWZP+muiGUtncfv/86HCfY4QSJo24hn3LHkiY7n/z1/S3W9XB93agP5C5I6W+ICaU6my1D2cdalhu+bMqbjvxd347hFnw7+7YReEAL60dAbu/v0OnHliNT42vQrl/pF7JqgYn9+RUIO3+pMzIITAvo4UtuztxN/2uU1x4xkLL+8+hJd3HwIA1AQNzGssw7ymMsw9YoCaY+n7E01lsb0lhm3Nbtja2RLvcQCJpnIvZtaFMKMuiFn1oX6bhkWS2QHtb4N9vwZjIIF4KE5+HbnOwT6fJEmFGqWPnVANQ5WR6Gd03kgqi8sXTER70jyqlrGna02VYSiHayV9utJ/za1PQ13YOyTv13DuiwPZ7pbjQMfIbfYrCSFK+uv9k5/8BMuWLcP999+PBQsW4J577sFPf/pTbNu2DbW1tbjiiiswYcIErFu3DgCwd+9efOhDH8KyZctw4403YufOnVixYgW+8IUv4NZbby08r+M4mDJlCj772c/izjvv7LbO3bt3Y/369TjvvPNQWVmJrVu3YtWqVWhsbDxq7q/eDHQWaxq7DkRS2LovgoZw701VaHwwVDk38azW7SzfSBh8YSSpDRt44pU9Pf5g3rT4hCEd/ez+je/0esA3mG2iyBKm1/jxkf/1fK8HMa/9+xJ8b8NOtMYyhdq1nsJcX8ci+WZCp63b0Ot6Xr91CXa3JkZNCB8Ljmdftx2BnS0x/G2fWxO2rTkG64htNrnSh5ObynDmCdU4Z05dr31/Xt3djjfe7yjUan3QeXSA8WoKZtQFMaMuiJl1QcysDR1VEzrc+8lg3q9jff5ivI7hNJDvlDduXYJdg9zXi/H9Wyx9bfdSjnY40GxQ8vAFAN///vcLkyyffPLJ+O53v1toBnj22Wdj8uTJePTRRwvLb9q0CatWrcKWLVswYcIEXHXVVUeNdvj73/++MF/YiSee2G19e/fuxec+9zm8+eabSCQSaGpqwkUXXYSvfvWrAw5SDF/UmTTxxnsdqOqn/TONHyN52PGRYKwc8A3FQYwQ7gh7hXCWthA3bSRy4SxgqLh0QRNOv/OPvT7H67cuQUfCHPEHl2PRYPb1dNbGW/uj+Nu+TvxtbyfeaTs8R9qDV8zH1n2Rbn1/8m78xHTMmRDG1f9nc7f7J5R5C0FrVl0ITRW+Af0mFfOk0bAPnz7KT36NhRNTxZbf7mGfhmjKQtirlXyer1EVvkYjhi9KZ2289m47DFXmoBtEAzQWDviKcRBTrLPhVHqRVBZb93Vid2sc37xodp+1na+sWYyrH3sD9WVezKoPYUZtcNCT+o6Vk0aj+XWMlRNTpRBNmagKGphRV/pjcYavYcbwRQDw+nvtSGXsEd3XgmgkGs0HSkBxDmLGUjMh6p+huqMEnvq/NvS6DGs7x66xcGKqFA7FMyj365jXZYTJUhloNuDpeqJBKPNq6OSgG0THbDgHQSmGYgwgUazBXGhksByRm/ev90ERwl4VB2MM3GNRMQdVGu3fv6MdwxfRIPgNFQ74BUY0Xg3nQUyxpxig0irF6H008jAYjX0MX0SD4NUUKJIE2xEcdIOIhtxonmKAjh1rO4nGPoYvokHw6gp0VUbGsjnoBhENG54NHx9Y20k09vFokWgQDFWGV1eQMm34OOYGERENEms7ica2kTv9M9EoIEkSyrwaz0YSEdGQsh2BjOUweBGNMQxfRIPkN1Q4nLGBiIiIiPrB8EU0SF5NgQyJZyeJiIiIqE8MX0SD5NUV6JoEk00PiYiIiKgPHHCDaJAMVYZXU5DO2vDqSqmLMySEEEiaNuIZC1nHQdijIejRSl0sIiIiolGN4YtokCRJQtirIZIa3fOvCCGQytqIpy1YwoFXU1EX9sCryXinLQlZkuA3+JVBREREdLx4JEU0BAIeDY5IlroYx6wQuDIWsrYDr66gJmygOuBB2KvBqysQwp1AekdLHJIEzmdGREREdJx4FEU0BPKDbjhCQJakUhenX6lck0LTtuHVFFQFDNSEDIS92lHhSpIkTKr0w3aAna0xyJIEjzY2mlcSERERFRPDF9EQ8OUG3chknRHb7yudzQcuB4YqozKgozroBq7+mhNKkoQpVX44QmD3wQSqAjoMdWS+TiIiIqKRiuGLaAgYqgyPqiBjjaxBN9JZG4mMhYzlBq4yn4aakNuk0K8rkI6hlk6WJUytDsB2BN47lEB1wANd5YCpRERERAPF8EU0BCRJQplPw9720g+6kbHcQTPStg1DkRH2aagJuoErYKjHFLiOpMgSptcE4AiBPe0p1AQNaAoDGBEREdFAMHwRDZGAR4MtEiVZt2k5iGcspLIWdFVGyKNhWiiAsE9DcJCB60iqIuOE2iAcAezrSKI26IHKAEZERETUL4YvoiHi1RRIRRx0w3YEoqkskqYFTXMD19RqP0JeDSHP0AauI2mKjBNq3SaIBzpTqAt7ocgjf6ARIiIiolJi+CIaIl5NgaHKMC1nWEcDFEIgmraQMLMo8+mYVBVCmVdH0KNCLmIAMlQFM+qCcIRAczSFuhADGBEREVFfGL6IhohHk+HRFKSz9rCFr0TGQmfKRNCj4UMNYdSGPCXtc+XRFMysC8F2BFqiadSFPaNiqH0iIiKiUmBHDaIhIkkSwl4NGcsZ8ufOWDb2R1JIWzam1wTw4YnlaCz3jYjBLry6gln1IZT7NbRE0xBClLpIRERERCMSa76IhlDAo8IWQxe+LNtBe8KEANBU7kVjhQ8hjzZkzz9U/IaKWfUhvLU/itZYBjVBY1j7nBERERGNRgxfREPIqymQpcEPuuEIgc5kFumshZqQB00VPlT69REdaIIezQ1gH0RwMJZBTchT6iIRERERjSilb7NENIb4dBW64g66cbxi6SwORNIwNBlzm8owt7EMVYHRUZMU9roBzNBktMUzpS4OERER0YjC8EU0hAxVhqEpx9XvK2Xa+KAzCcsRmFkXwCkTy1A/CodwL/frmFUfgiJLaE+YpS4OERER0YjB8EU0hGQ5N+hG1h7w32RtBy3RNOJmFpMr/Zg/qRyTqwIw1OEbrn64VQYMzKwPQkCgI8kARkRERASwzxfRkAt6VFgDGPHPdtxgYjkOaoIeTKzwodyvF6GExVET9MCpA/5xIIJIKouwd+QNFEJERERUTAxfREPMqymQgF4H3eg6SXKF38CkCh+qAkZRJ0gulrqwB7YQeHt/FIokIeDhVw4RERGNXzwSIhpiXl2BrrqDbhw52fJImyS5GCaUeeE4DrY1xyFJ7rD0REREROMRj4KIhphHVeDJDbqRD18Zy8ahhAlDlTG9JoAJZT549dHbp+tYNZb7YDsCO1rcAObT+dVDRERE4w+PgIiGmCxLCHlVHOhIw9KVUTFJ8nCTJAmTKv2wHWBnawyyJB1VK0hEREQ01jF8EQ2DoKHhXSuB1lh61EySPNwkScKUKj8cIbD7YBxVAWNUj+hIREREdKwYvoiGQcBQURvyYEK5FzVBz6ibq2u4yLKEqdUB2I7A+4cSqA6O/T5vRERERHkMX0TDoNyv48Pe8jE5guFgKbKE6TUBOEJgT3sKNUGDAYyIiIjGBR7xEA0TBq/eqYqME2qDaCz3ojWWhmU7pS4SERER0bBj+CKiktAUGSfUBlAf9qIlmkaWAYyIiIjGODY7JKKSMVQFM+qCEEKgLWHCdgQMRYbfUDkaIhEREY05DF9EVFIeTcGcxjLE0xai6SxaY2nEUhYOJTLQFBkBQ4VXU8b1SJFEREQ0NjB8EVHJKbKEsE9D2KehsdyLhGkjmsqiLZ5BZzKLzqQJRZbh1RT4DZWjRxIREdGoxPBFRCOKJEkIGCoChoqGMi/SWTeIdSRNtMVNHIxn4DgCXl1BwFA5UiIRERGNGgxfRDSieTQFHk1BTciDqbaDWNpCJGmiNZZBR8pE1nLgURX2EyMiIqIRj+GLiEYNTZFR4ddR4dcxqdKPWMYqNE+MpLJuPzFZhs9Q4dMVyOwnRkRERCMIwxcRjUqyLCHs1RD2uv3EkqaNaDqL9riJ9qSJlmgWkgR4NRV+XYHK5olERERUYgxfRDTqSZIEv6HCb6ioD7v9xGJpC51JE20xE4cSJizHgV9XEfRoHLCDiIhomDlCQAjwN/cIDF9ENObk+4lVBw1Mqcr1E0uZaI5m0BpLQ5IkhDwqfDq/AoUQMG0HpuVesraAg/wPpltz6NMVDmxCREQDYjsCnUkTGduBBPA39wh8F4hoTFMVGeV+HeV+HY3lPnQks2iJptEWdwfs8GkqQuOgNswRAtkuIcu0HQgAsgRosgxdlVHm0xHwKPDpKlRZyvWjc5txWrY7sIlPV+HRZM67RkRE3WRtB53JLCzHQblfx4xyLxRZwsFoxv3NTZrwaiqCnvE9UjHDFxGNG6oiozpooDpoIJ6x0B7PYH8kjdZYGrIkITgGzsw5QuRqsHI1WY4DRwASAF05HLKCHhVe3a0hNFQZhiof1S+uJuTBVEcUJsBui2cQTWXRnnSgyBJ8nHeNiGjcy9oOOhImHAhU5E50Vvr1wm9KTdCDRMZCe8JEczSN9oQJRwgEPRr8ujLuTuaN7qMMIqLj1HUusY5kFgdjaRyMuWfmfJqKkHdk14blQ1YhaOVqsiQAuipDz9X45UOWoSrwaDIMVTmm19V1AuymCh+SpoVY2kJHwkR7wkRbPAPLcQrNEznc/+hj2Q4sx60ZtR0BRZYOXyT3eqQdHNmOgO0IWI6TuxaFa0c4kCAV+pqosgRVkXPXElRZHtH7NtFoYVoO2pMmJABVAR0N5V5U+o0e9698v+yGMi8iKfc3tzWWwf6ICUNVEPJo0NXxURvG8EVE41rX2rD8mbkPOlM4GE9DwsioDbMdgYxlI5N1kLZst7kgJGiqBENRhiRkDZRPd9+P2pAHWdsp1IodjGUQT1toT2SgyjJ8ulseHuSWniMELNsNV9kuQUtAAAJQFBmaIhWan9qO2w/QNgUsIeA4AqLwbAISugezoQxrXUOVZYtuwcqBA0AChIAiy1BkQJHdUOUGfzlXk+t+7mxHIJ21kTRtpLI2LNtBOpt7PiG6vRYtF8hURYKWC2ojLXCOJkf2JTVtB0K4/XF9ulvbzvd39EpnbXSmTEiShNqQgYYyLyp8OuQBfN8rstRlyhgb7QkTLdF0oXl7QNcQ8KhjeqoYhi8iopzDIyZ60JnKojVXG9aZzMKrKQh61GEfst4RApmsg4xlI205cISAIknQVRk+Q0FDucedUFpVYAxjyBoIrUt/uokVPiRMG7HccP8dqSwOxtNwHMCrK/Dr6rg5q1kKXWuvsrmgZQu3szsgueEq1+y03MhtD00pNEU1VBmaIhdCS9caJcvucjv33JncQXU6aw8srHUJapIkwekjVKmyBEWRoEoyDEPuFqryNVe6Irv/z4XG/g76hBCF9yVfU5x/LW44s5AxBTKWg4RpwbLdcCaEBFnK15i56y78fxz3WckTwt2O+c+DaTlwILo1cy736wh5VEgS0BY3Ec9YOJRwoMpSIYyN5/4/o0nKtNGRMqEpEurDXjSUeVHu0447SHs0BQ1lXtSFPIilLbTFM2iOptEcTUOVJIS82phsTcHwRUR0BFWRURUwUBUwkKhwa8MORFJoS5gABEIeDV5t8O3UhXAPWjKWg0zWRtZx+1LpinuwWRMyEMytyzvCm/RJklRoylkf9sK0HMTSWURTWRyMZxBJm8haApoiw6e7r2csn9kcakeGh6wtYAs3nB9ZexX0KvDrBny5wKsrMrTc9UACsBuSBv5ZO5awlrFs2A5ygUqGV3PLqOXClCYfW6gaKEmSoKtSn6/fyYXIrgE2azvIZB2kshZSWfc1JE0bWUfAsh2osozwGD1APJKVC635bWk5AhKE+9lSFQS9CkIeT25QHrd2y6N1Pzk0qVIgadqIZyxEklkcys3LaDkOdFnhd8MIlTQtdKZMGIqCpgovGsJehL3HH7qOJHdp3t5Y4UVnMovmSBodCROHEpkxN02MJIQQ/S9GR4pGowiHw4hEIgiFQqUuDhENs/zQua25duop04FXUxA6htqwfE1BJtdPCxIKByghrzvqok9X4dWUMTWioOMIxHN9xdpiGUTTWaTMXPPJ/Gsc5C+R6OMJ8mH2yAPBkUwIgXTWQSprI23ZgBCFkKIpMvy91F7pytAFluEihBi1n20rF8zcEGKjNTd9RdYSCHs1+I3Rf07bdkShqWDGst1aQMkN5YXvq1xzbI92uLnn8dRe2Y5APGMhnun+3QAAHnXkn3Qa6+IZC9FUFh5NRn3Yi9qwB2GvVpR1C+F+NtyTn2lEU9keB8Y6FM+g3K9jXlNZUcrVl4FmA4av48TwRTR+Jc3DPwidiSwgCQQNDb4uozblz5inLRum5QYtXZHgUVUEvQrCXh1eXXFrtTRlxB8wD6X8JNjxdDbX98Ylof/3oK9j9p4eEgCiKQvxTBbprANLONAk92DRO4KaO3ULW1kbUi6Y+w0VlX4dAY8Gr64MuPaKikMIgWjKQks0jQPRFFKmjaChIehRR0XAFMKtiUpnbZi2A8A9IaKpEgxVQcBQEPLobj9S7XB/0uGSsWzE0xZi6WyhiWLGciBDglcvXhPFfD9Jy3HDdr5Zr50fOhYCEIe3ryxJkCW3hjV/LUnu/VLu8cLtXq5Hinzoiaaz8OkqGso8qA15EPQUJ3T1xLKdwwNjxQ+f/Ax6VERTWYav8YLhi4jytWEHYxm3NixrFx5TFQkeVUHAUFHm03IHDm6t1mipfRlLMpaNZMZGwnTP5HYms0hZNmzb7W/kyYXgYg0E4Ah3MIh01jkctjQZfv1w2AoYnFNtNElkLByMpbGvM414OjuiR01NmTZimWxhpNKwV0PIo8Kbm8fPU8R9oTf5YBjPWOhMmmiPZ5HIWoNuopjvp5YPV5YtkM01kxW5MWMVSTo8OqYi506SyfDqKlTFHUnTEe5k9F2b3RZG3HTgPrcjIBzkJq4XcIS7fiHc+xwB9/HcADDIhTUB958jR+tUZKnQN3M43u9o2j1RlR8JuDbkGXG1uV1PfkaSWSRNC5Mq/Qxf4wHDFxF1lTJttCdNmFkbPkMt1GqNlJoV6s52BBKmhWTGHSSkI2kiadrIZG0Ah5tXDVVTxXzYSpk2Mra7Do8mI9ClZsvPCazHhHTWRls8g30dKXQm3WG0w16t5N8F+X6YGcuBocko9+moDXlQ5hsdfdYs20Ei44bGQ3HzcPNlgW79Yu0u0yZYub552SOmIFBVN9TkR/j05b6vdU2Bprj9brXCZXAjX+ZDlyPccJYPbk6XMOZ0vc85XPOWsbqM1ukcUQPnPjtkKTeNQtcpFeSBjzrqCIFoKouEaSPgUdFY5kFtyAuvPrI/E7YjEEll0RbLQFMkTKkOlLpIDF/DjeGLiGjsEEIglbWRyNhIZLLoSGYRz1hIm+6ogVru7PdA+7YcGbakXO1awKOgwte1ZmtkH+DQ8cvaDg7FTXzQmUR7IgsJKPrgHJbtIJaxkDRtaKqEMq+WC1w6AiOsRuNYpbNurVgslUVbwkQibSFjO26tVa7GSpMPN1c0VKVLv0mpEK5GYs3kkfK1dV0Hgzk8mE0uoJl297CZC2hCoDBi55HTKsTTFpJZC2GvhgllXtSEPPxOGgSGr2HG8EVENLZ1baoYSWYRSblNFS3LgSzJ3ZpnCaDHsBX0qKjw6/DnRoLkgc344zgC7UkT+ztTOBjPwLIEynzasM0faDsCidwgFoosIeBRURcyUO43EDTUMdm/VAiBRK7vmibL0NTD4Wo8yQ8Ik3UcZHMjUuZHGU1n7cKInflmkj5dRWO5F9VBY1j78o0XA80Go/u0BxER0TAxVPdseblfR2O5e2CTzNqFportCRMJ00J7woYsuwMUlPn0XNhS3PnYGLbGPVmWUBUwUOnXEUm5Q2i3RN1htIOeoRmcIx8+EhkLjhDwGyqm1fhR4TcQHqH9zoZS16kuxjNVkaEqgBe9f+/YzuFpFI53lEoanPH9KSUiIhogVZERUmSEPBrqwp7CgABJ04YsgWGL+iRJEsp8Osp8OhorfGiNprG/M4X9kRR8ujvVxLGGpMMDZwj4dHcS9uqAOxw4R8WknuTn8eN3VekwfBERER0HSZLgN9QRNxoYjXwBQ0WgOoCGMi8OxjL4oDOFllgauiL3OzhHfjj2tOXAo8moDhqoCY6egTOIxjv+YhARERGVgEdT0FThQ13Yg0NxE/s6kjiUyECChDKfVuiHUxg4I2NB19yAdkLYi7IxMrEz0XjCPZaIiIiohDRFRl3Yg+qggfaEOzhHWyIDyzYL8z0FPSomVYZQ5tMRGiWTOBPR0Ri+iIiIiEYARZZQHTRQFXAH52iJpqHIEir9xoidsJmIjg3DFxEREdEI0nVwDiIaWzgUDhERERERUREwfBERERERERUBwxcREREREVERMHwREREREREVAcMXERERERFRETB8ERERERERFcGICF/33nsvJk+eDI/Hg4ULF+K1117rc/nOzk6sXLkS9fX1MAwDJ554In79618XHp88eTIkSTrqsnLlysIy6XQaK1euRGVlJQKBAC655BK0tLQM22skIiIiIqLxreTh6yc/+QlWr16NtWvX4i9/+QvmzZuHpUuXorW1tcflTdPEJz/5Sbz33nt46qmnsH37djz44IOYMGFCYZnXX38dBw4cKFz+8Ic/AAD++Z//ubDMqlWr8Oyzz+JnP/sZNm7ciP379+Piiy8e3hdLRERERETjliSEEKUswMKFC3Hqqafi+9//PgDAcRw0NTXhxhtvxFe+8pWjlr/vvvtw1113Ydu2bdA0bUDr+OIXv4hf/epX2LlzJyRJQiQSQXV1NdavX4/PfOYzAIBt27Zh1qxZ2LRpE0477bR+nzMajSIcDiMSiSAUCh3DKyYiIiIiorFkoNmgpDVfpmli8+bNWLJkSeE+WZaxZMkSbNq0qce/+eUvf4lFixZh5cqVqK2txezZs3HHHXfAtu1e1/H4449jxYoVkCQJALB582Zks9lu6505cyYmTpzY63ozmQyi0Wi3CxERERER0UCVNHy1tbXBtm3U1tZ2u7+2thbNzc09/s0777yDp556CrZt49e//jVuu+023H333fjWt77V4/I///nP0dnZiSuvvLJwX3NzM3RdR1lZ2YDXu27dOoTD4cKlqalp4C+UiIiIiIjGvZL3+TpWjuOgpqYGDzzwAObPn49LL70Ut956K+67774el3/ooYdw7rnnoqGhYVDrXbNmDSKRSOGyd+/eQT0fERERERGNL2opV15VVQVFUY4aZbClpQV1dXU9/k19fT00TYOiKIX7Zs2ahebmZpimCV3XC/e///77eP755/H00093e466ujqYponOzs5utV99rdcwDBiGcawvkYiIiIiICECJa750Xcf8+fOxYcOGwn2O42DDhg1YtGhRj39zxhlnYNeuXXAcp3Dfjh07UF9f3y14AcAjjzyCmpoanH/++d3unz9/PjRN67be7du3Y8+ePb2ul4iIiIiIaDBK3uxw9erVePDBB/HYY4/h7bffxnXXXYdEIoHly5cDAK644gqsWbOmsPx1112H9vZ23HTTTdixYweee+453HHHHd3m8ALcEPfII49g2bJlUNXuFXzhcBhXXXUVVq9ejRdeeAGbN2/G8uXLsWjRogGNdEhERERERHSsStrsEAAuvfRSHDx4EF/72tfQ3NyMk08+Gb/97W8Lg3Ds2bMHsnw4IzY1NeF3v/sdVq1ahblz52LChAm46aabcMstt3R73ueffx579uzBihUrelzvd77zHciyjEsuuQSZTAZLly7FD37wg+F7oURERERENK6VfJ6v0YrzfBERERERETBK5vkiIiIiIiIaL0re7HC0ylcYcrJlIiIiIqLxLZ8J+mtUyPB1nGKxGABwsmUiIiIiIgLgZoRwONzr4+zzdZwcx8H+/fsRDAYhSVKvy0WjUTQ1NWHv3r3sGzaOcLuPT9zu4w+3+fjE7T4+cbuPTwPd7kIIxGIxNDQ0dBss8Eis+TpOsiyjsbFxwMuHQiHuqOMQt/v4xO0+/nCbj0/c7uMTt/v4NJDt3leNVx4H3CAiIiIiIioChi8iIiIiIqIiYPgaZoZhYO3atTAMo9RFoSLidh+fuN3HH27z8YnbfXzidh+fhnq7c8ANIiIiIiKiImDNFxERERERUREwfBERERERERUBwxcREREREVERMHwREREREREVAcPXMLr33nsxefJkeDweLFy4EK+99lqpi0TD6Pbbb4ckSd0uM2fOLHWxaIi99NJLuOCCC9DQ0ABJkvDzn/+82+NCCHzta19DfX09vF4vlixZgp07d5amsDRk+tvuV1555VH7/znnnFOawtKQWLduHU499VQEg0HU1NTgwgsvxPbt27stk06nsXLlSlRWViIQCOCSSy5BS0tLiUpMQ2Eg2/3ss88+an+/9tprS1RiGgr//d//jblz5xYmUl60aBF+85vfFB4fyn2d4WuY/OQnP8Hq1auxdu1a/OUvf8G8efOwdOlStLa2lrpoNIw+9KEP4cCBA4XLn/70p1IXiYZYIpHAvHnzcO+99/b4+Le//W1897vfxX333YdXX30Vfr8fS5cuRTqdLnJJaSj1t90B4Jxzzum2///4xz8uYglpqG3cuBErV67EK6+8gj/84Q/IZrP41Kc+hUQiUVhm1apVePbZZ/Gzn/0MGzduxP79+3HxxReXsNQ0WAPZ7gDw+c9/vtv+/u1vf7tEJaah0NjYiDvvvBObN2/GG2+8gU984hP49Kc/jbfeegvAEO/rgobFggULxMqVKwu3bdsWDQ0NYt26dSUsFQ2ntWvXinnz5pW6GFREAMQzzzxTuO04jqirqxN33XVX4b7Ozk5hGIb48Y9/XIIS0nA4crsLIcSyZcvEpz/96ZKUh4qjtbVVABAbN24UQrj7tqZp4mc/+1lhmbffflsAEJs2bSpVMWmIHbndhRDirLPOEjfddFPpCkVFUV5eLn74wx8O+b7Omq9hYJomNm/ejCVLlhTuk2UZS5YswaZNm0pYMhpuO3fuRENDA6ZOnYrLL78ce/bsKXWRqIjeffddNDc3d9v3w+EwFi5cyH1/HHjxxRdRU1ODGTNm4LrrrsOhQ4dKXSQaQpFIBABQUVEBANi8eTOy2Wy3/X3mzJmYOHEi9/cx5MjtnvfEE0+gqqoKs2fPxpo1a5BMJktRPBoGtm3jySefRCKRwKJFi4Z8X1eHsrDkamtrg23bqK2t7XZ/bW0ttm3bVqJS0XBbuHAhHn30UcyYMQMHDhzA17/+dXzsYx/Dm2++iWAwWOriURE0NzcDQI/7fv4xGpvOOeccXHzxxZgyZQp2796Nf//3f8e5556LTZs2QVGUUhePBslxHHzxi1/EGWecgdmzZwNw93dd11FWVtZtWe7vY0dP2x0ALrvsMkyaNAkNDQ3YunUrbrnlFmzfvh1PP/10CUtLg/X3v/8dixYtQjqdRiAQwDPPPIOTTjoJW7ZsGdJ9neGLaIice+65hf/PnTsXCxcuxKRJk/DTn/4UV111VQlLRkTD7V//9V8L/58zZw7mzp2LadOm4cUXX8TixYtLWDIaCitXrsSbb77JfrzjTG/b/eqrry78f86cOaivr8fixYuxe/duTJs2rdjFpCEyY8YMbNmyBZFIBE899RSWLVuGjRs3Dvl62OxwGFRVVUFRlKNGQWlpaUFdXV2JSkXFVlZWhhNPPBG7du0qdVGoSPL7N/d9mjp1Kqqqqrj/jwE33HADfvWrX+GFF15AY2Nj4f66ujqYponOzs5uy3N/Hxt62+49WbhwIQBwfx/ldF3H9OnTMX/+fKxbtw7z5s3Df/3Xfw35vs7wNQx0Xcf8+fOxYcOGwn2O42DDhg1YtGhRCUtGxRSPx7F7927U19eXuihUJFOmTEFdXV23fT8ajeLVV1/lvj/O7Nu3D4cOHeL+P4oJIXDDDTfgmWeewR//+EdMmTKl2+Pz58+Hpmnd9vft27djz5493N9Hsf62e0+2bNkCANzfxxjHcZDJZIZ8X2ezw2GyevVqLFu2DB/5yEewYMEC3HPPPUgkEli+fHmpi0bD5Oabb8YFF1yASZMmYf/+/Vi7di0URcFnP/vZUheNhlA8Hu92dvPdd9/Fli1bUFFRgYkTJ+KLX/wivvWtb+GEE07AlClTcNttt6GhoQEXXnhh6QpNg9bXdq+oqMDXv/51XHLJJairq8Pu3bvx5S9/GdOnT8fSpUtLWGoajJUrV2L9+vX4xS9+gWAwWOjbEQ6H4fV6EQ6HcdVVV2H16tWoqKhAKBTCjTfeiEWLFuG0004rcenpePW33Xfv3o3169fjvPPOQ2VlJbZu3YpVq1bhzDPPxNy5c0tcejpea9aswbnnnouJEyciFoth/fr1ePHFF/G73/1u6Pf1oRuQkY70ve99T0ycOFHoui4WLFggXnnllVIXiYbRpZdeKurr64Wu62LChAni0ksvFbt27Sp1sWiIvfDCCwLAUZdly5YJIdzh5m+77TZRW1srDMMQixcvFtu3by9toWnQ+truyWRSfOpTnxLV1dVC0zQxadIk8fnPf140NzeXutg0CD1tbwDikUceKSyTSqXE9ddfL8rLy4XP5xMXXXSROHDgQOkKTYPW33bfs2ePOPPMM0VFRYUwDENMnz5dfOlLXxKRSKS0BadBWbFihZg0aZLQdV1UV1eLxYsXi9///veFx4dyX5eEEGIwSZGIiIiIiIj6xz5fRERERERERcDwRUREREREVAQMX0REREREREXA8EVERERERFQEDF9ERERERERFwPBFRERERERUBAxfRERERERERcDwRUREREREVAQMX0RERKPU5MmTcc8995S6GERENEAMX0RENKpceeWVkCQJ11577VGPrVy5EpIk4corrxzWMjz66KOQJAmSJEFRFJSXl2PhwoX4xje+gUgkMizrKysrG/LnJSKi4mL4IiKiUaepqQlPPvkkUqlU4b50Oo3169dj4sSJRSlDKBTCgQMHsG/fPrz88su4+uqr8aMf/Qgnn3wy9u/fX5QyEBHR6MLwRUREo86HP/xhNDU14emnny7c9/TTT2PixIk45ZRTui3729/+Fh/96EdRVlaGyspK/NM//RN2795dePxHP/oRAoEAdu7cWbjv+uuvx8yZM5FMJnstgyRJqKurQ319PWbNmoWrrroKL7/8MuLxOL785S8XlnMcB+vWrcOUKVPg9Xoxb948PPXUU4XHX3zxRUiShOeeew5z586Fx+PBaaedhjfffLPw+PLlyxGJRAq1bbfffnvh75PJJFasWIFgMIiJEyfigQceOPY3lIiIioLhi4iIRqUVK1bgkUceKdx++OGHsXz58qOWSyQSWL16Nd544w1s2LABsizjoosuguM4AIArrrgC5513Hi6//HJYloXnnnsOP/zhD/HEE0/A5/MdU5lqampw+eWX45e//CVs2wYArFu3Dj/60Y9w33334a233sKqVavwuc99Dhs3buz2t1/60pdw99134/XXX0d1dTUuuOACZLNZnH766bjnnnsKNW0HDhzAzTffXPi7u+++Gx/5yEfw17/+Fddffz2uu+46bN++/ZjKTURExaGWugBERETH43Of+xzWrFmD999/HwDw5z//GU8++SRefPHFbstdcskl3W4//PDDqK6uxj/+8Q/Mnj0bAHD//fdj7ty5+MIXvoCnn34at99+O+bPn39c5Zo5cyZisRgOHTqEcDiMO+64A88//zwWLVoEAJg6dSr+9Kc/4f7778dZZ51V+Lu1a9fik5/8JADgscceQ2NjI5555hn8y7/8C8LhcKGm7UjnnXcerr/+egDALbfcgu985zt44YUXMGPGjOMqPxERDR+GLyIiGpWqq6tx/vnn49FHH4UQAueffz6qqqqOWm7nzp342te+hldffRVtbW2FGq89e/YUwld5eTkeeughLF26FKeffjq+8pWvHHe5hBAA3GaJu3btQjKZLISqPNM0j2oemQ9nAFBRUYEZM2bg7bff7nd9c+fOLfw/H9BaW1uPu/xERDR8GL6IiGjUWrFiBW644QYAwL333tvjMhdccAEmTZqEBx98EA0NDXAcB7Nnz4Zpmt2We+mll6AoCg4cOIBEIoFgMHhcZXr77bcRCoVQWVmJd955BwDw3HPPYcKECd2WMwzjuJ7/SJqmdbstSVIhYBIR0cjCPl9ERDRqnXPOOTBNE9lsFkuXLj3q8UOHDmH79u346le/isWLF2PWrFno6Og4armXX34Z//Ef/4Fnn30WgUCgEOiOVWtrK9avX48LL7wQsizjpJNOgmEY2LNnD6ZPn97t0tTU1O1vX3nllcL/Ozo6sGPHDsyaNQsAoOt6oQ8ZERGNXqz5IiKiUUtRlELTPEVRjnq8vLwclZWVeOCBB1BfX489e/Yc1aQwFovh3/7t3/CFL3wB5557LhobG3HqqafiggsuwGc+85le1y2EQHNzM4QQ6OzsxKZNm3DHHXcgHA7jzjvvBAAEg0HcfPPNWLVqFRzHwUc/+lFEIhH8+c9/RigUwrJlywrP941vfAOVlZWora3FrbfeiqqqKlx44YUA3MmU4/E4NmzYgHnz5sHn8x3zYCBERFR6rPkiIqJRLRQKIRQK9fiYLMt48sknsXnzZsyePRurVq3CXXfd1W2Zm266CX6/H3fccQcAYM6cObjjjjtwzTXX4IMPPuh1vdFoFPX19ZgwYQIWLVqE+++/H8uWLcNf//pX1NfXF5b75je/idtuuw3r1q3DrFmzcM455+C5557DlClTuj3fnXfeiZtuugnz589Hc3Mznn32Wei6DgA4/fTTce211+LSSy9FdXU1vv3tbx/Xe0VERKUliXzPYCIiIiq6F198ER//+MfR0dGBsrKyUheHiIiGEWu+iIiIiIiIioDhi4iIiIiIqAjY7JCIiIiIiKgIWPNFRERERERUBAxfRERERERERcDwRUREREREVAQMX0REREREREXA8EVERERERFQEDF9ERERERERFwPBFRERERERUBAxfRERERERERfB/AbnmTcsRSEFrAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}